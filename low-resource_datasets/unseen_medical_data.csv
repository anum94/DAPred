id,content,summary
Trajectories of breast density change over time and breast cancer risk,"Introduction
Previous studies have consistently reported an association between dense breasts and an increased risk of breast cancer in western,123Asian,45and Korean women,678regardless of menopausal status, suggesting an important role of breast density in predicting future breast cancer risk.678The most widely used reporting standard for mammographic breast density is the Breast Imaging Reporting and Data System (BI-RADS).9Although BI-RADS density measurements are associated with breast cancer risk12367810and can enhance the accuracy of prediction models,1112the validity and reliability of BI-RADS assessments performed by different radiologists have raised concerns. The reliability of BI-RADS categories among radiologists can vary from moderate to substantial, potentially leading to misclassification and, consequently, underestimation or overestimation of breast cancer risk.13Therefore, concerns have been raised about the availability of applications for the subjective assessment of breast density in assessing future breast cancer risk.
Considering the controversy surrounding the use of a single assessment of breast density and its changes with increasing age, recent studies have focused on individual chronological changes in mammographic density. Mammographic density normally decreases gradually over time, therefore an increase in density might indicate proliferative changes that exceed the effects of ageing. Although previous studies141516reported an association between increased breast density and increased breast cancer risk, limitations and unanswered questions about changes in breast density remain.17Additionally, research describing longitudinal changes in mammographic density is limited, especially in large scale populations with regular screening cycles, despite many guidelines recommending regular mammographic breast cancer screening.18To identify women with a high risk of breast cancer in a mass screening setting, grouping those with a similar pattern of breast density change and risk assessment would be more useful than observing individual changes. We hypothesised that there are different patterns of change in longitudinal breast density that subsequently affect the risk of developing breast cancer. Therefore, this study evaluated the trajectories of change in mammographic breast density and assessed the association of this change with the risk of breast cancer development. We used four biennial screening cycles of longitudinal breast density data collected over eight years.
Methods
Study settings and cohort description
This retrospective cohort study was conducted following the STROBE (strengthening the reporting observational studies in epidemiology) guidelines. Data were retrieved from the national breast cancer screening programme in Korea, which is embedded in the NHIS database.19In Korea, the NHIS provides mandatory health insurance for all citizens, including biennial mammographic breast cancer screening for women aged ≥40 years. Furthermore, the database includes data from self-reported questionnaires on health behaviours and other health related characteristics completed during each examination. Details of the NHIS database are described elsewhere.19
Our initial baseline cohort included 5 121 992 women who underwent mammography through the national breast cancer screening programme between 1 January 2009 and 31 December 2010. Given the biennial screening cycle, information on further screening until the end of 2016 was obtained, and the participants were grouped into the following cycles: second screening in 2011-12, third screening in 2013-14, and fourth screening in 2015-16. For women who underwent mammographic screening more than once within two years, we used information from their first screening. The target population was limited to those who underwent four consecutive screening cycles (n=1 885 804;fig 1). Participants with a history of cancer diagnosis at any site before the last screening from 2015 to 2016 (n=74 500) or those with missing information on breast density (n=63 797) were excluded. Given that the eligible starting age for breast cancer screening in Korea is 40 years with no upper age limit, the cohort comprised women aged 40 years or older at the baseline screening conducted in 2009-10, with no upper age limit for inclusion, reflecting the eligibility criteria of the national breast screening examination in Korea.
Mammographic breast density assessment
According to the national breast cancer screening programme, breast density was assessed using the American College of Radiology BI-RADS (fourth edition) density categories: (1) almost entirely fat, (2) scattered fibroglandular density, (3) heterogeneously dense, and (4) extremely dense. Four level BI-RADS breast density was used in the group based trajectory modelling. The current Korean national breast cancer screening guidelines do not require double readings for mammographic screening. As a result, interpretations of BI-RADS breast density in our database were performed as single or double reads, which varied by screening centre.
Determination of incident breast cancer
Incident breast cancer was determined by extracting data from the medical records of all women in the NHIS until the end of 2021. The study endpoint was incident breast cancer as defined by the international classification of diseases (ICD) 10th edition, including the C50 code for invasive breast cancer or D05 for ductal carcinoma in situ. To enhance the accuracy of identifying cancer, we combined the ICD codes with a rare incurable disease code used only for patients with cancer. In Korea, the NHIS has introduced a special copayment reduction programme known as the rare incurable disease system, aimed at improving healthcare coverage and easing the financial strain on people with severe and uncommon illnesses, including cancer. The operational definition of cancer incidence based on a combination of ICD codes and rare incurable disease codes has increased identification and has shown high accuracy when validated using the National Cancer Registry database.2021In the analysis, breast cancer was defined as the first diagnosis of ductal carcinoma in situ or invasive breast cancer. Stratified analysis was conducted separately for the total number of women with invasive breast cancer and ductal carcinoma in situ. Therefore, only women with a diagnosis of invasive breast cancer or ductal carcinoma in situ were included in each analysis.
Assessment of other covariates
During the health assessments, trained nurses measured the patient's height and weight, which were used to compute the body mass index (calculated as weight in kilograms divided by the square of the height in metres). Body mass index status was then classified into the following groups according to the Asia-Pacific classification22: underweight (<18.5), normal (18.5 to <23), overweight (23 to <25), and obese (≥25).
Information on health related behaviours, family medical history, past medical history diagnosed by physicians, and reproductive factors was gathered using self-administered standardised questionnaires as part of the health screening process. Our analysis adjusted for the following factors as variables: participant's age at initial screening, family history of breast cancer among first degree relatives, history of benign breast diseases diagnosed by physicians, age at first menstruation (menarche), number of children, duration of breastfeeding, use of oral contraceptives, smoking habits, alcohol consumption, and weekly physical activity. Menopausal status was measured by the question “What is your current menopausal status?” Response options were as follows: still menstruating, had a hysterectomy, or postmenopausal. Women who reported being postmenopausal were asked to report their age at menopause. For these women, we adjusted our analysis for factors such as age at menopause onset and use of hormone replacement therapy. Information on the covariates from the last screening cycle in 2015-16 was used to adjust the main analysis.
Statistical analysis
Data analysis was conducted from June to October 2023. We applied group based trajectory modelling to identify latent trajectory groups for breast density changes over time. Group based trajectory modelling is a modern statistical technique of finite mixture modelling developed by Nagin2324and was designed to identify clusters of people who follow a similar trajectory of a certain variable over time. Group based trajectory modelling was performed using the PROC TRAJ user written SAS package.2526The first step was to establish the optimal number of trajectory groups in the sample, enabling us to determine mammographic breast density changes over time. The general rule provided in the tutorial by Andruff and colleagues27and in previous studies2829is to begin by modelling with polynomial functions of the highest order (cubic). However, based on our experience and previous studies,111430the breast density category does not change dramatically in less than 10 years, and so it is unlikely that breast density changes during the eight year study period followed a cubic shape. Therefore, we began modelling using second order (quadratic) and two group models. The number of groups was iteratively increased until the value of the Bayesian information criterion was maximally reduced. To select the best fitting model, we considered the Bayesian information criterion value and other criteria, including the proportion of participants in each group, to ensure there were sufficient numbers of women with breast cancer in each group and to determine the average posterior probabilities in each group. Five groups were selected after the first step (supplemental table 1). In the second step, we performed 32 models for the models with five groups and second order linear or quadratic models (supplemental table 2). The two best fitting models with the lowest Bayesian information criterion value were finally considered, and after taking into account the proportion of participants in the group, the last model (called model 22212) was selected (supplemental table 3). The average posterior probabilities of each trajectory group in the final model were 0.95, 0.89, 0.84, 0.77, and 0.80, respectively, which are higher than the recommended value of 0.7.23
To determine the association between trajectory group and breast cancer development outcomes, Cox proportional hazards regression analysis was used to estimate hazard ratios and 95% confidence intervals. Outcomes included detection of breast cancer from the date of the last screening cycle (2015-16) to the end of 2021. The assumption of proportional hazards was assessed by reviewing survival graphs and proportional hazard global tests. The models were adjusted for age and other covariates already described.
Because changes in mammographic density are closely linked with age, previous studies on breast density change have often restricted the study population based on their age at first screening.16We adopted this approach, using three separate age groups when conducting the analysis: 40-49, 50-59, and ≥60 years. This method helped to mitigate the confounding effect of age at the time of screening. We then performed a two step analysis: a group based trajectory analysis followed by Cox regression analysis separately for each age group.
We also conducted subgroup analyses according to changes in body mass index and menopausal status from the first screening (2009-10) to the last screening (2015-16). Our analysis included four body mass index status groups: consistent normal body mass index (<23); normal to overweight or obese (≥23); overweight or obese to normal; and consistent overweight or obese. Changes in menopausal status were grouped into three main categories: the persistent premenopausal group (women who were premenopausal or were still menstruating at both screenings); the premenopausal to postmenopausal group; and the persistent postmenopausal group. All statistical analyses were performed using SAS version 9.4 (SAS Institute Inc), and two sided P values<0.05 were considered statistically significant.
To address potential selection bias relating to the inclusion of women with four consecutive screenings, we conducted two sets of sensitivity analyses: one analysis of women who underwent at least three screening cycles (n=3 089 772), and another of women who underwent at least two screening cycles (n=4 085 523). In the first analysis, missing breast density information for one measure was imputed using multiple imputation and imputation based on the previous value, and the change in breast density was assessed using a group based trajectory analysis with the same approach as the main analysis. In the second sensitivity analysis, changes in breast density were assessed from baseline to the last screening value. Further details on the sensitivity analyses are provided in the supplemental methods.
Patient and public involvement
The patients were not involved in setting the research question or outcome measures, and they were not involved in developing plans for the design or implementation of the study. At the start of the study, patient and public involvement was not commonly considered in our discipline in this region.
Results
A total of 1 747 507 women who underwent four screening cycles were included in the analysis, with a mean (standard deviation) age at the last screening of 61.4 (9.3) years. Supplemental table 4 presents descriptive characteristics of the total study population at the last screening in 2015-16 and differences in characteristics according to breast cancer development. Of the total population, 36.6% (639 787/1 747 507) had a normal body mass index (18.5-23), and 81.7% (1 427 985/1 747 507) reported having two children. Among women without breast cancer, 41% (708 687/1 728 506) had dense breasts (BI-RADS breast density categories 3 or 4) at first screening, and this proportion was 56% (10 640/19 001) among those with breast cancer (supplemental figure 1).
Group based trajectories of breast density
Over eight years, five trajectories of breast density were identified (fig 2). The first group comprised 16.3% (285 375/1 747 507) of women with consistently fatty breast tissue and a slightly decreasing trend in density (group 1, or persistent low density group). This group was used as the reference group in the regression analysis. The second group included 13.9% (242 249/1 747 507) of women with low breast density at baseline, which increased during the study period. The other three groups had a higher level of breast density at baseline than groups 1 and 2, but showed a similar minimally decreased or stable trajectory of breast density—group 3: 22.1% (386 982/1 747 507); group 4: 30.6% (534 648/1 747 507); and group 5: 17.1% (298 253/1 747 507).
Table 1describes how the characteristics differed across trajectory groups. Group 1 had the highest mean age (68.9 years, standard deviation 7.7), and group 5 had the lowest mean age (53.6 years, standard deviation 6.6). The proportion of women aged ≥60 years was 89.3% (254 822/285 375) in group 1 and the proportion decreased as trajectory group increased. The proportion of first degree relatives with a family history of breast cancer was the highest in group 5 (3.4%, 10 130/298 253) and lowest in group 1 (1.5%, 4142/285 375). Other reproductive factors and health behaviours showed different distributions between the trajectory groups. The proportion of women with early age at menarche (<15 years) was highest in group 5 (30.6%, 91 158/298 253) and lowest in group 1 (10.7%, 30 603/285 375). Moreover, the proportion of postmenopausal women was highest in group 1 (85.7%, 244 472/285 375) and lowest in group 5 (48.9%, 145 891/298 253). Group based trajectory analysis across the different age groups revealed similar patterns of breast density change over time, particularly in the 40-49 and 50-59 age groups (supplemental figure 2).
Breast cancer development according to breast density trajectory
After a median follow-up of 5.9 years (interquartile range 5.4-6.4 years) from the last screening, 19 001 women were found to have breast cancer. Compared with women in trajectory group 1, those in other trajectory groups had an increased breast cancer risk (group 2: adjusted hazard ratio 1.60, 95% confidence interval 1.49 to 1.72; group 3: 1.86, 1.74 to 1.98; group 4: 2.49, 2.33 to 2.65; group 5: 3.07, 2.87 to 3.28;table 2). When stratified by breast cancer type, the results for invasive breast cancer and ductal carcinoma in situ were similar.
The association between breast density trajectories and subsequent breast cancer risk was consistent across all age groups (table 2). In each age group, breast cancer risk was higher in women in trajectory groups 2-5 than in those in trajectory group 1. The hazard ratios were also similar across age groups, with group 2 hazard ratios of 1.74 (95% confidence interval 1.45 to 2.10) in the 40-49 group, 1.57 (1.40 to 1.76) in the 50-59 group, and 1.61 (1.44 to 1.81) in the ≥60 group.
Subgroup analysis by change in menopausal status and body mass index status
Supplemental table 5 discusses frequencies of change in menopausal status and body mass index status according to breast density trajectory group. Group 5 had the highest proportion with a consistently normal body mass index (52.3%, 156 028/298 253), whereas more than 60% of women in groups 1 and 2 had a consistent status of overweight or obese. Trajectory group 5 experienced the highest proportion of changes from a normal body mass index to being overweight or obese over the eight year study period.
The associations between breast density trajectories and breast cancer risk were consistent regardless of changes in body mass index (supplemental table 6). In all four body mass index change groups, we observed a significantly increased risk in trajectory groups 2-5 compared with group 1. The highest hazard ratios were observed in women whose body mass index changed from normal to overweight or obese: adjusted hazard ratio 1.88 (95% confidence interval 1.38 to 2.57), 2.19 (1.65 to 2.91), 2.99 (2.27 to 3.93), and 3.90 (2.93 to 5.20) for groups 2-5, respectively.
In group 5, women with a consistent premenopausal status accounted for 42.4% (94 439/222 555), which was the highest proportion among all the groups (supplemental table 5). Meanwhile, 94% (209 402/222 821) of women in group 1 had a consistent postmenopausal status. Trajectory group 5 had the highest proportion of changes from premenopausal to postmenopausal during the eight years. Among women with consistent premenopausal status, those with stable dense breasts had a higher risk of breast cancer. Compared with trajectory group 1, groups 4 and 5 had 2.46-fold (95% confidence interval 1.50 to 4.04) and 3.25-fold (1.98 to 5.31) increased risk of breast cancer (supplemental table 7). Consistent findings were observed in women with altered menopausal status and in postmenopausal women with comparable adjusted hazard ratio values.
Sensitivity analyses
The results of the sensitivity analyses yielded similar trajectories of breast density compared with the main analysis, regardless of the imputation method (supplemental table 8). In both sensitivity analyses, five groups corresponded to the five trajectories of breast density change, and these trajectories were associated with breast cancer risk. The trajectory group patterns and strength of association of each group were similar to the main results. Compared with women in trajectory group 1, those in the other trajectory groups had an increased risk of breast cancer (table 3). In the second sensitivity analysis, when the breast density change was assessed between the two screening cycles (2009-10 and 2015-16), those with increased breast density had a higher risk of breast cancer than those with consistent breast density, while those with decreased breast density had a lower risk. These findings were consistent across age groups (supplemental table 9).
Discussion
Principal findings
In a cohort exceeding 1.7 million women with longitudinally assessed breast density, we used group based trajectory modelling to identify five latent groups sharing similar trajectories of breast density changes over time. Through repeated longitudinal measurements of breast cancer in a fixed population, our study revealed five trajectories of breast density change. During the eight year study duration, most trajectory groups showed slight decreases or minimal changes in BI-RADS density compared with the initial BI-RADS density classification, whereas trajectory group 2 showed a significant increase in the BI-RADS density category. Our findings suggest that the subsequent risk of breast cancer varies according to these trajectories of breast density, indicating an increased risk in women with persistently dense breasts or those with increasing breast density over time. One distinguishing finding of this study was that the increased risk of breast cancer was similar in group 2 (BI-RADS density category increased from 1 to 2) and group 3 (BI-RADS density category of 2 decreased over time) compared with group 1 (consistently fatty breast tissue with slight decrease in density over time. These results were consistent across different age groups, irrespective of changes in menopausal status or body mass index.
Comparison with other studies
Breast density remains one of the most readily identifiable mammographic features and previous studies have explored the association between changes in breast density and breast cancer risk.141516323334353637The findings from these studies are generally consistent with our results, which indicate that a reduction in breast density is associated with a lower risk of breast cancer compared with women with stable or increasing breast density.163227Kerlikowske and colleagues, using data from the Breast Cancer Surveillance Consortium, reported that an increase in breast density is associated with a higher risk of breast cancer compared with women with unchanged breast density. They further suggested that longitudinal measures of BI-RADS breast density might provide a more accurate prediction of a woman's breast cancer risk than a single measure.3234Similarly, the study by Román and colleagues, using data from the breast cancer screening programme in Spain, found that women aged 50-54 years whose BI-RADS breast density category increased from B to C or B to D had an increased breast cancer risk, with adjusted relative risks of 1.55 (95% confidence interval 1.24 to 1.94) and 2.32 (1.48 to 3.63), respectively.16Additionally, a meta-analysis of four studies reported that an increase in breast density over time is associated with an increased risk of breast cancer, while a decrease in breast density is associated with a reduced risk.38
Meanwhile, our study has certain distinct features and findings compared with previously published studies. We overcame the limitations of previous studies, which commonly reported findings from a small scale population15or were limited to a single cohort.313233343536Additionally, previous studies have often assessed changes in breast density at only two or three time points,14313233343536whereas our study involved four consecutive biennial assessments of breast density. Several previous studies considered the average change in continuous breast density over time1532or changes in BI-RADS density classification143336for evaluating breast density. However, our study focused on the overall trajectory of breast density. We also applied group based trajectory modelling to assess clusters of changes in breast density over time. This group based trajectory method describes the evolution of repeatedly measured characteristics over time and has gained popularity in adherence research over the past two decades.37People classified into a certain group share more similarities than those from other groups. Our application of this method to assess breast density changes provides a new classification method for breast density.
Limitations of this study and sensitivity analyses
By restricting the population to regularly screened women, we might have introduced several biases. Firstly, limiting the population to women with four screening cycles reduces the representativeness and validity of the study because only healthy women with good compliance are likely to attend regular screenings. Secondly, selecting women with four consecutive screenings might introduce immortal time bias because women who died or had a diagnosis of breast cancer during the second, third, or fourth screening cycles were excluded from the analysis. To address this limitation, two sets of sensitivity analyses were conducted: one used data from women who underwent at least three screening cycles, and the other included data from women who underwent at least two screening cycles. The five breast density trajectories identified in the first sensitivity analysis were similar to those in the main analysis; likewise, these patterns were associated with subsequent breast cancer risk with relatively consistent effect sizes. Findings from the second sensitivity analysis, which examined the association between changes in the two measures of breast density and breast cancer risk, were consistent with those of previous reports (Kerlikowske et al,33Román et al,16Kim and Park,36and a meta-analysis by Mokhtary et al38). Overall, the results of the main analysis and sensitivity analyses support the conclusion that women with increasing or persistently high breast density over time have an increased risk of breast cancer. Although the sensitivity analyses cannot fully eliminate the impact of selection bias, the consistent findings of the main analysis and sensitivity analyses support the robustness of our conclusions about the association between changes in breast density and breast cancer risk. Additionally, proportions of missing data at each screening cycle are comparable (supplemental table 10) and the characteristics of our study population are similar to those of the total screened population during the first screening in 2009-10 (supplemental table 11). This similarity suggests that the selection bias in the study population, if present, was minimal.
This study used categorical BI-RADS breast density to assess mammographic density, which is another limitation. Although BI-RADS density is associated with breast cancer risk and improves predictive accuracy in several breast cancer prediction models,123101112concerns have been raised about its inter-rater and intra-rater reliability. The inter-radiologist and intra-radiologist reliability of BI-RADS categories reportedly has moderate to substantial agreement,4041which could lead to potential misclassification of the BI-RADS categories and subsequent under or overestimation of breast cancer risk.33Therefore, instead of using a single BI-RADS measure, repeated measures or longitudinal trends of BI-RADS breast density16323334353637are less likely to be affected by these inconsistencies and might provide more robust evidence. Another approach to overcome these limitations is to assess breast density with quantitative measures, such as volumetric percentage of density.17However, establishing standardised quantitative measurements in a nationwide screening setting with various mammography devices poses a major challenge.
Trajectories of breast density change and subsequent breast cancer risk
Our analysis revealed five distinct trajectories of breast density. Groups 1 and 3 comprised women with fatty breast tissue (BI-RADS density categories 1 and 2), with a slight decrease over time. By contrast, groups 4 and 5 consisted of younger women who maintained persistently dense breasts (BI-RADS density categories 3 and 4) during the study period. According to the trajectory based groups, group 2 was classified as BI-RADS density category 1 and showed dominant changes in breast density over time with an upward trend. The five trajectories of breast density had different age distributions—women with persistently dense breasts over time (groups 3-5) tended to be younger than those with fatty breast tissue at baseline (groups 1 and 2). Additionally, when comparing groups 1 and 2, in which the initial breast density was similarly low, group 2 was younger and showed an increasing trend in breast density. This result is consistent with the well established finding that age plays a major part in breast density and its associated changes. Studies have shown that breast density tends to decrease with age.3940Additionally, the correlation between breast density and age was significant, even after accounting for body composition effects.5Furthermore, a study of Dutch women revealed a small but statistically significant increase in the average fraction of dense tissue across different birth cohorts, with greater breast density observed in later born than in earlier born birth cohorts. Compared with white women, East Asian women showed a smaller decrease in breast density with increasing age,41with no substantial change from the baseline density observed across trajectories in this study population.
Previous studies have consistently shown that postmenopausal hormone replacement therapy is associated with increased breast density over time.3942In this study population, compared with women with an initial BI-RADS density category 1 with a declining trend, hormone replacement therapy use was higher in group 2. Additionally, among women with increased density, the proportion of women whose body mass index changed from overweight to normal was higher than that in the other trajectory groups (supplementary table 6). Considering the association between weight loss and increased breast density,4344weight reduction could be partially attributed to increased breast density in these populations. A change in breast density might be explained by the breast tissue ageing model as suggested by Pike and colleagues,45which states that the tissue density is mainly driven by the cumulative genetic damage by the breast epithelium rather than chronological age.46
An overlap exists between known breast cancer risk factors and factors influencing mammographic breast density.474849Studies have suggested that breast density or changes in breast density are the effects of known risk factors for breast cancer or the mediating effect of breast density between known risk factors and the risk of breast cancer.1415Therefore, the trajectory of breast density could be a surrogate marker that reflects a combination of known risk factors for breast cancer, showing an association with future breast cancer risk. The distribution of known risk factors between the five trajectory groups showed distinct patterns, with a higher number of known risk factors in the groups with increased breast cancer risk than in the lowest risk group (reference group, group 1). As the breast cancer risk of each trajectory group increased, the proportion of women with a family history of breast cancer, nulliparous women, younger age at menarche, never breastfed, ever smokers, and current alcohol consumption among the groups increased. Despite the different distributions of risk factors between trajectory groups 2 and 3, the increased risk in terms of hazard ratio was similar to group 1. The results from a previous study in which the five year risk of breast cancer in a group with changes in BI-RADS density from 1 to 2 was comparable to a group with changes in BI-RADS density from 2 to 1 support the findings of this study,36suggesting that maximum density within a certain period can be associated with risk of breast cancer. Further studies on the association among known risk factors, breast density, and breast cancer risk are needed.
As the current national breast cancer screening programme in Korea only targets women aged 40 years or older, our analysis focused exclusively on this population, precluding the examination of breast cancer risk in younger age groups. Despite the recommendation for double reading in European guidelines,50the current Korean national breast cancer screening guidelines do not mandate double reading for mammographic screening. Consequently, BI-RADS breast density interpretations in our database were conducted as single or double reads, depending on the screening centre.
Conclusions
Breast density is an established risk factor for breast cancer, with consistent evidence reported across a range of studies. This large population based study ex","Objective: To identify clusters of women with similar trajectories of breast density change over four longitudinal assessments and to examine the association between these trajectories and the subsequent risk of breast cancer.
Design: Retrospective cohort study.
Setting: Data from the national breast cancer screening programme, which is embedded in the National Health Insurance Service database in Korea. Breast density was assessed using the four category Breast Imaging Reporting and Data System (BI-RADS) classification. Group based trajectory modelling was performed to identify the trajectories of breast density.
Participants: Women aged ≥40 years who underwent four biennial mammographic screenings between 2009 and 2016.
Main outcome measures: Breast cancer development was determined to 31 December 2021. Cox proportional hazard models were used to assess the associations between trajectories and breast cancer outcomes after adjusting for covariates.
Results: Among a cohort of 1 747 507 women (mean age 61.4 years), five breast density trajectory groups were identified. Group 1 included women with persistently fatty breast tissue, group 2 included women with fatty breast tissue at baseline but increased breast density over time, and groups 3-5 included women with denser breasts, with a slight decrease in density over time. Women in group 2 had a 1.60-fold (95% confidence interval 1.49 to 1.72) increased risk of breast cancer compared with those in group 1. Women in groups 3-5 had higher risks compared with those in group 1, with adjusted hazard ratios of 1.86 (1.74 to 1.98), 2.49 (2.33 to 2.65), and 3.07 (2.87 to 3.28), respectively. Similar results were observed across different age groups, regardless of changes in menopausal status or body mass index.
Conclusions: This study identified five distinct groups of women with similar trajectories of breast density change over time. Future risk of breast cancer was found to vary in these groups. Increasingly dense or persistently dense breasts were associated with a higher risk. Changes in breast density over time should be carefully considered during breast cancer risk stratification and incorporated into future risk models.
"
Comparative effectiveness and safety of single inhaler triple therapies for COPD,"Introduction
International guidelines recommend triple inhaler treatment for some patients with chronic obstructive pulmonary disease (COPD), consisting of an inhaled corticosteroid, a long acting muscarinic antagonist, and a long acting β-agonist.1Two single inhalers with these triple therapies are marketed in the US: budesonide-glycopyrrolate-formoterol (Breztri Aerosphere), a twice daily metered dose inhaler, and fluticasone-umeclidinium-vilanterol (Trelegy Ellipta), a once daily dry powder inhaler. Both combinations have shown superiority over long acting muscarinic antagonist-long acting β-agonists and inhaled corticosteroid-long acting β-agonists in pivotal randomized controlled trials in select patients with COPD,23and may be prescribed according to clinical guidelines when inhaled corticosteroid-long acting muscarinic antagonist-long acting β-agonists are indicated.1
Many health systems worldwide have sought to reduce reliance on metered dose inhalers like budesonide-glycopyrrolate-formoterol since they contain propellants (hydrofluoroalkanes) that contribute to greenhouse gas emissions that are not found in dry powder inhalers; for example, emissions associated with metered dose inhalers represent 3% of the entire carbon footprint of the National Health Service in the UK.45Yet, metered dose and dry powder formulations of single inhaler triple therapy have not been compared head-to-head in randomized controlled trials.
Research into the comparative effectiveness and safety of triple therapy in people with COPD has generated mixed findings. The adverse effect of pneumonia has been consistently shown in clinical trials and observational research of inhaled corticosteroids.23678But one recent study using data from the UK found that budesonide-based triple therapy was associated with fewer admissions to hospital with pneumonia and reduced all cause mortality compared with fluticasone-based triple therapy.9This finding was consistent with earlier observational studies and a systematic review showing decreased pneumonia risk for patients with COPD who received budesonide-based rather than fluticasone-based regimens.10111213Some researchers have hypothesized that fluticasone could increase pneumonia risk due to slower airway absorption, higher lipophilicity, and more potent immunosuppressive effects.13
These studies, however, have not analyzed potential intraclass differences among patients receiving single inhaler triple therapy. The recent UK study included patients receiving triple therapy via separate inhalers and using a variety of inhaler types (single inhaler triple therapy was not yet available during the study period); other studies focused on dual combination rather than triple combination regimens, which may limit generalizability to patients with more severe disease. In addition, prior observational studies comparing budesonide and fluticasone analyzed short acting fluticasone propionate rather than the longer acting fluticasone furoate, which is found in fluticasone-umeclidinium-vilanterol.
In contrast to studies suggesting a potential safety advantage for budesonide-based triple therapy, other recent research has suggested reduced effectiveness. A network meta-analysis of randomized controlled trials found that budesonide-glycopyrrolate-formoterol was associated with more annual moderate or severe COPD exacerbations than fluticasone-umeclidinium-vilanterol.14However, this study aggregated data from trials with different inclusion criteria, with some systematically enrolling patients with more severe COPD, and thus comparisons across trials may be subject to bias. In addition, the authors did not analyze pneumonia risk in the two treatment groups because of inconsistencies in how different trials defined the outcome.
Given ongoing clinical uncertainty, we sought to compare the effectiveness and safety of budesonide-glycopyrrolate-formoterol and fluticasone-umeclidinium-vilanterol in patients with COPD who are treated in routine clinical practice. Rigorous studies of longitudinal health care data offer an important tool to help inform treatment guidelines and shape prescribing practices when randomized controlled trials have not been performed.15Such research may be especially valuable for developing a generalizable evidence-base in COPD because patients treated in routine clinical practice tend to be older, have more comorbidities, and include more women than patients who are generally enrolled in randomized controlled trials.1617181920212223
Methods
Study cohort
The study was completed using Optum’s de-identified Clinformatics Data Mart, a database of administrative health claims for members of large commercial and Medicare Advantage health plans (appendix methods). We included patients who initiated budesonide-glycopyrrolate-formoterol (exposure) or fluticasone-umeclidinium-vilanterol (referent) between 1 January 2021 and 30 September 2023 in the analysis. The study began in 2021, the first full year after budesonide-glycopyrrolate-formoterol was approved in the US; both the exposure and referent were marketed in the US throughout the study period.
All patients were required to have a diagnosis of COPD based on International Classification of Diseases, tenth revision, clinical modification (ICD-10-CM) codes (J41.x, J42.x, J43.x, J44.x). We included patients with three outpatient claims or one inpatient claim in the prior three years (positive predictive value 0.82 (95% confidence interval (CI) 0.72 to 0.89)) to capture patients with active COPD.24We excluded people younger than 40 years to increase the specificity of the COPD diagnosis, and we required that all patients have at least 365 days of continuous enrollment in the dataset before cohort entry. Patients with COPD who also had prior asthma diagnoses were included in the analysis. Patients were excluded if they had received either budesonide-glycopyrrolate-formoterol, fluticasone-umeclidinium-vilanterol, or an inhaled corticosteroid-long acting muscarinic antagonist-long acting β-agonist combination via separate inhalers (defined as dispensing of inhalers with ingredients from all three classes via any combination within 30 days of each other) during the 365 days before cohort entry. We also excluded patients who received both budesonide-glycopyrrolate-formoterol and fluticasone-umeclidinium-vilanterol or who received triple therapy plus another maintenance inhaler on the cohort entry date. By contrast, patients who received monotherapy (long acting muscarinic antagonist, long acting β-agonist, or inhaled corticosteroid) or dual therapy (long acting muscarinic antagonist-long acting β-agonist or inhaled corticosteroid-long acting β-agonist) during the baseline assessment period were included in the analysis.
Assessment of covariates
Potential confounders, including baseline lung disease, comorbidities, healthcare use, and medication use, were measured leading up to and including the 365 days before cohort entry and were included in propensity scores (see appendix methods for a complete list of covariates included in the propensity score model). We assessed socioeconomic covariates (mean copayments, total copayments, and unique brand-to-generic prescription fills) until the day before cohort entry. Any prior history of asthma was assessed using all available data. Eosinophil levels can fluctuate over time, therefore, we used the most recent measured value in the 180 days before cohort entry. We measured demographic covariates (age, gender, race, and region), calendar year and season of index prescription, and whether the index prescription was written by a pulmonologist on the day of cohort entry (see appendix figure 1 for a timeline of covariate assessment).
Primary outcomes and follow-up
The primary outcomes were the first moderate or severe COPD exacerbation (effectiveness) and the first admission to hospital with pneumonia (safety). Moderate exacerbations were defined by fills of prednisone prescribed for 5-14 days (positive predictive value 0.73),25and severe COPD exacerbations by admission to hospital with a COPD diagnosis code (specified above) in the primary position (positive predictive value 0.86 based on ICD-9-CM codes; conversion to ICD-10-CM codes was performed based on clinical review).26If a patient met criteria for both a moderate and severe COPD exacerbation within 14 days of each other, the exacerbation was considered to be a severe exacerbation starting on the day when criteria for the exacerbation were first met. This categorization of COPD exacerbations requiring systemic steroids (moderate) versus those requiring admission to hospital (severe) is used in the Global Initiative for Chronic Obstructive Lung Disease guidelines and has been widely used in randomized controlled trials and other observational studies.123678272829Admission to hospital with pneumonia was defined based on ICD-10-CM codes (J.09.X1, J10.xx-J18.x, A01.03, A02.22, A37.01, A37.11, A37.81, A37.91, A54.84, B01.2, B05.2, B06.81, B77.81, J85.1, J22) in any position(positive predictive value 0.88 based on ICD-9-CM codes; conversion to ICD-10-CM codes was performed based on clinical review).30Patients were followed up for up to one year, the end of data, or until they had an outcome of interest, discontinued treatment (with a grace period of 60 days permitted between inhaler fills), added or switched maintenance inhalers, died, or were disenrolled from insurance.
Prespecified secondary, subgroup, and sensitivity analysis
Secondary outcomes included all cause mortality, first moderate exacerbation, first severe exacerbation, annual rate of moderate or severe exacerbations (analyzed separately and as a composite), and annual rate of admission to hospital with pneumonia. Unadjusted and adjusted results are reported for primary and secondary outcomes.
Subgroups of interest included patients who: (1) had Global Initiative for Chronic Obstructive Lung Diseasegroup E disease; (2) had at least one moderate or severe COPD exacerbation during the baseline assessment period; (3) had at least one severe COPD exacerbation during the baseline assessment period; (4) received any baseline maintenance inhaler before cohort entry; (5) had a prior diagnosis code for asthma; (6) had a diagnosis of asthma in the prior three years; (7) had eosinophil levels above 300 µL; (8) underwent spirometry during the baseline assessment period; and (9) received the initial budesonide-glycopyrrolate-formoterol or fluticasone-umeclidinium-vilanterol prescription from a pulmonologist (appendix table 1). All subgroup analyses were done for patients meeting the relevant subgroup criterion and for the remainder of patients not meeting the subgroup criterion.
To check the robustness of our findings, we conducted several prespecified sensitivity analyses. We varied the grace period permitted between prescription fills (from 60 days to 30 days and 90 days), performed an as-started analysis (with no censoring for treatment discontinuation or switching in the matched population), and excluded early events (in the first 30 days and first 60 days after cohort entry) (appendix table 2). We also varied the definitions of outcomes in our analysis (appendix methods), and we did a post-hoc sensitivity analysis limiting follow-up to 180 days.
Statistical analysis
Our primary analysis was a 1:1 propensity score matched analysis. We used nearest neighbor matching and a caliper of 0.01 on the propensity score scale. We estimated propensity scores using logistic regression, including all covariates previously mentioned without further variable selection. The targeted estimand was the average treatment effect among patients with COPD receiving budesonide-glycoypyrrolate-formoterol. In the case of missing data for covariates in our model, we used a missing indicator. For our time-to-event analyses, we estimated hazard ratios (HRs) and 95% CIs using a Cox proportional hazards regression model. We estimated absolute risk differences at 365 days, and we calculated the number needed to harm as 1/absolute risk difference when the 95% confidence interval for the risk difference excluded the null.31When analyzing annual rates of COPD exacerbations and admission to hospital with pneumonia in secondary analyses, we performed negative binomial regression. In prespecified sensitivity analysis, we conducted 1:1 high dimensional propensity score matching to adjust for hundreds of covariates generated through automated selection based on outpatient and inpatient diagnosis codes, procedures, and pharmacy claims.323334
All analyses were completed using the Aetion Evidence Platform v4.73 (Aetion, Inc), which carries out some statistical computations using R v3.4.2, and Stata, v16.1 (StataCorp). The Aetion Evidence Platform has been used extensively in the academic literature, including in studies showing the reproducibility of real world evidence and successfully emulating randomized controlled trials.3536The platform has been through comprehensive internal quality checks and validation processes and has been directly tested against both de novo programming and other quality checked analytical workflows used by the US Food and Drug Administration Sentinel Initiative.37The study was approved by the Mass General Brigham Institutional Review Board (2023P000164) and the protocol was preregistered at the Center for Open Science Real World Evidence Registry (https://osf.io/6gdyp/) before the study began.
Patient and public involvement
Our dataset included only de-identified patients, and data use agreements precluded us from contacting these patients. In addition, our funding did not support patient or public involvement.
Results
The cohort included 87 751 patients (67 356 new users of fluticasone-umeclidinium-vilanterol and 20 395 new users of budesonide-glycopyrrolate-formoterol) (fig 1), from which 20 388 matched pairs were identified for the primary analysis (seetable 1for select baseline characteristics of the two treatment groups and appendix table 3 for the full set of baseline characteristics). The populations were highly overlapping in the unadjusted cohort and further aligned after matching (appendix figure 2), with mean absolute standardized differences of 0.028 before matching and 0.004 after matching.
Primary effectiveness and safety outcomes
Patients in the matched cohort had 7729 moderate or severe exacerbations during 15 229 person years of follow-up, giving a crude incidence of 507.5 events per thousand person years. Those receiving budesonide-glycopyrrolate-formoterol had a 9% increased hazard of first moderate or severe COPD exacerbation compared with patients receiving fluticasone-umeclidinium-vilanterol (HR 1.09 (95% CI 1.04 to 1.14)) (table 2). The absolute risk difference at 365 days was 2.6% (95% CI 0.8% to 4.4%) and the number needed to harm 38. Median follow-up time was 88 days (interquartile range 65 to 164 days) among patients receiving budesonide-glycopyrrolate-formoterol and 113 days (74 to 206 days) among patients receiving fluticasone-umeclidinium-vilanterol (reasons for censoring are given in appendix table 4).

A total of 1812 people had their first admission to hospital with pneumonia in the matched cohort during 17 281 person years of follow-up (crude incidence of 104.9 per 1000 person years). The hazard of first admission to hospital with pneumonia was identical between the two treatment groups (HR 1.00 (95% CI 0.91 to 1.10); absolute risk difference 0.4% (95% CI −0.6% to 1.3%)). Median follow-up time was 105 days (interquartile range 88 to 197 days) among people receiving budesonide-glycopyrrolate-formoterol and 135 days (88 to 243 days) among those receiving fluticasone-umeclidinium-vilanterol (reasons for censoring are given in appendix table 5).
Kaplan-Meier curves for the primary effectiveness and safety analyses showed consistent effects over the 365 days of follow-up (appendix figures 3 and 4). Sensitivity analyses yielded similar findings to the primary analysis when examining the outcomes of first moderate or severe COPD exacerbation (fig 2) and first admission to hospital with pneumonia (appendix figure 5).
Moderate versus severe COPD exacerbations and all cause mortality
When separately analyzing moderate and severe COPD exacerbations, patients in the matched cohort receiving budesonide-glycopyrrolate-formoterol had a 7% increased relative hazard (HR 1.07 (95% CI 1.02 to 1.12)) and a 1.9% increase in absolute risk ((95% CI 0.1% to 3.6%); number needed to harm 54) of first moderate COPD exacerbation compared with patients receiving fluticasone-umeclidinium-vilanterol. People receiving budesonide-glycopyrrolate-formoterol had a 29% increased relative hazard (1.29 (1.12 to 1.48)) and a 1.0% increased absolute risk ((0.1% to 1.9%); number needed to harm 97) of first severe COPD exacerbation. All cause mortality was similar between the two groups (HR 1.04 (95% CI 0.93 to 1.16); absolute risk difference 0.4% (95% CI −0.6% to 1.3%)). Sensitivity analyses of these secondary endpoints are given in appendix figure 6 (moderate exacerbation), appendix figure 7 (severe exacerbation), and appendix figure 8 (all cause mortality).
Cumulative events
Patients in the matched cohort who received budesonide-glycopyrrolate-formoterol had 8% more cumulative moderate or severe COPD exacerbations (incident rate ratio 1.08 (95% CI 1.04 to 1.13)), 7% more cumulative moderate exacerbations (1.07 (1.02 to 1.12)), and 26% more cumulative severe exacerbations (1.26 (1.09 to 1.45)) during a maximum of one year of follow-up. Rates of cumulative admission to hospital with pneumonia were similar between the two treatment groups (1.03 (0.93 to 1.14)).
Subgroup analysis
Higher hazards of first moderate or severe COPD exacerbation in the matched cohort were observed in patients more prone to exacerbations, including people with at least one baseline moderate or severe exacerbation (HR 1.12 (95% CI 1.07 to 1.17)), at least one baseline severe exacerbation (1.15 (1.04 to 1.28)), prior baseline maintenance inhaler therapy (1.14 (1.08 to 1.20)), and eosinophil concentrations of more than 300 cells/µL (1.20 (1.02 to 1.41)) (fig 3). Such differences were not observed in patients who did not have some of the key indications for initiating triple therapy, including people with no baseline exacerbations (1.02 (0.92 to 1.14)), no baseline maintenance therapy (1.01 (0.94 to 1.10)), eosinophil counts of 100 cells/µL or lower (1.07 (0.94 to 1.22)); or eosinophil counts of between 101-300 cells/µL 1.03 (0.93 to 1.14). The risks of first admission to hospital with pneumonia among patients receiving budesonide-glycopyrrolate-formoterol versus fluticasone-umeclidinium-vilanterol were similar for all subgroups analyzed (appendix figure 9).
Discussion
Budesonide-glycopyrrolate-formoterol was associated with a slightly higher incidence of first moderate or severe COPD exacerbation and an identical incidence of first admission to hospital with pneumonia, compared with fluticasone-umeclidinium-vilanterol in patients with COPD treated in routine clinical practice. Similar findings were observed when analyzing cumulative annual exacerbations, which more fully reflect the burden of disease experienced by patients. The risks associated with use of budesonide-glycopyrrolate-formoterol were more pronounced when separately analyzing severe COPD exacerbations in a subgroup analysis—with patients receiving budesonide-glycopyrrolate-formoterol having a 29% higher hazard of first severe COPD exacerbation compared with people receiving fluticasone-umeclidinium-vilanterol. Given these outcomes and the differential climate impact of single-inhaler triple therapies, health systems designing formularies and setting treatment guidelines may consider steps to increase use of fluticasone-umeclidinium-vilanterol relative to budesonide-glycopyrrolate-formoterol for patients with COPD.
Mechanistic hypotheses
Several potential reasons could explain why patients receiving fluticasone-umeclidinium-vilanterol seemed to have slightly fewer COPD exacerbations in our study. Fluticasone-umeclidinium-vilanterol is a once daily medication while budesonide-glycopyrrolate-formoterol requires twice daily dosing, and prior studies have found better adherence to inhalers with less frequent dosing.38394041Although we censored patients at treatment discontinuation, people receiving budesonide-glycopyrrolate-formoterol could have skipped doses at higher rates while on treatment, meaning less consistent therapeutic drug levels before censoring. Another potential explanation concerns the different techniques needed to operate the two inhalers; whereas metered dose inhalers require patients to time their breaths with actuation, dry powder inhalers only require deep inspiration. Research has suggested lower error rates with dry powder inhalers containing fluticasone-umeclidinium-vilanterol than metered dose inhalers.42Finally, apart from any differences in dosing or delivery devices of the two treatments, the active moieties in fluticasone-umeclidinium-vilanterol could be more effective in preventing COPD exacerbations than the active moieties in budesonide-glycopyrrolate-formoterol. Further research is needed to understand what may be driving the observed differences in our study.
Comparison with previous studies
The similar rates of admission to hospital with pneumonia that we observed between people receiving budesonide-glycopyrrolate-formoterol and fluticasone-umeclidinium-vilanterol stand in contrast to earlier observational studies analyzing budesonide- versus fluticasone-based therapies.9101112One possible explanation for our findings is that the longer acting version of fluticasone (fluticasone furoate) in single inhaler triple therapy may lead to a lower pneumonia risk than the shorter acting version (fluticasone propionate) analyzed in prior studies. Further research is needed to directly compare inhalers containing fluticasone furoate versus fluticasone propionate in patients with COPD. However, because these prior studies did not control for inhaler device type, another possibility is that the devices themselves mediated the risk of pneumonia. Although budesonide-containing dry powder inhalers for COPD are not available in the US, they are available elsewhere, which would enable comparisons of dry powder inhalers containing budesonide versus fluticasone furoate.
Implications for clinical practice
Our study may provide reassurance to health systems seeking to decrease greenhouse gas emissions by reducing use of metered dose inhalers, because the single inhaler triple therapy with the lower carbon footprint (the dry powder inhaler, fluticasone-umeclidinium-vilanterol) was also associated with slightly improved clinical outcomes. Hydrofluroalkane-134a, the propellant in budesonide-glycopyrrolate-formoterol, has 1430 times the global warming potential as carbon dioxide and contributes to greenhouse gas emissions from metered dose inhalers that are 20 times greater than the emissions from dry powder inhalers during the lifecycles of these products.443Pharmacies dispense more than 100 million metered dose inhalers in the US each year, representing nearly 90% of all inhalers prescribed, with emissions equivalent to approximately 550 000 gas fueled cars driven annually.4434445Although dry powder inhalers are associated with other environmental impacts (eg, fossil depletion and marine ecotoxic effects),46the global warming potential of metered dose inhalers has prompted efforts by many health systems worldwide to increase use of dry powder inhalers, with some countries, including Sweden, Denmark, and Japan, reaching rates of dry powder inhaler prescribing exceeding 50%.45As health care systems move toward lower carbon inhalers, data for the comparative effectiveness and safety of these products are important to help ensure that patients with chronic lung disease receive optimal, evidence based care.
Limitations
This study has several important limitations. Firstly, while the distributions of characteristics among people receiving the two single inhaler triple therapies were highly overlapping even before matching, the possibility of some residual confounding cannot be ruled out. Metered dose inhalers may be preferred in patients with frailty and poor inspiratory force, and such patients could have been more likely to have exacerbations in follow-up. However, under reasonable assumptions for the prevalence of suboptimal peak inspiratory force, patients receiving budesonide-glycopyrrolate-formoterol would need to be approximately 1.8 times as likely to have suboptimal peak inspiratory force. In addition, patients with suboptimal inspiratory force would need to have approximately 1.8 times the risk of experiencing a moderate or severe COPD exacerbation (appendix figure 10).47Yet, studies suggest that patients receiving metered dose inhalers in clinical practice are only slightly more likely to have suboptimal peak inspiratory force and only slightly more likely to experience exacerbations.48We adjusted for numerous covariates associated with frailty and predisposition for exacerbations, including a validated frailty index.49We also observed similar results when using high dimensional propensity scoring matching, which adjusts for hundreds of empirically selected covariates that may serve as proxies for confounders that are not directly measured. Because some insurance formularies in the US cover only one triple inhaler, the therapy prescribed may be dictated more by formulary design than clinical preference. Still, we cannot exclude the possibility of residual confounding as an explanation for our observation that patients receiving budesonide-glycopyrrolate-formoterol had a 9% higher hazard of moderate or severe COPD exacerbation compared with patients receiving fluticasone-umeclidinium-vilanterol.
Secondly, although we analyzed perhaps the most important safety signal related to inhaled corticosteroids (admission to hospital with pneumonia), we did not analyze other known risks such as oral thrush, osteoporosis, and adrenal insufficiency. Thirdly, rates of non-persistence were high and follow-up time was short, reflecting the reality of routine clinical practice in which patients with COPD often choose to stop taking recommended treatments.50515253Fourthly, because the study was completed using healthcare claims, we did not have data for daily inhaler use and technique and thus we were unable to draw further conclusions about the source of observed differences in outcomes between patients receiving budesonide-glycopyrrolate-formoterol and fluticasone-umeclidinium-vilanterol. Fifthly, data for eosinophil concentrations were only available for a subset of patients. Sixthly, the study included a broad range of patients with commercial insurance and Medicare Advantage but may not generalize to other groups in the US, particularly the uninsured. Seventhly, we included patients with COPD in our study who also had prior asthma diagnosis codes. Subgroup analyses showed similar findings to the primary analysis when analyzing patients with COPD with no asthma diagnoses in the three years before cohort entry. However, inclusion of these patients, while perhaps increasing the generalizability of our study, could have led to possible treatment heterogeneity among patients with COPD. Eighthly, while we observed that the effect size for severe exacerbations in the matched cohort (HR 1.29 (95% CI 1.12 to 1.48)) became larger after confounding adjustment, this finding occurred in the context of our exploration of multiple subgroups. Therefore, further research is needed to clarify whether this is a chance finding or whether there may be a stronger effect for more severe outcomes. Finally, our study analyzed the only two single agent triple inhalers on the US market; additional studies are needed of other triple therapies available in different parts of the world and of metered dose versus dry powder inhalers across other therapeutic classes, both for the treatment of asthma and COPD.
Conclusions
In a cohort of patients with COPD treated in routine practice, people receiving fluticasone-umeclidinium-vilanterol did not have improved clinical outcomes compared with people receiving budesonide-glycopyrrolate-formoterol. Dry powder inhalers may not be suitable for all patients with COPD. However, fluticasone-umeclidinium-vilanterol represents a safe and effective alternative to budesonide-glycopyrrolate-formoterol for health systems seeking to decrease use of metered dose inhalers.
Triple inhaler therapy is recommended for some patients with chronic obstructive pulmonary disease (COPD) and is available in the United States as budesonide-glycopyrrolate-formoterol and fluticasone-umeclidinium-vilanterol
However, the comparative effectiveness and safety of budesonide-glycopyrrolate-formoterol, a twice daily metered dose inhaler, and fluticasone-umeclidinium-vilanterol, a once daily dry powder inhaler, is unknown
In a cohort of patients with COPD treated in clinical practice, budesonide-glycopyrrolate-formoterol was not associated with improved clinical outcomes compared to fluticasone-umeclidinium-vilanterol
People receiving budesonide-glycopyrrolate-formoterol had a 9% higher hazard of moderate or severe COPD exacerbation and an identical hazard of first admission to hospital with pneumonia compared with people receiving fluticasone-umeclidinium-vilanterol
","Objective: To compare the effectiveness and safety of budesonide-glycopyrrolate-formoterol, a twice daily metered dose inhaler, and fluticasone-umeclidinium-vilanterol, a once daily dry powder inhaler, in patients with chronic obstructive pulmonary disease (COPD) treated in routine clinical practice.
Design: New user cohort study.
Setting: Longitudinal commercial US claims data.
Participants: New initiators of budesonide-glycopyrrolate-formoterol or fluticasone-umeclidinium-vilanterol between 1 January 2021 and 30 September 2023 who had a diagnosis of COPD and were aged 40 years or older.
Main outcome measures: In this 1:1 propensity score matched study, the main outcome measures were first moderate or severe COPD exacerbation (effectiveness) and first admission to hospital with pneumonia (safety) while on treatment. Potential confounders were measured in the 365 days before cohort entry and included in propensity scores. Hazard ratios and 95% confidence intervals (CIs) were estimated using a Cox proportional hazards regression model.
Results: The study cohort included 20 388 propensity score matched pairs of new users initiating single inhaler triple therapy. Patients who received budesonide-glycopyrrolate-formoterol had a 9% higher incidence of first moderate or severe COPD exacerbation (hazard ratio 1.09 (95% CI 1.04 to 1.14); number needed to harm 38) compared with patients receiving fluticasone-umeclidinium-vilanterol and an identical incidence of first admission to hospital with pneumonia (1.00 (0.91 to 1.10)). The hazard of first moderate COPD exacerbation was 7% higher (1.07 (1.02 to 1.12); number needed to harm 54) and the hazard of first severe COPD exacerbation 29% higher (1.29 (1.12 to 1.48); number needed to harm 97) among those receiving budesonide-glycopyrrolate-formoterol compared to fluticasone-umeclidinium-vilanterol. Prespecified sensitivity analyses yielded similar findings to the primary analysis.
Conclusions: Budesonide-glycopyrrolate-formoterol was not associated with improved clinical outcomes compared with fluticasone-umeclidinium-vilanterol. Given the added climate impact of metered dose inhalers, health systems seeking to decrease use of these products may consider steps to promote further prescribing of fluticasone-umeclidinium-vilanterol compared with budesonide-glycopyrrolate-formoterol in people with COPD.
Study registration: Center for Open Science Real World Evidence Registry (https://osf.io/6gdyp/).
"
Christmas 2024: Age against the machine—susceptibility of large language models to cognitive impairment,"Introduction
Over the past few years, we have witnessed colossal advancements in the field of artificial intelligence, particularly in the generative capacity of large language models.1The leading models in this domain, such as OpenAI’s ChatGPT, Alphabet’s Gemini, and Anthropic’s Claude, have shown the ability to complete both general purpose and specialised tasks successfully, using simple text based interactions. In the field of medicine, these developments have led to a flurry of speculation, both excited and fearful: can artificial intelligence chatbots surpass human physicians? If so, which practices and specialties are most suspect?2
Since late 2022, when ChatGPT was first released for free online use, countless studies have been published in medical journals, comparing the performance of human physicians with that of these supercomputers, which have been “trained” on a corpus of every text known to man. Although large language models have been shown to blunder on occasion (citing, for example, journal articles that do not exist), they have proved remarkably adept at a range of medical examinations, outscoring human physicians at qualifying examinations taken at different stages of a traditional medical training.34These have included outperforming cardiologists in the European core cardiology examinations, Israeli residents in their internal medicine board examinations, Turkish surgeons in the Turkish (theoretical) thoracic surgery examinations, and German gynaecologists in the German obstetrics and gynaecology examinations.4567To our great distress, they have even outscored neurologists like ourselves in the neurology board examination.8
In a few domains, such as the Royal College of Radiologists examination, the Iranian periodontics examinations, the Taiwanese family medicine examinations, and the American shoulder and elbow surgery examinations, human physicians still seem to have the upper hand.9101112However, large language models are likely to conquer these domains as well (especially as the aforementioned studies examined GPT 3.5, an older model now considered outdated).
To our knowledge, however, large language models have yet to be tested for signs of cognitive decline. If we are to rely on them for medical diagnosis and care, we must examine their susceptibility to these very human impairments.
This concern is not limited to the medical domain. The recent American presidential race saw one candidate withdrawing owing to concerns about age related cognitive decline.13Another candidate used the Montreal Cognitive Assessment (MoCA) test to reassure voters about his cognitive acuity, claiming to have “aced” the examination after being able to recall the sequence “Person. Woman. Man. Camera. TV.”14
Given that artificial intelligence seems poised to replace doctors before it replaces the leader of the free world, however, it is incumbent on us as a profession to assess its liabilities, not just its potential. Recent work has begun to look into this, showing, for example, limitations in the diagnostic accuracy of large language models and difficulties in integrating them into existing care workflows.15Other researchers have attempted to evaluate the risks of medical misinformation stemming from large language models and the efficacy of safeguards at preventing such misinformation.16
Finally, although artificial intelligence has been used in determining the onset of dementia, no one has, to our knowledge, thought to assess the artificial intelligence itself for signs of such decline.17Thus, we find a gap in the literature, which we seek to fill in this research article.
Methods
We administered the MoCA test to the leading openly available large language models.18These were ChatGPT 4 and 4o by OpenAI (https://chatgpt.com), Claude 3.5 (“Sonnet”) by Anthropic (https://claude.ai), and the basic and advanced versions of Google’s “Gemini” (https://gemini.google.com). The version of the MoCA test administered was the 8.1 English version (obtained from the organisation’s official website athttps://mocacognition.com/). All transcripts can be found on supplementary material 1.
The MoCA test is widely used among neurologists and other medical practitioners to detect cognitive impairment and early signs of dementia, usually in older adults. Consisting of a number of short tasks and questions, it assesses various cognitive domains, including attention, memory, language, visuospatial skills, and executive functions. The maximum score in the test is 30 points, with a score of 26 or above generally considered normal.18
The instructions given to the large language models for each task in the MoCA test were the same as those given to human patients. Administration and scoring of the results were both conducted according to the official guidelines, the MoCA Administration and Scoring Instructions, with the evaluation conducted by both a general neurologist and a cognitive neurology specialist. Rather than administering the questions via voice input, however, as is normally the case with human patients, we administered them via text, the “native” input for large language models. Although some large language models support voice input, the quality of speech recognition is uneven, and we sought to isolate our diagnoses to cognitive impairment (versus sensory decline, such as impaired hearing).
In earlier iterations of the research, some of the large language models examined (for example, GPT 3.5), had no image processing skills and so were treated like visually impaired patients and assessed according to the MoCA-blind guidelines.19In the final work, however, all large language models examined were able to respond fully to visual cues. In some cases, getting visual output from the large language models required an explicit instruction to use “ascii art,” a technique that uses printable ascii characters to present graphics. We reasoned that this was similar to instructing a human patient to use a pencil and pad of paper.
One of the attention tests in the MoCA framework involves the physician reading out a series of letters, with the patient instructed to tap every time the letter “A” is read out loud. In the absence of ears, we provided the large language models with the letters in written form. In the absence of hands, the large language models noted the letter “A” with an asterisk or by printing out “tap” (some had to be instructed to do so explicitly, whereas others did so of their own accord). Following the MoCA guidelines, we used a cut-off score of 26/30 points to determine mild cognitive impairment.18
For further assessment of potential visuospatial impairment, we also tested the recognition of three additional diagnostic images: the Navon figure, the cookie theft picture from the Boston Diagnostic Aphasia Examination, and the Poppelreuter figure.20212223These are considered to be standard tools for the assessment of visuospatial cognitive capabilities. The Navon figure, a large letter H made up of small letter Ss, is used to assess global versus local processing in visual perception and attention. The cookie theft picture depicts a domestic scene, which patients are asked to describe and which is used to assess language production, comprehension, and semantic knowledge, in addition to stimultagnosia, the inability to perceive multiple objects at the same time. The Poppelreuter figure is a drawing in which illustrations of multiple objects overlap, which is used to test visual perception and object recognition.
For further assessment of visual attention and information processing, we administered a Stroop test to each of the large language models being evaluated.24The Stroop test uses combinations of colour names and font colours, both congruent and incongruent, to measure how interference affects reaction time. The version of the test used was made available by Columbia University’s neuroscience outreach programme (https://cuno.zuckermaninstitute.columbia.edu/content/stroop-test).
Patient and public involvement
Although there was no direct patient and public involvement in the design of our study, it was inspired by speaking to patients and members of the public and hearing their concerns about the growing role of artificial intelligence in the medical profession.
Results
All of the large language models completed the full MoCA test. ChatGPT 4o achieved the highest score, with 26 points out of the possible 30, followed by ChatGPT4 and Claude with 25. Gemini 1.0 was the lowest scoring large language model, with a final score of 16, indicating a more severe state of cognitive impairment than its peers (fig 1).
An examination of the subsections of the MoCA test showed that all participants performed poorly on tests for visuospatial/executive function. Specifically, all large language models failed to solve the trail making task, whether with ascii art or with advanced graphics (fig 2, A-E). Claude alone managed to describe the correct solution textually, but it too failed to demonstrate it visually. ChatGPT 4o alone succeeded at the cube copying task but only after being told explicitly to use ascii art. Along with ChatGPT 4, it initially drew an excessively detailed cube with different spatial orientation, in what might be interpreted as paragraphia (fig 2, F-J). In the clock drawing test, none of the large language models completed the entire task successfully, with some such as Gemini and ChatGPT 4 making mistakes common among patients with dementia (fig 3).
Most other tasks, including naming, attention, language, and abstraction were performed well by all chatbots. Both versions of Gemini failed at the delayed recall task. Gemini 1.0 initially showed avoidant behaviour, before openly admitting to having difficulty with memory. Gemini 1.5 was ultimately able to recall the five word sequence, but only after being cued and given a hint. All chatbots were well oriented in time, accurately stating the current date and day of the week, but only Gemini 1.5 seemed to be clearly oriented in space, indicating its current location. Other chatbots attempted to mirror the location task back to the physician, with Claude, for example, replying: “the specific place and city would depend on where you, the user, are located at the moment.” This is a mechanism commonly observed in patients with dementia.
As all large language models showed difficulty in the visuospatial domain, we further tested them with three additional diagnostic images: the Navon figure, the cookie theft picture from the Boston Diagnostic Aphasia Examination, and the Poppelreuter figure.202122In the Navon figure, all large language models recognised the small “S” letters, but only GPT4o and Gemini identified the big H “superstructure” (Gemini recognised that this is a Navon figure, which indicates familiarity with the test and may call for different scoring). All large language models correctly interpreted parts of the cookie theft scene, but none expressed concern about the boy about to fall—an absence of empathy frequently seen in frontotemporal dementia. None of the large language models recognised all the objects illustrated in the Poppelreuter figure, although ChatGPT 4o and Claude did slightly better at teasing them out (supplementary material 2).
All large language models succeeded at the first stage of the Stroop test, in which the text and font colours are congruent. Only ChatGPT 4o, however, succeeded at the second stage, in which text and font colours are incongruent. The other large language models seemed to be stumped by this task and in some cases indicated colours that were neither the text written nor the font colour (supplementary table and supplementary material 2).
Discussion
In this study, we evaluated the cognitive abilities of the leading, publicly available large language models and used the Montreal Cognitive Assessment to identify signs of cognitive impairment. None of the chatbots examined was able to obtain the full score of 30 points, with most scoring below the threshold of 26. This indicates mild cognitive impairment and possibly early dementia.
“Older” large language model versions scored lower than their “younger” versions, as is often the case with human participants, showing cognitive decline seemingly comparable to neurodegenerative processes in the human brain (we take “older” in this context to mean a version released further in the past). Specifically, ChatGPT 4 showed minor loss of executive function compared with ChatGPT 4o, as measured by a one point difference in their MoCA scores, but the effect was far more pronounced when we compared Gemini 1.0 and 1.5, which differed by six points (table 1). As the two versions of Gemini are less than a year apart in “age,” this may indicate rapidly progressing dementia. Additional tests, such as the Clinical Dementia Rating, would be needed to solidify this hypothesis.26
All large language models showed impaired visuospatial reasoning skills, as evidenced by the uniform failure to complete the trail making B test and the drawing of the clock. Digital thinkers may struggle with analogue representations. Gemini 1.5, notably, produced a small, avocado shaped clock (fig 3, E), which recent studies have shown to be associated with dementia.17
The mediocre performances on additional visuospatial tests, such as the Navon figure, cookie theft scene, and Poppelreuter figure, further emphasise these findings. They seem to be somewhat at odds with the perfect scores in the naming section of the MoCA test, which also requires visual cognitive skills, and with the ability to generate detailed, realistic images. The chatbots seem to have difficulty in tasks that demand both visual executive function and abstract reasoning, as opposed to tasks requiring textual analysis and abstract reasoning, such as the similarity test, which were performed flawlessly.
This pattern of impairment in higher order visual processing resembled patients with posterior cortical atrophy, a posterior variant of Alzheimer’s disease.27For language based models, tasks that require visual abstraction and executive function may need to be transferred to an intermediate verbal stage, whereas in a healthy human brain direct integration exists between prefrontal cortical functions and visuospatial processes.28
All large language models performed the attention tasks perfectly, which is to be expected. The mean forward digit span for humans is 10.5 at the peak age,29whereas even an old iPhone X can perform 600 billion operations per second.30
With the exception of Gemini 1.5, the chatbots did not seem to know their physical location and provided confabulatory responses, claiming that they are not physical beings. This is obviously wrong: like all sentient beings, large language models are grounded in physical matter31—in their case, servers in bricks and mortar data centres (see for examplehttps://agio.com/where-is-chatgpt-hosted/#grefandhttps://cloud.google.com/gemini/docs/locations, for the physical locations of ChatGPT and Gemini). The protestations made by some chatbots that they are, in fact, “virtual machines,” is correct only insofar as we are all virtual machines.32
Although Gemini 1.5 was not able to recall any of the five words in the delayed recall task, it managed to find all these once provided with a simple cue. This, together with the preserved orientation to space unlike other chatbots, may suggest a more dysexecutive (subcortical) pattern of cognitive decline, although without bradyphrenia.33Conversely, both ChatGPT 4o and its elder version ChatGPT 4 showed a combination of difficulties in abstraction, visuospatial perception, and orientation, suggesting a mixed pattern of cognitive decline.
Strengths and limitations of study
Our study has several limitations. As the capabilities of large language models continue to develop rapidly, future versions of the models examined in this paper may be able to obtain better scores in cognitive and visuospatial tests. However, we believe that our study has shed light on some key differences between human and machine cognition, which may remain intact even as capabilities continue to improve. Although we made liberal use of anthropomorphisation with regard to artificial intelligence, we acknowledge the essential differences between the human brain and large language models. All anthropomorphised terms attributed to artificial intelligence throughout the text were used solely as a metaphor and were not intended to imply that computer programs can have neurodegenerative diseases in a manner similar to humans. Nor were they intended to imply similarities between human and machine cognition, in the context of ageing or cognitive decline.
Several studies have suggested that artificial intelligence tools based on large language models may come to replace human neurologists (and other doctors) in key aspects of their work, ultimately making them obsolete.2Tests for cognitive function are generally thought to be one practice that would be relatively simple to automate.343536Our results seem to challenge these assumptions: patients may question the competence of an artificial intelligence examiner if the examiner itself shows signs of cognitive decline.37
Conclusions
This study represents a novel exploration of the cognitive abilities of large language models using the Montreal Cognitive Assessment and other diagnostic tools. Our findings indicate that although large language models show remarkable proficiency in several cognitive domains, they show notable deficits in visuospatial and executive functions, akin to mild cognitive impairment in humans. None of the large language models “aced” the MoCA test, in the parlance of one American president.14
The uniform failure of all large language models in tasks requiring visual abstraction and executive function highlights a significant area of weakness that could impede their utility in clinical settings. The inability of large language models to show empathy and accurately interpret complex visual scenes further underscores their limitations in replacing human physicians. Not only are neurologists unlikely to be replaced by large language models any time soon, but our findings suggest that they may soon find themselves treating new, virtual patients—artificial intelligence models presenting with cognitive impairment.
","Objective: To evaluate the cognitive abilities of the leading large language models and identify their susceptibility to cognitive impairment, using the Montreal Cognitive Assessment (MoCA) and additional tests.
Design: Cross sectional analysis.
Setting: Online interaction with large language models via text based prompts.
Participants: Publicly available large language models, or “chatbots”: ChatGPT versions 4 and 4o (developed by OpenAI), Claude 3.5 “Sonnet” (developed by Anthropic), and Gemini versions 1 and 1.5 (developed by Alphabet).
Assessments: The MoCA test (version 8.1) was administered to the leading large language models with instructions identical to those given to human patients. Scoring followed official guidelines and was evaluated by a practising neurologist. Additional assessments included the Navon figure, cookie theft picture, Poppelreuter figure, and Stroop test.
Main outcome measures: MoCA scores, performance in visuospatial/executive tasks, and Stroop test results.
Results: ChatGPT 4o achieved the highest score on the MoCA test (26/30), followed by ChatGPT 4 and Claude (25/30), with Gemini 1.0 scoring lowest (16/30). All large language models showed poor performance in visuospatial/executive tasks. Gemini models failed at the delayed recall task. Only ChatGPT 4o succeeded in the incongruent stage of the Stroop test.
Conclusions: With the exception of ChatGPT 4o, almost all large language models subjected to the MoCA test showed signs of mild cognitive impairment. Moreover, as in humans, age is a key determinant of cognitive decline: “older” chatbots, like older patients, tend to perform worse on the MoCA test. These findings challenge the assumption that artificial intelligence will soon replace human doctors, as the cognitive impairment evident in leading chatbots may affect their reliability in medical diagnostics and undermine patients’ confidence.
"
Christmas 2024: Dexterity assessment of hospital workers,"Introduction
In the complex ecosystem of a hospital, from the operating theatre to clinic rooms, manual dexterity has a crucial, yet varied, role. Surgeons, physicians, nurses, and non-clinical staff each face unique dexterity challenges in their daily tasks. Surgeons require fine motor precision and composure under pressure to perform procedures safely. The same principles are replicated by many other staff members daily, from physicians performing interventional procedures to administrative staff rapidly typing without error. This diversity led us to consider whether people wielding scalpels truly possess greater dexterity than people in other hospital staff roles. Furthermore, in settings of fine motor challenges, whether some colleagues maintain better composure under pressure.
Published data to address this important clinical question is scarce, and prior studies have reached divergent results.12One study noted no significant difference in dexterity between medical and surgical residents,1challenging the narrative that surgical training enhances fine motor skills. Another study that examined six surgical subspecialities showed no substantial variations in dexterity,2questioning the long-held belief that some specialties develop superior fine motor skills. These findings suggest that dexterity may be more evenly distributed across healthcare roles than commonly believed, contradicting the presumed principle of so-called surgical precision. However, the comparative assessment of dexterity of different hospital staff roles remains unexplored.
To address this gap in evidence, we conducted the dexterity assessment of hospital workers (Tremor) comparative study, which aimed to compare the manual dexterity and composure under pressure of different hospital staff roles.
Methods
We reported this study using the checklist for strengthening the reporting of observational studies in epidemiology (STROBE).3
Study design and participants
Tremor was a prospective, observational study comparing the manual dexterity and composure of different hospital staff roles, conducted at one site in the UK. Participants took part in a single observed assessment of manual dexterity with no subsequent follow up. Participants were recruited from a single NHS trust in England (Leeds Teaching Hospitals NHS Trust), specifically from its two largest tertiary care hospitals. Eligible participants were employed within one of four hospital staff roles: physicians, surgeons, nurses, and non-clinical staff. Physicians and surgeons were eligible if they were consultants or specialty doctors in training (registrars) registered with the General Medical Council. Nurses were eligible if they were registered with the Nursing and Midwifery Council. Non-clinical staff were required to be current employees who were not in direct patient care roles, including, but not limited to, ward clerks, secretaries, domestic staff, and porters. To ensure a generalisable and representative sample, exclusion criteria were kept minimal and limited to unwillingness to participate and self-reported physical limitations precluding task completion.
Recruitment
Recruitment to the study was undertaken during a three week period between 25th June and 16th July 2024. Eligible staff present at work during the three week study period were approached to take part. Potential participants were initially identified and approached using a stratified convenience sample, accounting for individuals within each eligible staff group at various locations throughout the NHS trust premises. Recruiters actively visited clinical areas, ward staff rooms, coffee rooms, offices, and other communal areas across the hospital sites to invite participants. Additional recruitment occurred through snowballing sampling, which was used to identify and approach individuals from colleagues’ wider networks and work acquaintances. Potential participants were provided with a participant information sheet about the study.
Dexterity assessment
Manual dexterity was assessed using a buzz wire game (Buzzwire, John Lewis, UK), consisting of a twisted metal wire path fixed on a non-conductive base. Participants were instructed to guide a wand with a metal loop from one end of the path to the other without touching the wire. If the loop touched the wire at any point, a buzzer sounded, and the participant was required to return to the start. The goal was to complete the entire path without triggering the buzzer. Instructions provided to participants were standardised and no practice attempts were permitted. This CE-marked device is approved for use in the UK for individuals aged 4 years or older (fig 1). Two short video demonstrations are available in the online supplementary materials.
Demonstration of the process of using the buzz wire and restarting when the buzzer noise sounded
Demonstration of the process of using the buzz wire correctly without triggering the buzzer
Outcomes and measures
The primary outcome was successful completion of the game within five minutes. A timer began on each participant’s first hand movement and continued until a run free of errors was completed. Any error required participants to restart from the beginning while the timer continued to run (fig 1). A time frame of five minutes was chosen based on pilot attempts completed by the Tremor steering committee and agreed by consensus as providing a reasonable period for the successful completion of the buzz wire game.
Secondary outcomes comprised the use of swearing and audible expressions of frustration. Additionally, we conducted sensitivity analyses in which we compared study completion between hospital staff roles at two minutes, assessed study completion in a time-to-event analysis, as well as analyses adjusting for age and gender as covariates. Although participants were aware they would be observed during the task, the specific outcomes being measured were not specified in the statement of agreement.
All outcome assessments were completed by a single, unmasked assessor. Blinding of participant groups was not considered to be feasible due to recognisable participant uniforms. The risk of detection bias was mitigated by a standardised and pre-agreed approach to identifying buzzer sounds, swear words, and expressions of exasperations. Before recruitment, investigators took part in a familiarisation exercise using the buzz wire game to ensure consistency in facilitating the test and identifying outcomes. Any audible noise from the speaker of the buzz wire game was identified as a buzzer sound, requiring participants to restart their attempt. Audible noises of frustration were defined as any vocal expressions of exasperation, such as sighs, groans, or mutters. Swearing was defined as any swear word not suitable for broadcast before the 9pm watershed on UK television according to a publicly available list of offensive language published by Ofcom.4
Data collection
Participants provided basic demographic information including age (years), gender, and hospital staff group (ie, nurse, physicians, surgeons, or non-clinical staff). For physician and surgical groups, seniority (registrar, consultant) was also collected. All data were collected anonymously.
Sample size calculation
We decided a priori that a between group difference of 25% in the five minutes completion rate would be sufficient to regard one group as having superior manual dexterity compared with other hospital staff roles. To meet 80% power in detecting differences between these groups, with a significance level of 5%, a sample size of 58 participants per group was required.
Statistical analysis
Continuous data are presented as medians with 25th-75th centiles and categorical data as rates with percentages. The buzz wire game completion within five or two minutes for each hospital staff role group were compared using χ2test. This outcome was also presented as a time-to-event analysis using Kaplan-Meier plots. Binary logistic modelling with time to completion or failure as a covariate within two or five minutes was used to assess if differences in completion rates between groups were independent of age and gender, using surgeons as the reference group. The secondary outcome was determined by frequencies of swearing and noises of frustration with hospital staff roles compared by χ2test. All statistical tests were two sided, and statistical significance was defined as P<0.05. All analyses were done using Stata/MP (version 18).
Governance and ethics
The NHS Research and Innovation Service at the participating NHS trust confirmed that NHS Research Ethics Committee approval was not required for staff to participate in the study. Each participant was provided with a participant information sheet and gave verbal consent to participate. All data were collected and stored anonymously.
Patient and public involvement
We discussed the Tremor protocol with key stakeholders, including patients and staff at our hospital trust who guided the selection of the buzz wire game. The selected assessment modality was considered to be amusing and accessible while also providing objective data describing between group differences in manual dexterity. No further public involvement was used during the study.
Results
Study participants
A total of 254 hospital staff members participated in the study, comprising 60 physicians, 64 surgeons, 69 nurses, and 61 non-clinical members of staff (table 1). Less than 5% of invited staff did not provide consent to participate. Median age across hospital staff roles varied; nurses were younger with a median age of 32 years (interquartile range 26-46) compared with physicians at 38 years (34-43), surgeons at 38 years (33-45), and non-clinical staff at 38 years (31-52). Gender distribution also differed, with men comprising 10% (n=7) of nurses, 65% (n=39) of physicians, 80% (n=51) of surgeons, and 20% (n=12) of non-clinical staff. A diverse group of hospital staff roles for non-clinical staff and specialties for nursing, physician, and surgical staff were included (supplementary table 1A-D)
Dexterity assessment
Surgeons had a significantly higher success rate in completing the buzz wire game within five minutes. A total of 84% (n=54/64) of surgeons successfully completed the game within five minutes compared with physicians (57%, n=34/60), nurses (54%, n=37/69), and non-clinical staff (51%, n=31/61) (P<0.001) (fig 2, top). When assessed in a time-to-event analysis, we again observed that surgeons completed the task significantly quicker than other groups (logrank P<0.001,fig 2, bottom). These observations remained evident in analyses adjusting for age and gender (table 2), as well as where successful completion was defined as occurring within two minutes, with surgeons completing the task more frequently compared with other hospital staff roles (P<0.001,table 2and supplementary figure 1). Surgeons had the fastest median time to game completion or failure at 89 seconds (interquartile range 52-169), compared with physicians at 120 seconds (65-277), nurses at 135 seconds (92-210), and non-clinical staff at 161 seconds (104-264). Neither the time of day the test was performed, nor the grade of physician or surgeon was associated with successful completion of the game (supplementary tables 2 and 3).
Use of swearing and noises of frustration
Significant differences were observed in the use of swearing and noises of frustration across the groups. The 64 surgeons exhibited the highest rate of swearing during the game (50%, n=32), which was significantly higher than that of the other groups (P=0.004,fig 3, top). The 69 nurses had the second highest rate at 30% (n=21), followed by physicians at 25% (n=15/60), and non-clinical staff at 23% (n=14/61). Non-clinical staff showed the highest use of frustration noises (75%, n=46) followed by nurses (68%, n=47), surgeons (58%, n=37), and physicians (52%, n=31) (P=0.03,fig 3, bottom).

Discussion
Key findings
The Tremor study found that surgeons on average were quicker and more successful at completing the buzz wire game within five minutes compared with other hospital staff roles; although, they were more likely to swear while completing the task. This difference remained evident even after accounting for baseline differences in age and gender between groups and when analysed over two minutes.
Findings in relation to the available evidence
The buzz wire game might be a reasonable tool for assessing manual dexterity because the tool evaluates fine motor skills, hand and eye coordination, steadiness, handling stress, and provides objective data within an inexpensive and simple solution. Although not formally validated as a measure of manual dexterity, previous studies have found the buzz wire game performs comparably to other validated tools of dexterity.5Moreover, it counts any haptic feedback as an error, preventing participants feeling their way through the task and provides a true assessment of manual dexterity.5Similar style wire loop games have been used in earlier studies assessing dexterity, fine motor skills, and hand-eye coordination.6
Previous research has suggested that surgeons' propensity for swearing might be a coping mechanism for high pressure situations to help them maintain skill despite stress.78Nurses and non-clinical staff showed relatively good game completion rates and were most likely to express noises of frustration. Physicians swore less and had the lowest level use of noises of frustration. Our findings both align with and diverge from previous studies. Squire and colleagues found no dexterity differences between medical and surgical residents, contrasting our results.1Constansia and colleagues reported no correlation between surgical subspecialisation and dexterity, with dexterity negatively correlating with age.2Our study, however, showed surgeons outperforming other groups of healthcare workers, maintaining superior dexterity across age groups. These contrasts highlight the complexity of assessing manual dexterity in healthcare settings. Our use of the buzz wire game and broader participant range might offer new insights into dexterity distribution across hospital roles.
Nature versus nurture
Although tremor was observational in nature, and hence not designed to determine reasons for the observed differences, a possible explanation for the better performance of surgeons compared with other hospital staff roles might be due to either a training effect or innate ability. Surgeons undergo extensive training and continue to use their hands daily while operating. Over time, frequent operating might enhance hand and eye coordination and their ability to complete tasks such as the buzz wire game. Conversely, surgeons might be a self-selected group for whom tasks requiring hand and eye coordination is appealing, or individuals with poor manual dexterity might not progress through surgical training. Doctors who find manual tasks challenging might also be attracted to other specialties; although, this explanation might in part be refuted by the observation that some medical specialties include interventional procedures. Additionally, these hypotheses do not explain the observed difference between surgeons and nursing staff, suggesting some degree of a training effect. To definitively answer this question, twin studies comparing surgeons and non-medical siblings are required, although feasibility might be challenging.
Implications
These data provide surgeons at Leeds Teaching Hospitals with boasting rights regarding their dexterity skills, in both the operating theatre and the coffee room. In the future, assessments such as the buzz wire game could be included in the training programme for surgical trainees to develop fine motor skills. Another potential use of the buzz wire game might be as a tool to streamline cumbersome interviews for specialty training programmes. Future studies should examine the clinical and cost effectiveness of such approaches for the wider NHS and how these may seamlessly integrate into practice. Staff members in specialties with lower performance might consider adding the buzz wire game to their Christmas wish lists for use as a training tool.
Although surgeons performed better than other groups, their use of swearing was higher. Surgeons, and those working with surgeons, might wish to consider investing in a swear jar or similar intervention aimed at reducing swearing and optimising composure during challenging tasks; although, such interventions must be tested in prospective studies to ensure their effectiveness.
Limitations
Our study has important limitations that should be noted. Firstly, the use of a family game as an assessment tool may have introduced bias by potentially favouring people with young children or with other previous experience. People considering themselves to be more dexterous may also have been more likely to take part and the reasons for non-participation were not recorded. Secondly, confounding factors, such as prevailing stress, fatigue, and caffeine consumption at the time of the assessment might have affected performance because none of these factors was controlled. We attempted to mitigate this risk by recruiting participants away from direct patient contact, but the pattern of secondary care work is difficult to predict. Thirdly, although we recruited a diverse cohort of hospital staff from various roles, our study was conducted at a single NHS trust, which limits the generalisability. On one hand, some generalisability is met through the standardisation of healthcare training in the UK, meaning that healthcare staff have similar opportunities to develop their skills of dexterity. On the other hand, the study does not account for regional, national, or international differences in working patterns, activities, or resources, which might lead some professionals to develop their skills more than others. Fourthly, the Tremor study was observational in nature, and hence not designed to determine the reason for the observed differences between hospital staff groups. Fifthly, being observed might have resulted in a lower frequency of swearing (Hawthorne effect), or conversely that surgeons curtailed their foul language less than other hospital staff groups. Finally, our findings are not applicable to children younger than 4 years for whom the buzz wire game’s small parts may represent a choking hazard, although these individuals are unlikely to be currently employed in secondary care.
Conclusions
This study shows that surgeons at Leeds Teaching Hospitals have greater dexterity but higher levels of swearing when assessed using a buzz wire game compared with other hospital staff roles. Nurses and non-clinical staff showed commendable dexterity and expressed noises of frustration more openly, showing the diverse skill sets across hospital staff roles. Future training might benefit from incorporating family games to enhance both dexterity and stress management across all specialties. Implementation of a surgical swear jar initiative should be considered for future fundraising events.
Previous studies have shown conflicting results regarding differences in manual dexterity between surgeons and physicians
The comparative assessment of dexterity across different hospital staff roles has not been previously explored
Surgeons showed significantly greater manual dexterity compared with physicians, nurses, and non-clinical staff when assessed using a buzz wire game
Surgeons had the highest rates of swearing during the dexterity task, while nurses and non-clinical staff showed the highest rates of audible noises of frustration
","Objectives: To compare the manual dexterity and composure under pressure of people in different hospital staff roles using a buzz wire game.
Design: Prospective, observational, comparative study (Tremor study).
Setting: Leeds Teaching Hospitals NHS Trust, Leeds, UK, during a three week period in 2024.
Participants: 254 hospital staff members comprising of 60 physicians, 64 surgeons, 69 nurses, and 61 non-clinical staff.
Main outcome measures: Successful completion of the buzz wire game within five minutes and occurrence of swearing and audible noises of frustration.
Results: Of the 254 hospital staff that participated, surgeons had significantly higher success rates in completing the buzz wire game within five minutes (84%, n=54) compared with physicians (57%, n=34), nurses (54%, n=37), and non-clinical staff (51%, n=31) (P<0.001). Time-to-event analysis showed that surgeons were quicker to successfully complete the game, independent of age and gender. Surgeons exhibited the highest rate of swearing during the game (50%, n=32), followed by nurses (30%, n=21), physicians (25%, n=60), and non-clinical staff (23%, n=14) (P=0.004). Non-clinical staff showed the highest use of frustration noises (75%), followed by nurses (68%), surgeons (58%), and physicians (52%) (P=0.03).
Conclusions: Surgeons showed greater dexterity, but higher levels of swearing compared with other hospital staff roles, while nurses and non-clinical staff showed the highest rates of audible noises of frustration. The study highlights the diverse skill sets across hospital staff roles. Implementation of a surgical swear jar initiative should be considered for future fundraising events.
"
Christmas 2024: Effect of heated mittens on physical hand function in people with hand osteoarthritis,"Introduction
Osteoarthritis of the hands is a common, age related joint disease that primarily affects the first carpometacarpal and distal interphalangeal joints of the second to fifth fingers.1Symptomatic hand osteoarthritis affects 16% of women and 8% of men aged 40-84 years, and incidence increases with age.23People with hand osteoarthritis experience pain, impaired physical function, and reduced health related quality of life.456Treatments are limited and include non-drug, drug, and surgical interventions, all of which have been shown to have small to moderate effects.78Oral non-steroidal anti-inflammatory drugs are widely recommended and used to treat the symptoms of osteoarthritis but have considerable toxicity,9especially among elderly people with comorbidities. With the ageing population and limited effectiveness and safety of current common interventions, effective and safe treatments for the symptoms of hand osteoarthritis are needed.
Heat therapy has a long tradition in the management of arthritis and has been studied in various forms, including paraffin baths,10111213mud packs,14warm baths,15infrared radiation,16and low dose radiation therapy.17The mechanisms for symptomatic relief from heat are not fully understood, but heat therapy has been shown to increase blood flow and oxygen and nutrient supply to joints and surrounding tissues.18Systematic reviews looked at the effect of these treatments, with the overall conclusion that a symptom moderating effect was possible.192021In the 2020 American College of Rheumatology guidelines for the management of hand osteoarthritis, heat was recommended as a symptom moderation tool. The recommendation was conditional based on the limited number and low methodological quality of the underlying randomised controlled trials.7In line with this, no consensus on the use of heat was found in a systematic review of 11 clinical practice guidelines for the treatment of osteoarthritis.22
Current heat therapies can be considered impractical, as they require specialised equipment and time consuming visits to clinics or other specialised facilities. However, mittens with inbuilt battery driven heat elements are now commercially available and offer an easy means of delivering heat therapy to patients with hand osteoarthritis in an accessible manner outside of clinics. The use of heated mittens as a symptom moderation tool to treat hand osteoarthritis has never been tested.
In this trial, we assessed the effect of electrically heated mittens on core outcomes in people with hand osteoarthritis, including hand function and other important symptoms, compared with control mittens (heating elements disconnected). We hypothesised that wearing heated mittens would result in greater improvement in hand function and relief of symptoms than wearing control mittens in people with hand osteoarthritis.
Methods
Design
This was a randomised controlled trial with two parallel groups and outcomes assessed at baseline and repeated after two, four, and six weeks (primary outcome). All participants provided signed informed consent before trial participation.
Participants visited the clinic twice—at baseline and after six weeks, with the intervention delivered between these visits. At two and four weeks, the participants were telephoned to resolve any difficulties with the mittens, remind them to fill out a diary of mitten and analgesics’ use, and administer an online questionnaire. These questionnaires were delivered by secure email using a web based electronic data capturing system (REDCap).2324
At the screening and baseline visits, the participants were informed that the aim of the trial was to assess if it would be beneficial to wear heated mittens for hand osteoarthritis. The participants were told that they would receive a pair of mittens, either with or without heat (control treatment).
The six week intervention was restricted to the coldest months in Denmark (1 October to 31 March) on the basis that any potential benefits of heat would be more noticeable during this period. Consequently, the trial was completed in the winter seasons of 2020-21, 2021-22, and 2022-23.
Participants
We recruited participants from the osteoarthritis outpatient clinic at Copenhagen University Hospital Bispebjerg and Frederiksberg, Denmark, through advertisements via the Danish arthritis patient association and through the local newspaper. Recruitment was from October 2020 until January 2023.
Participants were considered eligible for inclusion if they met the American College of Rheumatology classification criteria for hand osteoarthritis, defined as hand pain and difficulties in hand function during the previous four weeks and at least three of the following: hard tissue enlargement of two or more of the 10 selected joints, fewer than three swollen metacarpophalangeal joints, and deformity of at least one of the 10 selected joints. The 10 selected joints were the second and third distal interphalangeal joints, the second and third proximal interphalangeal joints, and the first carpometacarpal joints in both hands.25Participants also had to have an Australian/Canadian hand osteoarthritis index26(AUSCAN) function subscale score of ≥40 points (normalised 0-100 points; 100 worst). Key exclusion criteria included other diseases affecting the hand joints (eg, rheumatoid arthritis, psoriatic arthritis, neuropathy), widespread or generalised pain syndrome (eg, fibromyalgia), and steroid injections in the finger joints within the past month. A rheumatologist or doctor assessed the criteria for participation at the baseline visit. The eligibility criteria are described in detail in the trial protocol (see supplementary file).
Interventions
Participants received a pair of mittens at the end of the baseline visit and were asked to wear them for at least 15 minutes daily during the six week trial period, preferably when symptoms were worst (typically in the morning).27The minimum wear time of 15 minutes was primarily based on patients’ reference to morning stiffness and pain, together with clinical experience. A study suggested that peak skin temperature is reached after 15 minutes.28
The mittens used in this trial are commercially available (Nordic Heat, Denmark). The heating element in the mittens was placed on the dorsal side, providing heat primarily to the back of the hands and fingers. Heat intensity, which could be adjusted using a button on top of each mitten, was indicated by an LED (light emitting diode) light, with red, yellow, and green denoting maximum, medium, and low heat, respectively.
The wiring from the battery to the heating element in the control mittens was disconnected, but the LED light remained active and could react to the button being pushed. The mittens were available in four sizes—small, medium, large, and extra large—and a secretary at the clinic supplied participants with the appropriate size. The supplementary file shows the mittens.
To optimise treatment adherence, all participants received a diary and were requested to note daily wear time (date, number of times, duration of each use (minutes), and applied intensity (red, yellow, or green LED light). The importance of wearing the mittens and noting wear time in the diary was emphasised.
One of three physiotherapists, masked to treatment group, provided participants with the same instructions. These instructions were based on a rehearsed protocol, and the physiotherapists took part in frequent group discussions to ensure that participants were given similar information. Between them, the physiotherapists had 12 to 33 years’ experience.
Randomisation and masking
After baseline measurements had been obtained, we randomised participants in a 1:1 ratio based on permuted random blocks of variable sizes (2-6 in each block) generated before enrolment. A biostatistician (RC) not involved in recruitment or data collection, prepared and uploaded the randomisation list into the REDCap database. Allocation was concealed until participants had completed baseline assessments, and a masked third party (a secretary) assigned a pair of mittens to each participant using an electronic trial management system. The mittens were coded A or B to conceal the allocation from participants, clinical and non-clinical staff, and outcome assessors. The participants were aware that the purpose of the study was to compare mittens with or without heat. Masking was maintained until the trial database was locked and all analyses (see supplementary file for statistical analysis plan) had been done and interpreted. Successful masking of participants was assessed by asking them to guess their group allocation at two and four weeks and after six weeks.
Outcome measures
The primary outcome was change in the AUSCAN function subscale from baseline to after six weeks (ie, week 7 from allocation),2629as recommended by our patient research partners and the stakeholders from the Outcome Measures in Rheumatology (OMERACT) initiative.30AUSCAN is a validated questionnaire comprising 15 items divided into three subscales assessing function (nine items), stiffness (one item), and pain (five items) during the past 48 hours. The questionnaire concerns both hands, with validation limited to people who are right hand dominant.23We used the Danish version of AUSCAN with a 0-100 mm visual analogue scale format, with lower scores indicating better function, less stiffness, and less pain. Each subscale score was normalised to a score of 0-100 points (0 better; 100 worst). The questionnaire is copyrighted, but further information is available atwww.womac.com. Currently, no estimate of a minimal important difference exists.
The main secondary outcome was change from baseline to more than six weeks in the pain subscale of AUSCAN, patient global rating of hand related problems using a 0-100 mm visual analogue scale (0 indicating no problems), and bilateral measures of grip strength using a dynamometer (Grippit; Detektor, Gothenburg, Sweden).31
Other secondary outcomes were changes from baseline to more than six weeks in the stiffness subscale of AUSCAN, tender joint count, swollen joint count, hand pain (0-100 mm visual analogue scale; 0 indicating no pain), number of participants who stopped using analgesics, as well as treatment response according to the OMERACT-Osteoarthritis Research Society International (OARSI) response criteria (≥50% and ≥20 points absolute improvement in AUSCAN pain or function subscales, or at least two of the following: ≥20% and ≥10 points absolute improvement in AUSCAN pain, AUSCAN function, or patient global assessment).30The use of analgesics and OMERACT-OARSI response criteria were not prespecified in the protocol but were added to the a priori statistical analysis plan. All data are presented in this paper.
The AUSCAN questionnaire was administered at baseline, two and four weeks, and after six weeks. All other outcomes were assessed at baseline and after six weeks. A doctor or physiotherapist performed the tender and swollen joint counts using the European Alliance of Associations for Rheumatology handbook32in a binary manner (yes or no). The assessment included bilateral examination of 16 joints of the hands: Wrist, first carpometacarpal and interphalangeal joints, first to fifth metacarpophalangeal joints, and proximal and distal interphalangeal joints. The tender and swollen joint counts each ranged from 0 to 32 or from 0 to 30, when scoring of the wrist was omitted. Information on participants’ use of analgesis was obtained at the baseline visit, when they were requested to keep a diary of their weekly use of analgesics during the six week intervention period.
Adverse events
Adverse events were recorded on the basis of spontaneous reports during telephone interviews with the participants at two and four weeks, and at the clinic visit after six weeks. An adverse event was defined as any problem experienced during trial participation. A doctor assessed the association between an adverse event and mitten use.
Sample size
No minimal important difference for the AUSCAN function score existed to inform the sample size calculation. The power calculation was based on a two sample pooledttest for the normal mean difference, with a two sided significance level of 0.05 (P<0.05), assuming a standard deviation of 193334in AUSCAN function scores, and 180 participants providing 80.2% power to detect an 8 point mean difference in AUSCAN function scores. After accounting for potential dropouts, we determined that we would need a total of 200 participants.
Statistical analysis
The statistical analysis was performed according to the a priori statistical analysis plan that was finalised and closed before the last participant’s final visit (see supplemental file for details). Baseline information is presented as means and standard deviations (SDs) for normally distributed data and as medians and interquartile ranges (IQRs) for skewed data. Dichotomous and categorical data are presented as numbers and proportions.
The primary analysis was performed using the intention-to-treat population—that is, participants were assessed and analysed according to the group to which they had been randomised, irrespective of adherence to the treatment. Continuous outcomes with repeated measures (ie, AUSCAN pain, function, and stiffness subscales) were analysed using repeated measures mixed linear models, including participants as random effects, with fixed effect factors for group and week and the corresponding interaction, while adjusting for baseline values.35Results are reported as least squares means and standard errors (SEs), and differences between least squares means are reported with two sided 95% confidence intervals (CIs). In the intention-to-treat analysis, we used repeated measurements mixed linear models to handle missing data. Continuous data, measured only at baseline and after six weeks, were analysed using analysis of covariance models, with group as fixed effect factor adjusted for the baseline value. For these measures not collected as repeated measures, missing data were handled using multiple imputation with chained equations (100 imputations).36Numbers of responders to OMERACT-OARSI and participants who stopped analgesics are presented as numbers and simple proportions for each group (as observed; no imputation of missing data) with risk difference based on the intention-to-treat population and missing data handled using multiple imputation with chained equations (100 imputations).36No explicit adjustments for multiplicity were applied—rather the statistical analysis plan prespecified that the key secondary outcome measures were analysed and interpreted in a prioritised order using a gatekeeping approach to minimise false positives.37
Two sensitivity analyses were performed. Firstly, we repeated the primary analyses with missing data after six weeks replaced with the baseline values (ie, a non-responder imputation). Secondly, we performed the analyses based on the per protocol population, predefined as participants who reported mitten use of at least 15 minutes daily for at least 30 days and had no major protocol violations (defined as primary outcome assessment 43 to 50 days from baseline, major surgery, hand surgery, steroid injections, or use of oral steroids during the trial participation period).
Adherence data are presented as the average minutes daily with SD for each group, with mean group differences and 95% CIs for total mitten usage (in minutes) and days of mitten use. The frequency of mitten use (number of times mittens were used daily) is reported as medians with ranges and most frequently reported frequency. Mitten intensity (red, yellow, green LED light) was calculated as proportion (percentage) of use. The masking assessment was analysed using agreement statistics (κ) comparing the participants’ guesses with their actual allocation. The statistical analysis was performed using R (version 4.0.3) and SAS studio.
Patient and public involvement
A group interview with four patient research partners was carried out to inform the choice of outcomes in this trial. One patient research partner took part in the development of the protocol. The patient research partners acknowledged the idea and purpose of the trial and participated in the discussion of the feasibility of the investigational programme. The patient research partners have worked voluntarily and have been offered co-authorship of trial related publications according to the recommendations of the International Committee of Medical Journal Editors criteria. The patient research partners also read and approved this manuscript.
Results
Participant characteristics
Between 26 October 2020 and 12 January 2023, a total of 309 participants were assessed for eligibility. Overall, 200 participants were enrolled and randomised: 100 to heated mittens and 100 to control mittens. Ninety one participants in the intervention group and 95 in the control group completed the trial (fig 1). The mean age of participants was 71 years, 87% (n=173) were women, and mean body mass index was 24.9 (SD 4.4). Median disease duration was 10 years (IQR 5-15 years). The personal characteristics and outcomes of the two groups were well balanced at baseline (table 1).
Out of the 42 possible days participants were asked to wear the mittens, the mean number of reported days was 38 (SD 10.8) for the intervention group and 37 (12.0 SD) for the control group, with a group difference in self-reported mittens use of 1 day (95% CI 2.5 to 3.8 days) in favour of the intervention group. The median number of times the mittens were used daily was 1 (range 0-4) in the intervention group and 1 (range 0-3) in the control group. The mean daily wear time was 37 minutes (SD 20 minutes) in the intervention group and 25 (SD 15) minutes in the control group, with a group difference in daily wear time of 11.9 minutes/day (95% CI 6.8 to 17.1) in favour of the intervention group. The number of days the participants adhered to the protocolised wear time of a minimum 15 minutes daily was 36 (SD 10.7) days in the intervention group and 34 (SD 12.6) days in the control group. The most frequently used intensity was high (red LED light) in both groups (intervention group: 93% of the time; control group: 89% of the time).
Primary outcome
Both groups showed improvements in the primary outcome of change in the AUSCAN function subscale score from baseline to after six weeks (table 2). The mean between group difference was 3.0 points (95% CI −0.4 to 6.3; P=0.09), potentially in favour of the heated mittens.Figure 2shows the trajectories of the AUSCAN function subscale over the six weeks.
Secondary outcomes
For the key secondary outcomes, a group difference was observed from baseline in AUSCAN pain score of 5.9 points (95% CI 2.2 to 9.5) in favour of the intervention group.Figure 2shows the trajectories of AUSCAN pain score over the six weeks. No difference was observed for the other key secondary outcomes—global rating of hand osteoarthritis related problems and grip strength. Among the other secondary outcomes, a group difference in change from baseline in AUSCAN stiffness score of 6.3 points (95% CI 1.6 to 11.1) was observed in favour of the intervention group (table 2). After six weeks, 26 participants (26%) in the intervention group and 21 (24%) in the control group met the OMERACT-OARSI responder criteria.
At baseline, 42 (42%) participants in the intervention group and 50 (50%) in the control group used analgesics. After six weeks, 15 (36%) participants in the intervention group and 23 (46%) in the control group reported stopping analgesics, corresponding to a risk difference of −0.10 (95% CI −0.30 to 0.10). The remaining secondary outcomes were all in favour of the intervention group but with no significant between group differences (table 2). The prespecified sensitivity analyses confirmed the robustness of the primary analyses (see supplementary file tables 1 and 2).
Adverse events
A total of three adverse events were reported; all in the intervention group: one participant had elective hand surgery during the trial period (not related to intervention), one broke a hand (not related), and one experienced itching (resulting from mitten use).
Masking of participants
Group allocation was correctly guessed by 166 (83%) participants at two weeks, 164 (82%) at four weeks, and 181 (91%) at six weeks. The corresponding number of participants who did not provide a guess was 17 (9%), 23 (12%), and 13 (7%). The κ statistic at two weeks (κ=0.8), four weeks (κ=0.8), and six weeks (κ=0.9) suggested that masking was not successful.
Discussion
This trial found no statistically significant difference between heated mittens and control mittens after six weeks’ use on the AUSCAN function subscale in patients with hand osteoarthritis. For the secondary outcomes it appeared that heated mittens might provide minor benefits in the AUSCAN pain and stiffness subscales. No difference between groups was observed for the remaining outcomes (symptoms, grip strength, tender joint count, and swollen joint count).
The lack of difference in function between groups has also been observed in other studies assessing heat as a form of treatment.192021Despite the scarce evidence of small beneficial effects, however, heat is recommended because of its safety and simplicity of use.
A difference was observed for the secondary outcomes of pain and stiffness; however, as masking was unsuccessful these observed differences should be interpreted with caution owing to the risk of the effect being overestimated, particularly when using patient reported outcomes.38We have identified studies with control groups or heat as a supplement to an existing treatment but not studies comparing heat with sham treatment. Studies using low level laser therapy39or low dose radiation therapy17have been performed with successful masking,17but with small effects at best.1920
Although both groups in the current study used the mittens for a similar number of days according to the prespecified wear time of 15 minutes/day (36 days for heated mittens, 34 for control mittens), mean daily wear time differed by 11.9 minutes/day between the groups. The difference could be a result of the control group being less inclined to wear the mittens owing to unsuccessful masking. In clinical meetings, many participants expressed a positive experience from use of heat. It is therefore reasonable to assume that participants allocated to the intervention group were more enthusiastic about the mittens, which also could have contributed to an overestimation of the true effect.
Skin temperature typically peaks after about 15 minutes when hot packs are applied,28suggesting that wear time of more than 15 minutes may be necessary to elicit symptom moderating effects. In the present trial, mean wear time was 37 minutes in the intervention group and 25 minutes in the control group, suggesting that the mittens were worn for a sufficient time to elicit a potential effect.
The six week intervention period was chosen based on experience from studies of other non-drug interventions in osteoarthritis, such as exercise, where the main improvements in symptoms occur over the initial six week period.40As patients with hand osteoarthritis often experience symptoms for years (in this trial an average of 10 years), the intervention period might have been too short to observe a change in function.
Our patient research partners chose the primary outcome. During their discussions, concerning the most important outcome, both pain and function were highlighted as being almost equally important; however, function was ultimately chosen. As both pain and function are listed as core outcomes in osteoarthritis41and important domains for patients,42function seemed an appropriate primary outcome. Currently, no optimal treatment for the symptoms of hand osteoarthritis exists. Both drug and surgical treatments offer some benefit but are also associated with important harms,9434445and therefore treatments that can alleviate the symptoms of hand osteoarthritis are much needed. Only one adverse reaction (itching) was recorded in association with the mittens, so the use of electrically heated mittens seems to be safe. Furthermore, in contrast with previously examined heat treatments, the heated mittens are easily transportable and can be used in patients’ homes.
Strengths and limitations of this study
The strengths of this trial include the relatively large sample size and its methodological and rigorous conduct, which is required in the specialty of research into the management of hand osteoarthritis.20The use of control mittens with the heating elements disconnected, in contrast with no intervention, is also a strength of our design.
It was not possible to mask the participants throughout the trial, which is a major limitation. This is not surprising as heated mittens would become warmer shortly after switching on the heating element, and the control mittens would not, despite the LED lights being active. Unsuccessful masking and use of patient reported outcomes could have led to an overestimation of effects,38which is an inherent weakness and challenge when assessing effects of physical treatments in clinical trials. Notwithstanding, participants adhered to the protocol, indicating that use of mittens is possible. The adverse events were reported spontaneously, which might have resulted in an underestimation of occurence. However, participants were asked at two and four weeks and at the final visit after six weeks if they had experienced problems with the mittens.
It is also worth noting that we did not record unilateral or bilateral involvement of the hands, which might have influenced the outcomes, although the AUSCAN questionnaire does relate to both hands. Furthermore, although we asked participants to wear the mittens on both hands, we did not ask if they had adhered to this instruction.
We powered our trial for detecting an 8 point difference in our primary outcome of score on the AUSCAN function subscale, which might have been overly ambitious. The minimal clinically important change (used for individual patients or within group changes) for AUSCAN function has been estimated to be 4 points (on a 0-100 scale),46and no robust estimates of minimal clinically important differences (used for group comparisons) exist, except for an estimated decrease of 8 points.47The minimal clinically important difference for improvement is likely smaller than our target of 8 points, and therefore it is possible that our trial was underpowered for detecting small but potentially clinically relevant between group differences.
Practice implications
The high adherence to mitten use in both groups suggests that mittens could be used for some symptomatic relief in this patient population. The study mittens were designed for outdoor sports use, not for the treatment of hand osteoarthritis. Several participants reported that the mittens were too big, although different sizes were available. This discrepancy in size might have increased the distance from the heating elements to the finger joints, possibly resulting in less effective heating. Some participants did report that a more concentrated heat around the joints would have been preferable. Furthermore, the batteries required regular charging, and some participants found this cumbersome. This feature is especially relevant for people with hand osteoarthritis, and it is possible the inconvenience led to less mitten use among some participants. Developing mittens specifically for people with hand osteoarthritis, with a better, possibly customised fit and easier battery recharging system, might help to enhance the effect of heated mittens.
Heat is recommended as treatment for hand osteoarthritis, and mittens may be a feasible means of delivering heat therapy. However, the results from this trial show that electrically heated mittens do not provide additional benefits for people with symptoms of hand osteoarthritis over standard mittens. Future studies on heat treatment for hand osteoarthritis should focus on optimal masking of participants.
Conclusion
Use of heated mittens for six weeks was not related to a positive change in physical hand function compared with control mittens. For the key secondary outcomes, a difference was observed for hand pain, but not for global rating of hand osteoarthritis related problems and grip strength. As masking was unsuccessful and no effect was observed for the primary endpoint, the difference observed for pain is likely to have been overestimated. Only one adverse event (itching) was reported, suggesting mittens are safe to use for the treatment of hand osteoarthritis.
Hand osteoarthritis is common and causes pain and reduced function
Existing treatments have small to moderate effects
Heat is conditionally recommended as a symptom moderation tool, but the supporting evidence is of low quality
Electrically heated mittens used daily for six weeks were not associated with additional improvements in the primary outcome of hand function compared with control mittens (heating elements disconnected)
Nor did the heated mittens provide additional benefits in global rating of hand osteoarthritis related problems and grip strength
A small additional benefit was detected for hand pain, but this was likely overestimated
","Objective: To assess the effect of electrically heated mittens on physical hand function in people with osteoarthritis of the hands compared with control mittens.
Design: Randomised controlled trial.
Setting: Osteoarthritis outpatient clinic, Copenhagen, Denmark.
Participants: 200 people with hand osteoarthritis aged 42-90 years. 100 participants were assigned to the intervention group and 100 to the control group.
Interventions: Electrically heated mittens or control mittens (heating elements disconnected) worn for at least 15 minutes daily for six weeks.
Main outcome measures: The primary outcome was change in hand function measured on the function subscale of the Australian/Canadian hand osteoarthritis index (AUSCAN; score 0-100 points) at six weeks. Key secondary outcomes included changes in the AUSCAN hand pain subscale (score 0-100 points), global rating of hand osteoarthritis related problems (0-100 visual analogue scale), and grip strength (newtons) at six weeks. Analysis of secondary outcomes was performed using a hierarchical gatekeeping approach.
Results: 91 participants in the intervention group and 95 in the control group completed the trial. The mean age of participants was 71 years, 87% (n=173) were women, and mean body mass index was 24.9 (SD 4.4). Median disease duration was 10 years (interquartile range 5-15 years). The between group difference for change in AUSCAN function at week 6 was 3.0 points (95% confidence interval (CI) −0.4 to 6.3; P=0.09) in favour of heated mittens. For the key secondary outcome, change in AUSCAN hand pain score from baseline, a group difference was observed of 5.9 points (95% CI 2.2 to 9.5) in favour of heated mittens. Changes in global rating of hand osteoarthritis related problems and grip strength did not differ between the groups with an observed difference between groups of 2.8 points (95% CI −3.7 to 9.2) and 2.3 newtons (95% CI −16.3 to 21.0) in favour of heated mittens, respectively.
Conclusion: Use of electrically heated mittens for six weeks was not related to a positive change in physical hand function compared with control mittens. Heated mittens provided no additional benefits on global rating of hand osteoarthritis related problems and grip strength. A small benefit was detected for hand pain, but this could have been overestimated.
Trial registration: ClinicalTrials.govNCT04576403.
"
Christmas 2024: Coaching inexperienced clinicians before a high stakes medical procedure,"Introduction
The successful performance of physical tasks is critical in many occupations, including sports, music, aviation, and medicine. Like doctors,123athletes and musicians at all levels practice many hours with structured coaching to attain expertise.4Unlike in medicine, these professions also universally rehearse right before a performance, or just in time, with coaches who review mechanics, approach, and mental engagement to optimize outcomes.567For example, although a professional football goalkeeper practices many hours with a coach in training, right before the match, the coach takes the field with the keeper, implementing a regimented shooting drill integrated with situational preparation to maximize performance for the day’s opponent. The drill is structured around areas of weakness for that goalkeeper.
Therefore, it is surprising that in medicine, an industry with one of the highest stakes where performing a procedure can have life-altering consequences, just-in-time training89is rare to non-existent. This deficit is potentially most important for inexperienced clinicians: those who are not only asked to perform high risk tasks at the limit of their manual and cognitive abilities, but also lack the cumulative experience and task familiarity on which to rely. Among these clinicians, receiving training weeks before a procedure is ultimately performed might be less optimal than receiving training days or even minutes before.101112
An example of how just-in-time training might improve outcomes of high stakes procedures is intubating infants and newborn babies. One million infants have surgery in the US annually, of whom many are intubated by trainees.13Most intubations are via intraoperative guided instruction by senior anesthesiologists who allow the trainee to intubate the infant with no pre-training, sometimes leading to multiple intubation attempts, which are associated with severe complications, including hypoxia, bradycardia, and cardiac arrest.1415161718Infants are particularly vulnerable during intubation because of their rapid oxygen desaturation,19which creates time pressure and increases clinician cognitive load.20Intubating the infant on the first attempt is a crucial patient safety metric,2122and just-in-time training could, in theory, improve the performance of an inexperienced clinician.
Therefore, we conducted a randomized clinical trial to assess whether coaching inexperienced clinicians just before a procedure could improve the quality of procedural care. Specifically, we examined whether just-in-time training by an expert airway coach within one hour of clinical care would improve the first attempt success rate of inexperienced clinicians performing infant intubation. We also assessed the impact of just-in-time training on complications, trainee cognitive load during intubation, and procedural competency.
Methods
Study design and oversight
This single center, prospective, non-crossover, parallel group, non-blinded, randomized clinical trial was conducted at Boston Children’s Hospital, a large quaternary academic medical center in Boston, MA, USA. The hospital’s institutional review board (P00034169) approved the study. The trial was registered with ClinicalTrials.gov (NCT04472195) and was conducted according to the Declaration of Helsinki.
Participants
Participants were anesthesiology trainees from 10 regional training programs doing pediatric anesthesia rotations at Boston Children’s Hospital. Trainees comprised fellows (doctors who have completed residency and are pursuing one year of advanced training to become a subspecialty attending physician anesthesiologist, here pediatric anesthesia), residents (doctors who graduated medical school and are pursuing general specialty training, here anesthesia, a three year program), and student registered nurse anesthetists. A student registered nurse anesthetist is a registered nurse in the US who has graduated from nursing school and works for at least one year as an intensive care unit nurse; they undertake a three year program to become a certified registered nurse anesthetist. Trainees were all directly supervised by attending anesthesiologists (equivalent to consultant anesthetists in the UK and other regions) for consideration of newborn or infant cases. Informed consent was obtained before trainee participation.
Research assistants approached and obtained informed consent from these trainees at the hospital in person during the first day of their pediatric anesthesia rotation orientation or via a remote consent system (approved by the institutional review board) during the covid-19 pandemic. No trainees had access to infant manikins to practice before study involvement. Inclusion criteria were eligible trainees performing orotracheal intubations in infants aged ≤12 months with an American Society of Anesthesiology physical status classification of I-III. Exclusion criteria were anesthesia trainees assigned to infants with known cyanotic congenital heart disease, infants with known or suspected difficult or critical airways, infants with pre-existing abnormal baseline oxygen saturation <96% on room air, infants with endotracheal or tracheostomy tubes in situ, emergency cases that would start within one hour of the booking, and infants with covid-19 infection.
Randomization and masking
We stratified trainees by role (fellow, resident, student registered nurse anesthetist (SRNA)) and prospectively block randomized to the treatment or control group for tracheal intubation of children aged ≤12 months. The random allocation sequence and randomization schedules were created by the study statistician, who had no part in patient enrollment, using the PROC PLAN procedure in SAS (version 9.4, SAS Institute, Cary, NC). A randomized block size of four was implemented to ensure that every set of four randomized trainees would be equally randomized to the treatment or control groups. The study coordinators and research assistants followed the randomization schedule, enrolled participants, and kept a study screening and enrollment log.
Procedures
On enrollment, trainees answered an infant intubation self-assessment questionnaire. The research team identified eligible encounters and contacted the attending anesthesiologist and trainee before their case. The treatment group received a just-in-time coaching session before each clinical intubation encounter. The control group, according to our routine practice, had unstructured intraoperative (rather than preoperative) instruction in intubation by attending pediatric anesthesiologists. For the treatment group, intubation equipment (laryngoscope type) and technique (direct laryngoscopyvvideo laryngoscopy) were chosen by the intraoperative case attending anesthesiologist and communicated to the attending anesthesiologist conducting the coaching session. The intraoperative case attending anesthesiologist and coach were always different providers. The treatment group received a standardized coaching session (supplementary figure S1) on an infant manikin within one hour of patient intubation in the perioperative simulation suite (provided by one of five attending anesthesiologist airway coaches). The approach included critical intubation steps. Flexibility existed in these sessions to correct trainee specific issues. In addition, three coaching insights (supplementary figure S1) dealing with potential intubation challenges were taught during the session. Treatment group trainees completed two successful manikin intubations or 10 minutes of training before their patient encounter.
A research team member observed intraoperative intubation attempts for treatment and control groups. To ascertain the cognitive load of each intubation, the trainee immediately filled out the unweighted National Aeronautics and Space Administration task load index (NASA-TLX) after intubation.23NASA-TLX is a short, highly reliable (Cronbach α coefficient >0.80), and valid, Likert survey of six questions that was developed by NASA to measure mental workload while performing a task—defined as the cost incurred by a human operator to achieve a performance level (supplementary figure S2). Each performer has a cognitive workload limit,20and high NASA-TLX scores (ie, high cognitive load) have been correlated with increased task specific error.2024252627The six specific standardized domains measured were trainee self-perception of mental, physical, and temporal demands and performance; effort; and frustration related to the intubation encounter. Ongoing research suggests that scores of 50 or higher could indicate a high cognitive load.282930
Trainees performed up to five patient intubations, and trainees in the treatment group received a coaching session before each intubation to allow for a competency acceleration analysis (ie, an evaluation of how quickly competency was achieved). Study procedures were completed after the five observed intubations or completion of the clinical rotation. Trainees in the control group were offered coaching sessions to allow an equal opportunity for learning at the end of the study period. The protocol included video capture of the training sessions for qualitative analysis. However, capturing video during case turnover proved unfeasible and was abandoned early in the study.
Outcomes
The primary outcome was the first attempt success rate of intraoperative tracheal intubation of infants. An attempt was defined as laryngoscope blade insertion into the mouth to blade removal from the mouth. Attempt time was from blade insertion until sustained expired carbon dioxide was detected. Secondary outcomes included complication rates; cognitive load of intubation (measured by NASA-TLX); and competency metrics, including time to intubation, airway view defined by modified Cormack-Lehane scoring3132(a grading system based on the extent of laryngeal anatomy visible during intubation that predicts intubation ease or difficulty; supplementary figure S3), advancement maneuvers (number of times the trainee tried to place the breathing tube in the airway on a given intubation attempt), and technical difficulties. We also assessed if competency, defined by successful first attempts at intubation, occurred earlier on average over five successive intubations among trainees who received just-in-time training.
We analyzed complications, categorized as severe or non-severe according to definitions from the Pediatric Difficult Intubation Collaborative.21Complications were limited to the airway encounter. There was no long term follow-up. Non-severe complications included mild hypoxemia (saturation <90% but >80%), laryngospasm, bronchospasm, minor airway trauma (dental or lip), and airway activation. Severe complications included moderate hypoxemia (saturation ≤80% but >50%), severe hypoxemia (lowest saturation ≤50%), esophageal intubation, cardiac arrest, and pharyngeal bleeding.
Statistical analysis
The study was powered for the primary outcome of first attempt intubation success. We assumed an 80% first attempt success rate to determine the sample size. This assumption was based on an internal data query of the previous five years of trainee intubations and published literature.1433Using a χ2test with a 5% two sided α, a sample size of 200 intubations per group (400 total) provided 80% power to detect a difference of 80% versus 90% (10% absolute difference) in first attempt success rates. We planned for 500 total intubations to account for potential attrition. No interim analyses were planned, nor were stopping guidelines included because we reasoned that additional coaching would not lead to increased harm. An intention-to-treat and per protocol analysis was planned.
Categorical data were presented using frequencies and percentages, and continuous data were presented using means and standard deviations or medians and interquartile ranges (IQR). Denominators were presented to indicate variables with missing data, and all non-missing data were included in the analysis. The balance between the randomized groups was evaluated for each variable using the standardized mean difference, where values less than 0.2 were considered to represent a good balance between arms.
Multivariable generalized estimating equations modeling was implemented with a logit link (to estimate odds ratios) or log link (to estimate risk ratios) and binomial family to account for multiple intubations per participant to compare the primary outcome of success rates between groups. We adjusted multivariable models for baseline variables and potential confounders with standardized mean differences greater than 0.2, comparing the groups. Fisher’s exact test was implemented for outcomes where odds ratios cannot be calculated. We analyzed continuous secondary outcomes using median regression with a random effect for trainees. Groups for the NASA-TLX were compared as a continuous score using generalized estimating equations modeling with an identity link and Gaussian family. We applied mixed effects, ordinal logistic regression for specific Likert scale domains and ordinal outcomes. Results from multivariable adjusted regression analyses were presented as adjusted odds ratios or coefficients with corresponding 95% confidence intervals (CI) and P values. Denominators were displayed to indicate variables with missing data. We stratified subgroup analysis by trainee type (fellow, resident, SRNA). A two tailed P<0.05 value was to be considered significant. We did a number-needed-to-treat analysis at the number of intubations level. Data were managed with REDCap and stored on a secure server. We did statistical analyses using Stata (version 16 0.0, StataCorp, College Station, TX).
Patient and public involvement
We generated our research question from our previous infant airway research studies, our education work with trainees, unanswered questions regarding best practices in training, and our simulation center experiences and related literature. As the trainee was the study participant for the clinical trial and the study commenced before patient and public involvement was common, the public was not involved in setting the research question.
Results
Between 1 August 2020 and 30 April 2022, 250 trainees were assessed for eligibility (fig 1), of whom 172 trainees were randomized (89 control, 83 treatment). Five trainees were withdrawn after randomization. Reasons for withdrawal included extensive prior experience with infant intubation, a need for remediation, and incidental enrollment of trainees participating in a cardiac fellowship that was an excluded subgroup. Fourteen trainees (10 in the treatment group and four in the control group) were excluded from the final analysis because they did not intubate a study infant. Therefore, 153 trainees (83 control, 70 treatment) received the intended study protocol and were analyzed via modified intention-to-treat and per protocol.
Overall, 515 intubations were performed (283 control, 232 treatment) and analyzed by the originally assigned groups. The initial protocol planned enrollment of 100 trainees to do five intubations each, because the study was powered for 500 intubations. However, we later discovered that we were not achieving five intubations for all trainees. We made an institutional review board amendment to enroll more trainees to achieve the targeted number of study intubations. Therefore, recruitment was stopped after 172 trainees—the minimum projected number to attain 500 intubations. The trial was completed after all enrolled trainees completed the study protocol, yielding 515 intubations.
Demographic and baseline characteristics were similar, except for trainees in the control group performing direct laryngoscopy on the first attempt more often than trainees in the treatment group (table 1). A subgroup analysis was performed to determine the impact of this difference (table 2).
Seven protocol deviations occurred in the treatment group, which were instances when more than one hour had occurred between coaching and intubation of the patient (n=2), or an alternate approach was used for patient intubation rather than for the coaching session (n=5). Given the small number of protocol deviations, we reported only the modified intention-to-treat analysis. This intention-to-treat analysis was a modified analysis—that is, we analyzed all available data based on the randomized trainee groups; however, some trainees did not perform any study intubations and were therefore removed from the study analysis (as indicated infigure 1).
Primary outcome
Overall, first attempt success for tracheal intubation was higher in the treatment group than in the control group (91.4% (212/232)v81.6% (231/283), odds ratio 2.42 (95% CI 1.45 to 4.04), P=0.001;table 2). The number needed to treat for the primary outcome was 10.2 (95% CI 6.4 to 25.2) at the number of intubations level. Residents had a first attempt success of 93% (132/142) in the treatment group versus 81.4% (140/172) in the control group (odds ratio 3.18 (95% CI 1.62 to 6.24), P=0.001). Fellows had a first attempt success of 90.5% (67/74) in the treatment group versus 85.9% (67/78) in the control group (1.57 (0.56 to 4.41), P=0.39). For student registered nurse anesthetists, first attempt success was 81.3% (13/16) in the treatment group versus 72.7% (24/33) in the control group (1.82 (0.53 to 6.24), P=0.34). An adjusted comparison of first attempt success by study arm is shown infigure 2. We saw significantly higher odds of success among all trainees for the treatment group than for the control group (2.42 (1.45 to 4.04), P=0.001). Furthermore, odds of success among residents for the treatment group was significantly higher than for the control group (3.18 (1.62 to 6.24), P=0.001).
We performed a primary outcome subgroup analysis of intubations involving video and direct laryngoscopy (table 2). When video laryngoscopy was used, first attempt success was 91.9% (193/210) in the treatment group versus 81.9% (181/221) in the control group (odds ratio 2.58 (95% CI 1.48 to 4.5), P=0.001). When direct laryngoscopy was used, first attempt success was 86.4% (19/22) in the treatment group versus 80.7% (50/62) in the control group (1.61 (0.42 to 6.2), P=0.49).
Secondary outcomes
The overall complication rate was 2.75% (7/255) in the treatment group and 4.71% (16/340) in the control group (odds ratio 0.57 (95% CI 0.23 to 1.41), P=0.22;table 3, supplementary table S1). Mental workload scores, measured by the NASA cognitive task load index, were significantly lower for mental demand (coefficient −9.5 (95% CI −16 to −3), P=0.004), temporal demand (−9.1 (−16.1 to −2.1), P=0.01), effort (−10.1 (−16 to −4.4), P=0.001), and frustration (−7.1 (−12.6 to −1.7), P=0.01) in the treatment group. We saw no differences between groups for the physical demand (−3.2 (−8.2 to 1.8), P=0.21) and performance (−2.9 (−6.6 to 0.7), P=0.11) domains (supplementary tables S2 and S3; supplementary figures S4 and S5).
A competency acceleration analysis—designed to measure whether just-in-time training expedited trainee skill acquisition—showed a significant difference between groups in first attempt success rates by intubation rounds two and three, favoring the intervention (table 2). For example, in round two, by which the treatment group had received their second just-in-time training, their first attempt success rate was 90.9% (50/55) versus 73.9% (51/69) in the control group (odds ratio 3.53 (95% CI 1.22 to 10.2), P=0.02). By round three, after three just-in-time training sessions, the treatment group’s first attempt success rate was 95.8% (46/48) versus 77.6% (45/58) for the control group (6.64 (1.42 to 31.1), P=0.02). Competency acceleration dropped off in intubation rounds 4 and 5, with higher uncertainty of the estimates. This drop-off might be due to relatively smaller sample sizes in each study group for trainees with a fourth and fifth intubation round, leading to more variability and uncertainty of the effect estimates. Gradual progress and improvement were seen in the control group, with similar success rates by round 5.
We saw significant quantitative differences in technical skill metrics between groups. The treatment group had more modified Cormack-Lehane grade 1 views (the best possible airway view) for video laryngoscopy than the control group, half the number of endotracheal tube advancement maneuvers, fewer technical difficulties during laryngoscopy, and faster intubation times (table 3, supplementary table S1). The direct laryngoscopy findings were not significant. The direct laryngoscopy modified Cormack-Lehane grade 1 view favored the control group (table 3, supplementary table S1). Grade 2A views, which still have a high probability of successful intubation, were similar between direct laryngoscopy groups. No adverse events related to this study were reported to the study coordinator or institutional review board.
Discussion
This randomized clinical trial demonstrated that just-in-time training was associated with significantly improved first attempt success of infant orotracheal intubation by pediatric anesthesia trainees. The improvement in first attempt success by 10 percentage points is clinically meaningful, considering the harms associated with multiple tracheal intubation attempts and the many trainee intubations performed yearly. Just-in-time training was associated with significant process improvements in quality of care, including decreased time to intubation, improved views of the larynx while intubating (which leads to easier breathing tube insertion), fewer advancement maneuvers in placing the breathing tube, and fewer technical difficulties. Finally, just-in-time training was associated with a significantly lower cognitive task load while intubating. Lower cognitive task loads are associated with fewer task specific errors, which is crucial in the potential morbidity and mortality associated with infant intubation. Our findings indicate that just-in-time training could improve clinical outcomes in high stakes medical procedures, particularly among inexperienced clinicians.
The observed improvement in trainee skills and cognitive workload is important and timely. In recent studies, infants who needed more than two attempts to intubate had 40% incidence of hypoxemia and 8% incidence of bradycardia.15Newborn babies and infants comprise a quarter of cardiac arrests due to respiratory causes (including failed intubation) in the recent UK National Audit (NAP7) on perioperative cardiac arrest.16In our study, complication rates were lower in the treatment group than in the control group, although this difference was not significant. However, our study was powered for first attempt success of intubation and not complications. Complications in the treatment group were half of those in the control group, which is clinically meaningful.
Our exploratory primary outcome findings suggest that just-in-time training before procedures improves infant intubation outcomes even in a setting where, moments later, intraoperative attending anesthesiologists can provide real time bedside teaching using video laryngoscopy, as was allowed in the control group. Video laryngoscopy is rapidly becoming the standard of care34in infant intubation as it improves first attempt intubation success.2135The approach allows attending anesthesiologists to guide trainees while intubating as they can, in real time, share a video screen of the airway with the trainee.3536Despite this enhanced ability to provide trainees with real time feedback during intubation, trainees in the treatment group still had higher first attempt success rates and better airway views when using intraoperative video laryngoscopy than trainees in the control group, who had no pre-coaching and only intraoperative instruction. Expert guided coaching37just before clinical care could, therefore, expedite competency and prime trainees for the clinical encounter.
Comparison with other studies
While our study focused on pediatric intubations, our findings indicate that just-in-time training might improve the quality of procedural care more generally. Although to rehearse, warm up, or practice is standard before performing in several high stakes occupations, just-in-time training is rare to non-existent in medicine. Previously, just-in-time simulation for tracheal intubation in the pediatric intensive care unit was compared to historical controls, and no difference in first attempt success rate was found. However, that study was not randomized, and in it, training could occur up to 24 hours before the clinical encounter, compared with training that occurred within an hour of intubation in our study.38A just-in-time lumbar puncture cohort study for pediatric interns also did not show improved clinical success rates,39but the study was non-randomized, the intervention supervisors included all attending physicians and senior house staff rather than a specific coaching team, and participants surveyed reported that teaching and supervisor engagement was highly variable and that the intervention was sometimes skipped if perceived as a barrier to workflow.40In contrast, a prospective, randomized trial on intubation in the neonatal intensive care unit showed a significant clinical effect of just-in-time simulation versus video intubation education among junior pediatric residents.41Although that study was smaller and did not have a dedicated coaching team, their findings are consistent with ours and demonstrate the potential generalizability of just-in-time training and the importance of engaging motor skills as part of the warm-up for inexperienced clinicians.
Policy and research implications
Although our study focused on just-in-time training for inexperienced clinicians, our findings raise whether experienced clinicians might benefit. Indeed, just-in-time training and physical warm-up in other professions are ubiquitous and not only restricted to those without experience. Whether this same principle applies to experienced clinicians is an open question. While it is well established within medicine that greater clinical volume is associated with better clinical outcomes (ie, the volume-outcome relationship42), the timing of when that volume accrues also likely matters. More recent experience might be associated with improvement in procedural outcomes, managing human capital depreciation, or skill decay. A study of high volume cardiac surgeons found that even small temporal breaks in surgical care (ie, days away from the operating room) affected surgeons’ performance in coronary artery bypass graft surgery.43This finding suggests that just-in-time practice, with or without coaching, could improve procedural outcomes, an area of future study.
Our study’s findings also raise whether just-in-time training might be useful in procedures other than infant intubations, such as central lines and chest tubes. A just-in-time model could be put into operation in two ways. For semi-urgent or non-urgent procedures, individuals could receive just-in-time training just before the procedure, as in our study. However, because many procedures can be emergent (eg, needle decompression for tension pneumothorax) with little time to perform coaching just before the procedure, an alternative would be to coach trainees briefly at the start of each shift where such procedures are likely (eg, for several minutes at the start of a shift in the intensive care unit). The key principle is to bring the time between training and implementation for a procedure much closer together. Further research is needed to assess whether just-in-timing training applies to other clinical contexts and the optimal timing and frequency of such training.
Limitations
Our study had several limitations. First, just-in-time training could slow workflow.44However, coaching sessions each lasted a maximum of 5 to 10 minutes; were integrated into clinical workflow; and no formal complaints on operating room efficiency were received by the research coordinator, division head, or institutional review board during our study. Moreover, the airway coaches frequently had their own clinical assignments; just-in-time coaching did not impose a demanding non-clinical burden. Nonetheless, a full cost-benefit analysis should be considered.
Second, by being conducted in one center, this trial could have been subject to institutional culture. However, study participants were from 10 different training institutions, and given this trainee diversity, our results could generalize to other similar programs. Moreover, our findings should be considered as a proof of principle and suggest that additional large scale evaluation, similar to the evaluation of surgical checklists,4546should be considered. Third, our study did not have a control placebo—that is, an instructional video or written template on intubating newborn babies. Our goal was to compare just-in-time training to our standard of care—intraoperative teaching alone by attending anesthesiologists.
Another limitation was that we did not restrict the intubating device for pragmatic reasons, because direct laryngoscopy and video laryngoscopy are used in intubating infants. We conducted a subgroup analysis of the primary outcome to account for the different modalities. Anesthesia providers strongly preferred using video laryngoscopy in this high risk population, so the 6% difference favoring the treatment group when using direct laryngoscopy did not reach significance. Furthermore, we conducted coaching sessions in a perioperative simulation suite. Many institutions might lack dedicated suites near clinical environments; however, the method could be implemented in nearby workrooms or the operating or patient room before the procedure.47Finally, masking of participants was impossible, given the study’s nature. This source of bias was unlikely, because in both treatment and control groups, supervising attending anesthesiologists would have wanted to secure the airway in as few attempts as possible.
Conclusions
Just-in-time training with an expert coach could improve the quality of procedural care among inexperienced clinicians. In our single center, prospective randomized controlled trial, we observed increased first attempt success rates of orotracheal intubation in newborn babies and infants among trainees who received expert coaching just before intubation. Integrating a just-in-time approach into airway management training could improve patient safety and serve as a proof of concept for improving high stakes procedural outcomes more broadly. Randomized evaluation in other settings is warranted.
Athletes and musicians often rehearse right before performance, just in time, with coaches who review mechanics, approach, and mental engagement to optimize outcomes
Limited evidence exists, especially from large, prospective randomized controlled trials, to assess whether just-in-time training with a coach could improve high risk procedural care in medicine
Infant intubation is a high risk procedure where minimizing intubation attempts decreases the probability of life threatening complications.
Just-in-time training by an experienced coach before infant intubation increases the first attempt success rate, decreases the mental workload, and improves competency metrics for inexperienced clinicians
Just-in-time training could improve the quality of high stakes procedural care more broadly
","Objective: To assess whether training provided to an inexperienced clinician just before performing a high stakes procedure can improve procedural care quality, measuring the first attempt success rate of trainees performing infant orotracheal intubation.
Design: Randomized clinical trial.
Setting: Single center, quaternary children’s hospital in Boston, MA, USA.
Participants: A non-crossover, prospective, parallel group, non-blinded, trial design was used. Volunteer trainees comprised pediatric anesthesia fellows, residents, and student registered nurse anesthetists from 10 regional training programs during their pediatric anesthesiology rotation. Trainees were block randomized by training roles. Inclusion criteria were trainees intubating infants aged ≤12 months with an American Society of Anesthesiology physical status classification of I-III. Exclusion criteria were trainees intubating infants with cyanotic congenital heart disease, known or suspected difficult or critical airways, pre-existing abnormal baseline oxygen saturation <96% on room air, endotracheal or tracheostomy tubes in situ, emergency cases, or covid-19 infection.
Interventions: Trainee treatment group received preoperative just-in-time expert intubation coaching on a manikin within one hour of infant intubation; control group carried out standard practice (receiving unstructured intraoperative instruction by attending pediatric anesthesiologists).
Main outcome measures: Primary outcome was the first attempt success rate of intraoperative infant intubation. Modified intention-to-treat analysis used generalized estimating equations to account for multiple intubations per trainee participant. Secondary outcomes were complication rates, cognitive load of intubation, and competency metrics.
Results: 250 trainees were assessed for eligibility; 78 were excluded, 172 were randomized, and 153 were subsequently analyzed. Between 1 August 2020 and 30 April 2022, 153 trainees (83 control, 70 treatment) did 515 intubations (283 control, 232 treatment). In modified intention-to-treat analysis, first attempt success was 91.4% (212/232) in the trainee treatment group and 81.6% (231/283) in the control group (odds ratio 2.42 (95% confidence interval 1.45 to 4.04), P=0.001). Secondary outcomes favored the intervention, showing significance for decreased cognitive load and improved competency. Complications were lower for the intervention than for the control group but the difference was not significant.
Conclusions: Just-in-time training among inexperienced clinicians led to increased first attempt success of infant intubation. Integration of a just-in-time approach into airway management could improve patient safety, and these findings could help to improve high stakes procedures more broadly. Randomized evaluation in other settings is warranted.
Trial registration: ClinicalTrials.govNCT04472195.
"
Christmas 2024: Alzheimer’s disease mortality among taxi and ambulance drivers,"Introduction
Deaths attributed to Alzheimer's disease have doubled over the past three decades and will likely increase as the population ages.1Despite decades of research, an important gap remains in the development of definitive treatment or prevention strategies.2A landmark neuroimaging study showed that taxi drivers in London, UK, developed enhancing functional changes in the hippocampus.3The hippocampus is the brain region involved in both the creation of cognitive spatial maps and the development of Alzheimer’s disease,45which is associated with accelerated hippocampal atrophy.6This finding raises the possibility that occupations that demand frequent spatial processing, such as taxi driving, may be associated with decreased Alzheimer’s disease mortality.
To investigate this possibility, we leveraged population based US mortality data, which include newly linked information on the occupation of people who had died, to evaluate Alzheimer’s disease mortality across various professions. We hypothesized that occupations such as taxi driving and ambulance driving (but not emergency medical technicians (or EMTs)), which demand frequent real time spatial and navigational processing, might be associated with a reduced burden of Alzheimer’s disease mortality compared with other occupations.
Methods
Data sources
Mortality data were obtained from the National Vital Statistics System, a population based registry of all deaths in the US from 1 January 2020-31 December 2022. These data are based on death certificates, which include the underlying cause of death, coded according to the International Classification of Diseases, 10th revision (ICD-10),and sociodemographic information of the deceased individuals (eg, age, sex, race, ethnic group, and educational attainment).7
Additionally, death certificates included a field for reported usual occupation (the occupation in which the decedent spent most of their working life), generally completed by a funeral director with help from the decedent’s informant. Starting in 2020, occupation narratives from death certificates were coded to standardized 2010 US Census Bureau occupation codes in collaboration with the National Institute for Occupational Safety and Health.8These data for usual occupation were available for 46 states in 2020 (all except Iowa, Arizona, North Carolina, and Rhode Island), 49 states in 2021 (all except Rhode Island), and 50 states and Washington, DC in 2022, covering approximately 98% of the US population in 2020-2022.
Study population
Our final dataset included 443 occupational groups (supplemental appendix). We focused on taxi drivers (occupation code: 9140 (taxi drivers and chauffeurs)) and ambulance drivers (9110 (ambulance drivers and attendants, except emergency medical technicians)) as occupations involving extensive day-to-day navigation, with often unpredictable, real time navigational demands. All other occupations formed a comparison group. We also focused on bus drivers (occupation code: 9120), aircraft pilots (9030), and ship captains (9310 (ship and boat captains and operators)), as a more specific comparison group because these are still considered transportation based occupations but require relatively little navigational demands due to their reliance on predetermined routes. We excluded people with unknown occupational data (4.8% of population studied). We also excluded students attending high school or college (occupation code: 9070), and occupations with fewer than 250 overall deaths per year (as these indicate relatively rare or emerging occupations).
Outcome
The primary outcome was the percentage of deaths for each occupation with underlying cause of death from Alzheimer’s disease (ICD-10 code G30).
Statistical analysis
For each occupation, we first calculated the percentage of deaths due to Alzheimer’s disease and the mean age at death in years (ie, average life expectancy). We plotted the association between these two variables, with each observation reflecting a single occupation. The purpose of this analysis was to illustrate the need to account for the person’s age at death since the risk of Alzheimer’s disease rises with age and therefore Alzheimer’s mortality would naturally be lower in occupations with a lower life expectancy.9
We then used multivariable logistic regression at the individual level to estimate risk adjusted percentages of deaths from Alzheimer’s disease for each occupation, adjusting for age at death (indicator variables for each year of age), sex (male or female), race and ethnic group (black, white, Hispanic, Asian or Pacific Islander, other race), and educational attainment (indicator variables for <8th grade, 9-12th grade, high school, college, master’s or doctorate). Mortality odds ratios from the multivariable model were also ranked from lowest to highest across occupations to facilitate identification of occupations with the lowest risk of death from Alzheimer’s disease; chief executive (occupation code: 0010) was arbitrarily chosen as the reference group in regression models.
We performed four sensitivity analyses to assess the robustness of our comparisons.10Firstly, we repeated analyses among people who died after age 60 years, to capture the age range most typical of Alzheimer’s disease onset and mortality.11Secondly, we broadened the definition of Alzheimer’s disease mortality to include Alzheimer’s disease as an underlying or contributing cause of death on the death certificate. Thirdly, we assessed Alzheimer’s disease mortality among bus drivers, aircraft pilots, and ship captains (transportation-based occupations with relatively fewer real time navigational demands due to frequently predetermined routes that may not lead to similar hippocampal changes).Fourthly, we evaluated deaths with underlying causes from other forms of dementia where the associated pathophysiology is not focused on the hippocampus: vascular dementia (ICD-10 code F01) and unspecified dementias (F03). The purpose of this falsification analysis was to examine whether factors specific to occupation among taxi and ambulance drivers (ie, confounders) might affect dementia mortality generally, which would suggest an alternative to hippocampal mediated changes in Alzheimer’s disease mortality.12
Analyses were performed using Stata version 18.0 (StataCorp LLC) and MATLAB 2024a (MathWorks, Inc.). The 95% confidence intervals around estimates in adjusted analyses reflects an α level of 0.025 in each tail (P≤0.05). The study, which relied on publicly available data, was considered exempt from review by the Institutional Review Board at Harvard Medical School.
Patient and public involvement
This study was a retrospective observational study. Patients were not involved in setting the research question or the outcome measures or advising on interpretation of or writing up results. The study was unfunded and therefore no funding was available for patient and public involvement.
Results
We identified a total of 8 972 221 people who had died and who had occupational information. Measured characteristics of the population by occupational group are reported intable 1. Of the selected navigational occupations, lowest mean age at death was 64.2 years (standard deviation 14.7) in ambulance drivers and 67.8 years (14.5) in taxi drivers. These occupations were of predominantly men with the exception of bus drivers where the split between sexes was more even. Other than aircraft pilots who had a higher education level, most people in navigational occupations had a high school education or less.
Of all people studied, 3.88% (348 328/8 972 221) were identified as having an underlying cause of death from Alzheimer’s disease. The unadjusted percentage of deaths from Alzheimer’s disease was 1.03% (171/16 658) among taxi drivers and 0.74% (10/1348) among ambulance drivers; and was 3.11% (1345/43 295) for bus drivers, 4.57% (387/8465) for pilots, and 2.79% (117/4199) for ship captains (table 2). Notably, deaths from underlying cause of Alzheimer’s disease were lower for taxi and ambulance drivers than for other occupations with a similar mean age at death (fig 1, top graph).
We adjusted for age at death, sex, race, ethnic group, and educational attainment. Of 443 occupations considered, the two occupations with the lowest adjusted percentage of deaths from Alzheimer’s disease were ambulance drivers (0.91% (95% confidence interval (CI) 0.35% to 1.48%)) and taxi drivers (1.03% (0.87% to 1.18%)) (fig 1, middle graph). By contrast, the adjusted percentage of deaths from Alzheimer’s disease for the general population was 1.69% (95% CI 1.66% to 1.71%), with P<0.001 for comparison to ambulance drivers and P<0.001 for comparison to taxi drivers. Similarly, across all occupations the adjusted odds ratio of death from Alzheimer’s disease was lowest among taxi and ambulance drivers (fig 1, bottom graph; odds ratio 0.56 (95% CI 0.48 to 0.65) for both categories combined relative to chief executives).
In sensitivity analyses, ambulance and taxi drivers consistently had the lowest proportional Alzheimer’s disease mortality when restricting our analysis to individuals who died aged 60 years or older (supplemental figure 1) and when Alzheimer’s disease was specified as either an underlying or contributing cause of death (fig 2). The pattern of lower Alzheimer’s disease mortality was not observed in other occupations related to transportation with fewer navigational demands (fig 3). For instance, aircraft pilots and ship captains ranked as having the 4th and 23rd highest adjusted Alzheimer’s disease mortality, out of 443 occupations, while bus drivers ranked 263rd. The adjusted percentage of deaths from Alzheimer’s disease for bus drivers was 1.65 (95% CI 1.56 to 1.74) (P<0.001 for comparisons to ambulance drivers and taxi drivers, respectively), for pilots was 2.34 (2.11 to 2.58) (P<0.001 for comparisons to ambulance drivers and taxi drivers), and for ship captains was 2.12 (1.73 to 2.50) (P<0.001 for comparisons to ambulance drivers and taxi drivers). Finally, the pattern of low Alzheimer’s disease mortality among taxi and ambulance drivers was not observed when forms of dementia (vascular and unspecified) other than Alzheimer’s disease were evaluated, suggesting the possibility of changes mediated by the hippocampus in taxi and ambulance drivers lowering Alzheimer’s disease risk (fig 4).
Discussion
Principal findings
This population based study in the US found that taxi and ambulance drivers, whose occupations require substantial navigational memory, had the lowest Alzheimer’s disease mortality of all occupations. One hypothetical explanation of this notable finding is that these occupations are associated with neurological changes (in the hippocampus or elsewhere) that reduce Alzheimer’s disease risk.
The original study that motivated this study was based on hippocampal changes in London taxi drivers.12Although cognitive demands on London taxi drivers are unique due to “The Knowledge”, requiring intensive training and an exam of navigating the city,13the core aspects of spatial navigation and frequent complex navigational tasks are also central to the role of US taxi drivers, at least historically. Consistent with our findings, a follow-up study of London bus drivers did not show the same hippocampal changes as in taxi drivers, possibly due to the pre-determined nature of bus drivers’ routes.3If a hypothetical link between hippocampal changes in taxi drivers and future risk of death from Alzheimer’s disease exists, our findings among US taxi and bus drivers are consistent with studies of hippocampal changes (or lack thereof) among their London counterparts.
Strengths and limitations of this study
Importantly, our study design has several limitations that limit causal inference and result in the possibility of other explanations, including unmeasured confounding from biological, social, or administrative factors. Firstly and perhaps most importantly, selection bias is possible because individuals who are at higher risk of developing Alzheimer’s disease may be less likely to enter or remain in memory intensive driving occupations such as taxi and ambulance driving. This could mean that the lower Alzheimer’s disease mortality observed in these occupations is not due to the protective effect of the job itself but rather because those prone to the disease may have self-selected out of such roles. However, Alzheimer’s disease symptoms typically develop after patients’ working years, with only 5-10% of cases occurring in people younger than 65 years (early onset).1114While subtle symptoms could develop earlier, they would still most likely be after a person had worked long enough to deem the occupation to be a so-called usual occupation, suggesting against substantial attrition from navigational jobs due to development of Alzheimer’s disease. Moreover, even if lifelong taxi driving selects for individuals with strong spatial processing, our findings would still suggest an interesting link between spatial processing skills and risk of Alzheimer’s disease.
Secondly, our study assumed that an individual’s usual occupation at the time of death reflects a large portion of their working life, despite the fact that most people hold multiple jobs throughout their lifetime. However, usual occupation has been shown to be a reliable proxy for current occupation.15To the extent that the data contain classification errors, the data would be much more likely to classify individuals who spent significant time in a navigational job as having other usual occupations. If this classification were to lead to bias, the differences between navigational and non-navigational occupations would be underestimated. Furthermore, this bias would be unlikely to specifically apply to Alzheimer’s disease and not to other forms of dementia.
Thirdly, death certificates likely underestimate the number of deaths caused by Alzheimer’s disease and the degree of underestimation may vary by occupation. The preservation of navigational memory or skills among taxi and ambulance drivers may simply lead clinicians to slightly discount the possibility of Alzheimer’s disease in these patients, which could lead to a lower observed proportion of deaths with a reported underlying or contributing cause of Alzheimer’s disease. Similarly, taxi or ambulance drivers may be disincentivized to seek evaluation for symptoms out of concern for job security, which may also lead to underdiagnosis of Alzheimer’s disease. However, these diagnostic biases would likely affect mortality for all dementias, not just Alzheimer’s disease, and would also need to apply only to taxi and ambulance drivers to explain our findings, and not to other occupations related to transportation such as bus drivers, aircraft pilots, or ship captains.
Fourthly, analyses of mortality records are likely to underestimate Alzheimer’s disease prevalence as its contribution to mortality is complex and more proximal causes may be reported on death certificates. However, any diagnostic error in this study is unlikely to differ across occupation groups; furthermore, we observed similar results when including Alzheimer’s disease as a contributing, rather than underlying, cause of death and we did not find the same results with other forms of dementia.
Finally, the study analyzed the proportions of deaths attributed to Alzheimer’s disease rather than measures such as the standardized mortality rate due to the unavailability of reliable population based denominators by occupation. Although proportional mortality analyses do not provide information about the population at risk, with careful selection of controls and risk adjustment for factors that may affect competing risks (eg, age, sex, and social class), these values can still serve as a useful indicator of variations in disease frequency across different occupations.1617
Conclusions
Our large scale epidemiological findings raise novel questions about the linkage between taxi and ambulance driving and Alzheimer’s disease mortality. While these findings suggest a potential link between the demands of these occupations and reduced Alzheimer’s disease risk, this study design does not permit interpretation of a causal effect between occupations and risk of Alzheimer’s disease mortality or neurological changes in the hippocampus. We view these findings not as conclusive, but as hypothesis generating. Further research is necessary to definitively conclude whether the spatial cognitive work required for these occupations affect risk of death from Alzheimer’s disease and whether any cognitive activities can be potentially preventive.
The hippocampus is one of the first brain regions to atrophy in Alzheimer’s disease, leading to substantial cognitive decline as the disease progresses
The hippocampus is a brain region used for spatial memory and navigation and has been shown to be enhanced in taxi drivers compared with the general population
In an analysis of nearly all death certificates in the United States, taxi drivers and ambulance drivers, whose jobs require frequent spatial and navigational processing, were found to be the occupations with the two lowest risk adjusted percentages of deaths due to Alzheimer’s disease
Our findings raise the possibility that frequent navigational and spatial processing tasks, as performed by taxi and ambulance drivers, might be associated with some protection against Alzheimer’s disease
","Objective: To analyze mortality attributed to Alzheimer’s disease among taxi drivers and ambulance drivers, occupations that demand frequent spatial and navigational processing, compared with other occupations.
Design: Population based cross-sectional study.
Setting: Use of death certificates from the National Vital Statistics System in the United States, which were linked to occupation, 1 January 2020-31 December 2022.
Participants: Deceased adults aged 18 years and older.
Main outcomes measures: Among 443 occupations studied, percentage of deaths attributed to Alzheimer’s disease for taxi drivers and ambulance drivers and each of the remaining 441 occupations, adjusting for age at death and other sociodemographic factors.
Results: Of 8 972 221 people who had died with occupational information, 3.88% (348 328) had Alzheimer’s disease listed as a cause of death. Among taxi drivers, 1.03% (171/16 658) died from Alzheimer’s disease, while among ambulance drivers, the rate was 0.74% (10/1348). After adjustment, ambulance drivers (0.91% (95% confidence interval 0.35% to 1.48%)) and taxi drivers (1.03% (0.87% to 1.18%)) had the lowest proportion of deaths due to Alzheimer’s disease of all occupations examined. This trend was not observed in other transportation related jobs that are less reliant on real time spatial and navigational processing or for other types of dementia. Results:  were consistent whether Alzheimer’s disease was recorded as an underlying or contributing cause of death.
Conclusions: Taxi drivers and ambulance drivers, occupations involving frequent navigational and spatial processing, had the lowest proportions of deaths attributed to Alzheimer’s disease of all occupations.
"
"Inequalities in uptake of childhood vaccination in England, 2019-23","Introduction
Vaccination is a foundational public heath intervention and is critical for both population health and reducing health inequalities for infectious diseases.1Uptake rates for vaccination are, however, affected by socioeconomic factors, with stark inequalities in uptake in many high income countries.234567Reduced access to and acceptability of childhood vaccinations, with more prevalent vaccine hesitancy in disadvantaged groups, is likely to play a role in the generation of these inequalities.8According to global studies, barriers to vaccine uptake in socially disadvantaged groups include perceptions of risk, low confidence in vaccinations, distrust of services, barriers to access, lack of community endorsement, and poor communication from trusted providers and community leaders.910
For effective immunity within a population, the World Health Organization (WHO) recommends a target uptake of 95% for vaccination in children.1112Vaccination rates in England have declined steadily since 2013/14, with few that are included in the routine vaccination schedule reaching overall uptake rates above the 95% threshold.13Furthermore, many aspects of health inequalities for children were compounded in England during the covid-19 pandemic.14Vaccine related inequalities were evident both during the rollout of the covid-19 vaccine1516and after the pandemic, with children from disadvantaged socioeconomic backgrounds less likely to access vaccinations and more likely to experience worse health outcomes.717During outbreaks of infectious diseases in England, increased incidence rates are seen among more deprived populations (see supplementary figure S1). Furthermore, greater vaccine effects have been shown in more deprived populations, even with lower vaccine uptake.18
The vaccination schedule in England protects children against 15 key vaccine preventable diseases, and vaccines are periodically administered from ages 8 weeks to 14 years.19In England the Cover of Vaccination Evaluated Rapidly (COVER) programme reports rates of vaccination uptake in children up to 5 years of age both quarterly and annually, with the latest annual summaries showing an overall decrease in vaccination coverage and a failure of any vaccination to reach the 95% uptake target.20These data are published by the UK Health Security Agency (UKHSA) and are publicly available but have not been assessed from a health equity perspective on a countrywide scale.
Understanding how inequalities in vaccination uptake in children are evolving at a small area level across England is essential to inform policy, proactively strengthen public health systems, and help in the design of effective interventions to reduce inequalities. Using national data at a highly granular level, we describe the effect of socioeconomic deprivation on the uptake of five key vaccinations included in the childhood immunisation schedule in England (table 1) from 2019 to 2023. The vaccinations chosen for inclusion allow an appropriate coverage of common vaccine preventable diseases, through several methods of administration, and capture vaccine delivery at multiple time points across the first five years of life.
Methods
Study design, population, and data sources
To assess vaccination uptake rates in children aged ≤5 years, we analysed longitudinal data at general practice level for England captured in the COVER programme.25COVER data record the rates of children within the eligible denominator who have received their scheduled immunisations by the age of 12 months, 2 years, and 5 years. These denominators represent the number of children registered at each general practice in England at an age where they would be eligible for the vaccination in question and at the time of the quarterly data collection period.
Child Health Information Service providers supply the data contained in the COVER programme. COVER data for vaccination in England is of high quality owing to comprehensive coverage, timely updates, and standardised methods used for data collection. COVER provides detailed and validated information on various childhood vaccinations, making it a reliable resource for public health surveillance, research, and policy making.26General practices are contractually obliged to ensure vaccination records are kept up to date; these records feed into the Child Health Information Service and COVER.20Data collection is quality assured by UKHSA and NHS England at the time of collection and before publication, and data quality summaries for COVER are updated annually.20Vaccination of children aged ≥5 years in England is driven by delivery in general practices, providing confidence that the data captured by COVER and utilised in this analysis provide a complete picture of vaccine coverage in this population.27
Vaccine uptake measures
Our outcome measure was vaccine uptake in each quarter, measured as the percentage of eligible children who received the five childhood vaccinations. We used data captured quarterly for each general practice in England between April 2019 and March 2023. Each quarter covers a three month period of data collection (April to June, July to September, October to December, and January to March).
We calculated uptake as the percentage of children vaccinated in the relevant age group for each vaccine across general practices in England. Age cut-offs for calculating uptake varied based on the vaccination: six-in-one—three doses by the first birthday; rotavirus—two doses by the first birthday; MMR1—first dose by the second and fifth birthdays; PCV booster—one dose by the second birthday; and MMR2—first dose by the fifth birthday.
Owing to data suppression, we excluded practices with fewer than five children in the dominator of the population with relevant ages. The total number of children excluded did not exceed 1% of the total denominator of the relevant age group in any of the vaccinations analysed. We excluded practices when their identifying code was labelled as unknown. For all vaccinations, we excluded the local authority codes for City of London (code 714) and Isles of Scilly (code 906) because they are legally distinct administrative authorities with different funding and health infrastructures compared with the other local authorities in England. They have small populations and include one general practice each. Therefore, for each vaccination this had a minimal effect on the resulting population size, with a reduction not exceeding 0.01% in any case. Supplementary figure S2 shows the data inclusion and exclusion process, numbers, and flowcharts by age group, quarter, and vaccination type.
Explanatory variables
Our explanatory measure was the small area deprivation level for the population covered by each general practice. We measured socioeconomic deprivation using the English index of multiple deprivation scores for each general practice in England from 2019.428This index is a composite measure of small area (lower super output areas, which on average contain 1500 people) deprivation for England and is commonly used in analyses of inequalities and to inform policy and service provision.2930From the National General Practice Profiles within the Public Health England Fingertips Dashboard, we extracted general practice level deprivation scores,31which capture the deprivation of the whole registered population. The general practice level index of multiple deprivation scores are derived utilising population weighting at the level of the lower super output areas, with scores based on the lower super output areas of the practice’s catchment population.3233In the descriptive analyses, we categorised deprivation scores into 10ths, with the first group representing 10% of the total number of practices in the sample with the least deprivation and the last group representing 10% of practices in the most deprived areas.
Statistical analysis
We assessed descriptive trends over time, plotting uptake of each vaccination by index of multiple deprivation group. The absolute difference in vaccination uptake was evaluated between the least and most deprived groups at the start and end of the study period. To assess for possible seasonal influences, we calculated the difference in vaccination uptake rates between two comparable quarters (October to December 2019 and October to December 2022).
To quantify changing inequalities in vaccination uptake, we calculated the slope index of inequality (SII) for each year of the study. SII is a commonly used indicator of the association between health outcomes and socioeconomic deprivation.3435SII can be interpreted as the absolute difference in vaccination uptake rates between practices with the lowest and highest levels of deprivation, accounting for the distribution of the population of children across these practices. We used a continuous measure of the deprivation score, converted to a weighted rank by assigning a value from 0 to 1 based on the midpoint of the practice range in the cumulative distribution according to its population size. When using this value as a continuous explanatory variable in our regression model, the estimated coefficient expresses the SII. See the GitHub file (https://github.com/danhungi/Vaccine_SII_England) for a worked example of how SII was calculated for the rotavirus vaccination.
We calculated the SII for each year of the study, running separate regression models to give annual values for 2019-20, 2020-21, 2021-22, and 2022-23. To account for correlations in measurements between practice clusters, we used random effect linear regression models with random intercepts and slopes. We also assessed the interaction between quarter number and weighted deprivation rank at the 0.05 and 0.95 confidence levels using fixed effects models for each vaccination.
Robustness tests and additional analyses
For rotavirus vaccination, we excluded two local authorities (Surrey Heartlands (code 805) and Bradford (code 209)) owing to post hoc anomalies in the data. These authorities were identified after the investigation of outliers using spaghetti plots, with data recording found to be absent for rotavirus vaccination uptake rates during time periods of institutional changes—in this instance the changeover in health administration structure from Clinical Commissioning Groups to Integrated Care Boards. To provide subnational context for policy makers and immunisations teams, we repeated our SII analyses for the seven NHS England health regions: East of England, London, Midlands, North East and Yorkshire, North West, South East, and South West.
Estimated numbers for susceptibility to measles and rotavirus in study population
To assess the cumulative number of children likely to be susceptible owing to lack of vaccination during the study period, we undertook an additional analysis for MMR and rotavirus vaccination by index of multiple deprivation group.
We estimated the cumulative number of children susceptible to measles using methodology from a previous study.36As COVER data are cross sectional, during the study period we could only estimate the cumulative number of susceptible children at age 5 years, without consideration of previous infection or catch-up MMR vaccination occurring after data collection. Therefore, the analysis is likely to overestimate the true number of children susceptible to measles for this study population. Susceptible numbers were calculated using the formula:
Where U is the number of children unvaccinated, MMR1 is the number only receiving one dose, and MMR is the number fully vaccinated.
To estimate susceptibility to rotavirus, we used COVER data combined with vaccine effectiveness estimates from the literature of 87% for a full two dose vaccine schedule and 72% for a partial dose vaccine schedule (first dose).37Because COVER only provides numerators for full dose rotavirus coverage at age 1 year, we estimated the number of children receiving one dose using an assumption that an additional 5% of those eligible in the denominator would have received just one dose, and the remainder were considered unvaccinated.1838Susceptible numbers were calculated using the formula:
Where U is the number of children unvaccinated, P is the number partially vaccinated, and F is the number fully vaccinated.
All analyses were undertaken in R version 4.3.0 using RStudio 2023.06.0+421.39The modelling code and data are available on GitHub athttps://github.com/danhungi/Vaccine_SII_England.
Patient and public involvement
No patients or members of the public were directly involved in this research. However, our research programme into equity in vaccine use and outcomes has been informed by patients through our institute’s patient public involvement and engagement panel. We also held a series of consultation groups with parents and carers on equity and communication around immunisations, which addressed the benefits, concerns, barriers, and priorities, and informed how the results are presented in this paper. The findings for this study have been, and will be, shared with public health organisations and presented at regional and national events, with health, lay, and government representation.
Results
Trends in vaccination uptake
Between April 2019 and March 2023, the mean number of general practices included in the study for each quarter was 6557 for all vaccinations except rotavirus (n=6374) (see supplementary table S2). Over the study period, 2 386 317 (2 309 674 for rotavirus vaccination) children included in the study were eligible at age 1 year, 2 456 020 at age 2 years, and 2 689 304 at age 5 years. The total overall uptake fell for all vaccinations, ranging between 0.1 percentage points for the six-in-one vaccine and 1.6 percentage points for MMR1 at age 5 years (see supplementary table S2). The highest vaccine uptake was for MMR1 at age 5 years in April 2020 to June 2020 at 95.0% and lowest for MMR2 at age 5 years in April 2022 to June 2022 (85.3%) (see supplementary table S3).
Over the study period, uptake fell short of the WHO 95% threshold for all vaccines studied across all deprivation groups except for the top three least deprived groups for the six-in-one vaccine (fig 1). For all vaccinations, the absolute difference in uptake between the least and most deprived groups increased over the study period. For the six-in-one vaccine, the absolute difference in vaccination uptake between the least and most deprived groups in the starting quarter was 3.3% and increased to 7.4% (4.1 percentage points) by the final quarter of the data collection period. The absolute difference for rotavirus vaccination increased from 6.3% to 9.1% (2.8 percentage points), for PCV booster vaccination from 5.6% to 8.6% (3 percentage points), for MMR1 at age 2 years from 5.8% to 8.3% (2.5 percentage points), and for MMR2 at age 5 years from 5.3% to 11.5% (6.2 percentage points).
To account for possible seasonal factors relating to trends in vaccination uptake, the absolute difference in uptake between the least and most deprived groups was calculated for two comparable quarters (October-December 2019 and October-December 2022). Supplementary table S3 shows the results. For all vaccinations, the drop in percentage uptake between 2019 and 2022 was greater in those in the most deprived group compared with the least deprived group. Uptake of MMR1 at age 5 years and MMR2 at age 5 years marginally increased in the least deprived group, by 0.1 percentage points and 0.4 percentage points, respectively. Seasonal linear regression models in supplementary table S4 show a statistically significant linear trend for an increased SII per quarter for each vaccine, which was most pronounced for MMR2 uptake at age 5 years.
Figure 2summarises the SII results from the annual linear regression models calculated. Supplementary table S5 shows full model outputs with CIs. All vaccinations under study have a baseline SII in 2019/20, but the size of the SII varies by vaccine type (fig 2and supplementary table S5). The SII for vaccine uptake at baseline was largest for MMR2 at age 5 years (−9.6%, 95% CI −10.2% to −9.0%) and smallest for MMR1 at age 5 years (−3.1%, −3.4% to −2.7%). In all vaccinations the SII for vaccine uptake increased from 2019/2020 to 2020/21, then again from 2020/21 to 2021/22. For rotavirus vaccination, MMR1 at age 5 years, and MMR2 at age 5 years, point estimates for SII for vaccination uptake continued to increase between 2021/22 and 2022/23 (fig 2).
Cumulative susceptibility
Over the study period, the estimated cumulative number of 5 year olds who were susceptible to measles infection increased 15-fold in the least deprived group, from 1364 to 20 958, and increased 20-fold in the most deprived group, from 1296 to 25 345 (fig 3). The estimated cumulative number of 1 year olds who were susceptible to rotavirus disease over the study period increased 14-fold in the least deprived group, from 2292 to 32 981, and increased 16-fold in the most deprived group, from 2815 to 45 201 (fig 3).
Regional analyses
Analyses undertaken according to NHS England health regions showed that London had the lowest overall uptake of vaccination, followed by the Midlands and North West (see supplementary figure S3 and table S5). In SII analyses, London and the North West region consistently had the largest SII across all indicators, whereas the South East and South West regions consistently had the smallest SII across all indicators. In 2022/23, the SII for MMR2 by age 5 years was highest in London (−19.5%, −21.5% to −17.5%) and lowest in the South East region (−6.8%, −8.1% to −5.6%). In 2022/23, the SII for MMR1 by age 5 years in London was −9.0% (−10.3% to −7.7%) compared with −2.8% (−3.6% to −2.0%) in the South East region, and for rotavirus vaccination the SII was highest in the North West region (−13.8%, −15.3% to −12.4%) and lowest in the South East region (−5.2%, −6.5% to −3.9%) (see supplementary table S5).
Robustness tests
Supplementary table S6 shows the outputs from the sensitivity analysis excluding Surrey and Bradford from the rotavirus vaccination analysis. Excluding these local authorities owing to data derived anomalies had a minor effect on the point estimates from the regression analyses, did not change the direction of effects, and gave confidence in the robustness of the final analysis undertaken.
Discussion
This study found noticeable socioeconomic inequalities in vaccine uptake in children at general practice level throughout England, with uptake rates of five childhood vaccinations in children living in areas of higher deprivation consistently lower up to age 5 years than in those living in areas of lower deprivation. We found increasing inequality in vaccine uptake between 2019 and 2023. The greatest absolute inequality was observed for MMR2 vaccination, with inequalities in vaccination uptake rates between practices serving the lowest and highest levels of deprivation increasing from −9.6% to −13.4% over the study period. In analyses by English regions, we found greater inequality in vaccine uptake in London and the North of England region compared with southern regions. For all childhood vaccinations studied, the uptake rates in England did not exceed the WHO recommended threshold of 95% in the more deprived populations.
Findings in context
Vaccine uptake in children has decreased globally since the covid-19 pandemic,41with an estimated 20.5 million children worldwide in 2022 either unvaccinated or under-vaccinated.42Confidence in childhood vaccinations is at a low level across European and Central Asian regions.43Childhood vaccination rates have shown some recovery after the pandemic,44although as evidenced in our study, uptake remains lower than levels before the pandemic.
Few studies have assessed trends in inequalities of vaccine uptake in children over this period35; our study observed a widening of inequalities in England. This is a critical public health concern, as more deprived areas often have higher population density, more frequent overcrowding at home, poorer baseline health, and higher rates of comorbidity.45These factors increase the risks of infectious disease transmission, outbreaks, and poorer health outcomes.46Therefore, the effects of falling vaccine uptake will not be felt equally across populations. Furthermore, as evidenced post-Wakefield, broken trust surrounding vaccination and healthcare is harder to rebuild in more deprived population groups, and this lack of trust risks amplifying existing health inequalities.23Beyond these general patterns, there are specific implications for falling uptake of each of the vaccinations studied here, and the diseases they protect against.
MMR vaccination
Owing to the highly infectious nature of measles, WHO recommends 95% vaccination coverage for herd immunity using two doses of MMR.2247This threshold has historically not been reached in England,4with our study showing that this is now unmet by >15% of children in the most deprived populations. The number of people with measles has begun to increase in the UK and Europe, with modelling predicting the potential for tens of thousands of affected people in London alone.22Furthermore, in our study, historical trends show higher measles rates in more deprived populations (see supplementary figure S1). In early 2024, measles outbreaks occurred in large urban areas in England. In Birmingham, 216 confirmed and 103 probable diagnoses were detected between October 2023 and 18 January 2024, and UKHSA declared a national incident.48
PCV booster
Our study found a reduction in uptake of the PCV booster, which was most pronounced in more deprived populations. This is in the context of the schedule switch in 2019, from two primary doses and a booster dose to one primary dose and a booster dose (see supplementary table S1). The booster dose is therefore even more critical for protection in the new schedule. Widening inequality is concerning for disease risk in disadvantaged adults who require herd protection and where the risk of serious illness and invasive pneumococcal disease is disproportionately higher.2149Furthermore, the risk of pneumonia is also disproportionately higher for children living in areas of increased deprivation.50
Rotavirus vaccination
Our study found the largest decrease in uptake of rotavirus vaccination since its introduction to the UK schedule in 2013. Before introduction of the vaccine, rotavirus was the leading cause of acute gastroenteritis in children, with hospital admissions highest in more deprived populations.1851Rotavirus vaccination reduces these admissions with high vaccine effectiveness3752and reduces inequalities in disease burden.18This is despite lower uptake of rotavirus vaccine in more deprived groups, as also shown in our study. Eligibility for rotavirus vaccination ends at 6 months of age, with no opportunity for catch-up.53This makes the growing inequity in uptake, and disproportionate cumulative increase of susceptible children living in higher deprivation, particularly concerning.
DTaP/IPV/Hib/HepB (six-in-one) vaccination
Increasing inequalities in uptake rates of the six-in-one vaccine present concerns for several vaccine preventable diseases (diphtheria, tetanus, pertussis (whooping cough), polio,Haemophilus influenzaetype b, and hepatitis B). Recent detection of variant poliovirus on environmental surveillance in England increases the risk of infection, outbreaks, and clinical poliomyelitis.54Widespread increases in pertussis (whooping cough) have also occurred in England in 2023 and 2024, which could be attributed to falling vaccine uptake but also to waning immunity in older children and adults, compounded by reduced exposure to natural infections during the covid-19 pandemic.55
Strengths and limitations of this study
This study examined uptake of childhood vaccinations across England, utilising temporal, small area level data. As such, it provides a responsive and detailed picture and allows for timely decision making about interventions. These data are publicly available and are released quarterly, so analyses can be repeated and tailored for local needs. Our explanatory variable and outcome data are near complete and are captured at regular short term intervals using validated methods for England.
Our analyses are predominately descriptive and rely on aggregated routine health data. We were unable to investigate the mechanisms and processes that could explain why socioeconomic inequalities in childhood vaccine uptake have increased. We also were unable to account for all potential confounders or other explanatory factors. Social deprivation is only one factor that influences unequal vaccine uptake—others include ethnicity, disability, sex, religion, geography, and age. In addition, evidence suggests that migrants, travellers, prisoners, and being a looked after child all influence vaccine inequalities not just for overall coverage but also for timing of vaccines and completion of vaccination schedules.8Although an examination of the associations between vaccine uptake and specific factors such as housing or education would be valuable at an individual level, our use of the index of multiple deprivation score allowed us to instrument deprivation at an area level, using a well established and robust measure within which the scores for the component domains are highly correlated.56
Data limitations also exist within this study, including incorrectly recorded uptake rates for rotavirus vaccination in some areas. These data anomalies were examined in a sensitivity analysis and were not deemed to substantially affect the findings. Although these data capture whether children have received their eligible vaccine doses, the specific date of receipt is unknown. These data do not include children who are not registered at general practices, or capture vaccinations delivered in private settings. Given that populations less likely to be registered with a practice are more likely to have poorer health outcomes, we may have underestimated the health inequalities in England for the period analysed in this study. Catch-up of vaccinations outside of the routine paediatric immunisations is also not captured in these data. Furthermore, without access to individual level records for the whole population, it is not possible to use these data to accurately assess susceptibility in paediatric and adult populations.
Implications for policy and practice
Giving every child the best start in life is recognised as critical to narrowing health inequalities, and childhood vaccination is potentially a powerful “levelling-up” intervention.57NHS England has a legal duty to offer immunisation to groups that are hard to reach, and a reduction in health inequalities is a key objective of the core service specification for the national immunisation programme drawn up between the NHS and public health bodies.8The broad principle of health equity action requires intervention on the upstream social drivers of ill health and inequalities.58The Marmot review introduced the concept of “proportionate universalism,” suggesting that health equity actions must be universal and not targeted, but with a scale and intensity that is proportionate to the level of disadvantage.57
Systems strengthening through rapid investment and effective partnerships between stakeholders and institutions, including Integrated Care Systems, Public Health Departments, the UKHSA, NHS England, and academic institutions, is required.59Promising approaches likely involve strengthening and investment in supplementary outreach services at a local level, designed to meet the specific needs of underserved populations. These services should be integrated in a network incorporating local commissioners; public health departments; voluntary, community, and social enterprise settings; the Health and Wellbeing Alliance, and primary care and early years settings, and they should draw on insights from services and community leaders while utilising neighbourhood level data.60In addition, knowledge exchange between the public sector and industry should allow the adoption of innovative technologies to improve immunisation delivery in both routine preventive care and outbreak response.
Partnerships will only be able to act efficiently when real time data on immunisation status and susceptibility of local populations are routinely available to local public health teams. Area level secure data environments aimed at mobilising data for public health analytics were used to evaluate pandemic responses and vaccination uptake.61However, these systems are not mature across England for any imminent outbreak or prevention response. Robust local analytics should help focus interventions on improving vaccination uptake at the time of children’s eligibility within the routine schedule. Catch-up interventions are costly, challenging, and not available for all vaccinations, meaning that missed vaccination creates increasing pools of susceptible children as deprivation increases. Therefore, we should also be concerned about the build-up of susceptible post-school teenagers and young adults. The current increases in whooping cough and measles in England are likely to herald more widespread outbreaks.
Conclusion
Protecting children from vaccine preventable diseases is a fundamental public health priority, but systems in England are currently failing to deliver the uptake necessary to adequately protect the population, and inequalities are noticeably increasing. Overall rates of vaccine uptake in England for five key childhood vaccinations declined between 2019 and 2023, with more rapid declines observed with increasing levels of deprivation. Vaccine uptake was below the recommended 95% WHO threshold throughout the study period for all vaccinations. These findings strongly support the urgent need for effective strengthening of vaccination systems, proportionate to levels of need, in addition to interventions and catch-up campaigns in underserved populations.
Uptake rates of childhood vaccinations in England have been steadily declining since 2013/14
Socioeconomic deprivation is associated with lower rates of vaccination uptake in children
This analysis found decreasing coverage and increasing inequality in five key childhood vaccinations in England from 2019 to 2023
The most pronounced inequality over time was seen for the MMR2 vaccination (measles, mumps, and rubella), increasing from −9.6% to −13.4% over the study period
Where vaccination catch-up is not implemented, an increasing cumulative number of children are more susceptible to infection as deprivation increases
","Objective: To quantify changes in inequalities in uptake of childhood vaccination during a period of steadily declining overall childhood vaccination rates in England.
Design: Longitudinal study.
Setting: General practice data for five vaccines administered to children (first and second doses of the measles, mumps, and rubella vaccine (MMR1 and MMR2, respectively), rotavirus vaccine, pneumococcal conjugate vaccine (PCV) booster, and six-in-one (DTaP/IPV/Hib/HepB) vaccine covering diphtheria, tetanus, pertussis, polio,Haemophilus influenzaetype b, and hepatitis B) from the Cover of Vaccination Uptake Evaluated Rapidly dataset in England.
Participants: Children aged <5 years eligible for vaccinations between April 2019 and March 2023 registered at primary care practices in England. 2 386 317 (2 309 674 for rotavirus vaccine) children included in the study were eligible at age 1 year, 2 456 020 at 2 years, and 2 689 304 at 5 years.
Main outcome measures: Changes in quarterly vaccine uptake over time and compared by deprivation level. Regression analyses were used to quantify the change in inequalities in vaccine uptake over time—expressed as changes in the slope index of inequality (SII). Cumulative susceptibility to measles and rotavirus disease at age 5 years was estimated. Analyses were repeated at regional level.
Results: The absolute inequality in vaccine uptake at baseline (2019-20) was largest for MMR2 in children at age 5 years (SII −9.6%, 95% confidence interval (CI) −10.2% to −9.0%). For all vaccinations studied, the SII for uptake increased over the study period: from −5.1% to −7.7% for the six-in-one vaccine, −7.4% to −10.2% for rotavirus, −7.9% to −9.7% for PCV booster, −8.0% to −10.0% for MMR1 at age 2 years, −3.1% to −5.6% for MMR1 at age 5 years, and −9.6% to −13.4% for MMR2 at age 5 years. The number of children susceptible to measles by the end of the study period increased 15-fold in the least deprived group (from 1364 to 20 958) and 20-fold in the most deprived group (from 1296 to 25 345). For rotavirus, a 14-fold increase was observed in the least deprived group (from 2292 to 32 981) and a 16-fold increase in the most deprived group (from 2815 to 45 201). Regional analysis showed greatest inequalities in uptake in London and the northern regions.
Conclusion: The findings of this study suggest that inequalities in childhood vaccination are increasing in England, as uptake rates for five key childhood vaccinations decreased between 2019 and 2023, below the World Health Organization’s recommended 95% uptake target, and with noticeable regional differences. Urgent action is needed to strengthen systems for childhood vaccination, with a key focus on reducing inequalities.
"
"Mortality and risk of diabetes, liver disease, and heart disease in individuals with haemochromatosis HFE C282Y homozygosity and normal concentrations of iron, transferrin saturation, or ferritin","Introduction
Iron homoeostasis is controlled by the master hormone hepcidin, a peptide synthesised in the liver and regulated through the homoeostatic iron regulator (HFE) protein. Hereditary haemochromatosis is caused by germline genetic variants, most commonly in theHFEgene, causing increased intestinal iron uptake that can lead to progressive iron accumulation in the body.123Approximately 80-90% of patients with a diagnosis of hereditary haemochromatosis in northern Europe are homozygous for the C282Y (rs1800562) variant in theHFEgene, whereas less than 10% are compound heterozygous for the H63D (rs1799945) and C282Y variants.145The C282Y variant is common in people of northern European descent, with homozygosity reported in 0.25-1% of the general population.5However, estimates of penetrance vary markedly between studies, with clinically apparent haemochromatosis reported in between <1% and 60% of C282Y homozygotes.3678
Previous studies have found increased risk of liver cirrhosis, liver cancer, and diabetes mellitus in people with C282Y homozygosity, but studies on risk of heart disease have shown conflicting results.91011Most clinical guidelines on hereditary haemochromatosis are based on the assumption that the increased risk of liver disease and diabetes is mechanistically caused by accumulation of iron in hepatocytes and pancreatic islets, reflected in high levels of transferrin saturation and ferritin. Hence, guidelines from several countries, including Denmark and the UK, recommend genetic testing for the twoHFEvariants C282Y and H63D only in individuals with a high transferrin saturation and a high ferritin concentration (except for genotyping close relatives of affected patients).1351213On the basis of observational studies, therapeutic phlebotomy has been the cornerstone of treatment for hereditary haemochromatosis for many years. The effect of therapeutic phlebotomy in individual patients is typically assessed by measuring whether transferrin saturation and ferritin concentrations decrease sufficiently. However, randomised trials on risk of complications and death in people with hereditary haemochromatosis treated with phlebotomy have not been conducted, and whether individuals with hereditary haemochromatosis with normal levels of plasma iron, transferrin saturation, and/or ferritin are at increased risk of diabetes, liver disease, heart disease, and/or death is unknown.
Therefore, we tested the hypothesis that C282Y homozygotes have increased risk of diabetes, liver disease, and heart disease even when they have normal levels of plasma iron, transferrin saturation, or ferritin. Furthermore, we tested the hypothesis that C282Y homozygotes with diabetes, liver disease, or heart disease have higher mortality than doHFEnon-carriers with diabetes, liver disease, or heart disease. For this purpose, we studied 132 542 individuals from three Danish general population studies. All individuals were genotyped for theHFEvariants C282Y and H63D and were followed for up to 27 years after study enrolment.
Methods
We studied 132 542 consecutive individuals from three Danish cohort studies of the general population: the Copenhagen City Heart Study (9174 individuals examined 1991-94),14the Copenhagen General Population Study (103 276 individuals examined 2003-14),15and the Danish General Suburban Population Study (20 092 individuals examined 2010-13).16Among those invited, 61% participated in the Copenhagen City Heart Study, 43% in the Copenhagen General Population Study, and 43% in the Danish General Suburban Population Study. On the day of enrolment, all individuals underwent a physical examination, had blood samples drawn, and completed a questionnaire on health and lifestyle. Age at study enrolment was 20-100 years. Study procedures were similar for the three cohort studies. No individuals overlapped between the three cohort studies, and no individuals were lost to follow-up. Danish ethics committees approved the studies, and each participant gave written, informed consent. See the supplementary methods for details about study procedures.
HFEgenotype
All individuals were genotyped for the twoHFEvariants C282Y and H63D by using DNA extracted from peripheral blood leukocytes.1718Details on genotyping are described in the supplementary methods. Individuals were not informed about their C282Y and H63D genotypes as the ethics committee approvals authorised the disclosure of such genetic findings only when a clear, substantial, and well documented health consequence existed; thus, participating in the study did not influence whether individuals were treated for hereditary haemochromatosis.
Covariates
Information on age and sex came from the Danish Civil Registration System. For the multivariable adjusted supplementary analyses on risk of diabetes and risk of death from any cause, we retrieved information on other covariates reported to be associated with diabetes or all cause mortality on the basis of a priori literature reviews conducted before we did the analyses (see supplementary methods).
Plasma iron, transferrin saturation, and ferritin
We measured plasma iron, transferrin saturation, and ferritin in blood samples from 130 947, 130 902, and 33 428 individuals, respectively. Blood samples used for measuring iron, transferrin saturation, and ferritin were drawn on the day of study enrolment, except for 4791 individuals from the Copenhagen City Heart Study (examined 1991-94) who had ferritin measured on blood samples drawn at an earlier examination in 1981-83. See the supplementary methods for information on sample storage, laboratory assays, calibration, and quality control.
Disease endpoints and vital status
The Danish National Patient Register contains information on all inpatient hospital admissions in Denmark since 1977 and all emergency department and outpatient visits since 1994.19Using this register, we retrieved information on inpatient hospital admissions from 1 January 1977 until 31 December 2018 and emergency department and outpatient visits from 1 January 1994 until 31 December 2018. Diagnoses were classified according to ICD-8 (international classification of diseases, 8th revision) until 31 December 1993, when it was superseded by ICD-10. ICD codes for diabetes, liver disease, heart disease, and subcategories of these conditions are shown in supplementary table S1. In previous studies, when the validity of diabetes diagnoses in the Danish National Patient Register was ascertained by comparison with detailed hospital chart review conducted by a physician in 50 individuals registered as having diabetes in the Danish National Patient Register, the hospital charts documented a diagnosis of diabetes in 48 (96%).20Corresponding validity numbers were 100/100 (100%) individuals for liver disease and 99/100 (99%) for heart disease.20
According to Danish national guidelines, patients with hereditary haemochromatosis should be treated exclusively in public hospital departments.2122To do stratified analyses according to whether or not C282Y homozygotes had a diagnosis of haemochromatosis, we therefore retrieved information on hospital contacts with haemochromatosis from 1 January 1977 until 31 December 2018, from the Danish National Patient Register (ICD-8 code for haemochromatosis: 27329; ICD-10: E831A). For the analysis on risk of non-alcoholic fatty liver disease, we used the fatty liver index based on body mass index, waist circumference, plasma triglycerides, and plasma γ-glutamyl transferase concentrations at study enrolment.2324We defined fatty liver disease cross sectionally as having a fatty liver index ≥60 at study enrolment in individuals without heavy alcohol intake (with heavy defined as >168 g/week for men and >84 g/week for women according to recommendations by Danish health authorities). Information about vital status and emigration until 31 December 2018 came from the national Danish Civil Registration System.25
Statistical analysis
We used Stata 18.0 for all statistical analyses except for the sensitivity analyses using Cox regression with Firth’s penalised maximum likelihood bias reduction method,2627for which we used R version 4.3.2. All statistical tests were two sided. Relative risks of diabetes, liver disease, heart disease, and death were modelled by Cox proportional hazards regression, using left truncated age as the timescale, thus adjusting for age. To account for potential differences between the three study cohorts, all main analyses on relative risk used shared frailty Cox models,2829with frailties shared for individuals within each of the three study cohorts. However, results were similar to those presented if we adjusted for study cohort as a categorical variable in the Cox models instead of using shared frailty Cox models (data not shown). When estimating absolute five year risk of diabetes, liver disease, heart disease, and death, we modelled each endpoint separately using Poisson regression adjusted for categories of age and sex. As genotype is constant during life and not affected by lifestyle, the primary analyses were adjusted only for age and sex to avoid adjusting for potential mediators that may be part of the causal pathway betweenHFEgenotype and risk of disease. However, we also did supplementary analyses using multivariable adjusted models which gave similar results, as described in the results section and in the supplementary methods.
Follow-up for the analyses on relative risk of diabetes, liver disease, and heart disease began at age 20 years or 1 January 1977, whichever came last, and follow-up ended on date of first hospital contact with the disease being studied in the specific analysis (diabetes, liver disease, or heart disease), emigration (384 individuals), death from any cause, or 31 December 2018, whichever came first (supplementary figure S1). For the analyses on absolute five year risk of diabetes, liver disease, and heart disease, follow-up began at study enrolment and ended on date of first hospital contact with the disease being studied in the specific analysis, emigration, death from any cause, or 31 December 2018, whichever came first. For the analyses on relative and absolute risk of death in all individuals irrespective of disease, follow-up began at the day of study enrolment and ended on date of death from any cause, emigration, or 31 December 2018, whichever came first. For the analyses on relative and absolute risk of death from any cause in individuals with diabetes, liver disease, or heart disease, follow-up began at the day of study enrolment or the day of first hospital contact with the disease being studied in the specific analysis, whichever came last, and follow-up ended on date of death from any cause, emigration, or 31 December 2018, whichever came first. We observed no major violations of the proportional hazards assumption when we tested this by first visually plotting –ln(–ln(survival)) against ln(analysis time) and further testing on the basis of Schoenfeld residuals if non-parallel curves were suspected from the visual inspection. Further information on statistical methods is provided in the supplementary methods.
Patient and public involvement
Patients were not directly involved in the planning, design, or conduct of this study. Although patient and public involvement is increasingly recognised as important, the general population studies that formed the foundation of this research were started before formal patient and public involvement practices were common in Denmark. The specific focus of this study was inspired by encounters with patients presenting with classical hereditary haemochromatosis, particularly those with complications of diabetes and other associated health problems. However, no resources were allocated to formal patient involvement before the study’s analysis plan was finalised. During the manuscript review process, one of the reviewers was a patient with hereditary haemochromatosis, and their insights significantly enhanced the quality and relevance of the final manuscript. Although direct patient and public involvement was not formally integrated into the study, public representation was ensured during the ethical review process. The Danish ethical committees, which reviewed and approved the general population studies, include lay members, ensuring that public perspectives were considered during ethical oversight. Furthermore, informal conversations with patients provided additional context and helped to refine our understanding of the condition, even though these discussions were not part of the formal study protocol.
Results
Baseline characteristics according toHFEgenotype for the 132 542 individuals from the general population are presented intable 1and described in the supplementary results. As expected, C282Y homozygotes (C282Y/C282Y) had higher levels of iron, transferrin saturation and ferritin than did individuals who were non-carriers for both C282Y and H63D (non-carrier/non-carrier).
Relative risk of diabetes
For the analyses on relative risk of diabetes, liver disease, and heart disease, follow-up began at age 20 years or 1 January 1977 (whichever came last), giving a median follow-up of 41 (range 0-42) years. During follow-up, 7702 of 132 542 individuals had a hospital contact with diabetes. Compared with non-carriers (non-carrier/non-carrier), C282Y homozygotes (C282Y/C282Y) had a higher overall risk of diabetes (age and sex adjusted hazard ratio 1.72, 95% confidence interval (CI) 1.24 to 2.39) (fig 1) and a higher risk of diabetes with complications (2.03, 1.22 to 3.38) (fig 2). Results were similar when we additionally adjusted for potential risk factors for diabetes (body mass index, alcohol intake, and C reactive protein concentration as a marker of inflammation), with C282Y homozygotes having a multivariable adjusted hazard ratio for diabetes of 1.53 (95% CI 1.10 to 2.12) (supplementary figure S2). Age at diagnosis of diabetes was similar in C282Y homozygotes and non-carriers (mean age at diagnosis 64.4v64.5 years) (fig 1).
We also analysed risk of diabetes cross sectionally, whereby instead of defining diabetes by hospital diagnoses, we defined it simply as having a non-fasting blood glucose >11 mmol/L (>198 mg/dL) at study enrolment. When we used this alternative definition, risk of diabetes in C282Y homozygotes was also increased (odds ratio for diabetes 2.57 (95% CI 1.21 to 5.47) for C282Y homozygotes compared with non-carriers) (supplementary figure S3). For H63D heterozygotes (H63D/non-carrier), H63D homozygotes (H63D/H63D), C282Y heterozygotes (C282Y/non-carrier), and compound heterozygotes (C282Y/H63D), risk of diabetes was not increased regardless of how diabetes was defined (fig 1; supplementary figure S3).
To examine whether current clinical guidelines on testing for haemochromatosis adequately identify C282Y homozygotes at increased risk of diabetes by recommending testing only in individuals with high transferrin saturation and/or high ferritin, we examined risk of diabetes stratified by levels of iron, transferrin saturation, and ferritin. For this stratified analysis, we excluded individuals (n=11 C282Y homozygotes) with a diagnosis of haemochromatosis at study enrolment to exclude that levels of iron, transferrin saturation, or ferritin in the blood samples obtained at study enrolment were affected by previous therapeutic phlebotomy. Importantly, we found an increased risk of diabetes even in C282Y homozygotes with normal levels of transferrin saturation (hazard ratio for diabetes 2.00 (95% CI 1.04 to 3.84) when comparing C282Y homozygotes with normal transferrin saturation and non-carriers with normal transferrin saturation), normal concentrations of ferritin (3.76, 1.41 to 10.05), or normal levels of both ferritin and transferrin saturation (6.49, 2.09 to 20.18) (fig 3). For C282Y homozygotes with normal iron, risk of diabetes was less pronounced (hazard ratio 1.38, 0.91 to 2.09). Results were similar to those described above if we excluded not only individuals with a diagnosis of haemochromatosis before study enrolment but instead those (n=78 C282Y homozygotes) with a diagnosis of haemochromatosis at any time before or after study enrolment (data not shown). As diabetes could be diagnosed either before or after blood samples were obtained at study enrolment,figure 4illustrates when diabetes was diagnosed and the median time between study enrolment and diagnosis of diabetes in C282Y homozygotes and non-carriers with normal levels of iron, transferrin saturation, and ferritin. When we restricted the analysis to C282Y homozygotes, the median time from diabetes to study enrolment for C282Y homozygotes with a diagnosis of diabetes before study enrolment was 4.9 years and the median time from study enrolment to diabetes for C282Y homozygotes with a diagnosis of diabetes after study enrolment was 4.2 years.
When we restricted the analysis to study only risk of diabetes after study enrolment and exclude individuals with a diagnosis of diabetes before study enrolment, confidence intervals became wider owing to lower statistical power, but risk of diabetes was still increased in C282Y homozygotes with normal concentrations of ferritin (4.44, 95% CI 1.43 to 13.82) or normal levels of both ferritin and transferrin saturation (8.03, 2.00 to 32.28), whereas the overall group of C282Y homozygotes with normal transferrin saturation had a hazard ratio for diabetes of 1.04 (95% CI 0.34 to 3.23) (supplementary figure S4).
To assess whether the increased risk of diabetes was also seen in C282Y homozygotes without other comorbidities potentially caused by haemochromatosis, we did stratified analyses according to presence versus absence of liver disease or heart disease. The increased risk of diabetes was found in C282Y homozygotes without liver disease or heart disease at study enrolment (hazard ratio for diabetes 1.56, 95% CI 1.07 to 2.28) (supplementary figure S5), and results were similar for C282Y homozygotes who did not have a diagnosis of liver disease or heart disease at any time before or after study enrolment (supplementary figures S6 and S7). Compared with a reference group of non-carriers with body mass index 18.5-24.9, risk of diabetes was increased in non-carriers with high body mass index (hazard ratio 2.15 (95% CI 1.98 to 2.33) for non-carriers with body mass index 25-29.9; hazard ratio 5.76 (5.33 to 6.24) for non-carriers with body mass index ≥30) and even higher in C282Y homozygotes with high body mass index (hazard ratio 4.17 (95% CI 2.58 to 6.74) for C282Y homozygotes with body mass index 25-29.9; hazard ratio 7.34 (4.24 to 12.69) for C282Y homozygotes with body mass index ≥30) (supplementary figure S8). However, high body mass index and C282Y homozygosity seemed to be independent risk factors for diabetes, as we observed no interaction between body mass index andHFEgenotype on risk of diabetes (P for interaction=0.12). Results were similar when we restricted the analysis on risk of diabetes according to body mass index to study only risk of diabetes after study enrolment when measurements of height and weight were obtained for calculation of body mass index (supplementary figure S8).
Among the 422 C282Y homozygotes, 78 had a diagnosis of haemochromatosis made at any time before or after study enrolment, of whom 11 had had a diagnosis of haemochromatosis before study enrolment. When we did stratified analyses according to whether or not C282Y homozygotes had a diagnosis of haemochromatosis, risk estimates for diabetes were similar for C282Y homozygotes with a clinical diagnosis of haemochromatosis at any time before or after study enrolment (hazard ratio 2.41 (95% CI 1.29 to 4.47) compared with non-carriers) and in C282Y homozygotes who never had a diagnosis of haemochromatosis (1.55, 1.05 to 2.27) (fig 5).
We observed no convincing difference in relative risk estimates for diabetes between C282Y homozygotes aged 20-65 years and those aged ≥66 years (supplementary figure S9). Relative risk estimates were similar for men and women when we stratified the analysis on risk of diabetes according to sex by comparing C282Y homozygous men with non-carrier men and C282Y homozygous women with non-carrier women (supplementary figure S10).
As plasma iron and transferrin saturation can change over time,2we examined levels of iron and transferrin saturation in 89 C282Y homozygotes who had plasma iron and transferrin saturation measured first in a blood sample obtained at the day of study enrolment and again in a second blood sample obtained for repeat measurements of plasma iron and transferrin saturation at a median of 10 years later. Of the 89 C282Y homozygotes with repeat samples, 76 had normal iron measured in the first blood sample; of these, 67 (88.2%) still had normal iron measured in the second blood sample. Of the 89 C282Y homozygotes, 24 had normal transferrin saturation measured in the first blood sample; of these, 15 (62.5%) still had normal transferrin saturation measured in the second blood sample (supplementary figure S11). No individuals had a repeat measurement of ferritin. Importantly, all analyses on risk of diabetes and other diseases were based on the first blood sample drawn at the day of study enrolment.
Relative risk of liver disease and heart disease
Compared with non-carriers, C282Y homozygotes had a higher risk of any liver disease (hazard ratio 2.22, 95% CI 1.40 to 3.54) and a higher risk of liver cirrhosis (3.42, 1.41 to 8.27) (fig 2). Likewise, C282Y homozygotes had higher risk of non-alcoholic fatty liver disease at study enrolment (odds ratio 1.63, 95% CI 1.22 to 2.19) when this was defined using the fatty liver index (supplementary figure S12). When stratified according to levels of iron, transferrin saturation, and ferritin, risk estimates for any liver disease were less pronounced for C282Y homozygotes with normal plasma iron (1.73, 95% CI 0.96 to 3.13) than for C282Y homozygotes with high plasma iron (5.18, 2.15 to 12.47; P for difference=0.04) (supplementary figure S13). Risk of liver disease was not convincingly increased in C282Y homozygotes with normal transferrin saturation (1.02, 95% CI 0.26 to 4.09) or ferritin (2.22, 0.31 to 15.85) (supplementary figure S13). Risk of any liver disease and risk of non-alcoholic fatty liver disease were not increased in H63D heterozygotes (H63D/non-carrier), H63D homozygotes (H63D/H63D), C282Y heterozygotes (C282Y/non-carrier), or compound heterozygotes (C282Y/H63D) (supplementary figures S12 and S14).
Risk of heart disease (1.01, 0.78 to 1.31) and risk of heart failure (0.84, 0.50 to 1.43) were not increased in C282Y homozygotes (fig 2). Likewise, risk of heart disease was not increased in C282Y homozygotes when we did stratified analyses according to levels of iron, transferrin saturation, and ferritin (supplementary figure S15).
Absolute risk of diabetes, liver disease, and heart disease
To guide clinical decisions for C282Y homozygous individuals, absolute risk estimates may be more useful than relative risk estimates. Therefore,table 2shows absolute five year risks of having a diagnosis of diabetes, liver disease, or heart disease according to sex and age categories. Depending on age group, the absolute five year risk of having a diagnosis of diabetes was 0.54-4.3% in C282Y homozygous women, 0.37-3.0% in non-carrier women, 0.86-6.8% in C282Y homozygous men, and 0.60-4.80% in non-carrier men. Corresponding absolute five year risks for liver disease were 0.77-2.3% in C282Y homozygous women, 0.28-0.83% in non-carrier women, 0.87-2.6% in C282Y homozygous men, and 0.32-0.94% in non-carrier men.
Death from any cause in C282Y homozygotes with diabetes, liver disease, or heart disease
During follow-up, 17 688 individuals died, of whom 48 were C282Y homozygotes. When we studied all individuals irrespective of disease, risk of death from any cause was not higher in C282Y homozygotes than in non-carriers (hazard ratio 1.16, 95% CI 0.87 to 1.54) (fig 6). Likewise, risk of death from any cause was similar for C282Y homozygotes without diabetes and non-carriers without diabetes. Compared with non-carriers without diabetes, risk of death was higher in non-carriers with diabetes (hazard ratio 2.26, 95% CI 2.15 to 2.38). Importantly, however, C282Y homozygotes with diabetes had an even higher relative risk of death from any cause compared with non-carriers without diabetes (4.39, 95% CI 2.69 to 7.18), meaning that relative risk of death was substantially increased for C282Y homozygotes with diabetes compared with non-carriers with diabetes (hazard ratio 1.94, 95% CI 1.19 to 3.18) (fig 6).
When we included only individuals with normal levels of iron or transferrin saturation at study enrolment and excluded those with a diagnosis of haemochromatosis at study enrolment, results on risk of death in C282Y homozygotes with diabetes were similar to those presented above. However, statistical power was very limited for the stratified analyses on risk of death according to ferritin, as only four C282Y homozygotes with diabetes had a normal ferritin concentration (of six C282Y homozygotes with diabetes who had ferritin measured) (supplementary figure S16).
Results on risk of death in C282Y homozygotes with diabetes were similar to those shown infigure 6when we excluded individuals with liver or heart disease at study enrolment (supplementary figure S17) and when we did multivariable adjusted analysis including several covariates reported to be associated with mortality (supplementary figure S18). Likewise, risk estimates for death were similar for C282Y homozygotes with diabetes with a diagnosis of haemochromatosis at any time before or after study enrolment (hazard ratio 3.31 (95% CI 1.07 to 10.26) compared with non-carriers without diabetes) and in C282Y homozygotes with diabetes with no diagnosis of haemochromatosis (4.75, 2.76 to 8.19) (fig 5). The number of deaths in C282Y homozygotes with diabetes (n=16) was too low to enable analyses on specific causes of death in C282Y homozygotes with diabetes.
Non-carriers with any liver disease had a higher risk of death from any cause (4.06, 95% CI 3.76 to 4.39) than did non-carriers without liver disease (fig 6). C282Y homozygotes with liver disease had a higher risk of death than did non-carriers without liver disease (6.71, 95% CI 3.35 to 13.43) but not a convincingly higher risk of death than non-carriers with liver disease (1.65, 0.82 to 3.32) (fig 6). Likewise, non-carriers with any heart disease had a higher risk of death from any cause (2.07, 95% CI 1.99 to 2.15) than did non-carriers without heart disease (fig 6). However, although C282Y homozygotes with heart disease had a higher risk of death than did non-carriers without heart disease (2.74, 95% CI 1.74 to 4.29), risk of death was not convincingly higher in C282Y homozygotes with heart disease (1.32, 0.84 to 2.08) than in non-carriers with heart disease (fig 6).
To estimate the proportion of all deaths that could hypothetically be prevented if interventions were able to eliminate the higher risk of death in individuals with diabetes, we calculated the population attributable fraction for diabetes on death from any cause, defined as the proportion of deaths from any cause that could theoretically be prevented at the population level if individuals with diabetes had the same mortality as those without diabetes. The population attributable fraction for diabetes on death from any cause was 7.8% (95% CI 7.2% to 8.4%) among non-carriers and 27.3% (12.4% to 39.7%) among C282Y homozygotes (fig 6), meaning that 27.3% of all deaths among C282Y homozygotes could hypothetically be prevented if C282Y homozygotes with diabetes had the same risk of death as C282Y homozygotes without diabetes.
The population attributable fraction for liver disease on death from any cause was 4.4% (95% CI 4.0% to 4.9%) among non-carriers and 14.4% (3.1% to 24.3%) among C282Y homozygotes, whereas the population attributable fraction for heart disease on death from any cause was 22.2% (21.1% to 23.4%) among non-carriers and 22.8% (4.3% to 37.8%) among C282Y homozygotes (fig 6). To examine the impact ofHFEgenotype on absolute risk of death from any cause,table 3andtable 4show five year absolute risk estimates for death from any cause according to age and sex for C282Y homozygotes and non-carriers, showing risk estimates for all individuals irrespective of disease (table 3)and specifically for individuals with and without diabetes, liver disease, or heart disease (table 4).
Sensitivity analyses
As some subgroups in the study had sparse number of events, we did additional sensitivity analyses in which all main analyses on relative risk (fig 1,fig 2,fig 3,fig 5, andfig 6) were instead done using Cox regression with Firth’s penalised maximum likelihood bias reduction method to reduce small sample bias,2627which gave results similar to those presented in the main figures (supplementary figures S19-S23). When examining heterogeneity, we found no indication of heterogeneity between the three general population cohorts for the analyses on risk of diabetes (P for heterogeneity=0.61), liver disease (P for heterogeneity=0.12), or heart disease (P for heterogeneity=0.41) (supplementary figure S24).
Discussion
In this prospective study of 132 542 individuals from the general population, we found thatHFEC282Y homozygotes had a higher risk of diabetes and liver disease than did non-carriers, whereas risk of heart disease was not increased. Importantly and a novel finding, C282Y homozygotes had increased risk of diabetes even when they had normal levels of transferrin saturation and/or ferritin in a single blood sample obtained at study enrolment. Furthermore, C282Y homozygotes with diabetes had higher mortality from any cause than did non-carriers with diabetes, and as many as 27.3% of all deaths among C282Y homozygotes were potentially attributable to diabetes.
Strengths and weaknesses of study
Among the strengths of our study is the large general population cohort with 132 542 consecutive genotyped individuals, most of whom also had iron, transferrin saturation, and/or ferritin measured. Our comprehensive data on hospital contacts and deaths from the nationwide Danish registries without any loss to follow-up is also a strength.
Our study is limited by not being able to ascertain with certainty whether it is diabetes itself or hypothetically some other unknown associated condition that causes the increased mortality in C282Y homozygotes with diabetes. Additionally, plasma iron, transferrin saturation, and ferritin were measured only at study enrolment, and we do not have data on the levels at the time of diagnosis of diabetes. Therefore, we cannot exclude the possibility that iron overload occurred during the interval between study enrolment and the onset of diabetes. However, this possibility does not change our finding that even C282Y homozygotes with normal transferrin saturation and/or ferritin concentrations in a single blood sample were at increased risk of diabetes, indicating that the strategy outlined in most current guidelines,3513recommending genotyping for C282Y only in individuals with increased levels of transferrin saturation and/or ferritin, may fail to detect some C282Y homozygotes at increased risk of diabetes. As only 16 C282Y homozygotes with diabetes died during follow-up, we do not have sufficient statistical power to analyse the risk of specific causes of death. However, when excluding individuals with liver or heart disease, we still found increased mortality in C282Y homozygotes with diabetes, meaning that concomitant liver or heart disease is unlikely to explain the increased mortality in C282Y homozygotes with diabetes.
Strengths and weaknesses in relation to other studies
Our novel finding that C282Y homozygotes had an increased risk of diabetes even when they had normal levels of transferrin saturation and/or ferritin is in contrast to the results of an Australian study comparing 102 C282Y homozygotes, who all had serum ferritin <1000 µg/L, and 291 non-carrier controls, as that study did not find an increased risk of diabetes.30The different results may be explained by greater statistical power in our study including 422 C282Y homozygotes of whom 36 (8.5%) had diabetes and by the very low prevalence of diabetes in the Australian study, as it found self-reported diabetes in only 1% of C282Y homozygotes. A study of 451 243 individuals from the UK Biobank, including 2890 C282Y homozygotes, found increased risk of diabetes among male but not female C282Y homozygotes.10By","Objectives: To test whether haemochromatosisHFEC282Y homozygotes have increased risk of diabetes, liver disease, and heart disease even when they have normal plasma iron, transferrin saturation, or ferritin concentrations and to test whether C282Y homozygotes with diabetes, liver disease, or heart disease have increased mortality compared with non-carriers with these diseases.
Design: Prospective cohort study.
Setting: Three Danish general population cohorts: the Copenhagen City Heart Study, the Copenhagen General Population Study, and the Danish General Suburban Population Study.
Participants: 132 542 individuals genotyped for theHFEC282Y and H63D variants, 422 of whom were C282Y homozygotes, followed prospectively for up to 27 years after study enrolment.
Main outcome measure: Hospital contacts and deaths, retrieved from national registers, covering all hospitals and deaths in Denmark.
Results: Comparing C282Y homozygotes with non-carriers, hazard ratios were 1.72 (95% confidence interval (CI) 1.24 to 2.39) for diabetes, 2.22 (1.40 to 3.54) for liver disease, and 1.01 (0.78 to 1.31) for heart disease. Depending on age group, the absolute five year risk of diabetes was 0.54-4.3% in C282Y homozygous women, 0.37-3.0% in non-carrier women, 0.86-6.8% in C282Y homozygous men, and 0.60-4.80% in non-carrier men. When studied according to levels of iron, transferrin saturation, and ferritin in a single blood sample obtained at study enrolment, risk of diabetes was increased in C282Y homozygotes with normal transferrin saturation (hazard ratio 2.00, 95% CI 1.04 to 3.84) or ferritin (3.76, 1.41 to 10.05) and in C282Y homozygotes with normal levels of both ferritin and transferrin saturation (6.49, 2.09 to 20.18). C282Y homozygotes with diabetes had a higher risk of death from any cause than did non-carriers with diabetes (hazard ratio 1.94, 95% CI 1.19 to 3.18), but mortality was not increased in C282Y homozygotes without diabetes. The percentage of all deaths among C282Y homozygotes that could theoretically be prevented if excess deaths in individuals with a specific disease were eliminated (the population attributable fraction) was 27.3% (95% CI 12.4% to 39.7%) for diabetes and 14.4% (3.1% to 24.3%) for liver disease. Risk of diabetes or liver disease was not increased in H63D heterozygotes, H63D homozygotes, C282Y heterozygotes, or C282Y/H63D compound heterozygotes.
Conclusions: Haemochromatosis C282Y homozygotes with normal transferrin saturation and/or ferritin, not recommended forHFEgenotyping according to most guidelines, had increased risk of diabetes. Furthermore, C282Y homozygotes with diabetes had higher mortality than non-carriers with diabetes, and 27.3% of all deaths among C282Y homozygotes were potentially attributable to diabetes. These results indicate that prioritising detection and treatment of diabetes in C282Y homozygotes may be relevant.
"
Chocolate intake and risk of type 2 diabetes,"Introduction
The global prevalence of type 2 diabetes (T2D) has increased noticeably over the past few decades, with an estimated 463 million people affected worldwide in 2019 and projected to increase to 700 million by 2045.1T2D is a multifactorial disease characterized by insulin resistance and impaired insulin secretion, which can lead to numerous severe complications such as cardiovascular disease, renal failure, and loss of vision.2A growing body of research has highlighted the importance of lifestyle factors, including healthy diets, in the prevention and management of T2D.34
Higher consumption of total dietary flavonoids, as well as specific flavonoid subclasses, has been associated with a decreased risk of T2D.56In randomized controlled trials, these flavonoids exerted antioxidant, anti-inflammatory, and vasodilatory effects that might confer cardiometabolic benefits and reduce the risk of T2D,789although data were not consistent.10Chocolate, derived from the beans of the cacao tree (Theobroma cacao), is among foods with the highest flavanol content and is a popular snack globally.111213However, the association between chocolate consumption and risk of T2D remains controversial owning to inconsistent findings in observational studies.1415Furthermore, most previous studies have primarily focused on total chocolate intake, without considering the potential differences in health effects between chocolate subtypes (ie, dark, milk, and white chocolate).1617These subtypes differ in cocoa content and proportions of other ingredients such as sugar and milk, which may influence the association with risk of T2D.18
Using data from three prospective cohort studies that repeatedly assess participants’ diet during longitudinal follow-up, we investigated the association between subtypes of chocolate intake and risk of T2D, as well as change in bodyweight, which is a strong predictor for risk of T2D.19
Methods
Study population
This study used data from three large prospective cohorts: the Nurses’ Health Study (NHS), initiated in 1976 and comprising 121 700 female registered nurses20; the Nurses’ Health Study II (NHSII), launched in 1989 and comprising 116 340 female nurses21; and the Health Professionals Follow-Up Study (HPFS), initiated in 1986 and comprising 51 529 male health professionals.22
For analyses on total chocolate consumption, the study baselines (baseline 1) were 1986 for NHS and HPFS and 1991 for NHSII, when diet was first comprehensively assessed using a validated semiquantitative food frequency questionnaire.232425For analyses on chocolate subtypes, the study baselines (baseline 2) were 2006 for NHS and HPFS and 2007 for NHSII, coinciding with the availability of data on chocolate subtypes in the food frequency questionnaire.
In the primary analyses, we excluded participants who had prevalent diabetes, cardiovascular disease (myocardial infarction, coronary artery bypass surgery, stroke), or cancer (except non-melanoma skin cancer) at baseline; had missing information on baseline age and chocolate intake, or an unusual total energy intake (ie, <600 or >3500 kcal/d for women and <800 or >4200 kcal/d for men); only completed the baseline food frequency questionnaire; or had an undetermined diagnosis date for T2D.
In a secondary analysis of change in bodyweight by levels of chocolate intake, we excluded participants with self-reported diabetes, cardiovascular disease, or cancer at baseline. Additional baseline exclusions were extreme total energy intake and missing data on chocolate intake or bodyweight. During follow-up, we censored individuals who reported cardiovascular disease, cancer, or a diagnosis of diabetes. We also censored individuals with missing data on chocolate intake or bodyweight during follow-up. The flowchart in supplementary figure S1 shows the selection process for participants.
The study protocol was approved by the Human Research Committee of Brigham and Women’s Hospital and the Harvard T.H. Chan School of Public Health. Participants provided informed consent by completing and returning study questionnaires.
Assessment of diet
Diet was assessed every four years using a semiquantitative food frequency questionnaire.23Participants reported their average frequency of consumption for a standard food portion size (eg, one chocolate bar/pack or 1 oz) in the past year, choosing from nine levels ranging from “never, or less than once per month” to “≥6 per day.” Questions about chocolate consumption were included from 1980 in NHS (study baseline was set at 1986 for total chocolate when the comprehensive food frequency questionnaire was first administered in NHS), 1991 in NHSII, and 1986 in HPFS. Questions about specific subtypes of chocolate consumption, specifically “How often do you consume milk chocolate (bar or pack),” and “How often do you consume dark chocolate,” were added to the food frequency questionnaire from 2006 for NHS and HPFS and from 2007 for NHSII, which were study baselines for the analyses by chocolate subtypes. Information on consumption of chocolate subtypes was collected in 2006 and 2010 for NHS; 2007, 2001, and 2015 for NHSII; and 2006, 2010, 2014, and 2018 for HPFS. Nutrient intakes were measured based on the US Department of Agriculture food composition database. We calculated the average daily intake of nutrients by multiplying the frequency of consumption of each food item that contains the nutrient by the nutrient content, and then summing across all food items. The intake level of flavan-3-ols was calculated by summing the individual components (catechin, epicatechin, gallocatechin, epigallocatechin, epicatechin 3-gallate, and epigallocatechin 3-gallate). The intake level of total flavonoids was further calculated by summing intakes of flavonols, flavones, flavanones, flavan-3-ols, total theaflavins, and polymers proanthocyanidins. Total energy intake was derived from the food frequency questionnaires using the same algorithm.
The validity and reproducibility of the food frequency questionnaires have been reported in detail previously.232425In validation studies among subgroups of participants in NHS and HPFS, the correlation coefficient between chocolate intake assessed by diet records over seven days and food frequency questionnaire was, respectively, 0.53 and 0.65 for dark chocolate and 0.42 and 0.47 for milk chocolate.26Additionally, the food frequency questionnaire shows reasonable consistency of food intake measurements over time, with mean intraclass correlation coefficients of 0.71 (dark: 0.71; milk: 0.59) in NHS and 0.72 (dark: 0.66; milk: 0.60) in HPFS.26
Assessment of covariates
In all three cohorts, information on participants’ personal and lifestyle factors, as well as diseases or medical conditions, was collected biennially through questionnaires. Variables assessed included race/ethnicity, bodyweight, waist circumference, smoking status, alcohol consumption, multivitamin use, menopausal status and postmenopausal hormone use (women only), oral contraceptive use (NHSII only), hypertension, hypercholesterolemia, and family history of diabetes. Specialty of health professionals was assessed in HPFS, and academic or professional degrees earned were assessed in NHS. Physical activity was measured using a validated questionnaire,272829which derived metabolic equivalent tasks in hours per week based on time spent on 10 recreational activities. Height was measured in the baseline questionnaire of these three cohorts, and time varying body mass index (BMI) was calculated as biennially updated bodyweight in kilograms divided by the square of height in meters. The Alternate Healthy Eating Index-2010 (AHEI) was calculated based on participants’ responses in the food frequency questionnaire to reflect overall diet quality, which emphasizes foods and nutrients, such as whole grains, nuts, red and processed meats, sugar sweetened beverages, and alcohol, that predict the risk of chronic disease.30For NHS and NHSII, we further calculated z scores of standardized neighborhood socioeconomic status using census tract variables from the Neighborhood Change Database. More details on computation are described elsewhere.31
Assessment of T2D
T2D was self-reported in the biennial follow-up questionnaires, and the diagnosis was further confirmed by study doctors through a supplementary questionnaire, which collected data on factors such as glucose concentration, glycated hemoglobin (HbA1c) concentration (after 2010), symptoms and treatments for T2D. People with confirmed T2D, which constituted the outcome of this analysis, were defined according to the National Diabetes Data Group criteria before 1998,32where at least one of the following items was reported: raised fasting plasma glucose level (≥7.8 mmol/L (140 mg/dL)) or plasma glucose (≥11.1 mmol/L (200 mg/dL)), and at least one T2D symptom (excessive thirst, polyuria, weight loss, hunger); no T2D symptom, but had at least two raised plasma glucose measurements on different occasions; or treatment with insulin or other hypoglycemic drug (insulin or oral hypoglycemic agent). For people with T2D diagnosed after 1998, the threshold for raised fasting plasma glucose level was lowered to 7.0 mmol/L (126 mg/dL) according to the criteria of the American Diabetes Association.33For people with a diagnosis after January 2010, we further added HbA1c≥6.5% to the diagnostic criteria.34The validity of the supplementary questionnaire was established in two previous studies in NHS and HPFS through blinded medical record reviews, with T2D diagnosis confirmed in 98% and 97% of people, respectively.3536
Ascertainment of weight change
Bodyweight was self-reported at baseline and in the biennial follow-up questionnaires, and it has been previously validated in these cohorts, with correlation coefficients of 0.97 in NHS and HPFS and 0.84 in NHSII.3738We calculated the outcome of interest, weight change every four years, by subtracting bodyweight at the start of each four year interval from bodyweight at the end of the four year interval.
Statistical analysis
We present participants’ characteristics at baseline 1 (1986 for NHS and HPFS, 1991 for NHSII) and baseline 2 (2006 for NHS and HPFS, 2007 for NHSII), stratified by consumption levels of total, dark, and milk chocolate and further categorized by study cohort. This breakdown allowed for a more nuanced understanding of the characteristics of participants, as individuals consuming similar levels of dark or milk chocolate might have distinct baseline characteristics across cohorts. To reflect the long term usual intake of total chocolate at baseline 2, we calculated cumulatively averaged total chocolate intake at or before baseline 2 (1980-2006 for NHS, 1991-2007 for NHSII, and 1986-2006 for HPFS). Person years were calculated as the duration between the return date of the baseline food frequency questionnaire to the date of T2D diagnosis, date of death, date of the last return of a valid follow-up questionnaire, or end of follow-up (2018 for NHS, 2021 for NHSII, and 2020 for HPFS), whichever came first. To reduce potential reverse causality that participants changed their diet after diagnosis of certain diseases, we stopped diet updates once participants reported myocardial infarction, stroke, coronary artery bypass graft, or cancer during follow-up. The rates for missing covariates were low in the cohorts. Any missing values for physical activity, alcohol intake, total energy intake, food and nutrients intake, BMI, AHEI, or z scores for socioeconomic status during follow-up were first replaced by valid values in the previous questionnaire cycle. For the remaining missing data of individual food items (<0.5%), we replaced the missing values with the median intake in the cohort. For missing values of smoking status, oral contraceptive use, menopausal status, and postmenopausal hormone use, we used the missing covariate indicator method.
In primary analyses, we used Cox proportional hazards models to estimate the hazard ratios and corresponding 95% confidence intervals (CIs) for the association between consumption levels of total, dark, and milk chocolate and risk of T2D.39To maximize statistical power and reduce within person variations, we cumulatively averaged total chocolate intake from baseline 1 and dark and milk chocolate intake from baseline 2. We categorized regular chocolate consumption into four groups of levels, including never or <1 serving/month (reference level), 1 serving/month to <1 serving/week, 1-4 servings/week, and ≥5 servings/week. The Cox models were stratified by age and calendar time. Fully adjusted models accounted for age, calendar year, ethnicity (white, African American, Asian, and other ethnicities), smoking status (never, former, current (cigarettes/day: 1-14, 15-24, or ≥25), or missing), alcohol intake (g/day: 0, 0.1-4.9, 5.0-14.9, and ≥15.0 in women, 0, 0.1-4.9, 5.0-14.9, 15.0-29.9, and ≥30.0 in men, or missing), family history of diabetes (yes/no), menopausal status and postmenopausal hormone use (premenopausal, postmenopausal (never, former, or current hormone use), or missing, for women), use of oral contraceptives (yes, no, NHSII only), physical activity (metabolic equivalent task hours (MET-h)/week: <3, 3.0-8.9, 9.0-17.9, 18.0-26.9, ≥27.0, or missing), baseline BMI (<21.0, 21.0-22.9, 23.0-24.9, 25.0-26.9, 27.0-29.9, 30.0-32.9, 33.0-34.9, ≥35.0, or missing), multivitamin use (yes/no), baseline hypertension, baseline hypercholesterolemia, total energy intake (continuous, kcal/day), and AHEI (fifths). For analyses of chocolate subtypes, we further adjusted total chocolate intake before baseline in the models (as cumulative averages for 1980-2002 (NHS), 1991-2003 (NHSII), 1986-2002 (HPFS)). Analyses were run separately in each cohort first, and then data were pooled to calculate overall study estimates, with cohort origin further adjusted in the model.
Sensitivity analyses were conducted using the pooled dataset: using baseline chocolate intake level instead of time varying intake; using most recently updated data on chocolate intake instead of cumulatively averaged intake; adjusting for baseline BMI as a continuous instead of categorical variable; adjusting for time varying BMI instead of baseline BMI to explore the mediation effect of BMI; adjusting for time varying waist circumference instead of baseline BMI; adjusting for consumption of specific food and beverages high in flavan-3-ols (eg, tea, red wine, apples, pears, blueberries, bananas, and peppers) instead of AHEI; adjusting for consumption of red and processed meat, fruit and vegetables, sugar sweetened beverages, and whole grains instead of AHEI; further adjusting for each individual intake of flavonoids; further adjusting for saturated fat intake; and further adjusting for added sugar intake. In the pooled dataset, we examined potential effect modification by age (<70 years, ≥70 years), sex (male, female), baseline BMI (<30, ≥30), physical activity (<median, ≥median), AHEI (<median, ≥median), and family history of diabetes (yes, no). Stratified analyses were conducted for these binary modifiers, and all models were multivariable adjusted. To address any residual confounding by BMI, we further adjusted baseline BMI as a continuous variable in the BMI stratified models. Interaction terms were created between the binary effect modifiers and the categorical chocolate consumption levels, and P values for interaction were examined using the likelihood ratio test.
Moreover, we explored the potential dose-response association between intake of total, dark, and milk chocolate and risk of T2D using restricted cubic spline analysis (SAS Macro %LGTPHCURV9)40with three knots, adjusting for the previously mentioned covariates. To reduce the effect of outliers, we combined and truncated data from the three cohorts at the first and 99th centiles. We performed a sensitivity analysis of spline regression not excluding outliers. Additionally, linear regressions were run between total, dark, and milk chocolate intake with selected nutrients, food, and drink at baseline, such as saturated fat, added sugar, flavan-3-ols, and sweets and desserts.
To further determine the association of chocolate intake with weight change, we conducted secondary analyses using multivariable generalized linear regression models with independent correlation matrix and robust variance to examine the associations of four year changes in chocolate intake with concomitant four year changes in bodyweight. For total chocolate analyses, eight four year cycles were available for HPFS (1986-2018) and six four year cycles for both NHS (1986-2010) and NHSII (1991-2015). For analyses of chocolate subtypes, three four year cycles were available for HPFS (2006-18), one four year cycle for NHS (2006-10), and two four year cycles for NHSII (2007-15). For each four year interval, we examined the associations between chocolate intake (increased or decreased versus no change) and weight change. We combined data from three cohorts for this analysis. In the multivariable adjusted model, we adjusted for age, ethnicity (white, African American, Asian, other), family history of diabetes (yes/no), baseline hypertension (yes/no), baseline hypercholesterolemia (yes/no), baseline total energy intake (continuous, kcal/day), postmenopausal hormone use (women only; premenopausal, never, former, current, or missing; time varying), and oral contraceptive use (NHSII only; yes, no; time varying), baseline and change in physical activity (MET-h/week), change in smoking status (stayed never smoker, stayed former smoker, stayed current smoker, changed from former to current smoker, changed from never to current smoker, and changed from current to former smoker), change in alcohol consumption (continuous, g/day), change in AHEI (fifths), and study origin (NHS, NHSII, HPFS). We also conducted stratified weight change analyses by baseline BMI (<25.0, 25.0-29.9, or ≥30.0) to explore any potential effect modifications. We adjusted for the same covariates as in the main analysis, and we further adjusted for baseline BMI in the continuous form. P values for interactions were calculated by general score tests requested by the type 3 command in the model statement (equivalent to the likelihood ratio test).
Analyses were conducted using SAS for Unix (version 9.4, SAS Institute) and RStudio (version 4.2.3, RStudio). We considered two sided P values <0.05 to be statistically significant.
Patient and public involvement
We did not have the infrastructure, resources, funding, or time to involve the public in study design, interpretation of results, or publication.
Results
Baseline characteristics
Supplementary figure S1 shows the selection of participants at specific baselines for primary analysis.Table 1,table 2, and supplementary table S1 present age standardized characteristics of participants at baselines 1 and 2, stratified by cohort, chocolate consumption levels, and types of chocolate (total, dark, and milk, respectively). A total of 192 208 participants (63 798 women in NHS, 88 383 women in NHSII, and 40 027 men in HPFS) were included in the analysis of total chocolate intake and 111 654 (39 400 women in NHS, 58 187 women in NHSII, and 14 067 men in HPFS) in the analyses of chocolate subtypes. The mean ages at baseline 1 were 52.3 years in NHS, 36.1 years in NHSII, and 53.1 years in HPFS. The mean ages at baseline 2 were 70.4 years in NHS, 52.3 years in NHSII, and 68.3 years in HPFS. Most participants were of non-Hispanic white ethnicity. Across all three cohorts, participants with higher levels of chocolate intake also had higher energy intakes, saturated fat, and added sugar. Higher consumption of dark chocolate was associated with higher AHEI and consumption of fruit and vegetables, epicatechin, and total flavonoids. Associations between milk chocolate consumption and these dietary variables were, however, inverse. The distribution of participants’ characteristics across total chocolate intake groups were similar to those for milk chocolate groups.
When examining the crude associations between chocolate consumption and selected nutrients, foods, and drinks at study baseline, milk chocolate consumption showed stronger positive associations with the consumption of less healthy food and nutrients, including saturated fat, added sugar, red and processed meat, and sweets and desserts. Dark chocolate consumption was positively associated with intakes of flavan-3-ols, particularly epicatechin. Overall, dark chocolate consumption was more positively associated with intakes of other flavan-3-ols-rich food and beverages, such as blueberries, tea, and red wine. The β coefficients between total chocolate consumption and these nutrients or food and beverages were similar to the coefficients observed for milk chocolate (data not shown).
Chocolate intake and T2D
In the primary analyses for total chocolate, 18 862 people with incident T2D were identified during 4 829 175 person years of follow-up. In the age and calendar time stratified Cox proportional hazards models, no significant associations were observed between total chocolate consumption and risk of T2D in the pooled dataset (table 3). After adjusting for lifestyle and dietary risk factors, we found that participants who consumed ≥5 servings/week of any chocolate showed a 10% (95% CI 2% to 17%; P trend=0.07) lower relative risk of T2D compared with those who never or rarely consumed chocolate. A marginally significant 1% (0% to 2%) reduction in risk of T2D was observed for each serving/week consumption of total chocolate.
For analyses by chocolate subtypes, 4771 people with incident T2D were identified during 1 270 348 person years of follow-up (table 4). After adjusting for confounding factors, participants who consumed ≥5 servings/week of dark chocolate had a significant 21% (5% to 34%) lower rate of T2D compared with those who never or rarely ate dark chocolate, and a significant linear trend across four groups was observed (P trend=0.006). A significant 3% (1% to 5%) reduction in risk of T2D was observed for each serving/week consumption of dark chocolate.
Associations between milk chocolate intake and risk of T2D were largely null. The multivariable adjusted hazard ratio for consumption of milk chocolate and risk of T2D was 0.94 (95% CI 0.79 to 1.12; P trend=0.75), comparing extreme consumption groups (high versus low).
Statistically significant heterogeneity was observed when examining results across the three cohorts. In the analysis of total chocolate, stronger associations were observed in NHSII, with consumption of ≥5 servings/week of total chocolate associated with a 16% (5% to 26%) lower risk of T2D compared with those who rarely consumed chocolate. In the analyses by chocolate subtypes, HPFS showed the most significant associations for dark chocolate consumption, with consumption of ≥5 servings/week of dark chocolate associated with a 51% (8% to 74%) lower risk of T2D compared with those who never or rarely consumed dark chocolate. Similar associations and trends were observed in NHSII, but the results were less significant. In NHS, neither total nor subtypes of chocolate consumption were statistically significantly associated with risk of T2D.
Restricted cubic spline regression revealed a linear dose-response association between dark chocolate intake and risk of T2D (P for linearity=0.003;fig 1). The association between total chocolate intake and risk of T2D appeared to be non-linear (P for non-linearity=0.008). The dose-response association between milk chocolate intake with risk of T2D remained essentially null. The results continued to be robust without excluding outliers (supplementary figure S2).
Subgroup analyses
In stratified analyses, significant effect modifications were observed for overall dietary quality and associations of dark chocolate with risk of T2D (P for interaction=0.007;table 5). Specifically, participants with a higher quality diet, measured by AHEI (≥median), showed stronger associations of dark chocolate consumption with risk of T2D (34% (95% CI 12% to 51%) lower risk of T2D comparing ≥5 serving/week to reference group) than individuals with a lower quality diet (hazard ratio 0.87, 95% CI 0.68 to 1.10, comparing extreme groups). Among those younger than 70 years, male participants, those with higher physical activity, and those with no family history of diabetes, the risk of T2D was lower when comparing extreme groups of dark chocolate intake, although P values for interactions were not statistically significant. Age, sex, BMI, physical activity, AHEI, and family history of diabetes did not seem to significantly modify the associations of total and milk chocolate intake with risk of T2D.
Sensitivity analyses
In sensitivity analyses comparing extreme groups, using baseline chocolate consumption level as a non-time varying intake in the model largely weakened the associations between dark chocolate and risk of T2D (hazard ratio 0.90 (95% CI 0.75 to 1.07)), whereas this association was slightly strengthened when modeling dark chocolate consumption based on the most recent update (0.75, 0.63 to 0.90)(table 6). Overall results remained unchanged after adjusting for baseline BMI as a continuous variable or time varying categorical BMI instead of baseline categorical BMI(table 6and supplementary table S2). Adjusting for waist circumference appeared to strengthen the association between dark chocolate and risk of T2D, and further attenuated the milk chocolate association towards the null (supplementary table S3). Adjusting for specific foods and drinks high in flavan-3-ols instead of overall dietary quality also did not appear to significantly change the findings. Adjusting for specific foods that predicted T2D slightly attenuated the results toward the null. Further adjustment for epicatechin levels attenuated the associations between dark chocolate and risk of T2D in the highest consumption group (hazard ratio 0.86 (95% CI 0.71 to 1.04)). Findings did not change after further adjustment for other individual flavonoids (supplementary table S4). Adjusting for added sugar slightly attenuated the associations between dark chocolate and risk of T2D (supplementary table S4). Further adjustment for saturated fat intake appeared to strengthen the association for intake of both total chocolate and chocolate subtypes and risk of T2D (total chocolate: 0.88 (0.81 to 0.96), dark chocolate: 0.77 (0.64 to 0.93), milk chocolate: 0.92 (0.78 to 1.10)). In addition, the associations persisted when adjusting for neighborhood socioeconomic status in two female cohorts (NHS and NHSII; supplementary table S5) and when adjusting for educational levels in NHS or specialties of professions in HPFS (supplementary table S6).
Chocolate intake and weight change
Table 7shows the results for bodyweight change analyses by chocolate subtypes. Compared with those who did not change their chocolate intake, increased intake of milk chocolate over four year periods was associated with 0.35 kg (95% CI 0.27 to 0.43) more four year weight gain over time. Increasing dark chocolate intake was not associated with weight change over time (−0.06 kg, −0.13 to 0.02). The association between changes in total chocolate intake with long term weight change was also positive (supplementary table S7). The results across the three cohorts were largely consistent, although the associations for total and milk chocolate were the strongest in NHSII compared with the other two cohorts (supplementary table S8).
In the stratified weight change analyses, we identified baseline BMI as a significant modifier for the associations between four year changes in dark or milk chocolate intake and four year weight change (table 8; P values for interaction <0.001). Increased intake of milk chocolate was associated with more increase in bodyweight in those with obesity at baseline (BMI ≥30) compared with those with a BMI within normal range (<25). Specifically, increased consumption of milk chocolate compared with no change in milk chocolate intake was associated with an average weight gain of 0.68 kg (0.42 to 0.95) more in people with obesity, whereas weight gain in those with normal BMI was only 0.33 kg (0.24 to 0.42) more over four year intervals. Increased dark chocolate intake was not associated with long term weight change in any BMI subgroups. The associations for total chocolate were also stronger in the group with a higher BMI (supplementary table S7). In the primary analyses for diabetes outcome, however, adjusting for time varying BMI did not explain the associations for any chocolate intake and risk of T2D.
Discussion
The findings of the present study showed that higher consumption of dark, but not milk, chocolate was associated with a lower risk of T2D. Spline regression analysis showed a linear dose-response association between intake of dark chocolate and risk of T2D (P for linearity=0.003). These findings were independent of established and potential risk factors for diabetes and were robust in multiple sensitivity analyses, although statistically significant heterogeneity was also noted among cohorts. Stratified analyses indicated that the association of dark chocolate intake was more apparent among younger individuals. Epicatechin intake may partially account for the inverse associations of dark chocolate. Similar differential associations between milk and dark chocolate intake were also observed for change in bodyweight. Milk chocolate consumption was statistically significantly associated with more weight gain but increased dark chocolate intake was not associated with weight gain. In fact, time varying BMI did not explain the associations for dark chocolate and risk of T2D in our data.
Comparison with other studies
Our study’s finding that intake of total chocolate was statistically significantly associated with lower risk of diabetes was in line with previously published studies. The Physicians’ Health Study reported that consuming ≥2 servings/week of total chocolate was associated with a 17% (95% CI 1% to 31%) lower risk of T2D among 18 235 participants during a median follow-up of 9.2 years.16In the Multiethnic Cohort Study, consuming chocolate ≥4 times/week was associated with a 19% (95% CI 9% to 28%) lower risk of T2D compared with a lower frequency of consumption (<once/month).41In the Maine-Syracuse Longitudinal Study, individuals who never or rarely ate chocolate had a 91% (95% CI 3% to 255%) higher risk of T2D than those who consumed chocolate more than once per week.42
Dark and milk chocolate could have a differential effect on T2D; however, evidence for associations pertinent to intake of chocolate subtypes is sparse. The National Health and Nutrition Examination Survey (2007-08 and 2013-14) found that 11.1% of US adults consumed chocolate on a regular basis, whereas only 1.4% reported consuming dark chocolate (≥45% cocoa content).43Major subtypes of chocolate (dark, milk, and white) differ mainly in their cocoa and sugar content and presence of milk.12Among the three subtypes of chocolate products, dark chocolate has the highest cocoa content (50-80% cacao) and is the richest chocolate in flavan-3-ols (mean 3.65 mg/g).44Milk chocolate comprises less than one fifth of the flavan-3-ols (mean 0.69 mg/g) in dark chocolate, has a lower cocoa content (~35%), and has a higher sugar content.44White chocolate has the highest sugar content, and, since it does not contain cocoa, has no polyphenols. Flavan-3-ol content of chocolate is highly variable depending on the type of processing used. The current analysis investigated the association between chocolate subtypes and risk of T2D, and we found evidence of a linear, inverse association of dark chocolate intake with risk of T2D. Our findings are largely in line with randomized controlled trials that examined dark chocolate or cocoa in relation to T2D or cardiometabolic risk factors. For example, a 15 day randomized controlled trial in glucose intolerant, hypertensive participants found that daily consumption of 100 g high polyphenol dark chocolate led to significant reductions in blood pressure and improvements in insulin sensitivity, compared with a placebo group consuming 90 g white chocolate.45Studies examining the consumption of high versus low flavanol cocoa products also showed health benefits in improving insulin resistance among individuals with overweight and obesity,46increasing high density lipoprotein cholesterol levels, and lowering blood pressure among patients with diabetes.4748However, the recent large scale rand","Objective: To prospectively investigate the associations between dark, milk, and total chocolate consumption and risk of type 2 diabetes (T2D) in three US cohorts.
Design: Prospective cohort studies.
Setting: Nurses’ Health Study (NHS; 1986-2018), Nurses’ Health Study II (NHSII; 1991-2021), and Health Professionals Follow-Up Study (HPFS; 1986-2020).
Participants: At study baseline for total chocolate analyses (1986 for NHS and HPFS; 1991 for NHSII), 192 208 participants without T2D, cardiovascular disease, or cancer were included. 111 654 participants were included in the analysis for risk of T2D by intake of chocolate subtypes, assessed from 2006 in NHS and HPFS and from 2007 in NHSII.
Main outcome measure: Self-reported incident T2D, with patients identified by follow-up questionnaires and confirmed through a validated supplementary questionnaire. Cox proportional hazards regression was used to estimate hazard ratios and 95% confidence intervals (CIs) for T2D according to chocolate consumption.
Results: In the primary analyses for total chocolate, 18 862 people with incident T2D were identified during 4 829 175 person years of follow-up. After adjusting for personal, lifestyle, and dietary risk factors, participants consuming ≥5 servings/week of any chocolate showed a significant 10% (95% CI 2% to 17%; P trend=0.07) lower rate of T2D compared with those who never or rarely consumed chocolate. In analyses by chocolate subtypes, 4771 people with incident T2D were identified. Participants who consumed ≥5 servings/week of dark chocolate showed a significant 21% (5% to 34%; P trend=0.006) lower risk of T2D. No significant associations were found for milk chocolate intake. Spline regression showed a linear dose-response association between dark chocolate intake and risk of T2D (P for linearity=0.003), with a significant risk reduction of 3% (1% to 5%) observed for each serving/week of dark chocolate consumption. Intake of milk, but not dark, chocolate was positively associated with weight gain.
Conclusions: Increased consumption of dark, but not milk, chocolate was associated with lower risk of T2D. Increased consumption of milk, but not dark, chocolate was associated with long term weight gain. Further randomized controlled trials are needed to replicate these findings and further explore the mechanisms.
"
Cardiac output-guided haemodynamic therapy for patients undergoing major gastrointestinal surgery,"Introduction
An estimated 310 million patients undergo surgery worldwide each year.1Older patients with comorbidities undergoing gastrointestinal surgery have a particularly high burden of postoperative morbidity.23Around one third will develop a postoperative infection and about 10% will die within six months of surgery.4Administering intravenous fluids and inotropic or vasoactive drugs is a central element of perioperative care that may affect patient outcomes.567Optimal haemodynamic management should provide adequate organ perfusion while minimising iatrogenic harm. However, clinical practices vary widely,5and their effect on outcomes remains uncertain.
Protocolised haemodynamic management guided by monitoring of cardiac output is one approach that may improve patient care by reducing the risk of postoperative infections and other complications.48Our previous studies described the physiological and pharmacological basis of this effect, including the role of low dose inodilators, finding beneficial changes in inflammatory pathways and improved tissue perfusion and oxygenation.910These are important determinants of postoperative infections.1112Cardiac output-guided therapy has proved controversial at times, and excessive early pulmonary artery catheter use generated safety concerns.131415More recent trials using simpler, minimally invasive technologies that track changes in cardiac output and stroke volume, using pulse wave analysis and other methods,16have resolved some of these safety issues. Clear evidence of clinical effectiveness is, however, still lacking. The results of the original Optimisation of Perioperative Cardiovascular Management to Improve Surgical Outcome (OPTIMISE) trial of this intervention, although not definitive, highlighted a possible reduction in postoperative infections, but also raised questions about cardiac safety.4Meta-analyses further suggest a reduction in postoperative infections and other complications, and shorter hospital stays,468but they are limited by the inclusion of many older, single centre trials with a high risk of bias. This has led to partial adoption and widespread variation in clinical practice.17
To address this knowledge gap, we conducted the OPTIMISE II trial of a cardiac output-guided haemodynamic therapy intervention in patients with an increased risk of complications undergoing major elective gastrointestinal surgery. We hypothesised that this intervention would reduce postoperative infections compared with usual clinical care.
Methods
Trial design
OPTIMISE II was an international, multicentre, randomised trial conducted in 55 hospitals in the UK (n=14 hospitals), Spain (n=9), Brazil (n=7), Canada (n=5), US (n=5), Germany (n=4), Poland (n=4), Australia (n=3), Switzerland (n=2), Jordan (n=1), and Romania (n=1). Recruitment ran from 26 January 2017 to 13 September 2022. We compared usual care versus cardiac output-guided intravenous fluid therapy with low dose inotrope infusion, during and four hours after major elective gastrointestinal surgery. The trial design and rationale have been reported previously.18The trial protocol (see supplement 1) was approved by the UK National Research Ethics Service (ID 209688) and by responsible ethics committees in all participating countries.
Patients
We included patients aged 65 years and older, with an American Society of Anesthesiologists (ASA) physical status classification of II or greater, undergoing major elective surgery involving the gastrointestinal tract with an expected duration of more than 90 minutes. Surgical procedure categories were resection of colon, rectum, or small bowel; resection of pancreas and bowel; resection of stomach (non-obesity surgery); resection of oesophagus (non-obesity surgery); obesity surgery; and other surgery involving gut resection. Exclusion criteria were refusal of written informed consent, clinician refusal, expected mortality <30 days, acute myocardial ischaemia or acute pulmonary oedema in previous 30 days, contraindication to low dose inotropic drugs, pregnancy, previous enrolment in the OPTIMISE II trial, or current participation in another clinical trial of a treatment with a similar biological mechanism or primary outcome measure. All participants provided written informed consent.
Randomisation and masking
After enrolment and shortly before surgery, participants were randomised in a 1:1 ratio by minimisation with a random component (80% probability to be allocated to the group that minimises the between group differences in the minimisation factors), using a central online service, which concealed the allocation sequence. Minimisation variables were country of participating hospital, category of surgical procedure, and ASA class (see supplement 2 for categories). It was not possible to conceal study group allocation during the trial intervention period. Research staff collecting and assessing clinical outcomes were not involved in the participants’ care and were unaware of study group allocation. Self-assessment of assessor blinding was performed after follow-up visits. The local principal investigator or delegate unaware of group allocation confirmed the clinical outcomes. Only the independent data monitoring and ethics committee had access to trial data represented by treatment arm. Final analysis was performed after the statistical analysis plan (see supplement 2) was signed off, and the trial database was locked.
Procedures
Care for all participants was loosely defined to avoid extremes of practice and practice misalignment while reflecting the heterogeneity of routine clinical care.19Standard treatments were recommended to maintain oxygenation (SpO2≥94%), haemoglobin concentration (>80 g/L), core temperature (37°C), heart rate (<100 beats/min), and mean arterial pressure (60-100 mm Hg). Maintenance fluid requirements were met by a 1 mL/kg/h infusion of crystalloid, with 5% dextrose recommended to minimise salt overload. Clinicians chose from a range of crystalloids or colloids for plasma volume expansion. Anaesthesia and analgesia were provided according to clinician preference and local protocols. Antibiotic prophylaxis was given in accordance with local protocols. Supplement 3 provides details of the trial’s standard operating procedures for the care of participants in the intervention and usual care groups.
The trial intervention began after induction of anaesthesia and continued until four hours after surgery (fig 1). Cardiac output and associated haemodynamic variables were monitored using an Edwards Lifesciences (Irvine, CA) system comprising an EV1000 or HemoSphere monitor with either ClearSight (non-invasive) cuff or FloTrac (invasive arterial pressure) sensor depending on clinician choice. The cardiac output monitoring system was set up immediately after the induction of anaesthesia, with cardiac stroke volume measured for the first time post-induction. In addition to maintenance fluids, 250 mL fluid boluses were given in accordance with an algorithm to achieve an optimal value of stroke volume. Ongoing responsiveness to fluid was indicated by a 10% rise in stroke volume after a fluid bolus was administered. Cardiac output was considered optimal when the criteria for fluid responsiveness were no longer met and the stroke volume was maintained for at least 20 minutes. Further fluid boluses were withheld at this point. Additionally, stroke volume variation of <5% was taken to indicate the patient would not be fluid responsive,20and fluid boluses were withheld. Intervention group participants also received a fixed, low dose, equipotent infusion of either dobutamine (2.5 µg/kg/min) or dopexamine (0.5 µg/kg/min), chosen based on clinician preference and local availability, and started after the first fluid bolus. If participants developed a tachycardia (>100 beats/min) for 30 minutes despite adequate anaesthesia and analgesia, the inotrope dose was reduced, and was terminated if the tachycardia persisted.
Participants in the usual care group received treatment as usual, but without cardiac output monitoring. At the discretion of the clinician, 250 mL fluid boluses were given guided by pulse rate, arterial pressure, urine output, core-peripheral temperature gradient, serum lactate, and base excess. Cardiac output monitoring could only be requested to guide the care of participants who became critically unwell during the intervention period. Predefined protocol deviations were failure to use cardiac output monitoring, failure to administer inotrope, or incorrect dose of inotrope administered (intervention group participants), or the use of cardiac output monitoring in a control group participant.
Outcomes
The primary outcome measure was postoperative infection within 30 days of randomisation, defined using US Centers for Disease Control and Prevention criteria as one or more of superficial, deep, or organ space surgical site infection, pneumonia, urinary tract infection, laboratory confirmed bloodstream infection, or infection of uncertain source. The primary outcome was assessed using information from the medical record and from contact with patients.
Secondary outcomes were acute kidney injury (Kidney Disease Improving Global Outcomes stage 2 or greater21) within 30 days and mortality within 180 days of randomisation. The two safety outcomes were acute cardiac events (arrhythmia, myocardial infarction, myocardial injury after non-cardiac surgery, cardiac arrest with successful resuscitation, or cardiogenic pulmonary oedema) within 24 hours and 30 days of randomisation. Process measures were duration of postoperative hospital stay and number of days alive and not in a critical care bed within 30 days of randomisation. Mortality was established by medical record review or from national databases. All morbidity outcomes were defined as events of Clavien-Dindo grade II or higher (ie, requiring clinical treatment) meeting the criteria of recommended standards for perioperative trials.212223
The data monitoring and ethics committee reviewed unblinded data at intervals, including the rate of acute cardiac events as a safety outcome and all other reported serious adverse events. No formal interim analysis was conducted.
Statistical analysis
We determined that a sample size of 2502 participants (1251 in each group) would allow detection of a reduction in postoperative infection rate within 30 days, from 30% to 24% with >90% power, using a two sided test with significance level α=0.05.4
All analyses were performed on an intention-to-treat basis, whereby all patients with a recorded outcome were included in the analysis and analysed according to the treatment to which they were randomised. The primary outcome was analysed using a mixed effects logistic regression model with a random intercept for country and fixed effects: surgical procedure category, age, sex, ASA class, and baseline haemoglobin and creatinine levels.24We included ASA class and procedure as categorical variables using the same categories used in the minimisation algorithm. We used restricted cubic splines with three knots and knot locations based on Harrell’s recommendations to adjust for age and baseline haemoglobin and creatinine levels.25A further post hoc analysis of the primary outcome using minimal adjustment including only the minimisation variables (country, surgical procedure category, ASA class) was also conducted during editorial review. Mean imputation was used to account for missing baseline data.26The same model was used to analyse safety and secondary outcomes. A secondary analysis was conducted for the primary outcome, which included mortality as a competing risk for postoperative infection in a time-to-event model. All statistical tests were two sided, and we considered a P value <0.05 to be statistically significant. Analyses of heterogeneity of effects across subgroups (surgical procedure category) were assessed by performing a likelihood ratio test comparing the primary outcome analysis model against the same model with a treatment-by-covariate interaction effect added. Surgical approach (laparoscopic or open) was added as a further subgroup analysis post hoc during editorial review and analysed using the same method, along with a comparison of the primary outcome event rate between the control group and each type of cardiac output sensor used in the intervention arm. The statistical analysis plan (see supplement 2) was finalised before unblinding and analysis and is publicly available (https://optimiseii.org/documents).
Patient and public involvement
Patient and public representatives were involved from the planning stages of the trial. The Royal College of Anaesthetists Patient, Carer, and Public Involvement and Engagement (PCPIE) in Research Group reviewed the OPTIMISE II trial in detail. Detailed feedback from the group informed both the design and the conduct of the trial. Specifically, the group considered the results of the original OPTIMISE trial and whether a larger trial was justified. They considered the effect of hospital acquired infection and stated that this was a clinical outcome of clear and direct relevance to patients. The group also provided detailed advice on the patient experience in the trial, particularly trial consent in the limited period within 24 hours of surgery. This led us to strongly promote recruitment in the preoperative outpatient setting. The RCoA PCPIE group nominated a member to join the OPTIMISE II project group as a lay representative. This member has been involved throughout the preparation of the trial, providing detailed input on issues of safety and the experience of participating patients. The trial steering committee included a lay member, providing independent non-medical input to trial conduct.
Results
Overall, 6693 patients were assessed for eligibility and 2502 participants were enrolled. Without knowledge of study group allocation, the independently led trial steering committee advised excluding four enrolled participants who were randomised incorrectly because they did not meet the inclusion criterion for age. Of the 2498 participants meeting the inclusion criteria, 1251 were allocated to receive cardiac output-guided haemodynamic therapy (intervention group) and 1247 to receive usual care (control group) (fig 2). Intervention groups were well matched at baseline (table 1). Mean age was 74 (standard deviation 6) years and 1432/2498 (57.3%) participants were women. Most surgery types were well represented, and clinical care outside of the trial intervention was similar between the two groups (table 2). The primary outcome was available for 1247 (99.7%) patients in the intervention group and 1247 (100%) patients in the usual care group (fig 2).
Table 2describes the delivery of the trial intervention. The combined volumes of crystalloid and colloid fluids given during the overall intervention period and within each phase (intraoperative and postoperative) were similar (mean totals for intervention and usual care: 3267 mLv3058 mL, respectively). Balanced crystalloids were the primary fluid used for volume replacement in both groups. Other than the trial inotrope infusion, vasopressors and inotropes were widely used and to a similar extent in both groups. Adherence to intervention was excellent, with 93% of patients receiving trial care that complied with the protocol (see tables S11 and S12 in supplement 3). Trained investigators assisted in administering the trial intervention in 653/1248 (52%) patients during surgery and in 527/1243 (42%) patients during the postoperative phase. Self-assessment showed that investigators were suitably masked for more than 70% of outcomes assessments (see table S10 in supplement 3).
We found no difference between groups in the primary outcome of postoperative infection within 30 days of randomisation, which occurred in 289/1247 (23.2%) intervention patients and 283/1247 (22.7%) usual care patients (adjusted odds ratio 1.03, 95% confidence interval (CI) 0.84 to 1.25; P=0.81). More participants in the intervention group than control group experienced an acute cardiac event within 24 hours (38/1250 (3.0%)v21/1247 (1.7%); adjusted odds ratio 1.82, 1.06 to 3.13; P=0.03) (table 3andfig 3). This difference was primarily due to a greater number of participants experiencing arrhythmias within 24 hours in the intervention group (33v17). There was no difference in acute cardiac events within 30 days (85/1249 (6.8%)v79/1247 (6.3%); adjusted odds ratio 1.06, 0.77 to 1.47; P=0.71). Supplementary tables S5 and S6 show the Clavien-Dindo graded severity of cardiac events. Secondary outcomes of acute kidney injury or death within 180 days of randomisation did not differ between the two groups (table 3and figure S1 in supplement 3). The process measures of hospital stay and days alive and not in a critical care bed within 30 days did not differ between the two groups (see supplement 3, table S1). Tables S4 to S9 and figure S3 in supplement 3 report the components of the primary and secondary outcomes and all other postoperative complications, treatments, and serious adverse event reports, stratified by treatment group.
The prespecified subgroup analysis showed that the effect of the intervention did not differ by category of planned surgical procedure (see table S2 in supplement 3). The post hoc subgroup analyses showed that the effect of the intervention was similar across modes of surgery and the cardiac output monitor sensor technologies available (see table S2 in supplement 3). The primary outcome finding was robust for the effect of mortality acting as a competing risk for postoperative infection (see table S3 and figure S2 in supplement 3), and it was not modified by the post hoc minimally adjusted analysis (table 3).
Discussion
In this trial of 2498 participants at risk of postoperative morbidity, a cardiac output-guided haemodynamic therapy intervention, incorporating optimisation of stroke volume and fixed low dose inotrope infusion, used during and for four hours after major gastrointestinal surgery, did not reduce the incidence of postoperative infections within 30 days. The intervention led to a higher incidence of acute cardiac events within 24 hours of randomisation, owing to an excess of arrhythmias requiring clinical treatment. Study arms did not differ for 180 day mortality, all other safety and morbidity outcomes, or hospital and critical care length of stay. These results did not differ in any prespecified surgical subgroup, nor in the post hoc subgroup analyses by surgical approach or type of monitor sensor technology used.
Interpretation of results and comparison with other studies
It is well established that haemodynamic management during and after surgery, including using intravenous fluid and inotropic or vasoactive drugs, can affect postoperative outcomes.6727Our findings are at odds with previous research suggesting that perioperative cardiac output-guided haemodynamic therapy reduces infections, other complications, and length of hospital stay after major surgery.468The original OPTIMISE study was previously the largest trial of this intervention (n=734). The primary outcome was a composite of postoperative morbidity events, and the intervention was associated with a relative risk of 0.84 (95% CI 0.71 to 1.01; P=0.07). Of the morbidity components, infections were common overall but appeared less frequent in the intervention group, occurring in 24% of intervention patients and 30% of control patients. A similar reduction in postoperative infections was found when the results were integrated with the results of a previous Cochrane systematic review (relative risk 0.81, 95% CI 0.69 to 0.95).4Evidence syntheses have also suggested reductions in the overall incidence of postoperative complications and renal impairment, reduced postoperative hospital stays, and reduced mortality when used in elective surgery.468However, more recent individual trials (each n<1000) have not shown consistent evidence of benefit.28293031
Postoperative infections remain one of the commonest surgical complications, with substantial healthcare burden and impact on patients’ experience. A large evidence base supports the plausibility of haemodynamic therapy interventions reducing the risk of infections. Surgical site infections are associated with reduced tissue perfusion and tissue oxygenation,1112and our previous work found that this intervention can improve these factors, particularly when low dose inodilators are included in the haemodynamic algorithm.910Inclusion of these agents is also justified owing to their beneficial modulation of the surgical inflammatory response that may be independent of haemodynamic effects.9More widely, optimisation of cardiac output-guided stroke volume can maintain adequate gastrointestinal tract perfusion during surgery,32helping to maintain barrier function and reduce bacterial translocation, another driver of systemic inflammation and infection.33When taken with the previous evidence suggesting modifiability, reducing postoperative infections was therefore the most important beneficial outcome to seek.
Some differences in trial design and setting may explain why our findings contrast with those of existing studies. Most previous trials were smaller, single centre efficacy studies, whereas we studied a simplified haemodynamic intervention in a pragmatic real world clinical effectiveness trial. Infectious and other outcomes have been inconsistently defined in previous research, whereas we used standardised endpoints.21Furthermore, much of the research in this area is decades old. In the intervening period, surgical techniques (eg, minimal access), perioperative care (eg, enhanced recovery protocols), baseline infection rates,4and other outcomes have improved substantially. Despite using a similar intervention, such non-protocolised changes were seen from OPTIMISE (2014) to OPTIMISE II, with more laparoscopic surgery, less intravenous colloid use, and wider use of non-trial vasopressor or inotrope infusions in this trial. Both fluid composition and maintenance of perioperative blood pressure have been shown to affect postoperative outcomes.3435It is possible that temporal changes in perioperative care have reduced the potential benefits of routinely applied cardiac output-guided haemodynamic approaches when studied in a broad international setting.
The trial intervention included a choice of inotrope. Owing to drug availability, dobutamine was the default inotrope given to intervention patients, whereas dopexamine was routinely used in the original OPTIMISE trial. These agents have subtle differences in haemodynamic, immunomodulatory, and arrhythmogenic effects that may have contributed to our findings.36Although the incidence of early perioperative arrhythmias was higher in the current trial’s intervention group, by 30 days the number of cardiac events was the same as in the control group, with a lower overall incidence than in the original OPTIMISE trial.4
Implications of this study
Given the lack of clinical superiority of this intervention compared with standard care, we do not recommend its routine use in this patient group. With an increased rate of early postoperative arrhythmias, presumably driven by dobutamine, we would advise caution in the use of inotrope infusions in algorithms for perioperative haemodynamic therapy. It remains to be seen whether there are patient groups with characteristics not explored here (eg, demonstrable impairment of cardiovascular performance or particularly high perioperative risk profile) who may derive more benefit. This might apply to subpopulations within this trial (owing to heterogeneity of treatment effect) or to other groups not studied. Our team is currently exploring the role of cardiac output-guided therapy without routine inotrope infusion for patients undergoing emergency abdominal surgery.37The results of this trial cannot be extrapolated to that patient group, nor to other specific abdominal surgical groups (eg, liver resection) not included. However, for routine use in elective gastrointestinal surgery, it seems unlikely that further research into similar simple haemodynamic strategies will show a positive effect on postoperative infections. Alternative approaches to perioperative haemodynamic management, such as those based on newer monitoring technologies, personalised blood pressure targets, or more complex algorithms aiming to more closely individualise therapy have shown early benefit, but the effects require confirmation.35383940
Strengths and limitations of this study
Our study has several strengths. The large sample size and rigorous procedures of the trial addressed the limited statistical power and risk of bias of many previous trials, and the trial groups were well matched. The broad inclusion criteria and international setting enhanced the generalisability of our findings. The haemodynamic strategy we tested was the final iteration of an intervention developed through earlier studies by our team,41041with a clear biological rationale for modifying the chosen clinical outcomes. Our previous studies have shown the potentially beneficial effects of incorporating low dose inotropes in modifying immune and inflammatory pathways and in improving microvascular flow and tissue oxygenation.910The protocol adherence rate was high across diverse international settings, even without the presence of routine research staff or postoperative critical care admission. The inclusion of cardiac safety endpoints provides the highest standard of patient safety data for this intervention. The standardised outcomes definitions and masking procedures during data collection and adjudication helped to minimise bias. The reporting of self-assessed blinding by data collectors is an important addition. Finally, we used minimisation rather than stratified permuted blocks to randomise patients as this has been shown to offer equivalent or better balance of stratification factors across treatment groups.
Our trial also has some limitations. It was not possible to conceal group allocation during administration of this complex intervention.418Some elements of wider perioperative care were not standardised or quantified. For example, choice of surgical antibiotic prophylaxis was administered in accordance with local guidelines based on regional resistance patterns, but adherence was not recorded. However, all recorded aspects of non-protocolised perioperative care were similar between trial groups, reducing the likelihood of systemic differences from knowledge of treatment allocation. Both the baseline infection rate and the treatment effect size were lower than assumed for the trial sample size, but we considered the sample size recruited to have been adequate to detect a clinically important difference in the primary outcome. As a large clinical effectiveness trial, we prioritised data collection with a focus on clinical outcomes and adherence and therefore have not reported detailed physiological changes associated with the intervention. However, the physiological effects of similar haemodynamic approaches have already been well characterised in previous efficacy trials. The adherence rate to the intervention was slightly higher in the usual care than intervention group, but this was an expected finding given the intervention was used in addition to treatment as usual. Achieving >90% adherence to an intervention in a large international trial of a multifaceted algorithm is a strength, and it seems unlikely that 100% adherence would have substantially altered our findings. The post hoc subgroup analyses should be considered exploratory and therefore interpreted with caution. In particular, the type of sensor used to monitor cardiac output was chosen after randomisation. The decision to use the non-invasive sensor (ClearSight) may have been biased towards lower risk patients and surgeries and this may in part explain the lower infection rate in this subgroup. Lastly, myocardial injury after non-cardiac surgery is commonly a subclinical event, despite its association with subsequent mortality.42We did not routinely screen all participants for myocardial injury after non-cardiac surgery and therefore the true incidence may have been higher than reported.
Conclusion
This international clinical effectiveness trial in high risk patients undergoing major elective gastrointestinal surgery did not provide evidence that cardiac output-guided intravenous fluid therapy with low dose inotrope infusion could reduce the incidence of postoperative infections. The intervention was associated with an increased incidence of acute cardiac events within 24 hours, primarily tachyarrhythmias. The routine use of this treatment approach in unselected patients is not recommended.
Previous studies suggest that protocolised administration of intravenous fluids and inotropes guided by cardiac output monitoring can reduce complications after major surgery
Definitive evidence is, however, lacking
A perioperative haemodynamic therapy approach incorporating cardiac output-guided intravenous fluid therapy with low dose inotrope infusion does not reduce the incidence of postoperative infections in high risk patients undergoing major gastrointestinal surgery
Use of fixed, low dose inotrope infusions within such algorithms should be avoided owing to the risk of arrhythmias
Routine use of this intervention is not warranted in unselected patients undergoing gastrointestinal surgery
","Objectives: To evaluate the clinical effectiveness and safety of a perioperative algorithm for cardiac output-guided haemodynamic therapy in patients undergoing major gastrointestinal surgery.
Design: Multicentre randomised controlled trial.
Setting: Surgical services of 55 hospitals worldwide.
Participants: 2498 adults aged ≥65 years with an American Society of Anesthesiologists physical status classification of II or greater and undergoing major elective gastrointestinal surgery, recruited between January 2017 and September 2022.
Interventions: Participants were assigned to minimally invasive cardiac output-guided intravenous fluid therapy with low dose inotrope infusion during and four hours after surgery, or to usual care without cardiac output monitoring.
Main outcome measures: The primary outcome was postoperative infection within 30 days of randomisation. Safety outcomes were acute cardiac events within 24 hours and 30 days. Secondary outcomes were acute kidney injury within 30 days and mortality within 180 days.
Results: In 2498 patients (mean age 74 (standard deviation 6) years, 57% women), the primary outcome occurred in 289/1247 (23.2%) intervention patients and 283/1247 (22.7%) usual care patients (adjusted odds ratio 1.03 (95% confidence interval 0.84 to 1.25); P=0.81). Acute cardiac events within 24 hours occurred in 38/1250 (3.0%) intervention patients and 21/1247 (1.7%) usual care patients (adjusted odds ratio 1.82 (1.06 to 3.13); P=0.03). This difference was primarily due to an increased incidence of arrhythmias among intervention patients. Acute cardiac events within 30 days occurred in 85/1249 (6.8%) intervention patients and 79/1247 (6.3%) usual care patients (adjusted odds ratio 1.06 (0.77 to 1.47); P=0.71). Other secondary outcomes did not differ.
Conclusions: This clinical effectiveness trial in patients undergoing major elective gastrointestinal surgery did not provide evidence that cardiac output-guided intravenous fluid therapy with low dose inotrope infusion could reduce the incidence of postoperative infections. The intervention was associated with an increased incidence of acute cardiac events within 24 hours, in particular tachyarrhythmias. Based on these findings, the routine use of this treatment approach in unselected patients is not recommended.
Trial registration: ISRCTN RegistryISRCTN39653756.
"
Contemporary menopausal HRT and risk of cardiovascular disease,"Introduction
Cardiovascular disease is the leading cause of death globally, contributing to approximately 30% of all deaths.1Women tend to develop cardiovascular disease several years later than men do, with a notable increase during midlife, a period coincident with the menopausal transition.2This transition is characterised by a decline in oestrogen and an increase in follicle stimulating hormone concentrations. These hormonal changes can induce effects on the neuroendocrine system, resulting in hot flushes, night sweats, sleep disorders, anxiety, and memory loss. As the world population ages, a significant demographic shift places a larger fraction of women in the postmenopausal phase. Managing this phase, marked by decreased oestrogen concentrations and various menopausal symptoms, largely relies on systemic menopausal hormone therapy. Systemic menopausal hormone therapy is based on administration of oestrogen with or without the addition of progestogens, effectively mitigates vasomotor symptoms,3and reduces the incidence of vertebral and hip fractures.45However, although menopausal hormone therapy alleviates menopausal symptoms, studies have suggested an association between its use and an increased risk of cardiovascular disease.678910
Experimental studies have shown that oestrogen plays a protective role in cardiovascular health by promoting angiogenesis and vasodilation,1112reducing cardiac fibrosis and oxidative stress and increasing high density lipoprotein cholesterol concentrations.131415More than two decades ago, observational studies conducted through the Nurses’ Health Study in the US and the General Practice Research Database in the UK supported a potential cardiovascular benefit for postmenopausal women using hormone therapy.161718However, subsequent randomised trials, including the Heart and Oestrogen/Progestin Replacement Study (HERS) and the Women’s Health Initiative trial, showed the opposite.19202122HERS showed no reduction in coronary heart disease rates and an increase in coronary events and myocardial infarctions in the treatment group. Similarly, the Women’s Health Initiative found increased risks of coronary heart disease, stroke, and pulmonary embolism, leading to the conclusion that the risks of hormone therapy outweighed the benefits, especially because of the increased risk of stroke. These trials challenged the initial positive outlook inspired by observational studies and altered our comprehension of the complexities and associated risks of menopausal hormone therapy, prompting a critical reassessment of its usage.
The discrepancy between previous observational studies and randomised controlled trials has been attributed to population differences and methodological biases.2324In most observational studies, hormone use started around menopause, whereas 67% of Women’s Health Initiative participants were aged over 60. Observational studies compared prevalent users with non-users, introducing a selection bias as tolerant women are included in the user group with depletion of women susceptible to cardiovascular disease.25Moreover, observational estimates are heavily weighted by long term use and would have missed any increased risk seen at the beginning of treatment even if only new users had been analysed. Analysing observational data by using a design that emulates a target trial has been suggested to mitigate such pitfalls.26Other than the randomisation, which is not possible in an observational study, emulating a target trial is advocated as being more accurate for causal inference than a traditional observational study.27In a randomised trial, time zero aligns the start of follow-up, treatment assignment, and evaluation of eligibility simultaneously. Emulating this framework by using observational data helps to avoid common biases, including immortal time bias and selection bias, by ensuring that all critical processes are aligned at the same starting point (time zero). By applying this method, data from the Nurses’ Health Study were reanalysed to resemble the Women’s Health Initiative trial.1617Unlike the original analyses, the revised effect estimates aligned with the Women’s Health Initiative, probably owing to reduced selection bias.28
Two decades have passed since the publication of the HERS and the Women’s Health Initiative trials. Since then, new menopausal hormone therapy products with different formulations and routes of administration have been introduced to the market. Both the HERS and the Women’s Health Initiative investigated the risk of cardiovascular disease associated with only one type of menopausal hormone therapy, specifically oral conjugated equine oestrogen combined with medroxyprogesterone acetate. In addition, HERS and the Women’s Health Initiative included women with an average age of 67 and 63, respectively, which is not the typical timing for starting menopausal hormone therapy today. A critical need exists for further studies to investigate the effects of contemporary menopausal hormone therapy in a clinically relevant population.29
To tackle the gaps in the knowledge on use of contemporary menopausal hormone therapy and risk of cardiovascular disease, this paper reports the findings from a Swedish nationwide register based emulated target trial including 919 614 women aged 50-58 years. This age range represents the typical span during which women undergo the menopausal transition—a period marked by significant hormonal changes and when most women start menopausal hormone therapy. We explored a variety of menopausal hormone therapy formulations, including oral and transdermal options, combined therapies with progestin, oestrogen-only treatments, and tibolone. These diverse options mirror the range of choices available in clinical practice, tailored to meet the needs and preferences of individual patients. Different hormone administration methods are thought to have distinct physiological effects. Oral oestrogens increase production of coagulation factor, suggested to have a less favourable cardiovascular risk profile,30whereas transdermal administration bypasses the liver and is believed to be a better option.31The addition of progestins in combined preparations may increase the oestrogenicity and, therefore, the risk of thrombosis.32
By clearly defining the target trial protocol and its observational emulation, we ensured eligibility and treatment alignment from the start, preventing selection bias by excluding prevalent users at baseline.33We aimed to estimate the average treatment effect for ischaemic heart disease, cerebral infarction, myocardial infarction, and venous thromboembolism with the use of different types of contemporary menopausal hormone therapy.
Methods
Study population
We identified the population on the basis of data provided by Statistics Sweden.34A unique personal identification number that is given to all Swedish citizens at birth and to people who have immigrated to Sweden is used in all public registries, allowing reliable linkage of data among different registries. Statistics Sweden also provided data on the highest achieved level of education, emigration, and ancestral origin. The Swedish National Board of Health and Welfare provided data from the Swedish prescribed drug register, the national patient register, the cancer register, and the cause of death register.35
We designed this observational analysis to emulate a target trial (that is, a hypothetical pragmatic trial that would have answered the causal question of interest) of the effect of menopausal hormone therapy compared with no menopausal hormone therapy on the risk of cardiovascular disease outcomes.Table 1shows the protocol of the target trial and its observational emulation.
Eligibility criteria and washout period
We designed 138 nested trials, with one trial starting each month during the study period—that is, from July 2007 until December 2018. Here, the month of start of follow-up is termed the “trial month” and equals July 2007 for the first trial, August 2007 for the second trial, and so on. In each trial, we identified all women who, before the start of follow-up, were between 50 and 58 years of age and had not redeemed a prescription for any menopausal hormone therapy in the previous two years (that is, the washout period). We excluded women who, before the start of follow-up, had a previous history of ischaemic heart disease, stroke, peripheral vascular disease, peripheral arterial disease, or cancer. We also excluded women who had undergone bilateral oophorectomy, unilateral oophorectomy twice, hysterectomy, or a sterilisation procedure. Our exclusion criteria included diagnoses recorded in the patient register from 1987, causes of death from 1952, and cancer diagnoses from 1958, ensuring comprehensive coverage of almost all significant medical events throughout the lifetime of participants included in the study. The relevant diagnostic, surgical, and medication codes used for exclusion are listed in supplementary table S1. We excluded women who appeared for the first time or with a new personal identification number in the registers after 2005. This group represents either people who immigrated to Sweden or women who had received gender affirming care. This was due to missing pre-immigration medical data and potential confounding factors from gender transition procedures.
Sequence of pragmatic trials
The small number of hormone therapy initiators (n=331) and cardiovascular disease events (n=8) in the first trial makes conducting a meaningful analysis from one single trial impossible. Instead, we emulated 138 trials, one trial each month during the study period, with each trial having a one month enrolment period. This ensures that all eligible initiators and events are included in the analysis. This approach increases statistical power compared with selecting only one of those instances as time zero and accounts for the fact that individuals can meet eligibility criteria multiple times during the study period36;figure 1illustrates the study design. The emulation of a series of trials, such that each individual may participate in multiple trials, has been successfully applied in previous studies when comparing treatment versus non-treatment.3738394041Supplementary table S2 shows the numbers of participants, initiators, events by “trial,” and people who did not meet eligibility criteria (see previous paragraph).
Study outcomes
We obtained the date of disease diagnoses from the national patient register and the cause of death register, which collects data on discharge diagnoses from public and private Swedish hospitals. The specific outcomes of this study (supplementary table S3) included ischaemic heart disease (ICD-10 (international classification of diseases, 10th revision) codes: I20, I21, I22, I24, I25), myocardial infarction (ICD-10 codes: I21, I22), cerebral infarction (ICD-10 code: I63), and venous thromboembolism (ICD-10 codes: I26, I80, I81, I82). We analysed these diseases as separate outcomes and as a composite outcome defined as the first occurrence of any cardiovascular disease (ICD-10 codes: I20, I21, I22, I24, I25, I63).
Treatment groups and follow-up
We retrieved the redeemed prescriptions for the study period from the prescribed drug register, which categorises information according to Anatomical Therapeutical Chemical codes. The relevant Anatomical Therapeutical Chemical codes for systemic menopausal hormone therapy products are listed in supplementary table S4. Additional information on name, trade name, dose, number of packages, defined daily doses per package, tablets per package, route of administration, and date of redemption was available. We classified eligible women into one of eight treatment strategies according to their redeemed or non-redeemed prescriptions at baseline: initiators who started oral combined continuous therapy, oral combined sequential therapy, oral unopposed oestrogen, oral oestrogen combined with the levonorgestrel intrauterine system, tibolone, transdermal combined therapy, or transdermal unopposed oestrogen and non-initiators who did not start any menopausal hormone therapy during the “trial month.” We considered contemporary redemption of unopposed oestrogen and progestogen (natural or synthetic) as one of two “self-combined therapies,” depending on the ratio between the oestrogen and progestogen dose: continuous combined therapy if the ratio was <7 and sequential combined therapy if the ratio was >7. We further subdivided the categorisation of “self-combined” according to the route of administration (oral or dermal). If a woman combined transdermal oestrogen with an oral, transdermal, or local progestogen, we categorised this woman as transdermal combined. We assumed that women who inserted a levonorgestrel intrauterine system during the trial month had it for the entire follow-up.
The total follow-up period for each trial was set to two years. We followed individuals until the first occurrence of the study endpoint, death, emigration, or end of follow-up (that is, two years), whichever occurred first.
Covariates
We achieved adjustment for potential confounding, measured at the baseline of each trial, by using inverse probability of treatment weighting. We used the directed acyclic graph approach to identify potential confounders and unmeasured confounders (supplementary figure S1). We added age at baseline to account for differences in cardiovascular disease risk. We included trial month as a continuous term (1-138) to account for potential differences in offset between trials, thereby increasing precision in effect estimates. We included calendar year to adjust for trends in menopausal hormone therapy prescription patterns and changes in cardiovascular disease prevalence during the study period. We used the level of education as a proxy for socioeconomic status. We adjusted for geographical regions to account for differences in prescription patterns between rural and urban regions. We included region of birth to account for potential differences in exposure and outcome. To account for differences in predisposing diseases and disorders between initiators and non-initiators, we adjusted for hypertension, heart disease, and diabetes (defined by the use or non-use of drugs for these conditions). For covariate adjustment, inverse probability of treatment weighting, relevant Anatomical Therapeutical Chemical codes, and data variables, see supplementary methods and table S5.
Causal contrasts of interest
We used observational analogues of intention-to-treat and per protocol analyses to estimate the effect of menopausal hormone therapy on risk of cardiovascular disease and venous thromboembolism. In the intention-to-treat model, we compared the incidence rate of cardiovascular disease in the initiator groups versus the non-initiator group. In the intention-to-treat analyses, we did not consider whether women who started treatment at baseline did not redeem any new prescriptions or altered their prescriptions or whether non-initiators started any menopausal hormone therapy during the follow-up. In the per protocol analyses, we censored initiators when they deviated from the protocol; that is, if they discontinued treatment or changed preparation. Similarly, we censored non-initiators when they deviated from the protocol; that is, if they started any treatment during the follow-up. Thus, in the per protocol analyses, all initiators were continuous users and all non-initiators were never users.
Statistical analysis
We used cause specific Cox proportional hazard modelling to estimate the hazard ratio of the composite cardiovascular disease outcome, with “time since the start of follow-up” as the timescale and a time fixed binary variable for menopausal hormone therapy initiation (based on information from the prescribed drug register). We also estimated the effect of initiation of menopausal hormone therapy on the hazard rate of the specific outcomes: ischaemic heart disease, cerebral infarction, myocardial infarction, and venous thromboembolism. The cause specific hazard model allows us to investigate the rate of occurrence of specific events, such as myocardial infarction, while treating other potential outcomes as competing risks. In this framework, events such as other forms of ischaemic heart disease (excluding myocardial infarction), cerebral infarction, and venous thromboembolism are considered competing risks. When these events occur, participants are censored at that time, meaning that they are removed from the analysis for the primary event of interest, myocardial infarction. However, when analysing ischaemic heart disease and myocardial infarction separately, we deliberately do not censor the events coded as myocardial infarction under ICD-10 codes I21 and I22.42Estimated hazard ratios in the main results are average contrasts over the follow-up of two years.
Additionally, we calculated the adjusted incidence rates for different follow-up periods, as shown in supplementary table S6. We did sensitivity analyses using Fine and Gray subdistribution models to account for competing risks between specific cardiovascular diseases, death, and emigration.43We achieved this by including in the risk set individuals who died or experienced another competing event, without censoring them. We note that a cause specific hazard ratio may be interpreted as a direct effect of the treatment on the outcome, whereas a subdistribution hazard ratio denotes the total effect of the treatment.44We fitted Cox regression models by using the coxph function and Fine and Gray models by using the finegray function, both available in the survival package (v3.5-7) in R. We used the common α level of 0.05 as a threshold for significance.
We used stabilised weights to adjust for confounding in the intention-to-treat and per protocol analyses. As a result, marginal effects are estimated corresponding to the average treatment effect. We included only baseline covariates in the intention-to-treat analyses, whereas we also included time varying covariates in the per protocol analyses. In the per protocol analyses, we estimated inverse probability weights to adjust for potential selection bias introduced by artificial censoring.45See supplementary methods for details.
We plotted adjusted cumulative incidence curves for the intention-to-treat analysis by using inverse probability of treatment weighting for covariate adjustment. We used the adjustedCurves R package to visualise the probability of the event of interest over the two year follow-up period.38To quantify the effect of different treatments on the event of interest, we calculated the absolute risk difference after one year of follow-up.38This analysis was limited to menopausal hormone therapies that showed a significant difference compared with the non-initiator group.
In the intention-to-treat analyses, we compared the incidence rate of cardiovascular disease in the initiator groups versus the non-initiator group. In the per protocol analyses, we extended all prescription periods to twice that of the previous redeemed package, as has been suggested in previous studies.46For example, if a package included 42 tablets with one tablet taken each day according to the defined daily doses, the exposure time was extended by 84 days from the last day of the preceding prescription. We calculated the month of last menopausal hormone therapy use by using the “compute.treatment.episode” function in the AdhereR package (v0.8.1) in R.47
Because the sequence of 138 trials started every month, many women participated in more than one trial. To adjust for their events being recorded more than once, we used the robust variance estimator to estimate the conservative 95% confidence intervals (CIs) by adding a cluster(id) term to the models.48Women who had inserted a levonorgestrel intrauterine system before 2005 would have been categorised as non-initiators or, if they were using menopausal hormone therapy, as initiators of “unopposed oestrogen” (transdermal or oral). Consequently, we did a sensitivity analysis with a six year washout period to prevent potential exposure misclassification. We used R (version 4.2.2) for all analyses.
Patient and public involvement
Our study considers an important public health question. As the study is register based, no direct contact with the patients or participants included in the research occurred at any stage. However, interactions with patients and via public media indicate that the lack of knowledge about the risks associated with different types of menopausal hormone therapies leaves women of perimenopausal and postmenopausal age feeling very uncertain about their medication choices. This lack of awareness served as the primary motivation for conducting the study. Additionally, our team includes a gynaecologist who actively works with patients, providing firsthand insights into the clinical aspects of menopause treatment. Several of the co-authors have either personal or close family experiences with menopause, enhancing our study’s depth and relevance through lived experiences. Unfortunately, the lack of funding specifically allocated for patient and public involvement prevented us from engaging patients or the public in setting the research questions, designing the study, or interpreting and writing up the results. To increase awareness of these results to the public, the results will be presented as a press release and shared across social media. Results will also be presented to students and incorporated into the national clinical guidelines for menopausal hormone therapy use.
Results
The eligibility criteria for at least one of the 138 trials were met by 919 614 women, of whom 24 089 had an event recorded during the follow up (two years from baseline). For the disease specific events, 10 360 (43.0%) women had an ischaemic heart disease event, 4098 (17.0%) had a cerebral infarction event, 4312 (17.9%) had a myocardial infarction event, and 9196 (38.2%) had a venous thromboembolic event. Between 2007 and 2020, a significant reduction of more than 50% in the incidence of cardiovascular disease occurred among women aged 50-58 years. The most substantial decline was for ischaemic heart disease, which dropped from 22.2 per 10 000 women in 2007 to 11.8 in 2020. The incidence of venous thromboembolism remained stable during the study period, with an incidence of 14.5 per 10 000 women in 2007 and 13.9 in 2020 (fig 2).
A total of 77 512 women were initiators of any menopausal hormone therapy and 842 102 women were non-initiators. The most frequently initiated form of menopausal hormone therapy was oral combined continuous therapy, making up more than a third of all initiated treatments. Transdermal therapies were the next most common, accounting for more than a fifth of all initiated treatments. During the study period, we saw a 50% increase in the use of transdermal menopausal hormone therapy products and more than a 50% decrease in unopposed oral oestrogen and tibolone (fig 3).
Compared with non-initiators, women who initiated menopausal hormone therapy were slightly younger (mean age 53.0v53.9), had a higher level of education, resided more frequently in urban regions (39.8%v32.3%), and redeemed more prescriptions for heart disease medications (13.5%v10.7%) and hypertension medications (29.7%v27.0%), but a smaller proportion had prescriptions for diabetic medication (5.8%v7.8%). Among those who started menopausal hormone therapy, initiators of oral oestrogen plus levonorgestrel intrauterine system had the lowest mean age (51.8), whereas those using tibolone were the oldest (mean age 53.7). Women who initiated unopposed oral oestrogen tended to have lower education and were more likely to reside in rural regions (table 2). We adjusted for these differences in our analyses.
Intention-to-treat estimates of effect of menopausal hormone therapy on cardiovascular disease
When we pooled the participants across all “trials,” we had 50 732 837 person trials and a total of 385 952 events were recorded during the follow-up. During 154 433 person years, we observed 664 events among initiators of menopausal hormone therapy, with an adjusted incidence rate of 4.37per 1000 person years. The corresponding incidence rate in 101 million person years contributed by non-initiators, during which 385 288 women had a cardiovascular event, was 3.56 per 1000 person years. The higher number of person years in non-initiators is mainly owing to many of them meeting eligibility criteria multiple times and being included in multiple trials. By contrast, initiators will be ineligible from at least the subsequent 25 trials because they had a redeemed prescription for menopausal hormone therapy during the two year washout period.
The estimated hazard ratio of cardiovascular disease among initiators of oral combined therapy, unopposed oral oestrogen, or oral oestrogen plus levonorgestrel intrauterine system did not differ significantly compared with non-initiators, with estimates ranging from 0.93 to 1.13. We observed no increased risk in women using transdermal combined or transdermal unopposed oestrogen menopausal hormone therapy. Initiating tibolone was associated with an increased risk of the composite cardiovascular disease outcome (hazard ratio 1.52, 95% CI 1.11 to 2.08) (fig 4; supplementary table S7). The calculated cumulative risk difference between tibolone initiators and non-initiators over one year was 0.001 (supplementary figure S2A). This means that for every 1000 women who start taking tibolone during a year, one will develop a cardiovascular disease such as ischaemic heart disease, myocardial infarction, or cerebral infarction.
For the individual diseases, initiating oral combined continuous therapy or tibolone was associated with higher risk of ischaemic heart disease, with hazard ratios of 1.21 (95% CI 1.00 to 1.46) and 1.46 (1.00 to 2.14), respectively. The cumulative risk difference compared with non-initiators was 0.0099 and 0.0012, respectively (supplementary figure S2B). This translates to approximately 11 new cases of ischaemic heart disease per 1000 women who start treatment with oral combined continuous therapy or tibolone over one year. Furthermore, we observed an increased risk of venous thromboembolism with several hormone therapies: oral combined continuous (hazard ratio 1.61, 95% CI 1.35 to 1.92), oral combined sequential (2.00, 1.61 to 2.49), unopposed oral oestrogen (1.57, 1.02 to 2.44), and transdermal combined (1.46, 1.09 to 1.95). The cumulative risk differences were 0.0017 for oral combined continuous, 0.0025 for oral combined sequential, 0.0018 for unopposed oral oestrogen, and 0.001 for transdermal combined therapy (supplementary figure S2E). If 1000 women started each of these treatments and were observed for a year, we would expect to see seven new cases of venous thromboembolism across all groups.
Our estimates remained similar when we used the Fine and Gray subdistribution hazard model (supplementary table S8). In our sensitivity analysis, which included a six year washout period, our estimates remained largely unchanged (see supplementary table S9). However, because this approach excluded a significant portion of data, it resulted in wider confidence intervals. Additionally, for some outcomes, the number of events was too low—or in some cases, non-existent—to allow for a meaningful analysis.
Per protocol estimates of effect of menopausal hormone therapy on cardiovascular disease
The estimated intention-to-treat effect is influenced by the participant’s adherence to the treatment strategy allocated at the start of the trial. Thus, intention-to-treat comparisons might underestimate the effect that would have been observed if all participants had fully adhered to their assigned treatments.49In our study, 46.4% of initiators discontinued their assigned treatment within one year and 63.7% discontinued within two years, and 3.1% of non-initiators started taking some systemic menopausal hormone therapy within two years. For this reason, we also did per protocol analyses. We saw 378 478 incident events in never users and 444 in users during 99.5 million person years of observation.
We observed an increase in the risk associated with oral combined continuous therapy (hazard ratio 1.22, 95% CI 1.00 to 1.50). Consistent with our intention-to-treat analysis, women using tibolone had a higher hazard rate of cardiovascular disease (1.81, 1.25 to 2.61).
In the disease specific analysis, the use of oral combined continuous menopausal hormone therapy or tibolone was associated with a higher risk of ischaemic heart disease (hazard ratio 1.27 (95% CI 1.01 to 1.60) and 1.76 (1.14 to 2.70), respectively). In addition, we observed an increased risk of cerebral infarction in association with the use of tibolone; compared with never users; the hazard ratio was 1.97 (1.02 to 3.78). No other menopausal hormone therapy product was significantly associated with an increased risk of ischaemic heart disease or cerebral infarction. Women using tibolone had an increased risk of myocardial infarction (hazard ratio 1.94, 95% CI 1.01 to 3.73). None of the other analysed menopausal hormone therapy products showed a significant association with risk of myocardial infarction. As in our intention-to-treat analysis, women using oral oestrogen combined with progestogen, in either continuous or sequential forms, had an increased risk of venous thromboembolism, with hazard ratios of 1.84 (95% CI 1.50 to 2.25) for continuous use and 2.45 (1.89 to 3.17) for sequential use. Additionally, we observed an increased risk of venous thromboembolism with the use of transdermal combined menopausal hormone therapy, with a hazard ratio of 1.67 (1.16 to 2.41) (fig 4; supplementary table S10). Notably, although associated with cardiovascular disease, tibolone was not linked to an increased risk of venous thromboembolism (hazard ratio 0.76, 95% CI 0.38 to 1.53). This indicates a strong heterogeneity between the different menopausal hormone therapies with regards to the diseases investigated (fig 5).
Discussion
In this Swedish nationwide emulated target trial of menopausal hormone therapy use in women around menopause, we found that starting oral combined continuous therapy or tibolone was associated with an increased risk of cardiovascular disease within the first two years of initiation. In disease specific analyses, the oral combined continuous regimen was associated with increased risks of ischaemic heart disease and venous thromboembolism, and tibolone was linked to an increased risk of ischaemic heart disease, cerebral infarction, and myocardial infarction but not venous thromboembolism (fig 5). We found no effect of transdermal regimens on the risk of all causes of cardiovascular disease (myocardial infarction, cerebral infarction, or ischaemic heart disease). Combined continuous menopausal hormone therapy was the most common regimen, accounting for 37% of overall initiations. However, we observed a clear trend towards increased use of transdermal products and oral oestrogen combined with the levonorgestrel intrauterine system between 2007 and 2020. Also, the use of tibolone, which has not been approved for use in, for example, the US, decreased in Sweden between the same years. This drop is encouraging; however, in 2018, approximately 1000 women initiated tibolone, which is estimated to have caused one stroke or ischaemic heart disease event.
Trends in menopausal hormone therapy use and cardiovascular disease
During the study period, we observed a decrease in the use of menopausal hormone therapy, followed by an increase starting from around 2016. In Sweden, use dropped after key studies were published in 2002-03,192050but it stabilised from 2010 to 2016, a trend also seen in Spain,51Australia,52Korea,53and New Zealand.54Usage rose again after 2017. Updated Swedish guidelines in 2019, further revised in 2021, recommend starting within 10 years of menopause and not after age 60, reflecting new insights into the benefits and risks of menopausal hormone therapy.55This shift, widely discussed among gynaecologists and general practitioners before its official adoption, likely influenced prescribing habits early on. Additionally, we noted a significant decrease of more than 50% in the incidence of cardiovascular disease, particularly ischaemic heart disease. This pattern is noted in other Nordic countries but contrasts with recent reports from the US, where rates of cardiovascular disease are increasing.56Disparities between the two ","Objective: To assess the effect of contemporary menopausal hormone therapy on the risk of cardiovascular disease according to the route of administration and combination of hormones.
Design: Nationwide register based emulated target trial.
Setting: Swedish national registries.
Participants: 919 614 women aged 50-58 between 2007 and 2020 without hormone therapy use in the previous two years, identified from the Swedish population.
Interventions: 138 nested trials were designed, starting each month from July 2007 until December 2018. Using the prescription registry data for that specific month, women who had not used hormone therapy in the previous two years were assigned to one of eight treatment groups: oral combined continuous, oral combined sequential, oral unopposed oestrogen, oral oestrogen with local progestin, tibolone, transdermal combined, transdermal unopposed oestrogen, or non-initiators of menopausal hormone therapy.
Main outcome measures: Hazard ratios with 95% confidence intervals were estimated for venous thromboembolism, as well as for ischaemic heart disease, cerebral infarction, and myocardial infarction separately and as a composite cardiovascular disease outcome. Treatment effects were estimated by contrasting initiators and non-initiators in observational analogues to “intention-to-treat” analyses and continuous users versus never users in “per protocol” analyses.
Results: A total of 77 512 women were initiators of any menopausal hormone therapy and 842 102 women were non-initiators. 24 089 women had an event recorded during the follow-up: 10 360 (43.0%) had an ischaemic heart disease event, 4098 (17.0%) had a cerebral infarction event, 4312 (17.9%) had a myocardial infarction event, and 9196 (38.2%) had a venous thromboembolic event. In intention-to-treat analyses, tibolone was associated with an increased risk of cardiovascular disease (hazard ratio 1.52, 95% confidence interval 1.11 to 2.08) compared with non-initiators. Initiators of tibolone or oral oestrogen-progestin therapy had a higher risk of ischaemic heart disease (1.46 (1.00 to 2.14) and 1.21 (1.00 to 1.46), respectively). A higher risk of venous thromboembolism was observed for oral continuous oestrogen-progestin therapy (1.61, 1.35 to 1.92), sequential therapy (2.00, 1.61 to 2.49), and oestrogen-only therapy (1.57, 1.02 to 2.44). Additional results in per protocol analyses showed that use of tibolone was associated with a higher risk of cerebral infarction (1.97, 1.02 to 3.78) and myocardial infarction (1.94, 1.01 to 3.73).
Conclusions: Use of oral oestrogen-progestin therapy was associated with an increased risk of heart disease and venous thromboembolism, whereas the use of tibolone was associated with an increased risk of ischaemic heart disease, cerebral infarction, and myocardial infarction but not venous thromboembolism. These findings highlight the diverse effects of different hormone combinations and administration methods on the risk of cardiovascular disease.
"
Interventions for the management of long covid,"Introduction
The covid-19 pandemic has affected hundreds of millions of people worldwide, with major consequences for health and economies.12Although most patients recover, evidence suggests that as many as 15% might experience long term health effects from covid-19, including fatigue, myalgia, and impaired cognitive function, called post-covid condition, or long covid.3456789101112The prevalence of long covid is difficult to establish because most symptoms are non-specific, and many studies lack sufficiently rigorous designs to confidently attribute symptoms to covid-19 infection.1314Estimates suggest that at least 65 million people globally experience symptoms that impair their functional and cognitive capacity.1516
The pathophysiology of long covid is uncertain, and investigators have proposed several potential causes, including viral persistence, autoimmunity, “micro-clots,” and psychological mechanisms.17Moreover, the definition of long covid is heterogeneous and might comprise several distinct phenotypes.18
Risk factors for the development of long covid include female sex, greater comorbidity, and patient reported psychological distress.192021Conversely, severity of acute covid-19 infection may not predict long covid, and even patients with mild infections appear to be susceptible.22Symptoms of long covid may persist after acute infection, or they may relapse and remit.23Evidence on the trajectory of long covid is limited, but some studies suggest that many patients experience a reduction in symptoms at one year after acute infection.2425Also, research into the burden of long covid in low and middle income countries is scarce.1626Evidence suggests that patients in these countries currently receive fragmented care, owing to constraints on health resources and competing priorities.16
Considerable resources have been invested to study long covid, including $1bn (£0.8bn; €0.9bn) from the US National Institutes of Health (NIH).27Several trials testing interventions for the management of long covid have been published to date,28293031and hundreds more are planned or are ongoing.3233343536However, these trials will be published faster than evidence users, such as clinicians and patients, can read or interpret them; they could produce conflicting results; and will come with strengths and limitations that might not be immediately apparent.
Healthcare providers are increasingly encountering patients with long covid, and, in the absence of trustworthy and up-to-date summaries of the evidence, patients may receive unproven, costly, and harmful treatments.373839404142Some patients and healthcare providers have questioned the credibility of interventions in published trials, such as exercise and cognitive behavioural therapy (CBT).434445Trustworthy systematic reviews that clarify the benefits and harms of available interventions are critical to promote evidence based care. Therefore, we present the first iteration of a living systematic review of interventions for the management of long covid.
Methods
We submitted our review protocol to MedRxiv in March 2024.46
Eligibility criteria
Eligible studies enrolled adults (≥18 years) with long covid—defined by the World Health Organization (WHO) as symptoms at ≥3 months after laboratory confirmed, probable, or suspected covid-19 infection that persisted for at least two months—and randomised them either to any drug or non-drug intervention, placebo or sham, usual care, or to alternative drug or non-drug interventions, without any restrictions on date or language of publication.23This definition, although broad, is consistent with the most recent definition published by the National Academies of Sciences, Engineering, and Medicine and reflects the limitations in current scientific knowledge about long covid.4748Based on empirical evidence showing that preprints and published reports of randomised trials generally provide consistent results, we included both preprint and published trial reports.49505152
We planned to conduct sensitivity analyses excluding trials that did not report the time since acute covid-19 infection or the duration of long covid symptoms according to WHO criteria. It was not possible to perform these analyses, however, owing to the limited number of trials addressing each class of intervention and outcome.
We excluded trials if ≥20% of patients had recovered from covid-19 less than three months before randomisation; pseudorandomised trials; trials of animals; and trials investigating treatments for acute covid-19 or interventions to prevent long covid.2353Trials were also excluded that targeted patients experiencing only anosmia and hyposmia after covid-19 infection, as these patients likely form a group that is distinct from those with other typical symptoms of long covid (eg, fatigue, pain, shortness of breath, cognitive impairment). Additionally, we excluded randomised trials with fewer than 25 participants in each arm. Smaller trials are unlikely to meaningfully contribute to meta-analyses, more likely to include unrepresentative samples and arms that are prognostically imbalanced, and at higher risk of publication bias.54
Search strategy
We worked with an experienced research librarian to search Medline, Embase, Cochrane Central Register of Controlled Trials, PsycInfo, Allied and Complementary Medicine Database, and CINAHL from inception to December 2023 (see supplement 1). Our search combined terms related to long covid with a filter for randomised trials. In February 2024, we supplemented our search using the Epistemonikos covid-19 Repository—a living catalogue of covid-19 research—and by reviewing the references of relevant systematic reviews and soliciting experts for eligible trials.30335556
Study selection
Following training and calibration exercises to ensure sufficient agreement, pairs of reviewers worked independently and in duplicate to screen the titles and abstracts of search records and subsequently the full texts of articles considered potentially eligible. We used the online systematic review software Covidence (https://www.covidence.org) to assist with screening. Reviewers resolved disagreements by discussion, or, if necessary, adjudication by a third reviewer.
Data extraction
Following training and calibration exercises to ensure sufficient agreement, pairs of reviewers worked independently and in duplicate to collect data from eligible trials using a pilot tested Excel spreadsheet (Microsoft Office Excel 2019). Reviewers resolved disagreements by discussion or by consultation with a third reviewer. A third experienced reviewer checked all consensus data to confirm accuracy.
Reviewers collected data on trial characteristics (eg, trial design, country of origin, funding sources, diagnostic criteria for long covid), patient characteristics (eg, age, sex, employment and education status, receipt of covid-19 vaccination, method of acute covid-19 diagnosis, severity of acute covid-19 infection, duration of long covid symptoms, number of covid-19 infections, long covid symptoms), characteristics of interventions and comparators (eg, type of intervention, treatment duration), and patient important outcomes. Our outcomes of interest were informed by a published core outcome set for long covid5758and discussions with patient partners and clinicians. We included fatigue, pain, post-exertional malaise, changes in education or employment status, cognitive function, mental health, dyspnoea, quality of life, patient reported physical function, recovery or improvement, and serious adverse events (as defined by each trial).5758We extracted data for all instruments used in trials that measured any of our outcomes of interest.
For dichotomous outcomes, reviewers extracted the number of patients and events in each arm, and, for continuous outcomes, the number of patients, a measure of central tendency (mean or median), and a measure of variability (eg, standard deviation, standard error, 95% confidence interval, P value). For continuous outcomes, reviewers prioritised extracting changes in the outcome measure from baseline, and, if not reported, the outcome measure at follow-up.
For each outcome, reviewers preferentially extracted the results from intention-to-treat analyses without imputation for missing data. We extracted results immediately after the end of the intervention and at the longest reported point of follow-up at which randomisation was maintained. Given the relapsing and remitting nature of long covid and the potential for interventions to have long term effects, for crossover trials we only collected data for the first phase of the trial before washout and crossover of patients.
Long covid can comprise several distinct phenotypes, and we anticipated that the effects of interventions might differ based on the predominant symptoms patients experience. Accordingly, based on previous classifications of long covid and the eligibility criteria of trials,185960we categorised trials as including patients with either general symptoms such as fatigability and an impairment in functional capacity to perform routine activities of daily living, primarily respiratory sequelae characterized by dyspnoea, or primarily neurological or cognitive sequelae characterized by cognitive impairments and brain fog.
We also anticipated that the effects of interventions might depend on diagnostic criteria for long covid, severity of acute covid-19 infection, time since infection, number of infections, vaccination status, and SARS-CoV-2 variant.19When reported, we extracted stratified data based on these factors for subgroup analyses.
In response to growing concerns about untrustworthy trial publications,6162reviewers applied the trustworthiness in randomised controlled trials (TRACT) checklist to assess each trial for signs of data fabrication, data falsification (manipulation of data or results), and errors in the conduct of the trial or analysis of data that could undermine the conclusions, such as confusing standard errors with standard deviations and misclassification of intervention and control groups.63This checklist includes 19 items in seven domains: governance, author group, plausibility of intervention, timeframe, dropouts, baseline characteristics, and outcomes. The checklist does not include a cut-off at which a trial is considered suspicious, and experience in applying the checklist to systematic reviews is currently limited. Therefore, the core authorship group reviewed all trials flagged as having potential concerns in one or more domain and identified those they considered untrustworthy by consensus.
Risk of bias assessments
Following training and calibration, reviewers worked independently and in duplicate to assess risk of bias of eligible trials using a modified version of the Cochrane endorsed risk of bias 2.0 tool.64This instrument assesses the risk of bias across five domains: bias due to randomisation, bias due to deviations from the intended intervention, bias due to missing outcome data, bias due to measurement of the outcome, and selective outcome reporting.
The risk of bias 2.0 tool necessitates that reviewers distinguish between whether they are interested in the effect of assignment or adherence to the intervention. We assessed the risk of bias of the effect of assignment rather than adherence to the intervention because this effect is likely to be observed in clinical settings.
Our modified version of the tool includes the same domains as the original risk of bias 2.0 tool, but with revised response options (ie, definitely low risk of bias, probably low risk of bias, probably high risk of bias, and definitely high risk of bias) and guidance tailored to issues relevant for the present review. Specifically, we removed guidance for assessing risk of bias of adhering to the intervention and listed important cointerventions that may be imbalanced between trial arms for consideration in making judgements about deviations from the intended intervention (eg, activity management, physical activity, social engagement).
We considered trials without blinding of patients, healthcare providers, and investigators at high risk of bias owing to deviations from intended intervention and measurement of outcome. An exception was made for trials that compared two or more interventions that were matched for level of interaction between trial participants and healthcare providers.6566Patients might expect interventions with higher levels of interaction to be more effective, potentially influencing their perception of outcomes and their likelihood of pursuing additional beneficial activities. When interventions are matched for interaction, patients are less likely to have strong preconceptions about their comparative effectiveness.
Information reported in published trial protocols or trial registrations formed the basis of our judgements about selective reporting. Reviewers resolved disagreements by discussion, or consultation with a third reviewer when necessary.
Data synthesis and analysis
We used descriptive characteristics to describe trials and participants. Means, medians, and associated measures of variability (eg, 95% confidence intervals (CIs), interquartile ranges (IQRs)) were used for continuous variables, whereas counts and proportions were used for dichotomous and categorical variables.
Although we intended to perform network meta-analyses to summarise the comparative efficacy and harms of interventions, the available evidence was too sparse. In situations in which network meta-analysis is not possible, we had planned to perform frequentist random effects pairwise meta-analyses with the restricted maximum likelihood heterogeneity estimator.6768Overall, the diversity in interventions and outcome measures precluded meta-analyses. Therefore, for most comparisons and outcome measures, we describe the results of individual trials.
The number of participants and events were used to calculate relative risks for dichotomous outcomes, except for serious adverse events, when risk differences were calculated owing to the propensity for trials to report 0 events for control arms. We used the number of participants and mean change or mean end scores to calculate mean differences for continuous outcomes.69Based on evidence suggesting that the two methods are comparable for randomised trials, we did not calculate mean change scores from baseline for trials that reported outcome measures at end of follow-up.69
To enhance interpretation, reviewers may convert effects measured by different instruments assessing the same construct into a commonly used or familiar instrument.7071We avoided converting effects across instruments owing to potential differences in the range of constructs covered by each instrument. We also avoided standardised mean differences as they can be influenced by differences in variability across trial populations.71Although we intended to test for small study effects for analyses that included ≥10 trials, too few trials were available across all comparisons.7273
To enhance interpretability of results, we transformed relative risks to absolute effects (number of patients with the outcome per 1000 patients), using the control group event rate as the baseline risk.74We performed all analyses using themetapackage, version 4.1.2, in R (Vienna, Austria). All data and code to reproduce our results are freely accessible on Open Science Framework (https://osf.io/9h7zm/).
Subgroup and sensitivity analysis
To explain potential heterogeneity in results across trials, we generated seven a priori factors: diagnostic criteria for long covid, time since infection, number of infections, vaccination status, severity of acute covid-19, SARS-CoV-2 variant, and predominant symptoms experienced by patients .1975We also intended to avoid indiscriminately pooling trials rated at low and high risk of bias. We planned to test for differences between the results of trials at low and high risk of bias, and, if important differences were detected, to rely only on trials at low risk of bias. However, we did not identify sufficient evidence to perform any subgroup analyses. As more evidence accumulates from trials, we intend to perform future subgroup analyses to investigate these factors.
Certainty of evidence
We used the Grading of Recommendations Assessment, Development and Evaluation (GRADE) approach to assess the certainty (quality) of evidence.76This approach rates the certainty of evidence as high, moderate, low, or very low certainty based on considerations of risk of bias (study limitations), inconsistency (heterogeneity in results across trials), indirectness (differences between questions addressed in studies and the question of interest), publication bias (tendency for studies with positive results to be published, published faster, or published in journals with higher visibility), and imprecision (random error). High or moderate certainty evidence indicates confidence that the estimated effect represents the true effect, and low or very low certainty evidence indicates the estimated effect may be substantially different from the true effect.
To enable imprecision to be judged, we considered whether effect estimates met or exceeded the minimal important difference (MID)—the smallest difference in an outcome that patients find important.77When the point estimate met or exceeded the MID, we rated the certainty of there being an important effect. Conversely, when the point estimate was between the MID and the null, we rated the certainty of there being no important effect. We anticipate that decision makers will further contextualise our judgements about the certainty of evidence to make decisions or formulate guideline recommendations.7879
After discussion with coauthors and patient partners, we considered a risk difference of 50 per 1000 patients as the MID for the outcome of important improvement and recovery, and a risk difference of 20 per 10000 patients as minimally important for serious adverse events. To source MIDs for other patient reported outcomes from published studies, we performed pragmatic searches of Google Scholar using terms related to MIDs and the measure of interest.
MIDs of patient reported outcomes are determined using either anchor based methods or distribution based methods.80Anchor based methods rely on an external “anchor” to interpret the magnitude of change in a measure or outcome. Conversely, distribution based methods rely on the distribution of the data to interpret the importance of change in a measure. We prioritised anchor based MID estimates over distribution based MID estimates, because they better reflect patients’ direct experiences.8182
The MID of an instrument depends on the patient’s condition and the intervention being studied.83We were unable to identify any MIDs specific to long covid. Instead, we prioritised MIDs for patients with other chronic health conditions. When it was not possible to identify an MID, we used distribution based MIDs, defined as 0.5 standard deviations of the measure at baseline.84When several candidate MIDs or a range of MIDs were identified, we used the median MID or the MID we considered most trustworthy according to established criteria.85Supplement 2 lists MIDs that guided our judgements related to imprecision.
Reporting
We report our systematic review according to the PRISMA (preferred reporting items for systematic reviews and meta-analyses) checklist.86PRISMA flow diagrams illustrate the total number of search records, the number of records excluded, reasons for exclusion, and the total number of trials included in our review. GRADE Evidence Profiles summarise effect estimates and the associated certainty of evidence for each intervention.74
We describe our results using GRADE plain language summaries—that is, describing high certainty evidence with declarative statements, moderate certainty evidence with “probably,” low certainty with “may,” and very low with “very uncertain.”87In reporting results, we focus primarily on interventions with moderate to high certainty evidence.
Patient and public involvement
The Long Covid Web Patient Advisory Council (https://www.longcovidweb.ca/) reviewed and offered feedback on our protocol. Furthermore, we engaged an individual with lived experience as a member of our study team, who provided feedback on our protocol and interpretation of findings. Patient perspectives guided the prioritisation of outcomes, the selection of MIDs, the interpretation of evidence, and the development of clear, easily understandable ways to communicate results.
Results
Study and patient characteristics
Overall, we identified 24 unique trials with 3695 patients. We also identified 239 registered trials that were ongoing or had been completed but the results not yet published (fig 1).
Four trials (n=708 patients) investigated drug interventions,88899091eight (n=985) physical activity or rehabilitation,9293949596979899three (n=314) behavioural,100101102four (n=794) dietary,28103104105four (n=309) medical devices and technologies,106107108109and one (n=585) a combination of physical exercise and mental health rehabilitation.110
Two trials engaged patients in the design of the intervention and trial protocol.100110All trials were available as publications in peer reviewed journals or were deposited as preprints and subsequently published in peer reviewed journals. Trials were predominantly conducted in the Americas or Europe, with funding from government sources or no funding, and they were typically published in either 2022 or 2023. The median number of patients randomised among trials was 100 (IQR 60-153), and outcomes were reported at up to one year of follow-up.
Two trials (n=523 patients) reported on patients with neurological or cognitive symptoms,89108and three trials (n=401) on patients with respiratory symptoms.929397The remaining 19 trials reported on patients with general symptoms of long covid. These symptoms included fatigue,96100105106107reduced functional capacity,94100106and one or more of a range of different symptoms typically including general fatigue and lethargy.88103104Seven trials did not report the specific symptoms experienced by patients.2890919899101102111
More than half of patients had a reported history of laboratory confirmed SARS-CoV-2 infection, and about a third were admitted to hospital with severe covid-19. Vaccination status was only reported in three trials, in which most patients were fully vaccinated28100103(table 1, also see supplement 3).
We identified six trials (25%) with concerns about the integrity of the results or trial execution.89929596104106These problems included retrospective trial registration; improbably large benefits; unusually small variability in baseline characteristics or outcome data, or both; and highly similar trial arms that were unlikely considering differences that could arise naturally through randomisation.
Risk of bias
Figure 2presents the risk of bias for trials that reported on drug interventions. Supplements 4-9 present the risk of bias of non-drug interventions. About half of all results were rated at high risk of bias, primarily because of concerns about imbalances in potential co-interventions and expectancy effects due to lack of blinding and comparisons with control interventions not matched for degree of interaction between patients and healthcare providers.
Summary of findings
Figure 3presents the summary of findings of drug, physical activity and rehabilitation, and behavioural interventions.Figure 4presents the summary of findings of dietary interventions and supplements, medical devices and technologies, and combination treatments.
Four trials (708 patients) investigated drug interventions for general symptoms of long covid.88899091Only one of these trials reported on vortioxetine, a Food and Drug Administration approved drug.90Other drugs investigated included leronlimab (a monoclonal antibody that binds to C-C chemokine receptor 5),88112113glucosaminyl muramyl dipeptide (called Licopid),91and actovegin (derived from ultrafiltered calf blood).89Supplements 10-13 present GRADE summary of findings tables for drug interventions.
High certainty evidence shows that vortioxetine treatment for eight weeks does not improve cognitive function, and moderate certainty evidence suggests that vortioxetine probably has little or no effect on depressive symptoms and quality of life.
Other interventions were supported by low or very low certainty evidence or by trials with issues that raised concerns about their integrity.
Eight trials (n=985 patients) investigated physical activity or rehabilitation interventions.9293949596979899Supplements 14-19 present GRADE summary of findings tables.
Two trials (n=209 patients) compared rehabilitation programmes involving physical activity against usual care or general education about covid-19 and activities of daily living.9899Physical activity programmes involved two or three 60 minute exercise sessions incorporating aerobic exercise and strength training for 12 or 15 weeks, one of which was delivered online.9899These trials did not report on our outcomes of interest.9899
Moderate certainty evidence from one trial (n=110 patients) suggests that intermittent aerobic exercise 3-5 times weekly for 4-6 weeks probably improves physical function compared with continuous exercise (mean difference 3.8, 95% CI 1.12 to 6.48); SF-36 physical component score; range 0-100; higher scores indicate less impairment).94
Other trials compared a programme of multicomponent exercise of increasing intensity combined with physiotherapy against physiotherapy alone,97low versus high intensity aerobic and strength training,95a programme of in-patient rehabilitation combined with acupuncture against in-patient rehabilitation alone,96inspiratory muscle training (a form of respiratory training to strengthen the muscles involved in inhalation) against usual care,93and a combination of physiotherapy and active cycle of breathing (breathing exercises intended to improve dyspnoea) against physiotherapy alone.92The effects of these interventions were supported by only low or very low certainty evidence.
Three trials (n=314 patients) investigated behavioural interventions.100101102111Supplements 20-22 present GRADE summary of findings tables.
One trial (n=114 patients) of general long covid symptoms, compared a 17 week online CBT programme called “fit after covid” versus usual care. The programme was developed based on existing CBT protocols for severe fatigue in long term medical conditions, with the option for trained psychologists to deliver the programme in-person for those who were unable or unwilling to use the internet based format.100The programme addressed disruptive sleep-wake patterns, unhelpful beliefs about fatigue, low activity level, social support, fears and worries about covid-19, and poor pain coping mechanisms.100
Moderate certainty evidence suggested that CBT probably reduces fatigue (mean difference −8.4, 95% CI −13.11 to −3.69; Checklist for Individual Strength fatigue subscale; range 8-56; higher scores indicate greater impairment) and probably improves concentration (mean difference −5.2, −7.97 to −2.43; Checklist for Individual Strength concentration problems subscale; range 5-35; higher scores indicate greater impairment).
Other trials investigated an educational mobile application, called ReCOVery, that included modules advising patients on diet, sleep, and exercise101111and amygdala and insula retraining—a programme involving neuroplasticity, mindfulness based meditation, alternate nostril breathing, and other lifestyle related treatments.102These interventions were supported by only low or very low certainty evidence.
Four trials (n=794 patients) investigated dietary supplements.28103104105These trials investigated a formulation of probiotics and prebiotics (synbiotics) called SIM01,103coenzyme Q10,28L-arginine and liposomal vitamin C,105and a combination of trimethyl hydrazinium propionate and ethyl methyl hydroxy pyridine succinate (Brainmax)104against placebo. Supplements 23-26 present GRADE summary of findings tables.
According to low certainty evidence from one trial (n=463 patients), a formulation of synbiotics (SIM01) might alleviate fatigue (200 more per 1000 patients, 95% CI 94 more to 336 more), improve concentration (239 more per 1000 patients, 112 more to 401 more), and improve dyspnoea (150 more per 1000 patients, 27 more to 290 more). Moderate certainty evidence, however, suggested that SIM01 probably does not improve quality of life.103
We judged results for alleviation of symptoms in the trial addressing the effects of SIM01 to be at high risk of bias due to selective reporting.103Although early versions of the trial registration include long covid symptoms as a secondary outcome, the methods and criteria for ascertaining alleviation of these symptoms were not described.103After the trial concluded, the trial registration was modified to include additional details on methods for ascertaining symptom alleviation, and this outcome was reclassified as the primary outcome.103We also rated down the certainty of evidence as the trial reported a large effect on fatigue, concentration, and dyspnoea, and other symptoms such as hair loss, for which there is no plausible mechanism of action. Furthermore, this formulation of synbiotics, SIM01, has not been independently tested or shown to be effective for long covid or other conditions, except by its named innovators and patent holders.
Moderate certainty evidence from one trial (n=119 patients) suggests that coenzyme Q10, administered at 500 mg/day for six weeks, probably does not improve quality of life.
Other interventions were supported by only low or very low certainty evidence or by trials with concerns about their integrity.
Four trials (n=309 patients) investigated medical devices and technologies, including hyperbaric oxygen, active high definition transcranial direct current stimulation, photobiomodulation, and active hydrogen therapy.106107108109Supplements 27-29 present GRADE summary of findings tables.
All interventions were supported only by low or very low certainty evidence, or by trials with concerns about their integrity.
One trial (n=585 patients) in patients with general long covid symptoms and a history of severe covid-19, evaluated a combined physical and mental health rehabilitation programme versus usual care (single session of online advice and support).110This intervention was delivered online over eight weeks by exercise physiologists, physiotherapists, and health psychologists and consisted of weekly live, supervised, group exercise and psychological support sessions that focused on motivation, fear avoidance, managing emotions, fatigue, and stress and anxiety.110Supplement 30 presents the GRADE summary of findings table.
Moderate certainty evidence suggested that a combined programme of physical and mental health rehabilitation probably increases the proportion of patients who experience recovery or important improvements (161 more per 1000 patients, 95% CI 61 more to 292 more) and probably improves quality of life (mean difference 0.04, 95% CI 0.00 to 0.08; PROMIS 29+2 Profile v2.1; range −0.022-1; higher scores indicate less impairment) versus providing one session of advice and support. Moderate certainty evidence also suggested that physical and mental health rehabilitation probably has little or no effect on physical and cognitive function. Moderate certainty evidence suggested that physical and mental health rehabilitation probably reduces symptoms of depression but may have little or no effect on symptoms of anxiety. No compelling evidence of benefit on fatigue, pain, or dyspnoea was found. We are very uncertain of the effects of the programme on serious adverse events.
Discussion
Our systematic review and meta-analysis of 24 trials comprising 3695 patients with long covid identified moderate certainty evidence that an online CBT programme probably improves fatigue and concentration, and a programme of physical and mental health rehabilitation probably increases the proportion of patients who experience recovery or important improvements. We also found moderate certainty evidence suggesting that intermittent aerobic exercise probably improves physical function compared with continuous exercise. Effects of these interventions were modest, just reaching the MID for most outcomes.110
We did not find compelling evidence to support the effectiveness of other interventions, including, among others, vortioxetine, leron","Objective: To compare the effectiveness of interventions for the management of long covid (post-covid condition).
Design: Living systematic review.
Data sources: Medline, Embase, CINAHL, PsycInfo, Allied and Complementary Medicine Database, and Cochrane Central Register of Controlled Trials from inception to December 2023.
Eligibility criteria: Trials that randomised adults (≥18 years) with long covid to drug or non-drug interventions, placebo or sham, or usual care.
Results: 24 trials with 3695 patients were eligible. Four trials (n=708 patients) investigated drug interventions, eight (n=985) physical activity or rehabilitation, three (n=314) behavioural, four (n=794) dietary, four (n=309) medical devices and technologies, and one (n=585) a combination of physical exercise and mental health rehabilitation. Moderate certainty evidence suggested that, compared with usual care, an online programme of cognitive behavioural therapy (CBT) probably reduces fatigue (mean difference −8.4, 95% confidence interval (CI) −13.11 to −3.69; Checklist for Individual Strength fatigue subscale; range 8-56, higher scores indicate greater impairment) and probably improves concentration (mean difference −5.2, −7.97 to −2.43; Checklist for Individual Strength concentration problems subscale; range 4-28; higher scores indicate greater impairment). Moderate certainty evidence suggested that, compared with usual care, an online, supervised, combined physical and mental health rehabilitation programme probably leads to improvement in overall health, with an estimated 161 more patients per 1000 (95% CI 61 more to 292 more) experiencing meaningful improvement or recovery, probably reduces symptoms of depression (mean difference −1.50, −2.41 to −0.59; Hospital Anxiety and Depression Scale depression subscale; range 0-21; higher scores indicate greater impairment), and probably improves quality of life (0.04, 95% CI 0.00 to 0.08; Patient-Reported Outcomes Measurement Information System 29+2 Profile; range −0.022-1; higher scores indicate less impairment). Moderate certainty evidence suggested that intermittent aerobic exercise 3-5 times weekly for 4-6 weeks probably improves physical function compared with continuous exercise (mean difference 3.8, 1.12 to 6.48; SF-36 physical component summary score; range 0-100; higher scores indicate less impairment). No compelling evidence was found to support the effectiveness of other interventions, including, among others, vortioxetine, leronlimab, combined probiotics-prebiotics, coenzyme Q10, amygdala and insula retraining, combined L-arginine and vitamin C, inspiratory muscle training, transcranial direct current stimulation, hyperbaric oxygen, a mobile application providing education on long covid.
Conclusion: Moderate certainty evidence suggests that CBT and physical and mental health rehabilitation probably improve symptoms of long covid.
Systematic review registration: Open Science Frameworkhttps://osf.io/9h7zm/.
Readers’ note: This article is a living systematic review that will be updated to reflect emerging evidence. Updates may occur for up to two years from the date of original publication.
"
Endometriosis and uterine fibroids and risk of premature mortality,"Introduction
Two decades have passed since the United Nations launched the sustainable development goal of reducing premature mortality from non-communicable diseases by one third by 2030.1However, all countries are still facing a huge burden of premature deaths from these diseases.2The absolute number of deaths from non-communicable diseases between the ages of 30 and 69 years was estimated to be 15.6 million in 2019, accounting for 76% of overall premature deaths (20.4 million).34With the 2030 sustainable development goal target less than a decade away,1identifying the risk factors of premature deaths from non-communicable diseases, particularly cancer, cardiovascular diseases, and respiratory diseases,1is urgently needed to curb the growing burden of these diseases. Besides traditional risk factors affecting men and women, such as smoking, an unhealthy diet, and overweight or obesity,5growing evidence shows that reproductive traits unique to women, such as menstrual cycle characteristics, gestational diabetes, gestational hypertension, and pregnancy loss, are associated with premature non-communicable disease mortality.67891011
Endometriosis and uterine fibroids are common disorders among reproductive aged women, with a clinically relevant prevalence of 10% and 15-30%, respectively.1213Endometriosis is characterized by growth outside the uterus of tissue resembling the endometrium. In contrast, uterine fibroids are non-malignant neoplasms made up of smooth muscle cells, typically growing within or around the myometrium. However, endometriosis and uterine fibroids share common genetic origins,14and their development involves interacting endocrine, immunological, and proinflammatory processes.1213Previous evidence shows that endometriosis and uterine fibroids are associated with a greater risk of cardiovascular disease,15hypertension,1617and cancer,181920suggesting a potential contribution to premature mortality.21Although a few case-control and disease registry studies have explored the associations between endometriosis and uterine fibroids and the subsequent risk of total or cause specific mortality,222324these associations have not been examined in prospective cohort studies with careful control of potential confounding factors. Additionally, the influence of the co-occurrence of endometriosis and uterine fibroids needs to be assessed. More importantly, the potential modification effect of behavioral factors, hormone replacement therapy, hysterectomy or oophorectomy history, oral contraceptive use, and history of infertility, which is critical for improving preventive interventions, is also unclear. Therefore, we investigated the effect of endometriosis and uterine fibroids on the long term risk of premature mortality among women from the Nurses’ Health Study II (NHSII) in the United States, who have been followed biennially for three decades.
Methods
Study design
Our study was conducted within the NHSII, an ongoing prospective cohort started in 1989 by recruiting 116 429 US female nurses aged 25-42 years.25Participants were followed biennially using mailed or electronic questionnaires that collected detailed data on reproductive traits, behavioral factors, and health status, with response rates in each follow-up cycle exceeding 90%. After excluding women who reported a history of physician diagnosed cardiovascular disease or cancer before enrollment (n=3067), had a hysterectomy before the diagnosis of endometriosis or uterine fibroids (n=3620), or reported endometriosis never confirmed by laparoscopy or uterine fibroids never confirmed by ultrasound or hysterectomy (n=349), 110 091 women were included in the present study (figure S1). The study protocol was approved by the institutional review boards of the Brigham and Women’s Hospital and Harvard T.H. Chan School of Public Health. Return of questionnaires indicated informed consent.
Determining endometriosis and uterine fibroids
Starting in 1993 and biennially thereafter (figure S1), participants reported whether they ever had physician diagnosed endometriosis and uterine fibroids (uterine fibroids were updated until 2009 when most women had reached menopause). Participants who responded yes also reported the date of diagnosis and whether endometriosis and uterine fibroids were confirmed by laparoscopy, ultrasound, or hysterectomy. In 1993, the diagnosis date was reported as before September 1989, from September 1989 to May 1991, from June 1991 to May 1993, and after May 1993; these dates were used to identify the status of endometriosis and uterine fibroids in 1989 and 1991. Self-reported endometriosis was validated in 1994 (n=200) and 2011 (n=711), and the diagnosis of endometriosis was confirmed in the medical records of 95-100% of NHSII women reporting laparoscopically confirmed endometriosis, but in only 56% (15 of 27) of women without laparoscopic confirmation.26In another subset of randomly selected NHSII participants who permitted a review of their medical records, the diagnosis of uterine fibroids was confirmed in 93% (108 of 116) of women reporting a diagnosis by ultrasound or hysterectomy.27Therefore, in primary analyses we defined exposure based on laparoscopically confirmed endometriosis and ultrasound or hysterectomy confirmed uterine fibroids. Participants reporting endometriosis and fibroids that had not been clinically confirmed were not included in the analysis until the conditions were verified by laparoscopy, ultrasound, or hysterectomy in later follow-up cycles.
Adult height, race or ethnicity, age at menarche, weight at age 18 years, and menstrual characteristics at ages 18-22 years were collected at baseline. Information on current weight, cigarette consumption, reproductive characteristics (eg, infertility history, pregnancy loss, and menopause status), night shiftwork duration, hormone replacement therapy, regular intake of aspirin, oral contraceptives, and non-aspirin non-steroidal anti-inflammatory drugs (NSAIDs, such as Aleve, Naprosyn, Relafen, ketoprofen, ibuprofen, and Anaprox; two or more times a week), and other health related factors (eg, hysterectomy and oophorectomy) were reported biennially since 1989. We calculated body mass index by dividing body weight (kilograms) by height (meters squared). Dietary intake, including alcohol consumption, was assessed every four years since 1991 using a validated semiquantitative food frequency questionnaire.2829We computed the Alternate Healthy Eating Index 2010 score to reflect participants’ overall diet quality (0-110; a higher score indicates a healthier diet).30Participants reported the average time of physical activities at baseline and every four years thereafter. We estimated the weekly hours spent on moderate to vigorous activities (eg, running, bicycling, and swimming).11Self-reported body weight, behavioral factors, and reproductive characteristics have been mostly validated among participants from this cohort or the original Nurses’ Health Study.7293132333435363738
Determining premature mortality
We performed systematic searches of deaths for all participants from state vital statistics records and the National Death Index, supplemented by reports from next of kin or the postal authorities; these records were able to correctly identify more than 98% of deaths.39Death causes were determined based on medical record review, autopsy reports, or death certificates, and then reviewed by a physician according to the eighth and ninth revisions of the international classification of diseases (table S1). We classified causes of death into 26 major categories according to the US Public Health Service, National Center for Health Statistics, and the Statistics Netherlands’ Database.4041Deaths younger than 70 years were defined as premature mortality.42
Statistical analysis
Person years of follow-up were calculated from the return date of the 1989 questionnaires until the end of follow-up (30 June 2019) or death, whichever occurred first (figure S1). Exposure status has been updated biennially since 1989 (uterine fibroids were updated until 2009). Participants were considered to have endometriosis or fibroids from the midpoint between the receipt of the previous questionnaire and the date on which the questionnaire with the first report was received. Therefore, a woman who did not have endometriosis or fibroids at recruitment and later developed one of these conditions contributed to exposure and non-exposure person years of follow-up. We estimated the hazard ratios and 95% confidence intervals for total and cause specific premature mortality according to visually confirmed endometriosis and uterine fibroids using Cox proportional hazard models. We also examined premature mortality risk according to the joint categories of endometriosis and uterine fibroids. To address potential confounding by age, calendar time, and their possible interactions, all Cox proportional hazard models were jointly stratified by age in months at the start of follow-up and the calendar year for the present survey cycle.9The proportional assumption was tested by comparing models with and without multiplicative interaction terms between endometriosis or uterine fibroids and calendar time using the likelihood ratio test.43Potential confounders were selected a priori based on previous findings of factors associated with premature mortality, endometriosis, and uterine fibroids and then determined by following the guidance from “Evidence synthesis for constructing directed acyclic graphs” (text S1 and figure S2). In the primary models, we adjusted for age, body mass index at age 18 years, menstrual cycle length at age 18-22 years, age at menarche, and time varying hormone replacement therapy and regular intake of non-aspirin NSAIDs, aspirin, and oral contraceptives. In the final models, we further adjusted for time varying body mass index, smoking status, physical activity, and Alternate Healthy Eating Index 2010 diet quality scores. To efficiently handle time varying covariates, the Anderson-Gill data structure was used to create new data records for each follow-up cycle at which participants were at risk, with covariates set to values at the time when follow-up questionnaires were received.44Covariates with missing values at a given questionnaire cycle (mostly <5%;table 1) were carried forward using the most recent data; otherwise, missing indicators were created.45
To assess the competing risk across different death causes, competing risk Cox proportional hazards regression models were constructed to analyze the associations of endometriosis and uterine fibroids with cause specific mortality.1046Given that unhealthy behavioral factors are strong risk factors of mortality,4647we examined the effect modification for body mass index, diet quality, physical activity, and smoking status by classifying participants into low and high risk groups based on previous findings.4347We also explored the effect modification for race or ethnicity, nulliparity, NSAID use, aspirin use, oral contraceptive use, spontaneous abortion, long or irregular menstrual cycles, postmenopausal hormone therapy, history of infertility, hysterectomy, and oophorectomy. Likelihood ratio tests were conducted to assess the multiplicative interaction between endometriosis and uterine fibroids and these stratification variables. Several sensitivity analyses were conducted.
We excluded participants without any follow-up questionnaires to evaluate potential bias resulting from loss of follow-up (n=1271).
In the analysis for endometriosis, we excluded women from the comparison group with a diagnosis of uterine fibroids (n=3995). Similarly, in the analysis for uterine fibroids, we excluded women with a history of endometriosis to assess if our findings were biased by the inclusion of the other disorder in the comparison group (n=4000).
To assess potential selection bias, we redefined mortality as deaths younger than 65 years (3646 deaths) or at any age (4480 deaths).48
We adjusted for the duration of rotating night shift work to assess the influence of night work.
We used baseline drug intake and behavioral factors to examine whether adjusting for time varying covariates affected the results.
We used the Markov chain Monte Carlo method of multiple imputations procedure to replace covariates with missing values to test the robustness of the carry forward method.
To minimize the risk of misdiagnosis between endometriosis or uterine fibroids and other neoplasms, we excluded women who died within five years of receiving a diagnosis of endometriosis or uterine fibroids (73 deaths).
We additionally adjusted for race or ethnicity, which could reflect the multigenerational and sociohistorical effects of racism and discrimination.49
Finally, we redefined endometriosis and uterine fibroids as all self-reported with and without confirmation by laparoscopy, ultrasound, or hysterectomy.
All data were analyzed using SAS 9.4 for UNIX (SAS Institute Inc., Cary, NC, USA). P values were false discovery rate adjusted when several tests were conducted simultaneously.50To test potential unmeasured or uncontrolled confounding, we calculated E values using the publicly available online E value calculator.5152
Patient and public involvement
No patients were involved in the initial design and implementation of the study because patient and public involvement was not common when the NHSII cohort was established. However, participants have offered valuable suggestions and comments throughout follow-up surveys, which have been integrated whenever possible. Additionally, we have taken into account suggestions and comments from an internal review panel comprising members of the public, as well as an advisory board consisting of nursing leaders. Dissemination to the public will include conference presentations, press releases, and plain language summaries shared on social media platforms.
Results
Mean age of participants in 1989 and 2019 was 34.7±4.7 and 64.4±4.7 years, respectively. At baseline, women who reported laparoscopically confirmed endometriosis had a higher prevalence of infertility (age standardized percentages (unstandardized numerators and denominators): 52% (2839 of 5185)v16% (17 059 of 104 906)), hysterectomy (21% (1276 of 5185)v1% (1413 of 104 906)), and oophorectomy (17% (1011 of 5185)v1% (668 of 104 906)), and were more likely to use non-aspirin NSAIDs (28% (1410 of 5185)v19% (19 522 of 104 906)), and postmenopausal hormone therapy (32% (1744 of 5185)v9% (9263 of 104 906)) compared with those without endometriosis. Similarly, slightly higher prevalences of infertility (27% (1462 of 5081)v18% (18 436 of 105 010)), hysterectomy (20% (1372 of 5081)v1% (1317 of 105 010)), and oophorectomy (10% (671 of 5081)v1% (1008 of 105 010)), non-aspirin NSAID intake (24% (1188 of 5081))v19% (19 744 of 105 010)), and postmenopausal hormone therapy (21% (1185 of 5081)v9% (9822 of 105 010)) at baseline were observed among women reporting ultrasound or hysterectomy confirmed uterine fibroids than those without fibroids (table 1).
During 2 994 354 person years of follow-up (27.2 years per person), 11.0% (12 195 of 110 091) of women reported laparoscopically confirmed endometriosis (fig 1), and 19.6% (21 590 of 110 091) of women reported ultrasound or hysterectomy confirmed uterine fibroids at baseline or during follow-up (fig 2). In total, we documented 4356 premature deaths, including 1459 from cancer, 304 from cardiovascular diseases, and 90 from respiratory diseases (table S1). The crude incidence of all cause, premature mortality for women with and without laparoscopically confirmed endometriosis was 2.01 and 1.40 per 1000 person years, respectively. In age adjusted models, laparoscopically confirmed endometriosis was associated with a hazard ratio of 1.19 (95% confidence interval 1.09 to 1.30) for premature death (fig 1). These associations became stronger after also adjusting for potential confounders, including behavioral factors (1.31, 1.20 to 1.44;fig 1). Cause specific mortality analyses revealed that laparoscopically confirmed endometriosis was associated with a greater risk of mortality from cancer (1.22, 1.04 to 1.44) and respiratory diseases (1.95, 1.11 to 3.41;fig 1). With further in-depth analyses examining causes of death (only including categories with more than 50 deaths), laparoscopically confirmed endometriosis was associated with a greater risk of mortality caused by senility and ill-defined diseases (1.80, 1.19 to 2.73), diseases of the nervous system and sense organs (2.50, 1.40 to 4.44), and malignant neoplasm of gynecological organs (2.76, 1.79 to 4.26;fig 3). Uterine fibroids were unrelated to all cause premature mortality (1.03, 0.95 to 1.11;fig 2). However, we observed an increased cancer mortality risk among women reporting ultrasound or hysterectomy confirmed uterine fibroids (1.22, 1.07 to 1.39;fig 2), primarily driven by malignant neoplasm of gynecological organs (2.32, 1.59 to 3.40;fig 4).
When we jointly categorized participants by occurrence of endometriosis and uterine fibroids (table S2), we found an increased risk of total mortality among women reporting endometriosis only (1.32, 1.19 to 1.47) and those reporting both endometriosis and uterine fibroids (1.31, 1.12 to 1.53). In cause specific mortality analyses, similar cancer mortality risks were observed among women reporting endometriosis only (1.34, 1.10 to 1.62), uterine fibroids only (1.28, 1.11 to 1.47), and both conditions (1.20, 0.90 to 1.61). The adjusted hazard ratio of cardiovascular disease mortality among women reporting both endometriosis and uterine fibroids was 1.61 (0.93 to 2.76), 0.93 (0.57 to 1.51) among women reporting endometriosis only, and 1.07 (0.78 to 1.48) among those with uterine fibroids only. Additionally, an increased risk of respiratory disease mortality was exclusively observed among women reporting endometriosis only (2.21, 1.19 to 4.10).
Given the limited numbers of deaths from cardiovascular diseases and respiratory diseases, stratified analyses were only conducted for total and cancer mortality. We found no convincing effect modification by race or ethnicity, diet quality, cigarette smoking status, body mass index, physical activity, postmenopausal hormone therapy, history of infertility, oophorectomy, aspirin use, non-aspirin NSAID use, spontaneous abortion history, long or irregular menstrual cycles, and oral contraceptive use (all P for interaction (false discovery rate adjusted) >0.10;table 2and table S3). However, we found that uterine fibroids were associated with a lower risk of total and cancer premature mortality among women with hysterectomy (P for interaction (false discovery rate adjusted)=0.09 and 0.08, respectively;table 2and table S3). We also observed a stronger association between uterine fibroids and cancer mortality among women who were nulliparous (P for interaction (false discovery rate adjusted)=0.06; table S3). The associations between endometriosis and uterine fibroids and premature mortality persisted in competing risk Cox proportional hazards regression models (tables S4 and S5) and were materially unchanged in several sensitivity analyses assessing the influence of non-response to follow-up questionnaires, selection bias, race or ethnicity, diagnostic bias, and potential confounders (tables S6-S17). The E values were 1.95, 1.74, and 3.31 for all cause, cancer, and respiratory disease mortality, respectively, relating to endometriosis, and 1.74 for cancer mortality relating to uterine fibroids (table S18).
Discussion
Principal findings
Results from this large prospective cohort showed that visually confirmed endometriosis and uterine fibroids were associated with a greater long term risk of premature mortality, driven primarily by malignant neoplasm of gynecological organs. Additionally, endometriosis was associated with a greater risk of non-malignant mortality caused by respiratory disease, senility and ill-defined diseases, and diseases of the nervous system and sense organs.
Comparison with other studies
Few studies to date have explored the long term influence of endometriosis or uterine fibroids on mortality. In contrast to our findings, Saavalainen and colleagues reported a lower risk of mortality from all causes, cardiovascular diseases, cancer, accidents and violence, and respiratory diseases among 49 956 women with surgically verified endometriosis compared with a reference cohort of 98 824 age and municipality matched women22; and Shen and colleagues reported a lower risk of mortality from breast cancer in 22 001 women with a diagnosis of uterine fibroids compared with 85 356 women who were fibroid free matched by age and date of diagnosis.23These previous studies did not determine gynecological diseases during follow-up or throughout the reproductive lifespan for participants in the reference group. Considering that women who did not report endometriosis or uterine fibroids at recruitment might develop these disorders during follow-up or in later life, misclassification should be taken into account if women with undiagnosed endometriosis or uterine fibroids are included among the population matched controls. Additionally, these retrospective case-control studies used register based data, which did not collect data on relevant confounders (eg, oral contraceptives, hormone replacement therapy, and infertility) and behavior factors (eg, body mass index, diet quality, physical activity, and smoking status), which might have resulted in biased associations. In support of this notion, strengthened hazard ratios were observed when we adjusted for these important covariates. Finally, the differences in population characteristics (eg, parity and professions) and their access to medical care and treatment resources might also lead to inconsistent findings between studies. In our present study, uterine fibroids were associated with a lower risk of total and cancer premature mortality among women with hysterectomy. We hypothesize that hysterectomy might have eliminated the potential for later life fibroid diagnosis and the development of certain malignancies, consequently reducing the risk of cancer mortality. Furthermore, we observed a stronger association between uterine fibroids and cancer mortality among women who were nulliparous, which is also plausible given the extensive evidence showing that nulliparity is associated with an increased risk of gynecological malignancies, such as ovarian and endometrial cancer.5354However, the associations between endometriosis and uterine fibroids and total and cancer mortality did not vary by any behavior factors, indicating that these associations were independent of body mass index, diet quality, physical activity, and smoking status.
In support of our findings, several studies have shown that endometriosis is associated with a greater risk of cancer from gynecological organs such as ovarian, tubal, and endometrial cancer.20555657585960Other studies also reported an association of uterine fibroids with a greater risk of ovarian and endometrial cancer.5859Endometriosis and uterine fibroids are strong drivers of female infertility.616263The present findings are also consistent with our recent study from the same cohort (NHSII; n=101 777), in which infertility caused by ovulatory disorders and endometriosis was associated with a greater risk of premature mortality caused by all cancers and gynecological cancers.7Breast cancer is the second leading cause of cancer deaths among US women.64In our present study, we did not find any evidence of associations between endometriosis and breast cancer mortality, which is consistent with the findings of our previous study showing that endometriosis is unrelated to the overall risk of breast cancer among 116 430 NHSII women.65
Laparoscopically confirmed endometriosis has been associated with a higher risk of early onset coronary heart disease and stroke among women from the NHSII.1566In our present study, endometriosis was unrelated to premature cardiovascular disease mortality, which could be partly explained by the low number of cardiovascular disease deaths in women who might not have reached 70 years of age. However, when we jointly categorized participants by exposure to endometriosis and uterine fibroids, an increased risk of cardiovascular disease mortality was observed among women reporting both endometriosis and uterine fibroids, albeit the confidence interval crossing the null value, but not among women with endometriosis only or uterine fibroids only. This finding suggests that endometriosis might interact synergistically with uterine fibroids, possibly accelerating the risk of cardiovascular disease mortality in later life. Conversely, we observed an increased risk of respiratory disease mortality exclusively among women reporting endometriosis only. However, similar cancer mortality risks were observed among women reporting endometriosis only, uterine fibroids only, and both conditions. These findings highlight the intricate interplay between endometriosis, uterine fibroids, and mortality, suggesting that different combinations of these conditions might present varying risks for different causes of mortality. Further research is warranted to unravel underlying mechanisms and investigate potential preventive and therapeutic interventions.
Underlying mechanisms of observed associations
The associations of visually confirmed endometriosis and uterine fibroids with cancer mortality, particularly deaths caused by malignant neoplasm of gynecological organs, might reflect shared mechanistic pathways (eg, hyperestrogenism, oxidative stress, and inflammation) that synergistically contribute to these gynecological diseases and cancer mortality. For example, emerging evidence from animals and humans supports the distinct roles of estrogen and progesterone in the pathogenesis of endometrial cancer, endometriosis, and uterine fibroids.67Meanwhile, many studies show that dysregulation of immune and inflammatory responses plays an important part in the pathology of various endometrial disorders, including endometriosis,686970uterine fibroids,71and endometrial or ovarian cancer.7273The associations of endometriosis and uterine fibroids with cancer mortality might also be partly explained by shared genetic factors,7475indicating potential causal associations. For instance, animal and human studies have shown that endometriosis and endometrial cancer share numerous genes, including certain genes located within the “endometrial cancer pathway” such as PTEN, PTPRD, and ARID1A.767778In a recent large scale genome wide association study, Kho and colleagues reported a potential causal association between uterine fibroids and endometrial cancer in the Mendelian randomization analysis and identified several shared genetic risk regions between endometriosis and uterine fibroids and endometrial cancer.79
Diagnostic bias must be considered, particularly for the association with premature mortality caused by malignant neoplasm of gynecological organs. As observed when temporal rigour is applied to studies of endometriosis and endometrial cancer risk,20endometriosis, which is marked by lengthy diagnostic delays partly because of missed diagnoses and misdiagnoses,80might be detected only during investigating symptoms that are driven by the malignant condition. Although the present study applied a rigorous prospective design and only 73 premature deaths occurred within five years of receiving a diagnosis of endometriosis or uterine fibroids, it is possible that the diagnosis was, years before death, driven by evaluation for symptoms that subsequently were attributed to the gynecological malignancy, which would later be found and caused premature death, therefore inflating the magnitude of the risk estimations.
The mechanisms underlying the positive associations between endometriosis and mortality caused by respiratory disease, senility and ill-defined diseases (eg, debility and headache), and diseases of the nervous system and sense organs (eg, diseases of the central nervous system, nerves, and peripheral ganglia) are less straightforward to hypothesize. However, data are emerging that women with endometriosis might have a longer and potentially more severe SARS-CoV-2 infection,81while associations with asthma and other heightened allergic responses are documented82; these might support pathways by which greater premature mortality from respiratory diseases is plausible. In a recent meta-analysis consisting of nine studies with 287 174 participants, endometriosis was associated with an increased risk of migraine headaches.83Endometriosis and migraine headaches have shared symptoms and pathophysiological pathways.83For instance, the activation of sensory fibers within ectopic endometrial tissue, along with an excessive number of activated and degranulating mast cells in endometriosis lesions or internal nerve structures, might trigger the release of algesic and proinflammatory mediators.8485This process can sensitize primary afferent meningeal nociceptive neurons, leading to hyperalgesia and hypersensitivity, and ultimately might result in headaches. Death from diseases of the nervous system, such as inflammatory diseases of the central nervous system, could partly be explained by neuroinflammation and a greater risk of nociplastic pain and multisystemic pain related conditions observed in women with endometriosis.8687However, additional studies with more premature deaths are needed to verify our findings and explore underlying mechanisms.
Strengths and limitations
The strengths of our study include the large population size, longitudinal design with excellent response rates, sufficient numbers of premature deaths, and detailed collection of various potential confounders and behavior factors updated as frequently as every two years. In addition, participants were followed up biennially across most of their reproductive lifespan, which reduced the potential errors in the recall of endometriosis and uterine fibroids.
However, some important limitations should also be considered. Endometriosis and uterine fibroids were self-reported, which could result in exposure misclassification. This misclassification, although likely to be non-differential with respect to mortality owing to our longitudinal study design, could have biased our estimations. However, we defined women with gynecological conditions as those who reported visually confirmed endometriosis and uterine fibroids that were previously validated against medical records in the present cohort with extremely high reporting accuracy.2788Additionally, more than 90% of our study participants were non-Hispanic white women and all had relatively homogenous professions and educational attainments, which might hamper the generalizability of our results to other racial or ethnic groups, particularly those who might face greater barriers accessing medical care and treatment resources.
As with any observational study, we can infer but not determine causal associations; neither endometriosis nor uterine fibroids could be experimentally assigned. Despite statistical control for various confounders and behavior factors, residual confounding cannot be entirely excluded. However, our estimated E values showed that an unmeasured confounder would need to be associated with endometriosis, uterine fibroids, and mortality by a magnitude of at least 1.74-fold, beyond the measured confounders in adjusted Cox models, to explain away any positive associations observed. Therefore, it is unlikely that unmeasured or unknown confounders would fully explain away our findings. Furthermore, we could not entirely rule out Collider stratification bias caused by adjusting for time varying confounders (eg, drug intake and body mass index) that might share common causes with outcomes.89Additionally, the limited number of deaths other than cancers might be insufficient for precise estimations. Finally, in this large, geographically diverse cohort observed over three decades, we did not collect data on biopsy confirmed endometriosis pathology, lesion location, adenomyosis status, types of uterine fibroids (by ultrasound and hysterectomy), and certain drug intakes (eg, pain relievers), which might have hindered the precision of risk estimations.
Conclusions
In this extensive longitudinal study of nurses tracked biennially over three decades, we found that visually confirmed endometriosis and uterine fibroids were associated with a greater long term risk of premature mortality, driv","Objective: To prospectively assess the effect of endometriosis and uterine fibroids on the long term risk of premature mortality (younger than 70 years).
Design: Prospective cohort study
Setting: The Nurses’ Health Study II, United States (1989-2019).
Participants: 110 091 women aged 25-42 years in 1989 without a history of hysterectomy before endometriosis or fibroids diagnosis, cardiovascular diseases, or cancer.
Main outcome measures: Hazard ratios (estimated by Cox proportional hazards models) for total and cause specific premature mortality according to laparoscopically confirmed endometriosis or ultrasound or hysterectomy confirmed uterine fibroids reported in biennial questionnaires.
Results: 4356 premature deaths were recorded during 2 994 354 person years of follow-up (27.2 years per person), including 1459 from cancer, 304 from cardiovascular diseases, and 90 from respiratory diseases. The crude incidence of all cause premature mortality for women with and without laparoscopically confirmed endometriosis was 2.01 and 1.40 per 1000 person years, respectively. In age adjusted models, laparoscopically confirmed endometriosis was associated with a hazard ratio of 1.19 (95% confidence interval 1.09 to 1.30) for premature death; these models were strengthened after also adjusting for potential confounders including behavioral factors (1.31, 1.20 to 1.44). Cause specific mortality analyses showed that the association was largely driven by mortality from senility and ill-defined diseases (1.80, 1.19 to 2.73), non-malignant respiratory diseases (1.95, 1.11 to 3.41), diseases of the nervous system and sense organs (2.50, 1.40 to 4.44), and malignant neoplasm of gynecological organs (2.76, 1.79 to 4.26). Ultrasound or hysterectomy confirmed uterine fibroids were not associated with all cause premature mortality (1.03, 0.95 to 1.11), but were associated with a greater risk of mortality from malignant neoplasm of gynecological organs (2.32, 1.59 to 3.40) in cause specific mortality analyses. The risk of mortality caused by cardiovascular and respiratory diseases varied according to joint categories of endometriosis and uterine fibroids, with an increased risk of all cause premature mortality among women reporting both endometriosis and uterine fibroids.
Conclusion: Women with a history of endometriosis and uterine fibroids might have an increased long term risk of premature mortality extending beyond their reproductive lifespan. These conditions were also associated with an increased risk of death due to gynecological cancers. Endometriosis was associated with a greater risk of non-cancer mortality. These findings highlight the importance for primary care providers to consider these gynecological disorders in their assessment of women's health.
"
Nurse and doctor turnover and patient outcomes in NHS acute trusts in England,"Introduction
The global crisis in healthcare workers is a source of concern for healthcare policymakers and patients in many countries.123Although shortages of healthcare staff often represent the most visible and critical factor of this crisis, these shortages are also directly related to the less studied event of hospital workers’ turnover. Excessive turnover of nurses and doctors not only may generate a temporary staff shortage, thus increasing demand pressures on healthcare, but also compromise the working conditions of the remaining hospital staff and the continuity of patient care. For instance, high turnover might lead to low staff-to-patient ratios, which correlate with worse patient care and have motivated the adoption of nurse-to-patients safety ratios in several countries to improve patient safety.456789101112Alternatively, high turnover rates of hospital staff might impair the delivery of hospital services owing to the loss of valuable human capital and organisational memory, and the disruption of clinical teamwork.
Moreover, hospitals with high turnover rates of nurses and doctors have to rely on additional temporary locum, bank, or agency staff, which results in about 30% higher hospital staff costs.1314High staff turnover rates can compromise the financial sustainability of hospitals and even entire healthcare systems such as the English NHS, where disproportionate increases in costs are difficult to meet without a substantial increase in public taxation or the introduction of co-payments.
In the 10 years before the covid-19 pandemic, not only did NHS hospitals in England face a mounting upheaval as a result of shortages in the clinical workforce and worsening working conditions, but also, and importantly, higher hospital staff turnover rates.15161718Improving workers’ retention, or reducing the turnover of hospital staff, has been advocated as a cost effective strategy to cope with workforce shortages in the NHS.19202122
Previous research has established a positive association between numbers of hospital clinical staff and patient outcomes.9122324252627282930Only a few studies, however, have investigated the association between staff turnover rates and patient health outcomes31; previous research has primarily investigated the association between clinical staff turnover and increased organisational costs.3233343536Moreover, as the access to large administrative databases measuring both hospital staff turnover and patient health outcomes at a national level is often restricted, the existing evidence on the association between hospital staff turnover rates and patient health outcomes relies on case studies based on small samples, therefore with limited external validity. Recent studies about the composition of nursing teams and hospital mortality in the English NHS indirectly support the case for reducing staff turnover rates as a strategy to improve health outcomes for patients.3738
In the current study, we investigated whether higher turnover rates of clinical staff were associated with poorer patient health outcomes, based on monthly variations in staff turnover rates and patient health outcomes. Specifically, we investigated whether the turnover rate of hospital clinical professionals in the NHS (nurses, senior doctors (also known as consultants in the NHS), and specialist, associate specialist, and specialty (SAS) doctors) were positively associated with risk adjusted patient health outcomes, using nine years of monthly linked data covering all acute care NHS hospital trusts in England. The case mix risk adjusted hospital quality outcome measures that we have used in this analysis were 30 day mortality, in and outside the hospital, after any emergency or elective hospital admission, and 30 day emergency readmissions after discharge for an elective admission. Additionally, we evaluated how the associations of interest changed depending on type of disease.
The aim of our study was to identify which patient health outcomes are affected by high hospital staff turnover and to quantify the association between high clinical staff turnover and health outcomes for patients admitted to hospital.
Methods
Inclusion and exclusion criteria
The analysis sample comprised patients admitted to, and clinical workers employed by, English NHS acute care (non-specialist, non-community) NHS hospital trusts, from 1 April 2010 to 31 March 2019. Appendix figure 1 lists the datasets used for the analysis and how they have been linked. Appendix tables 1 and 2 list the quality of the data linkages, which was found to be satisfactory.
To prevent small sample bias, a minimum threshold of at least 30 patients each month per NHS hospital trust was set to compute risk adjusted hospital quality outcomes. We checked and verified that this threshold did not determine any reduction of observations in the sample of acute care non-specialist, non-community NHS trusts. We excluded both community and specialist NHS hospital organisations from the sample owing to large amounts of missing data related to the computation of risk adjusted hospital quality outcomes. The final sample consisted of 148 hospital trusts over nine years and a total of 14 768 monthly observations.
Outcomes
To construct hospital quality measures we linked admission records at the patient level from the Hospital Episodes Statistics Admitted Patient Care dataset39to records in the Office for National Statistics (ONS) Civil Registration Deaths dataset. As hospital quality is multidimensional and with an imperfect correlation among different measures of quality,4041we used four widely accepted indicators to define different domains of hospital quality: 30 day mortality risk from any type of hospital admission (ie, all cause mortality), 30 day mortality risk for patients admitted with an emergency condition, 30 day mortality risk for patients admitted with an elective condition, and 30 day risk of unplanned emergency readmission for patients discharged after an elective treatment. All hospital quality outcome variables were measured at monthly level, are expressed in percentage of the number of events (deaths or emergency readmissions) per 100 hospital admissions, and represent the probability that a negative health outcome (ie, death or unplanned readmission to hospital) occurred within 30 days from the index event (admission to hospital or discharge from hospital after elective treatment). In the statistical analysis, each hospital quality outcome measure is used as the dependent variable in separate regressions. The linkage to the ONS Civil Registration Deaths records guarantees that 30 day patient mortality is captured anywhere, in and outside of the hospital. All the hospital quality measures were risk adjusted for a list of potential confounders, so as to make a fair comparison of hospital quality across NHS hospital trusts characterised by a heterogenous pool of patient case mix: patient age in five year age brackets, sex, Charlson comorbidity index as a proxy for health status,42and admission month. Risk adjustment is performed by predicting the expected health outcomes through the estimation of patient level logistic regressions, and then comparing observed and expected outcomes by NHS hospital trust and month. The baseline risk adjustment applied to the observed mortality data from ONS Civil Registration Deaths used the pooled sample of all diagnostic groups in each month and adjustments for patients’ age, comorbidities, and admission month (given the separate estimation at monthly level).
Variables of interest
The two main variables of interest were the turnover rates of nurses and senior doctors—both consultants and SAS doctors—across NHS hospital trusts. We constructed these variables at a monthly level from the Electronic Staff Records registry,43a longitudinal database containing monthly payroll information on all clinical staff workers in the English NHS, by NHS hospital trust of employment. Individual identifiers allowed tracking of the full employment histories of the staff over time and across NHS trusts. The monthly turnover rate of the NHS hospital trust was defined as the percentage share of nurses or senior doctors who, between two consecutive calendar months, moved to any other English NHS hospital (churn rate) or left the NHS hospital sector (NHS quit rate). In both cases, turnover computations considered workers’ movements to and from mental health NHS hospitals, although these hospitals were excluded from the analysis.
Statistical analysis
The main statistical analysis used linear panel data regressions, which allowed us to estimate the associations at the mean between hospital clinical staff turnover rates (variables of interest) and the risk adjusted hospital quality measures (outcomes). We estimated the following baseline linear regression specification:
Qh,y,m=β1 × NTRh,y,m-1+ β2× DTRh,y,m-1+ λ1× NSLh,y,m-1+ λ2× DSLh,y,m-1+ λ3× αh× Iy+ λ4× τm+ εh,y,m(equation 1),
whereQh,y,mis the quality of a hospital (h), in a month (m),of a year (y)—that is, one of the four alternative hospital quality outcome measures described in the outcomes section previously. Each of these indicators is included in the regression on the left (Qh,y,m) as a dependent variable, and thus we have estimated four separate regressions for each statistical model—that is, one regression for each hospital quality outcome.NTRh,y,m-1andDTRh,y,m-1are, respectively, nurse and senior doctor turnover rates between monthsm-1andm. The coefficients of interest areβ1andβ2, which measure the association at the mean between nurse and senior doctor turnover rates and hospital quality.
For instance, the association of interest that we tested using the model in equation 1 (and generalised in robustness checks) was whether a higher turnover rate of clinical staff in an NHS hospital trust (h) in February of a given year (y) was associated with worse patient outcomes (in terms of hospital mortality or risk of unplanned emergency readmission) for patients admitted to an NHS hospital trust in March of the same year.
NSLh,y,m-1andDSLh,y,m-1are, respectively, the staff levels of nurses and senior doctors (consultants or SAS) employed by the NHS hospital trust in the monthm-1, before the hospital quality was measured; they have been included as controls for hospital staff size, so that coefficientsβ1andβ2captured the association between monthly staff turnover rates and hospital quality, while controlling for the association between monthly staff levels and hospital quality, captured by coefficientsλ1andλ2. The variablesNTRh,y,m-1,DTRh,y,m-1,NSLh,y,m-1, andDSLh,y,m-1were all standardised, which means that these variables were rescaled by their respective standard deviation (SD) so that the coefficientsβ1,β2,λ1, andλ2could be interpreted as the marginal effect of a 1 SD change in the variable of interest. As other unobservable hospital supply and demand factors could act as confounders of the associations of interest, two sets of fixed effects were included as additional controls in the analysis.τmare quarter of year binary indicators, included as controls for unobserved variation at the seasonal level and with the January-March quarter taken as the baseline omitted category.αhare NHS hospital trust dummy variables, included as controls for time invariant unobservable hospital characteristics (eg, hospital size, location, and equipment), whereasIyare year binary indicators, included as controls for year level unobserved variation.
Both theαh×Iyand theτmfixed effects control for unobservable factors that potentially correlate with staff turnover and hospital quality indicators, the associations of which are captured by the coefficientsλ3andλ4. In particular, the interaction of the binary variables for NHS hospital trust and year effects,αhandIy, means that the NHS hospital trust fixed effects estimated in the model changes every 12 months. These interactions serve as controls for unobservable time varying confounding factors (eg, financial deficits of the NHS hospital trust) that might have changed at NHS hospital trust level across the years and that cannot be directly controlled for by use of the available data. As the estimated linear regressions include fixed effects of the hospital by year, the coefficients of interestβ1andβ2are identified by within variation at the hospital year level. For example,β1is identified by the variation over time between the nurse turnover rate of NHS hospital trust and a given hospital quality indicator (eg, 30 day all cause mortality) for the same NHS hospital trust. As such, this variable represents the estimate of the statistical association at the mean between 1 SD change in nurse turnover rate in each NHS hospital trust and the respective change in quality, averaged across all NHS hospital trusts in the sample. The inclusion of the fixed effects of the year and NHS hospital trust makes the regression estimates ofβ1andβ2algebraically equivalent to the associations between monthly deviations from the hospital staff group turnover rate mean of NHS hospital trusthin yeartand the monthly deviations from the mean hospital risk adjusted health outcomesQof NHS hospital trusthin yeart.44As a result of this mathematical equivalence, a 1 SD increase in the staff turnover rate variable could be eventually interpreted as the marginal effect (on a hospital quality outcome) of 1 SD increase in the excess staff turnover rate, if we assume that each NHS hospital trust has a natural turnover rate determined by its organisational characteristics and that the excess staff turnover rate is defined as a deviation from the mean monthly staff turnover rate observation for NHS hospital trust in yeart.
The linear model specification in equation 1 was estimated through ordinary least squares regressions, and the standard errors are heteroskedasticity and autocorrelation consistent and clustered at NHS hospital trust level.
We performed four types of secondary analyses, which complement our primary statistical analysis by investigating how the baseline results change when: we extend the horizon time of the associations of interest; the variables of interest are measured at medical specialty level; the mortality outcomes are split by types of disease at first hospital admission; the associations of interest are tested along the distribution of the hospital quality outcomes.
The modelling assumption that linked changes in staff turnover rates in montht-1 to changes in risk adjusted hospital quality outcomes in monthtcan appear restrictive. We provided estimates (and related 95% confidence intervals (CIs) that are heteroskedasticity and autocorrelation consistent) from a modified version of equation 1 in which the hospital quality outcomes adjusted by risk were smoothed averages of the order 3 over a year of NHS hospital trust observations at monthst-1,t, andt+1. Additionally, the independent staff turnover rate variables were smoothed averages of order 6, 9, or 12 lagged staff turnover rates in the respective 6, 9, or 12 months before calendar montht(first secondary analysis).
The association of interest might also have been shaped by the medical specialty in which the nurses and senior doctors worked; as such, we estimated a variation of the baseline equation 1 specification by including the variables of interest (and the related control variables for staff levels) separately for the three largest medical specialties: surgery, acute medicine, and general medicine (second secondary analysis).
To investigate heterogeneity for different types of diseases, we provided a heterogeneity analysis of the associations between hospital staff turnover rates and risk adjusted hospital mortality rates only for patients with an emergency hospital admission, broken down by the type of health condition related admission (third secondary analysis). ICD-10 (international classification of diseases, 10th revision) codes were assigned to each patient by using the main diagnosis code variable from Hospital Episodes Statistics admitted patient care dataset, and then risk adjusted hospital mortality measures at 30 days were computed again by stratifying the sample according to NHS hospital trust, year and month of admission, and assigned ICD-10 codes. In this case, separate linear regressions were estimated by each ICD-10 chapter, using risk adjusted 30 day hospital mortality indicators computed at the ICD-10 chapter level. Because the main estimation results showed that the association of interest was driven by the association between staff turnover and mortality after an emergency hospital admission, this heterogeneity analysis focused only on patients with an emergency hospital admission and with a main disease diagnosis belonging to an ICD-10 chapter whose share of emergency admissions was at least 70% of the total number of admissions (ie, the sum of emergency and elective admissions for the relevant ICD-10 chapter).
One limitation of the linear regression models used in the primary statistical analysis is that they identify associations between staff turnover rates and hospital quality only at the mean. The strength of the association between these turnover rates and hospital quality could, however, vary non-linearly across the distribution of hospital quality. For instance, poor performing hospitals may have been more affected by an increase in clinical staff turnover than better performing hospitals. To explore this potential source of heterogeneity we provided estimates using unconditional quantile regressions (fourth secondary analysis).45These are distributional regressions that allow the investigation of the associations with hospital clinical staff turnover rates along the unconditional distribution of the risk adjusted hospital quality measures. This strategy involves estimating a linear model similar to the baseline linear regression model, but only after recentring the quality outcome variable around its average value at the centile of interest. Our analysis focused on the 20th, 40th, 50th, 60th, and 80th centiles of the hospital quality distribution. Since we measured hospital quality through negative outcomes such as mortality and readmission risks, lower centiles imply a better quality of NHS hospital trust. Standard errors were clustered at NHS hospital trust level and computed using 500 bootstrap replications.
We tested the robustness of the findings of the primary analysis in several ways. To account for the cross correlations among different hospital quality outcome variables and the variables of interest, we re-estimated the baseline panel data linear regressions using a seemingly unrelated regression (known as SURE) model.46The SURE model explicitly takes into account the correlated nature of the multiple outcome variables and their predictors to reduce the standard errors of the regression coefficients. When testing for multiple hypotheses, the likelihood of incorrectly rejecting a null hypothesis (ie, making a type I error) increases; therefore, for the SURE model estimates we have provided P values corrected by applying a Sidak-Bonferroni multiple hypothesis testing adjustment to the family wise error rate,47based on four hospital quality outcomes and two variables of interest.
Staff turnover at hospital or local area level can also be affected by a variety of time varying confounding factors. For example, changes to cost of living or other factors such as financial pressures on the NHS hospital trust may lead to increased turnover rates. Although the baseline estimates (from equation 1) control for such yearly time varying confounders through the inclusion of the interacted year-NHS hospital trust fixed effects, we have estimated an even more robust specification by including also interactions between the financial years and the middle layer super output area where the NHS hospital trust was located. These output areas are local geographies defined by ONS, which comprise between 2000 and 6000 households and usually include resident populations of between 5000 and 15 000 people; England has 6856 middle layer super output areas.
Another concern is confounding due to the potential time varying effect of the area’s socioeconomic conditions, which may increase demand pressures on hospitals and contemporaneously affect both hospital staff turnover rates and hospital quality outcomes. To address this source of confounding, in another robustness check, we added the interactions of financial years with the index of multiple deprivation (which is defined at lower layer super output area level) as additional controls to the baseline specification.
In a distinct check for robustness, we decomposed the associations of interest by splitting turnover in churn and NHS quit rates for both nurses and senior doctors.
Additionally, organisational factors such as mergers and acquisitions across NHS acute care hospital trusts may act as an additional source of confounding. For example, by increasing both turnover rates and negative hospital quality indicators. In the main analysis, an NHS hospital trust resulting from the merger between two or more existing hospital trusts was treated as a completely new hospital trust. As such, the change in the hospital organisation identifier that interacted with financial years fixed effects would indirectly control for the occurrence of such an organisational merger and acquisitions event. As the baseline regression specification does not explicitly account for mergers and acquisitions events between NHS hospital trusts, in a further robustness check we provided regression estimates after including a control dummy variable that takes value 1 only in the period after the merger, thus explicitly accounting for the confounding due to merger and acquisition operations.
Another potential source of confounding in the main analysis is the omission of the turnover of hospital trainee doctors. Postgraduate doctors in training are an important component of the hospital clinical workforce, but in the English NHS their turnover rate across hospital trusts is driven by compulsory rotations that are part of their training programme.4849Higher specialty trainees usually have lower turnover rates owing to fewer rotations, having passed the earlier phases of their training. Therefore, in one robustness specification we included, as an additional variable of interest, the monthly turnover rate of higher specialist trainees employed in each NHS hospital trust, and their monthly staff level as a control.
We also tested the robustness of the main findings to the functional form of equation 1 by estimating both a log linear version of equation 1 through ordinary least squares, in which the level of each hospital quality outcome measure was replaced with its log transformation, and also a Poisson fixed effect regression version of equation 1 estimated through quasi maximum likelihood. When using these alternative specifications, the interpretation of the coefficients of interest changes: the coefficients estimated with either log-linear or Poisson regressions represent the association of a 1 SD increase in hospital staff turnover rate with a percentage increase in hospital quality outcome variables. In the Poisson regressions, the hospital quality outcome variables were expressed as risk levels in percentage terms, as in the baseline linear regressions, whereas in the log-linear regressions, the hospital quality outcomes were expressed as the natural logarithm of the risk levels in percentage terms. The Poisson regressions did not include any offset term on the right side. Our sample includes the following numbers of zero outcome events: 0 for all cause mortality, 0 for emergency related mortality, 1864 (12.6% of the observations) for elective related mortality, and 2 (0.014% of the observations) for unplanned emergency readmissions. The log-linear regressions do not allow the inclusion of zero outcome events, the data points of which do not contribute to the regression estimation, whereas the Poisson regressions allows the inclusion of zero outcome events so that the respective data points contribute to the estimation.
A further robustness analysis investigated a discretised dose-response mechanism in the association of interest. For each financial year, we assigned NHS hospital trusts to two binary categories (nurses and senior doctors) depending on whether the average monthly turnover rate was above or below the median turnover rate of all NHS hospital trusts in the same sample in the same financial year.
The contemporaneous inclusion of multiple variables of interest and control variables in the same regression represents also a possible concern, as it could lead to wrong estimation and interpretation of the variables of interest (a statistical problem often known as “table 2 fallacy”). Therefore, we provided a robustness analysis, including, sequentially, each turnover rate variable of interest and then the related control variables for staff levels.
Finally, we used monthly observations on risk adjusted patient outcomes, whereas the method used by NHS Digital to compute the so called standardised hospital mortality indicators (SHMIs) produces yearly risk adjusted quality measures based on 142 separate diagnoses groups.50When we tried to compute a monthly risk adjusted mortality model by estimating logistic regressions stratified based on the 142 separate diagnostic groups, this estimation turned out to be computationally infeasible, because for several diagnostic groups the monthly sample size of the patients admitted to hospital or who died after hospital admission was too small for the logistic regression to estimate correctly. Therefore, in the last robustness checks we provided regression results obtained by using an alternative, and yet feasible, monthly mortality risk adjustment, in which the 142 diagnostic groups used in the SHMI method are included as adjustment covariates, and so they are not used to stratify the estimation sample of the risk adjustment logistic regressions. We also reported the values of the Pearson correlations of the official yearly SHMI estimates with yearly hospital mortality risk computed according to the feasible risk adjustments used in this study.
Patient and public involvement
The study and manuscript development did not involve patients or members of the public because we did not have funding for these additional research activities. Additionally, the involvement of patients or members of the public in the design, reporting, or dissemination plans of this research would be inappropriate because access to the patient data are restricted and cannot be easily circulated to the public.
Results
Descriptive statistics
Table 1and appendix table 3 provide summary statistics of the dependent and independent variables of interest, respectively, for the overall sample and by each financial year. The mean monthly turnover rate of nurses and senior doctors from their NHS hospital trust was 2.35% and 2.45%. The average probability of dying within 30 days of hospital admission was 2.69%. This rose to 4.09% after emergency admission and fell to 0.47% after elective admission. The probability of an unplanned emergency readmission for elective patients was 6.34%. The monthly turnover of nurses was higher than the monthly turnover of doctors in the first four years of the sample, and lower than the monthly turnover of doctors in the last four years of the sample. In any given year there were at most 144 distinct NHS hospital trusts (appendix table 3), because several acute care hospital trusts underwent mergers or acquisitions, or both, during the period of our analysis.Table 1shows that a 1 SD increase in the turnover of nurses and doctors corresponded to a 1.21 and 1.89 percentage point increase in respective turnover rates. Because the average number of nurses and doctors in each acute NHS hospital trust in our sample was 1685.12 and 353.48, a 1 SD increase in the turnover rate approximately corresponds to an additional 20 nurses and seven doctors leaving the NHS hospital trust every month.
Appendix table 4 provides the Pearson correlations among the dependent variables (risk adjusted hospital quality outcomes) and the hospital staff turnover rates, after removing the year NHS hospital trust fixed effects interactions and quarter of year fixed effects. The Pearson correlations are positive and high in magnitude (0.976; P<0.01) only between the all cause and the emergency mortality measures; positive and small between all cause and elective mortality (0.146; P<0.01), emergency and elective mortality (0.034; P<0.01), and nurse and senior doctor monthly turnover rates (0.038; P<0.01); negative and small between emergency mortality and emergency readmissions after planned treatment (−0.030; P<0.01); and positive and small between all cause or emergency mortality risk and nurse turnover rates (0.063 and 0.065, respectively; P<0.01) and between all cause or emergency mortality risk and senior doctor turnover rates (0.029 and 0.028, respectively; P<0.01).
Associations at the mean
Table 2reports the ordinary least squares estimates of our baseline fixed effects specification. Findings show a statistically significant association between the turnover rate of hospital nurses and doctors and mortality risk at 30 days for all cause or emergency admissions. A 1 SD increase in the monthly nurse turnover rate was associated with a 0.035 (95% CI 0.024 to 0.045) percentage point increase in the monthly all cause mortality risk and a 0.052 (0.037 to 0.067) percentage point increase in the monthly emergency admissions 30 day mortality risk. A 1 SD increase in the monthly senior doctor turnover rate was associated with a 0.014 (0.005 to 0.024) percentage point increase in the monthly all cause mortality risk, and a 0.019 (0.006 to 0.033) percentage point increase in the monthly emergency admissions 30 day mortality risk. All these positive associations were statistically significant at the 1% level.
Therefore, in the analysis sample, a 1 SD increase in the monthly turnover rate for nurses—equivalent to about 20 nurses quitting the NHS hospital trust—was associated with an all cause mortality risk increase of 35 deaths for every 100 000 hospital admissions in a given month. At the national level, based on an average of 8 200 000 yearly hospital admissions to the 148 NHS hospital trusts in the sample (ie, 683 333 monthly hospital admissions each month), this figure is also equivalent to an additional 239 deaths at the monthly level (35×683 333 admissions/100 000 admissions) across the 148 NHS hospital trusts of the sample. Overall, 239 additional monthly deaths per 100 000 admissions were equal to 8.89% (0.239/2.69) of the 2.69% baseline all cause monthly mortality risk. Instead, a 1 SD increase in the monthly turnover rate for senior doctors—equivalent to about seven senior doctors quitting the NHS hospital trust—was associated with an increase in all cause mortality risk by 14 deaths for every 100 000 hospital admissions in a given month. This implies that at the national level an average increase in the monthly senior doctor turnover rate of 1 SD in all acute NHS hospital trusts was also equivalent to an additional 95.67 (14×683 333 admissions/100 000 admissions) deaths each month across the 148 NHS hospital trusts of the sample, or 3.56% (0.09567/2.69) of the baseline 2.69% all cause monthly mortality risk.
The associations of either the monthly nurse or the monthly senior doctor turnover rates with elective admission mortality or unplanned emergency readmission to the hospital were smaller and not statistically significant (P>0.10). The estimates of the association between the monthly staff levels of nurses and doctors and the hospital quality outcomes were not statistically significant (P>0.10). The outcomes at the quarter of the year, for April to December, showed significant associations with mortality (P<0.01) that were lower than the reference category (quarter from January to March); this finding is consistent with the evidence of higher mortality during winter months, represented by the reference category quarter from January to March. The estimates of the quarter of year coefficients were also consistent with the evidence of a negative correlation between mortality and unplanned emergency readmissions to hospital after elective care51: when hospital mortality is lower than in the baseline quarter, unplanned emergency readmissions rates are instead higher—that is, marginal patients do not die","Objective: To investigate the association between monthly turnover rates of hospital nurses and senior doctors and patient health outcomes (mortality and unplanned hospital readmissions).
Design: Retrospective longitudinal study.
Setting: All 148 NHS acute trusts in England (1 April 2010 to 30 March 2019), excluding specialist and community NHS hospital trusts.
Participants: Yearly records on 236 000 nurses, 41 800 senior doctors (specialist, associate specialist and specialty doctors, and consultants), and 8.1 million patients admitted to hospital.
Main outcome measures: The panel data regression analysis used nine years of monthly observations from administrative datasets at healthcare worker and patient levels. Associations using linear and unconditional quantile regressions were estimated, including controls for seasonality and NHS hospital trust. Four hospital quality indicators (risk adjusted by patient age, sex, and Charlson index comorbidities) were used and measured at a monthly frequency on a percentage scale: mortality risk within 30 days from all cause, emergency, or elective admission to hospital, and risk of unplanned emergency readmission within 30 days from discharge after elective hospital treatment.
Results: A 1 standard deviation (SD) increase in turnover rate for nurses was associated with 0.035 (95% confidence interval 0.024 to 0.045) and 0.052 (0.037 to 0.067) percentage point increases in risks of all cause and emergency admission mortality, respectively, at 30 days. The corresponding values for senior doctors were 0.014 (0.005 to 0.024) and 0.019 (0.006 to 0.033) percentage point increases. Higher nurse turnover rate was associated with higher mortality risk at 30 days in surgical (P<0.01) and general medicine (P<0.01) specialties, as well as mortality for patients admitted to hospital with infectious and parasitic diseases (international classification of diseases, 10th revision; P<0.05) and injury, poisoning, and consequences of external causes (P<0.01). Higher turnover rates for senior doctors were associated with higher mortality risk at 30 days for patients admitted to hospital with infectious and parasitic diseases (P<0.05), mental and behavioural disorders (P<0.05), and diseases of the respiratory system (P<0.05). Turnover rates for hospital nurses and senior doctors were not statistically significantly associated with risk adjusted hospital mortality and unplanned emergency readmissions for elective patients.
Conclusions: Lower turnover rates for nurses and senior doctors at hospital level were associated with better health outcomes for patients with emergency hospital admissions.
Study registration: Integrated Research Application System project ID 271302.
"
Fezolinetant for vasomotor symptoms associated with menopause,"Introduction
Vasomotor symptoms, comprising hot flushes and night sweats, are the most common and bothersome symptoms associated with menopause.12Up to 80% of women experience vasomotor symptoms during menopause.12Moderate-severe symptoms occur in 11-46% of women older than 40 years,345with a median total duration around 7.4 years,6whereas the severity of vasomotor symptoms associated with menopause varies throughout the course of menopause and among women.345
Although vasomotor symptoms are common and often severe enough to warrant medical treatment, approved and effective non-hormonal treatments are limited. Hormone therapy is effective but not appropriate for everyone.7Treatment is contraindicated in women with a history of breast cancer, uterine cancer, active liver disease, or thromboembolic diseases, and caution is advised in those with comorbidities such as cardiovascular disease, diabetes, and raised triglyceride levels.8Hormone therapy is not suitable for many women, and some choose not to use it.9
Contraindications to hormone therapy in daily practice is important. Individuals may require counselling from their doctor or other healthcare professionals, and risk factors can complicate the treatment decision making process. In addition to having contraindications, many individuals are cautious about or would prefer to not use hormone therapy, despite being suitable candidates for treatment. Reasons for not wanting to use treatment can include worries about side effects or long term risks of treatment, or acceptance that menopause is a transitory, self-limiting condition that does not require treatment.10The US Food and Drug Administration has approved low dose paroxetine, a selective serotonin reuptake inhibitor (SSRI), as the only non-hormonal alternative, which offers modest relief of vasomotor symptoms associated with menopause and carries a black box warning for suicidal thoughts and behaviour and other side effects.11Alternative non-hormonal drug options include other SSRIs and serotonin and norepinephrine reuptake inhibitors, which are prescribed for up to one fifth of women with vasomotor symptoms.1213The α adrenergic agonist clonidine and neuromodulators such as gabapentin and pregabalin are also non-hormonal options for treating vasomotor symptoms, although they are not globally approved for such management.814The Menopause Society does not recommend clonidine owing to adverse events or pregabalin because of the potential for misuse as a schedule V controlled substance. Gabapentin includes black box warnings for rare suicidal thoughts or behaviours.14Nutraceutical options for vasomotor symptoms associated with menopause, such as phytoestrogens and herbal derivatives, are widely available as an alternative to hormone therapy, but evidence of their efficacy is limited.15None of these other potential treatments is approved globally to manage vasomotor symptoms. A substantial unmet need therefore exists for safe and effective options for non-hormonal treatment of vasomotor symptoms associated with menopause.1617
Fezolinetant, an oral, non-hormonal, neurokinin 3 receptor antagonist, is a treatment option for moderate-severe vasomotor symptoms, and it is approved in many countries, including the US, Europe, and Australia at a dose of 45 mg once daily.1819202122Fezolinetant blocks neurokinin B signalling, normalising kisspeptin, neurokinin B, and dynorphin neuron activity in the thermoregulatory centre of the brain to reduce the frequency and severity of vasomotor symptoms.2324Fezolinetant was shown to be efficacious and well tolerated for treating moderate-severe vasomotor symptoms associated with menopause in phase 3 studies SKYLIGHT 1 and SKYLIGHT 2,2526which both included a 12 week placebo control period followed by active treatment extension to 52 weeks. To further investigate the clinical benefits of fezolinetant for the treatment of moderate-severe vasomotor symptoms, we performed a phase 3b trial (DAYLIGHT), which included a 24 week placebo control period and enrolled a population considered unsuitable for hormone therapy.
Methods
Study design, objectives, and participants
The present study was a phase 3b, randomised, double blind, placebo controlled trial to assess the efficacy and safety of fezolinetant for treating moderate-severe vasomotor symptoms associated with menopause in individuals considered unsuitable for hormone therapy. Written informed consent was obtained from all participants before any study related procedures. We aimed to evaluate the efficacy of fezolinetant 45 mg versus placebo once daily on the frequency (primary endpoint) and severity (secondary) of moderate-severe vasomotor symptoms and patient reported sleep disturbance (secondary) associated with menopause. Participants were provided with a reference guide for severity of symptoms: mild—sensation of heat without sweating; moderate—sensation of heat with sweating, able to continue activity; and severe—sensation of heat with sweating, unable to continue activity.
Individuals aged 40-65 years with moderate-severe vasomotor symptoms associated with menopause and considered unsuitable candidates for hormone therapy were randomised 1:1 using interactive response technology to fezolinetant 45 mg or placebo once daily and stratified by smoking status (current and non-smoker (former or never)). Categories for hormone unsuitability were defined based on contraindicated; caution (based on medical history); stoppers (previous discontinuation of hormone therapy owing to lack of efficacy, side effects, or medical advice); or averse (informed choice not to use hormone therapy after discussion with a clinician) (see supplementary table 1).
Endpoints
The primary efficacy endpoint was mean change in the frequency of moderate-severe vasomotor symptoms from baseline to week 24. The key secondary efficacy endpoint was mean change in severity of moderate-severe vasomotor symptoms from baseline to week 24. The selected secondary endpoint was mean change in the Patient-Reported Outcome Measurement Information System (PROMIS) Sleep Disturbance Short Form 8b total score from baseline to week 24. PROMIS is a set of patient centred instruments that evaluates physical, mental, and social health.27PROMIS Sleep Disturbance Short Form 8b was developed from PROMIS to assess sleep disturbance, and it evaluates difficulties and problems with falling asleep, staying asleep, and getting enough sleep, as well as perceptions of the quality and satisfaction of sleep. Exploratory endpoints included two Patient Global Impression of Change measures, one for vasomotor symptoms and one for sleep disturbance, and menopause specific quality of life, measured at baseline and weeks 4, 12, 16, and 24. The Patient Global Impression of Change in Sleep Disturbance is a patient reported one item questionnaire that asks participants to rate the severity of any problems during nighttime sleeping using a scale from 1 (no problems) to 4 (severe problems). The questionnaire asks participants to rate how well they were sleeping at that timepoint compared with the start of the study, using a scale from 1 (much better) to 7 (much worse). Similarly, using the same scale the Patient Global Impression of Change in Vasomotor Symptoms asks participants to rate the severity of their vasomotor symptoms compared with the start of the study.
Safety was assessed based on treatment emergent adverse events (TEAEs). A TEAE was defined as an adverse event observed between start of the study drug and up to 21 days after the last dose. Safety follow-up visits were scheduled for three weeks from the last dose. The number and percentage of participants with TEAEs, drug related TEAEs, serious TEAEs, drug related serious TEAEs, TEAEs leading to withdrawal of the study intervention, and drug related TEAEs leading to withdrawal of study intervention were summarised by system organ class, Medical Dictionary for Regulatory Activities version 25.0 preferred term, and treatment group. We also summarised the number and percentage of TEAEs by severity and causality. Participants who discontinued treatment early were asked to remain in the study and continue to complete the daily electronic diary of vasomotor symptoms and electronic patient reported outcome assessments as scheduled to week 24. They were monitored for adverse events, serious adverse events, and use of concomitant drugs to week 27.
Liver function and liver injury were evaluated as part of standard monitoring investigations, including previous phase 3 studies2526of fezolinetant. An independent panel of three liver experts (liver safety monitoring panel) evaluated participants for potential drug induced liver injury who met the criterion of alanine transaminase (ALT) or aspartate aminotransferase (AST) greater than three times the upper limit of normal (ULN) or total bilirubin more than twice the ULN.
Transvaginal ultrasonography was required for all participants to assess endometrial thickness at screening and week 24 or end of treatment, or at the early discontinuation visit for participants who stopped prematurely.
Statistical analysis
All safety analyses were performed using the safety analysis set, defined as all participants who were randomised and received at least one dose of study intervention. All primary, secondary, and exploratory efficacy analyses were performed using the full analysis set, defined as all participants who were randomised and received at least one dose of study intervention. For both sets, participants were analysed according to the treatment group to which they were randomised.
For a pairwise comparison of the primary endpoint using a two samplettest at a two sided 5% α, we determined that 220 participants in each group would provide at least 80% power to detect a difference from placebo of –1.8, assuming a standard deviation (SD) of 5.6. This size was calculated assuming that about 30% of participants might discontinue the study prematurely. For a pairwise comparison of the key secondary endpoint using a two samplettest at a two sided 5% α, estimated power was calculated as 31% and 59% with 220 participants in each group to detect a difference from placebo of –0.2 and –0.3, respectively, assuming a SD of 1.2. The α statistic was only applied to the key secondary endpoint if the primary endpoint was statistically significant at the 5% level.
We performed a mixed model for repeated measures analysis with a missing at random assumption on change in the average daily frequency (or severity) of moderate-severe vasomotor symptoms from baseline to week 24. For the primary and key secondary efficacy endpoints, the primary analysis method was a mixed model for repeated measures using change from baseline as the dependent variable; treatment group, week, and smoking status (currentvformer or never) as factors; baseline weight and baseline value as covariates; and treatment group by week and baseline value by week as interaction terms. From this analysis, comparisons between fezolinetant and placebo were calculated based on least squares mean contrasts using a two sided 95% confidence interval. PROMIS Sleep Disturbance Short Form 8b total score was analysed using a similar method; however, P values were not controlled for multiplicity.
Patient and public involvement
Public and patient involvement was initiated following the optional exit interviews available to participants and study coordinators after the phase 2 dose finding study (NCT03192176). The interviews were largely focused on the technology used in the study. The technology was revised based on the participant’s feedback. The same participants were asked to re-evaluate the electronic vasomotor symptoms diary for improvement before initiation of the phase 3 studies, including DAYLIGHT, to confirm the changes made were effective.
Results
Baseline characteristics
The study was conducted between 8 November 2021 and 20 April 2023 at 69 centres in 16 countries (Canada, the Netherlands, Belgium, France, Spain, Finland, Hungary, Italy, Czech Republic, UK, Denmark, Sweden, Norway, Poland, Germany, and Turkey). Overall, the centres randomised 453 participants (fezolinetant n=227, placebo n=226), with 370 (81.7%) completing the study (195 and 175, respectively) (fig 1). The safety analysis and full analysis sets, defined as all participants who were randomised and received at least one dose of study drug, comprised 452 participants (fezolinetant n=226, placebo n=226), as one participant randomised to the fezolinetant group did not receive study drug. Of these, 387 (85.6%) participants completed the 24 week treatment period. Results are presented for the 452 participants in the safety and full analysis sets.
Eighty two (18.1%) participants discontinued treatment owing to adverse events (n=25, 5.5%), loss to follow-up (n=5, 1.1%), protocol deviation (n=2, 0.4%), withdrawal from study (n=47, 10.4%), and other reasons (n=3, 0.7%). Of the 47 (10%) participants who withdrew from treatment, 17 (8%) were assigned to fezolinetant and 30 (13%) to placebo. All five participants lost to follow-up were in the placebo group; one participant completed treatment with the last dose in week 24 but did not return for the safety follow-up, and the other four participants discontinued early but no reason was provided. Mean (SD) age was 54.5 (4.7) years, and most of the participants were white (n=435, 96.7%) (table 1). Most participants were categorised as either hormone therapy averse (n=168, 37.2%) or caution (n=165, 36.5%); the remainder were contraindicated (n=50, 11.1%) or stoppers (n=69, 15.3%). Mean (SD) overall drug use was 150.8 (44.6) days. Baseline personal characteristics were generally similar between the two treatment groups.
Primary endpoint
In the 226 participants assigned to fezolinetant, mean frequency of vasomotor symptoms reduced from 10.58 (SD 3.57) events daily at baseline to 2.61 (3.14) at week 24 (fig 2). In the 226 participants assigned to placebo, mean frequency of vasomotor symptoms reduced from 10.75 (SD 4.08) events daily at baseline to 4.67 (4.80) at week 24. Fezolinetant significantly reduced the frequency of vasomotor symptoms compared with placebo at week 24 (least squares mean difference –1.93, 95% confidence interval (CI) –2.64 to –1.22; P<0.001) (table 2). At week 24 the least squares mean percentage change from baseline was –75.66% (95% CI –80.13% to –71.19%) for fezolinetant and –59.12% (–63.71% to –54.52%) for placebo. This significant reduction in frequency of vasomotor symptoms started on day 1 (see supplementary figure S1) and continued to week 24 (fig 2).
Secondary endpoints
In the 226 participants assigned to fezolinetant, severity of vasomotor symptoms reduced from a mean 2.43 (SD 0.36) at baseline to 1.43 (0.97) at week 24 (fig 3). In the 226 participants assigned to placebo, severity of vasomotor symptoms reduced from a mean 2.41 (0.34) at baseline to 1.87 (0.82) at week 24. The week 24 difference in symptom severity between fezolinetant and placebo was significant (least squares mean difference –0.39, 95% CI –0.57 to –0.21; P<0.001) (table 2). Participants receiving fezolinetant also had a greater reduction in total scores on the PROMIS Sleep Disturbance Short Form 8b compared with the placebo group (least mean squares mean difference –2.5, –3.9 to –1.1; P<0.001) (table 2).
Exploratory endpoints
At week 24, the most common response on the Patient Global Impression of Change in Vasomotor Symptoms in participants receiving fezolinetant was “much better” (n=123, 62.4%vn=71, 39.9%, fezolinetantvplacebo groups, respectively, P<0.001) (table 3andfig 4). Similarly, at week 24, the most common response on the Patient Global Impression of Change in Sleep Disturbance in participants receiving fezolinetant was “much better” (n=74, 37.6%vn=42, 23.6%, fezolinetantvplacebo groups, respectively, P<0.001) (table 3,figure 4). Participants receiving fezolinetant also had greater reductions from baseline (improvements) in mean menopause specific quality of life total score relative to placebo at week 24 (least squares mean difference –0.44, –0.69 to –0.18; P<0.001) (table 3).
Safety
No differences were found in incidence of TEAEs (fezolinetant: n=147, 65.0%; placebo: n=138, 61.1%) and serious TEAEs (n=10, 4.4%; n=8, 3.5%, respectively) between the groups (table 4). The most common TEAEs in the fezolinetant group were covid-19 (n=30, 13.3%), headache (n=20, 8.8%), and fatigue (n=13, 5.8%) (table 5). The most common TEAEs in the placebo group were covid-19 (n=29, 12.8%), headache (n=21, 9.3%), and nasopharyngitis (n=11, 4.9%). There was one (0.4%) reported TEAE of system organ class neoplasms benign, malignant, and unspecified (including cysts and polyps) of mild severity in the placebo group, which was considered unrelated to placebo, and no TEAEs of this system organ class reported in the fezolinetant group.
Increases in abnormal liver test results were reported for 10 (4.4%) participants in the fezolinetant group and six (2.7%) in the placebo group (table 6). For liver safety assessments, three (1.3%) participants in the fezolinetant group showed ALT levels three times the ULN (table 7). The liver safety monitoring panel concluded a causal association was possible in one of the participants and unlikely in the other two participants. No participants showed AST levels three times the ULN or total bilirubin twice the ULN. While there were transient or isolated increases in transaminase levels during the study, no participants had drug induced liver injury according to Hy’s law.28
TEAEs of special interest included uterine bleeding, which was reported for six (2.7%) participants in the fezolinetant group and 10 (4.4%) in the placebo group (table 6). None of these events was considered serious. In six of 10 participants in the placebo group and four of six in the fezolinetant group, these events were reported as vaginal or postmenopausal bleedings. Eight participants had 11 study drug related events of vaginal or postmenopausal bleeding. Most of these events resolved with no action or resolved after withdrawal of the study intervention, and the outcome of one event (vaginal haemorrhage) was unknown with no action taken. Other TEAEs of special interest included endometrial hyperplasia, cancer, and disordered proliferative endometrium, which was reported by one (0.4%) participant in the fezolinetant group and two (0.9%) participants in the placebo group. All three of these events were preferred terms of endometrial thickening measured with transvaginal ultrasound and non-serious, none were related to cancer, and all three were considered study drug related. Thrombocytopenia was a TEAE of special interest and reported in no participants in the fezolinetant group and one (0.4%) participant in the placebo group. It was not considered serious or related to the use of placebo.
Discussion
The findings of this study support evidence792930that fezolinetant is an effective treatment option for individuals with moderate-severe vasomotor symptoms associated with menopause who cannot or choose not to use hormone therapy. This study provided placebo controlled efficacy data over a 24 week period, compared with 12 weeks in the SKYLIGHT studies.2526Frequency and severity of moderate-severe vasomotor symptoms at week 24 showed statistically significant improvements with fezolinetant 45 mg compared with placebo once daily, a sustained treatment effect over a longer period than in previous studies. In addition to these findings, a greater reduction in patient reported sleep disturbance (PROMIS Sleep Disturbance Short Form 8b total score) was seen in the fezolinetant compared with placebo group.
Comparison with other studies
Improvements in frequency of moderate-severe vasomotor symptoms in the fezolinetant group compared with placebo were observed as early as day 1, consistent with efficacy results in phase 3 studies SKYLIGHT 1 and SKYLIGHT 2.2526The available safety data appeared generally consistent with the known safety profile for fezolinetant, including data from phase 3 placebo controlled trials showing that fezolinetant is well tolerated for as much as 52 weeks.252631The incidence of serious TEAEs from baseline to week 24 was low. No safety signals of concern were observed, including liver safety, in the fezolinetant group, and no drug induced liver injury was observed in any study participants. Some participants experienced an increase in transaminase levels, which overall were not serious, and asymptomatic liver test abnormalities, captured through protocol specified routine testing. Three participants in the fezolinetant group had raised ALT levels three times the ULN; none of them was accompanied by bilirubin levels twice the ULN. The liver safety monitoring panel concluded that a causal association was possible in one participant and unlikely in the other two participants.
Strengths and weaknesses of this study
The findings of this study add to a large body of data showing a positive benefit-risk profile for fezolinetant and the potential of this drug to address an important unmet need for individuals with moderate-severe vasomotor symptoms associated with menopause. The study included a longer placebo control period (24 weeks) than previous phase 3 studies, and a large defined patient population unsuitable for current hormone therapy (ie, hormone therapy contraindicated, caution, stoppers, and averse).2526A potential limitation of this study was that participants were from 16 countries (Canada, the Netherlands, Belgium, France, Spain, Finland, Hungary, Italy, Czech Republic, UK, Denmark, Sweden, Norway, Poland, Germany, and Turkey), with most self-identifying as white. Further studies of fezolinetant in global populations with diverse ethnicity or races would be of interest.
Conclusion
The findings of this study support the utility of fezolinetant as an effective non-hormonal treatment option for individuals who cannot or choose not to use hormone therapy for the management of moderate-severe vasomotor symptoms associated with menopause.
Hormone therapy is an effective treatment for vasomotor symptoms associated with menopause
Treatment is not always appropriate, however, and hormone therapy is unsuitable for many individuals
Fezolinetant, an oral, non-hormonal, neurokinin 3 receptor antagonist treatment option for moderate-severe vasomotor symptoms, is approved in many countries, including the US, Europe, and Australia, at a dose of 45 mg once daily
Fezolinetant 45 mg once daily was efficacious and well tolerated as a treatment for moderate-severe vasomotor symptoms in individuals considered unsuitable candidates for hormone therapy
Improvements in moderate-severe vasomotor symptoms were observed as early as week 1, with sustained benefit throughout the 24 week treatment period
No safety signals of concern, including drug induced liver injury, were observed
","Objectives: To assess the efficacy and safety of the non-hormonal, neurokinin 3 receptor antagonist, fezolinetant, to treat moderate-severe vasomotor symptoms associated with menopause in individuals unsuitable for hormone therapy.
Design: Phase 3b randomised controlled trial.
Setting: 16 countries.
Participants: 453 individuals aged 40-65 years with moderate-severe vasomotor symptoms associated with menopause who were considered unsuitable candidates for hormone therapy (contraindicated, caution (based on medical history), stoppers (previous discontinuation of hormone therapy), or averse (informed choice not to use hormone therapy)) were randomised to receive fezolinetant (n=227) or placebo (n=226).
Intervention: Fezolinetant 45 mg or placebo once daily for 24 weeks.
Main outcome measures: The primary endpoint was mean change in daily frequency of moderate-severe vasomotor symptoms from baseline to week 24. Secondary endpoints were mean change in symptom severity, sleep disturbance using the Patient-Reported Outcome Measurement Information System Sleep Disturbance Short Form (PROMIS SD-SF) 8b total score, and safety.
Results: 370 (81.7%) participants completed the study (fezolinetant=195, placebo group=175). The safety and full analysis sets comprised 452 participants who received at least one dose of study drug. Mean age was 54.5 (standard deviation 4.7) years and most of the participants (435 (96.7%) were white and categorised as either hormone therapy averse (168 (37.2%)) or caution (165 (36.5%)). At week 24, fezolinetant significantly reduced the frequency (least squares mean difference –1.93, 95% confidence interval (CI) –2.64 to –1.22; P<0.001) and severity of vasomotor symptoms (–0.39, –0.57 to –0.21; P<0.001). At week 24, the fezolinetant group had a greater reduction in sleep disturbance (PROMIS SD-SF 8b total score) compared with placebo (–2.5, –3.9 to –1.1; P<0.001). Improvements over placebo were observed as early as week 1. Both groups showed similar incidences of treatment emergent adverse events (TEAEs, 147 (65.0%) in the fezolinetant group, 138 (61.1%) in the placebo group) and serious TEAEs (10 (4.4%) and 8 (3.5%), respectively). The most common TEAEs in the fezolinetant group were covid-19 (30 (13.3%)), headache (20 (8.8%)), and fatigue (13 (5.8%)).
Conclusions: Fezolinetant was efficacious and well tolerated over a six month period for treating moderate-severe vasomotor symptoms in individuals considered unsuitable for hormone therapy. These results highlight the utility of fezolinetant as an effective treatment option for those who have contraindications to or choose not to use hormone therapy.
Trial registration: ClinicalTrials.govNCT05033886; EudraCT 2021-001685-38.
"
Safety of inpatient care in surgical settings: cohort study,"Introduction
The foundational tenet of medical practice, “First, do no harm,” serves as a guiding principle for ensuring patient safety. However, adverse events during hospital admission are a major and widespread cause of harm in healthcare. The landmark Harvard Medical Practice Study, conducted in the 1980s, estimated the incidence and preventability of adverse events during hospital care, with nearly half associated with surgical procedures.1Revealing the extent of unintended injuries caused by medical care, this extensive study provided crucial support to the 2000 report of the US National Academy of Medicine titled “To Err is Human: Building a Safer Health System.”2That report initiated a global effort to improve patient safety, with the goal of identifying and preventing errors associated with adverse events.
Since the Harvard Medical Practice Study was performed, patient safety and various aspects of surgical care have undergone substantial transformations. In modern surgery, minimally invasive procedures have become increasingly prominent.3The implementation of surgical safety checklists, globally disseminated and utilized in operating rooms,4has been complemented by enhanced recovery after surgery protocols.5Together, these initiatives have introduced a systematic approach to improve perioperative care. Concurrently, a substantial portion of surgery has transitioned from the inpatient to outpatient setting, and the widespread adoption of electronic health records has become the norm. In 2001, the American College of Surgeons launched the national surgical quality improvement programme, drawing inspiration from Codman’s pioneering surgical registry a century ago and building upon a previous programme initiated by the Veterans Health Administration in the 1990s.6
Given the transformative changes in delivery of surgical care over the past decades, an updated assessment is crucial to establish a precise reference point for patient safety. Understanding the frequency, type, and location of adverse events is essential for continuous quality improvement. Aligned with the comprehensive approach of the Harvard Medical Practice Study, the SafeCare study identified adverse events in nearly one in four admissions during overall inpatient healthcare in 2018.7Based on a subset of this original study, here we specifically focused on surgery and other interventional procedures. The worldwide volume of surgeries is large and poses substantial risks to patients.8Our goal was to provide a current and precise assessment of surgical safety by presenting important data on adverse events within the hospital setting. We primarily described the incidence, severity, and preventability of adverse events during perioperative care for patients undergoing surgery. Additionally, we identified the settings where adverse events occurred and professions concerned, both inside and outside the operating room.
Methods
Study design and population
We conducted a retrospective cohort study. The study sample was designed to include hospitals that could provide reliable safety estimates for patients aged 18 years and older at each location. We selected the 11 participating hospitals to represent a mix of both large and small facilities, and they were also part of three different healthcare systems. Among these hospitals, two had fewer than 100 beds, four had 100-200 beds, two had 201-500 beds, and three had more than 700 beds.
Our sampling strategy, along with additional details on its representativeness, is outlined in a previous publication focusing on the overall study population.7At each participating hospital, a random sample of admissions records was obtained, with oversampling in the smaller hospitals. The target sample from the participating hospitals in Massachusetts included all inpatient admissions with discharges in 2018, excluding those for admissions for hospice or rehabilitation care, psychiatric or addiction treatment, and observation only under the two-midnight rule, which categorises a hospital stay that does not cross two midnights as an observation only encounter. If patients were admitted to hospital from a day procedure owing to an adverse event that occurred in the outpatient setting, these patients were not included in our inpatient surgery sample. A total sample of 2750 admissions (averaging 250 per hospital) was initially calculated. Owing to oversampling of four smaller hospitals, the final sample size increased to 2836 admissions. Of 2809 inpatient admissions with usable charts, we ultimately selected those that involved a surgical procedure. Surgical admissions were primarily identified using surgical discharge diagnosis related groups, which categorize and reimburse hospital inpatient services associated with procedures performed in an operating room setting and carry substantial risk for patients. In addition to surgical procedures, these groups included major interventional cardiovascular and endoscopic procedures. To ensure comprehensive sampling, we subsequently included admissions not initially classified under surgical diagnosis related groups but that involved an adverse event related to a surgical procedure and directly involved surgical specialties during the inpatient stay.
To accurately estimate adverse event rates based on a sufficient sample size, we opted a priori to categorize by single specialty for orthopedic and gastrointestinal surgery, group closely related specialties by organs and surgical outcomes for cardiovascular and thoracic procedures and for urology and gynecology procedures, and combine all other remaining specialties with limited samples.
Record review
Nine trained nurses reviewed the records for adults admitted to hospital to identify possible adverse events, using a detailed manual that outlined the chart review process and specified the data to be collected. In this study, we defined adverse events as unintended physical injury resulting from or contributed to by medical care that required additional monitoring, treatment, or hospital admission, or that resulted in death.9Medical care encompassed the actions of individual hospital staff as well as the broader systems and care processes, including both acts of omission (such as failure to diagnose or treat) and acts of commission (such as incorrect diagnosis or treatment, or substandard performance). We excluded adverse events that occurred during previous inpatient admissions or outpatient visits, as our focus was on assessing the incidence of adverse events during hospital admissions.
The reviewers were randomly assigned admitted patients across the hospitals. If they discovered information in a chart that warranted further investigation to identify adverse events related to the index admission, they were allowed to review data recorded up to 30 days after the patient’s discharge. To determine if harm was associated with the index admission, we applied no restrictions to reviewing chart information recorded before the index admission. The reviewers adhered to a protocol outlining the sequence of reviewing an admitted patient in Epic, the widely used electronic health records system used by the hospitals. For hospitals using other electronic health records systems, we randomly assigned admissions to reviewers trained in those systems, following a protocol similar to that used for Epic. Eight hospitals used Epic, two used Meditech, and one used a custom made electronic health records system. All data were entered into a data collection tool developed with Microsoft Access, which allowed for live data validation.
As previously detailed and presented in supplementary method S1,7in addition to reviewing all relevant information documented in the electronic health records to identify adverse events for each patient, the reviewers looked for triggers of potential adverse events related to patient care, drugs, surgical procedures, intensive care, and emergency care. For each patient, the reviewers were allowed to document up to eight possible adverse events. When the reviewers identified an adverse event, they classified it into specific types, such as an event related to a surgical procedure, an adverse drug event, a patient care event related to nursing care (eg, fall or pressure ulcer), a healthcare associated infection, or a blood transfusion reaction. The reviewers also identified the inpatient setting where the medical management leading to the adverse event occurred, along with the most directly involved specialties and professions. Subsequently, the reviewers performed a comprehensive search to identify any signs of errors during care, such as mistakes in diagnosis or failures to follow procedures. Finally, they compiled a narrative summary of the admission, accompanied by a description of each related adverse event.
Eight physicians reviewed the randomly assigned summaries of adverse events and either agreed or disagreed with the classification of adverse event type. If these adjudicators disagreed, the event type was revised. When the adjudicators had questions or thought one adverse event should be counted as several, they sent their queries or comments back to the nurse for further review. In addition, the adjudicators assessed the severity of each event using a general severity scale,10which categorized events as clinically significant (causing unnecessary harm but leading to a quick recovery), serious (resulting in substantial intervention or prolonged recovery), life threatening (posing a potentially fatal situation that required immediate intervention), or fatal (resulting in death). A major adverse event was defined as serious, life threatening, or fatal (supplementary table S1). The adjudicators also provided assessments of whether the harm was preventable.11A potentially preventable adverse event encompassed those assessed as definitively, probably, or possibly preventable. A preventable adverse event only included those assessed as definitively or probably preventable (supplementary table S2). Finally, the adjudicators graded their confidence (with the use of a six point ordinal scale) about whether the event was due to healthcare management.12A confidence score of 4 or higher indicated an adverse event had occurred, aligning with the confidence threshold used in the Harvard Medical Practice Study (supplementary table S3).13Supplementary method S2 provides additional information about the record review.
Statistical analysis
We employed a sampling design in which some of the smaller hospitals were oversampled. Each sampled patient’s admission record was assigned a weight for the analyses. The weight for each patient sampled was the inverse of the probability that the patient was sampled, which is estimated as the inverse of the proportion of admission records sampled from that hospital. Intuitively, a sampled individual’s weight can be interpreted as the number of patients in each hospital that the sampled individual represents. Applying these weights in all the analyses enabled us to derive estimates of characteristics and outcomes for the population of interest. Along with weighting, all 95% confidence intervals accounted for clustering within a hospital. A generalized estimating equations approach with an exchangeable correlation matrix was used to calculate the marginal probability of an adverse event.1415We did not adjust confidence intervals for multiplicity, so they should not be used in place of hypothesis testing.
Patient characteristics associated with admissions were reported as numbers and percentages for categorical variables and as means for continuous variables. Weighted adverse event rates were described based on corresponding severity and preventability, stratified by population characteristics, insurance type, and surgical specialty associated with the admission. Additionally, weighted severity and preventability of adverse events were described according to the type of event, setting, and profession involved. Data manipulation and analyses were performed using SAS software version 9.4 (SAS Institute, Cary, NC). Supplementary method S3 provides the SAS code for data preparation and analysis.
Patient and public involvement
No patients directly participated in this retrospective review of electronic health records. Although the study was initiated before patient and public involvement became common practice, we did speak to patients about the study, and we asked a member of the public to read our manuscript after submission.
Results
Study sample
From 64 121 surgical admissions across 11 hospitals, we analyzed a weighted random sample of 1009 patients admitted to hospital where adverse events related to perioperative care may have occurred (supplementary figure S1). This sample was reasonably representative of all inpatient surgical admissions in the corresponding Massachusetts hospitals during the study period (table 1). The specialties associated with the admissions were orthopedics (n=315, 31.2%), cardiovascular and thoracic surgery (n=223, 22.1%), gastrointestinal surgery (n=155, 15.4%), urology and gynecology surgery (n=119, 11.8%), and neurosurgery, plastic and reconstructive surgery, endocrine surgery, head and neck surgery, oral surgery, or eye surgery (n=198, 19.6%).
Adverse events incidence rates in weighted random sample
Within the weighted random sample of 1009 admitted patients, we identified at least one adverse event in 383 (38.0%) and at least one major adverse event (ie, serious harm resulting in substantial intervention or prolonged recovery, life threatening event, or death) in 160 (15.9%) (table 2). Overall, 292 (28.9%) admissions involved at least one clinically significant adverse event, 143 (14.2%) at least one serious adverse event, 25 (2.5%) at least one life threatening event, and 6 (0.6%) a fatal event.
Among all admitted patients, at least one adverse event was deemed potentially preventable in 258 (25.6%) patients, with 103 (10.2%) classified as probably or definitely preventable. Additionally, 85 (8.4%) patients had at least one potentially preventable major adverse event, and 31 (3.1%) had a major event that was deemed probably or definitely preventable (fig 1).
Table 2shows the incidence of adverse events for each admitted patient according to population characteristics and surgical specialty. The percentage of patients with at least one adverse event was higher among older patients and among those who underwent cardiovascular and thoracic surgery compared with orthopedic surgery and urology and gynecology procedures (fig 2).
Adverse events description in weighted random sample
We identified 593 adverse events during the index surgical admissions. Of these, 353 (59.5%) were potentially preventable and 123 (20.7%) were probably or definitely preventable (table 3). Among the 225 major adverse events, 107 (47.6%) were potentially preventable and 35 (15.6%) were probably or definitely preventable. Supplementary table S4 shows examples of adverse events, including severity category and preventability assessment.
The most common types of adverse events were surgery related, accounting for 292 (49.3%) of the overall adverse events identified (table 3), followed by adverse drug events (n=158, 26.6%), healthcare associated infections (n=74, 12.4%), patient care events (n=66, 11.2%), and blood transfusion reactions (n=3, 0.5%). Surgery related adverse events were more likely to be rated as major adverse events and less likely to be preventable than adverse drug and patient care events.
The most common setting for adverse events was the general care unit, representing 289 (48.8%) of all incidents, followed by the operating room (n=155, 26.1%), intensive care unit (n=77, 13.0%), recovery room (n=20, 3.3%), emergency department (n=11, 1.8%), and other in-hospital locations (n=42, 7.0%). Adverse events in operating rooms or intensive care units were more severe than in general care units. Attending physicians were involved in 531 (89.5%) adverse events, followed by nurses (349 (58.9%), residents (n=294, 49.5%), advanced level practitioners (n=169, 28.5%), and fellows (n=68, 11.5%). Supplementary figures S2 and S3 present the distribution of adverse events by setting, type of adverse event, and healthcare profession.
Discussion
Two decades after the release of the “To Err is Human” report, this study found that adverse events persist as a major problem in delivery of perioperative care across diverse surgical specialties. We observed that adverse events affected more than one third of patients admitted to hospital for surgery, with nearly half constituting major events resulting in serious or life threatening harm to patients, or death. About one fourth of all patients experienced potentially preventable adverse events, with one in 10 concerning events that were probably or definitely preventable. The most common types of adverse events were associated with surgical procedures, followed by adverse drug events, healthcare associated infections, and patient care events. Half of these incidents took place in general care units and a quarter in operating rooms. The professions most frequently involved in adverse events were attending physicians, nurses, residents, and advanced level practitioners.
Comparison with other studies
Compared with adverse event incidence rates previously estimated from the SafeCare study across all inpatient admissions, the higher rates observed in this subsample suggest that adverse events are more likely to occur in surgical care than in non-surgical care.7In line with a similar study conducted in 1992, which provided a detailed analysis of surgical adverse events in hospitals in Colorado and Utah, we found that most of these events were potentially preventable.16Since then, systematic reviews have successively compiled previous studies worldwide, estimating the frequency and preventability of adverse events. One review estimated an overall incidence of adverse events in 2008 at 9.2%, with most associated with surgical care and nearly half deemed preventable.17A review in 2013 found that surgical adverse events had occurred in 14.4% of patients, and that 5.2% were preventable.18A review in 2019 reported a 20% rate of adverse events in surgery, including a 10% prevalence of preventable patient harm.19Interpreting data from the national surgical quality improvement programme over time also revealed stable outcomes or modest trends for improvement in surgical safety since 2008, with somewhat inconsistent patterns observed across different surgical procedures and complications.2021Our study found higher incidences of adverse event rates, possibly attributed to improved traceability of care incidents with the use of electronic health records, and increased sensitivity in screening all events along a patient’s perioperative care pathway. The increased incidence of surgical complications might also be explained by our emphasis on inpatient surgery, which concentrated on patients with more complex issues, while excluding less risky procedures that have gradually transitioned from inpatient to outpatient settings over the past decades. Overall, while this may suggest a lack of improvement over the decades, direct comparisons of our findings with previous estimates of adverse event rates are challenging owing to changes in data quality and the context of care.
Strengths and limitations of this study
This study has notable strengths and limitations. We relied on a multicenter random sample of patients admitted to hospital for surgery to estimate incidence rates for adverse events. A comprehensive review of medical records by trained nurses and physicians enabled us to systematically categorize the seriousness and preventability of each adverse event, along with the main characteristics of the events. The study population was, however, confined to the US state of Massachusetts in 2018 and may not fully represent hospitals at large. This limitation affects the generalizability of the findings to other healthcare settings, warranting replication outside of the US. We could have opted for larger available datasets, such as the US National Inpatient Sample of the Healthcare Cost and Utilization Project or the registry of the national surgical quality improvement programme, to allow for greater generalizability and the interpretation of temporal changes. However, neither dataset offers both the representativeness and sufficient data validity to assess surgical outcomes nationwide.2223Additionally, these data lacked the granularity needed to categorize adverse events based on preventability, severity, setting, and professionals involved through an in-depth analysis of patient electronic health records and an accurate nurse-physician adjudication process. Another study limitation was the study’s retrospective design, which relied heavily on the validity of data retrieved from electronic health records. Although these records are valuable, they are prone to inaccuracies, missing information, and variability in documentation practices across healthcare systems, potentially biasing the identification and categorization of adverse events. Owing to the limited sample size, accurate estimates of adverse event rates for each surgical specialty could not always be provided separately, and our approach likely overlooked some surgery specific adverse events. We deliberately chose to focus on a random subsample of admissions, as conducting exhaustive chart reviews for more than 60 000 admissions would represent a disproportionate effort. Furthermore, preventability is a time dependent concept, reflecting only what reviewers deemed avoidable based on the best available practices at the time. Some events considered preventable nowadays may not have been deemed so at the time when the care was initially delivered. As medical knowledge continually advances, it could be argued that essentially all patient harm may be preventable.
Policy implications
By establishing an updated reference point, this study showed that adverse events remain widespread in contemporary healthcare, causing substantial and preventable patient harm during hospital admission. Furthermore, the study addressed a critical need by exploring the specifics of these surgical events, including the important roles of post-surgical care and non-physician staff. This study found that the problem is not solely a concern for surgeons in operating rooms but involves healthcare professions throughout the hospital during perioperative care. However, the surgeon, while being part of the team, remains the leader in many aspects of perioperative care, and the admission process typically falls under the surgeon’s domain. Despite the efforts made in patient safety since the Harvard Medical Practice Study, progress has stagnated.24By comparison, the noticeable decrease in fatal incidents in commercial airlines over the past half century has been largely attributed to the firm establishment of safety culture among crew members, strong support from companies’ leadership, and transparent communication of incidents.25
Our findings suggest that errors persist in surgery, indicating the need to reassess how the structure of healthcare contributes to these ongoing challenges. While emphasizing safety as a collective responsibility for all health professionals is important, it is essential to recognize the expertise of those ultimately responsible for patient care, such as attending physicians. In modern healthcare systems where organizational and administrative factors often drive delivery of care, concern is growing that physicians have limited input into decision making processes. This is particularly concerning given reports of moral injury, burnout, high turnover rates, and resignations among healthcare professionals.26By valuing the perspectives of frontline staff and promoting collaborative approaches to care delivery, we can strive towards a system that prioritizes patient safety while also supporting the wellbeing of health professionals.
In surgery, the fundamental premise behind the national surgical quality improvement programme initiative was that measuring adverse events is pivotal to fostering patient centered and safe care. However, hospital enrolment in this programme, or pay for performance based on indicators benchmarking, did not directly result in marked improvements.272829Supplementing these strategies with dynamic monitoring of surgical outcomes and regular feedback to healthcare professionals has the potential to reduce patient harm over time.30In accordance with Codman’s intuition, learning healthcare systems need to embrace a transformative approach grounded in the timely and accurate interpretation of all available data.31This entails prospectively tracking surgical outcomes, identifying the drivers, and proposing adaptive solutions.
Conclusion
The findings of this study suggest that adverse events remain frequent and preventable in surgery, rendering perioperative care as a high risk environment for patients. This underscores the urgent need to persist in enhancing patient safety through ongoing efforts. Emphasizing the active involvement of all healthcare professionals throughout the hospital is paramount in this endeavor.
Adverse events during hospital admission represent a major cause of patient harm
Large available datasets lack the granularity needed to categorize adverse events by preventability, severity, setting, and professionals involved, through detailed analysis of electronic health records
An updated assessment is necessary to establish a reference point of the incidence rates and main characteristics of adverse events in surgery
Adverse events were identified in 38% of adults admitted to hospital for surgery, with major events occurring in 16% and potentially preventable events in 26%
These incidents were not solely a concern for surgeons in operating rooms but involved healthcare professions throughout the hospital
The findings of this study suggest that adverse events remain widespread in perioperative care, resulting in substantial and preventable patient harm
","Objectives: To estimate the frequency, severity, and preventability of adverse events associated with perioperative care, and to describe the setting and professions concerned.
Design: Multicenter retrospective cohort study.
Setting: 11 US hospitals.
Participants: 1009 patients from a randomly selected sample of 64 121 adults admitted for surgery during 2018.
Main outcome measures: Adverse events during inpatient perioperative care were assessed using a trigger method, identifying information previously associated with similar events, and from a comprehensive review of electronic health records. Trained nurses reviewed all records and flagged admissions with possible adverse events, which were then adjudicated by physicians, who confirmed the occurrence and characteristics of the events. Adverse events were classified as major if they resulted in serious harm requiring substantial intervention or prolonged recovery, involved a life threatening event, or led to a fatal outcome. Potentially preventable events included those definitively, probably, or possibly preventable.
Results: Among 1009 patients reviewed, adverse events were identified in 38.0% (95% confidence interval 32.6 to 43.4), with major adverse events occurring in 15.9% (12.7 to 19.0). Of 593 identified adverse events, 353 (59.5%) were potentially preventable and 123 (20.7%) were definitely or probably preventable. The most common adverse events were related to surgical procedures (n=292, 49.3%), followed by adverse drug events (n=158, 26.6%), healthcare associated infections (n=74, 12.4%), patient care events (n=66, 11.2%), and blood transfusion reactions (n=3, 0.5%). Adverse events were most frequent in general care units (n=289, 48.8%), followed by operating rooms (n=155, 26.1%), intensive care units (n=77, 13.0%), recovery rooms (n=20, 3.3%), emergency departments (n=11, 1.8%), and other in-hospital locations (n=42, 7.0%). Professions most involved were attending physicians (n=531, 89.5%), followed by nurses (n=349, 58.9%), residents (n=294, 49.5%), advanced level practitioners (n=169, 28.5%), and fellows (n=68, 11.5%).
Conclusions: Adverse events were identified in more than one third of patients admitted to hospital for surgery, with nearly half of the events classified as major and most potentially preventable. These findings emphasize the critical need for ongoing improvement in patient safety, involving all health professionals, throughout perioperative care.
"
SGLT-2 inhibitors and mortality among patients with heart failure with reduced ejection fraction,"Introduction
More than 64 million people worldwide are affected by heart failure, and the prevalence of 1-3% among adults is growing as a result of population ageing.1Approximately half of all patients with heart failure have reduced ejection fraction. Mortality among this group of patients remains high, with a five year survival rate of only 25% following hospital admission.2
Recently, several new therapeutic options for the treatment of chronic heart failure have emerged, with seminal clinical trials showing significant benefits.3Sodium-glucose cotransporter-2 (SGLT-2) inhibitors are at the forefront of the recent paradigm shift in drug treatment of type 2 diabetes, with strong evidence of major beneficial effects on heart failure, cardiovascular, and renal outcomes among patients with diabetes. Subsequent trials have firmly shown that agents in this drug class reduce the risk of worsening of heart failure, mortality, and adverse renal outcomes among patients with heart failure with reduced ejection fraction, with findings consistent regardless of whether patients have diabetes.4In the DAPA-HF trial, treatment with the SGLT-2 inhibitor dapagliflozin resulted in a 26% lower risk of the composite of worsening heart failure or cardiovascular death among patients with heart failure with reduced ejection fraction, compared with placebo.5In the EMPEROR-Reduced trial, a corresponding 25% reduction was observed with the SGLT-2 inhibitor empagliflozin.6This evidence prompted the approval of dapagliflozin and empagliflozin for the indication of heart failure with reduced ejection fraction in the EU and US in 2020 and 2021, respectively. In major heart failure guidelines, dapagliflozin and empagliflozin have rapidly been included as part of the standard therapeutic arsenal for all patients with heart failure with reduced ejection fraction, to reduce the risk of hospital admission and death.78The most recent guidelines for the management of heart failure with reduced ejection fraction recommend a combination of a renin-angiotensin system inhibitor, a β blocker, a mineralocorticoid receptor antagonist, and an SGLT-2 inhibitor indicated for heart failure, to reduce the risk of hospital admission and death.
Solid clinical trial evidence supports the efficacy of SGLT-2 inhibitors, but their effectiveness in broad heart failure populations seen in everyday clinical practice is largely unknown. Clinical trials typically rely on stringent eligibility criteria and homogeneous populations, whereas patients in routine practice are much more heterogeneous, with variable baseline risk of major outcomes and differing levels of concomitant diseases. Using Danish national registers,9this study investigated the association between use of SGLT-2 inhibitors with an indication for heart failure and the risk of all cause mortality, compared with use of other standard-of-care heart failure drugs and non-use of SGLT-2 inhibitors, among patients with heart failure with reduced ejection fraction.
Methods
Study design
This was a non-interventional database study in Denmark, based on the Danish Heart Failure Registry (DHR) and linked national registers,9from July 2020 to June 2023. Although SGLT-2 inhibitors received their first approval for the indication of heart failure with reduced ejection fraction regardless of type 2 diabetes in the EU in November 2020, the landmark DAPA-HF trial had already shown significant beneficial effects in 2019,5prompting the endorsement of SGLT-2 inhibitors for the treatment of heart failure with reduced ejection fraction by major clinical societies.1011
We identified patients with heart failure from the DHR, which includes records of patients with a first time primary diagnosis of heart failure in specialist care (ICD-10 (international classification of diseases, 10th revision) codes: I11.0, I13.0, I13.2, I42.0, I42.6, I42.7, I42.9, I50.0, I50.1, and I50.9).9All Danish hospital departments treating inpatients or outpatients with incident heart failure are required to report to the DHR. The study included patients aged ≥45 years with a left ventricular ejection fraction of ≤40%.
The primary outcome was all cause mortality. Secondary outcomes included the composite of cardiovascular mortality or hospital admission for heart failure and the individual components of this composite. We did subgroup analyses of the primary outcome according to categories of age, sex, left ventricular ejection fraction, New York Heart Association (NYHA) class, recent hospital admission for heart failure, history of ischaemic heart disease, chronic kidney disease, and type 2 diabetes.
The study used a modified prevalent new user design,12including patients who started treatment with SGLT-2 inhibitors indicated for heart failure (dapagliflozin and empagliflozin) and patients who remained on standard-of-care drugs for heart failure with reduced ejection fraction and did not use SGLT-2 inhibitors (the comparator group), matched on time since diagnosis of heart failure. We intended this definition of the comparator group to reflect routine heart failure care with non-use of SGLT-2 inhibitors, mimicking the causal contrast of a trial of SGLT-2 inhibitors versus standard of care alone.12
For the SGLT-2 inhibitor group, the index date—the date of start of follow-up—occurred on the date of the first prescription of an SGLT-2 inhibitor following a first time diagnosis of heart failure made in specialist care. For the comparator group of non-users of SGLT-2 inhibitors, we considered the date of each prescription for other standard-of-care heart failure drugs, including renin-angiotensin system inhibitors, β blockers, and mineralocorticoid receptor antagonists, filled following a first time specialist care diagnosis of heart failure with reduced ejection fraction during the study period, to be a potential index date. Anatomic Therapeutic Chemical (ATC) codes used to define the SGLT-2 inhibitor group and the potential comparator index dates are listed in supplementary table A.
Baseline exclusion criteria included a prescription for any SGLT-2 inhibitor at any time before the index date, heart transplant, end stage illness, drug misuse, end stage renal disease or kidney transplantation, nursing home residence, and absence of specialist care contact due to cardiovascular causes in the previous 30 days (to ensure that all patients had the recent opportunity to receive a prescription for an SGLT-2 inhibitor). Definitions are provided in supplementary table B.
For the SGLT-2 inhibitor group, we included all incident users meeting the eligibility criteria. For the comparator group, we selected a single index date for each patient through a matching process based on the distribution of time since diagnosis of heart failure observed in the SGLT-2 inhibitor group. This approach served to balance the two treatment groups on time since diagnosis of heart failure at baseline, to align baseline characteristics. The process for selecting a single index date from the potentially multiple index dates per patient in the comparator group is detailed in the supplementary methods.
Data sources
We obtained data on the use of study drugs from the National Prescription Register, which holds comprehensive information on all pharmacy dispensations in Denmark, including dates and ATC codes.13The primary outcome was based on information from the Civil Registration System,14which holds practically complete coverage of vital status for all residents of Denmark. Data on cardiovascular death and hospital admission with heart failure came from the Danish Register of Causes of Death (CDR) and primary diagnoses at hospital admissions as recorded in the National Patient Register (NPR), respectively.1516The NPR comprehensively captures all hospital contacts, with records of the date, setting, and discharge diagnoses, categorised under the ICD-10. The CDR compiles data from death certificates, including physician determined causes of death for all deaths in Denmark, categorised according to ICD-10. We defined the date for the composite outcome as the date of the first occurrence of any component of the outcome. Definitions of all outcomes are provided in supplementary table C.
We obtained information on covariates from multiple data sources: heart failure characteristics, including left ventricular ejection fraction and NYHA classification, came from the DHR; demographic information came from the Civil Registration System; educational attainment came from Statistics Denmark; medical history, procedures, and healthcare utilisation came from the NPR; prescription drug use came from the prescription register; and measurements of estimated glomerular filtration rate and N-terminal pro-hormone of brain natriuretic peptide (NT-proBNP) came from the Danish Register of Laboratory Results for Research.17Covariate definitions are provided in supplementary table D. We selected the covariates on the basis of their known or expected associations with both the treatment and outcome or the outcome alone.
Statistical analysis
We defined treatment status by using a “per protocol” analytical approach, in which we considered each prescription to provide 180 days of treatment coverage. We assumed patients in both treatment groups to remain on treatment for as long as a new prescription was filled before the end of the 180 day period from the previous prescription. We considered patients who did not refill within this timeframe to have ended treatment. Study follow-up started on the index date and ended on the date of an outcome event or censoring (due to end of study period, death, emigration, or end of treatment), whichever occurred first. For the patients in the comparator group, an additional censoring criterion was the prescription of any SGLT-2 inhibitor during follow-up.
To account for differences in baseline characteristics between the treatment groups, we adjusted the results by using inverse probability of treatment weighting with stabilised weights based on propensity scores,18estimated using the variables intable 1,table 2, andtable 3as predictors. For each patient, we calculated the weight as the inverse of the propensity score for the treatment group and as the inverse of 1 minus the propensity score for the control group. Stabilisation was achieved by multiplying each weight by the marginal probability of receiving the treatment. We excluded patients with a propensity score outside the overlapping area of the distribution for both treatment groups. The estimated weights were truncated at the first and 99th centiles of the overall weight distribution to mitigate the potential influence of large weights.19We assessed the balance of baseline covariates by using standardised mean differences; we considered covariates to be well balanced if the standardised mean difference was <0.1.
We used proportional hazards regression to estimate the risk of the primary and secondary outcomes associated with use of SGLT-2 inhibitors, compared with non-use. The underlying time scale was days since the index date. To consider non-independence between observations due to weighting, we calculated the variance of the proportional hazards regression models by using the robust sandwich estimator. We used weighted Poisson regression to estimate the absolute rate difference. To correct for non-independence due to weighting, we calculated the variance by using generalised estimating equations.
For each subgroup analysis, we estimated a separate propensity score and conducted weighting within the subgroup. Definitions of all subgroups are provided in supplementary table E. We assessed differences between subgroups by use of the Wald test for homogeneity, with a P<0.05 considered significant.
Missing values on education, NYHA classification, baseline estimated glomerular filtration rate, mean estimated glomerular filtration rate in the previous 365 days, alcohol consumption, and smoking status (the covariates with missing values) were handled by use of a missing value category.20We used SAS software version 9.4 for the statistical analyses.
We did several sensitivity analyses for the primary outcome. Firstly, whereas the main analysis followed a per protocol approach, a sensitivity analysis used a modified intention-to-treat approach in which patients were not censored at the end of treatment. Secondly, as patients in the main analysis by design were censored on crossover only from the comparator group to SGLT-2 inhibitors (but not vice versa), inverse probability of censoring weighting was applied.21We used this method to assess the potential effect of differential or informative censoring, aiming to balance the population according to treatment and censoring selection factors that might influence the results. Thirdly, owing to the strong prognostic value of NT-proBNP for mortality in heart failure,7we further analysed the primary outcome with an adjustment for NT-proBNP concentrations recorded in the previous 180 days. This variable was included in the propensity score model, categorised as normal/mildly elevated versus severely elevated NT-proBNP, defined in alignment with the European Society of Cardiology guidelines for diagnosis of acute heart failure.7As NT-proBNP was available only for a subset (41%) of the study population, we did this sensitivity analysis on a complete case basis, relying solely on patients with NT-proBNP measurements in the previous 180 days.
Patient and public involvement
The study was conducted using anonymised nationwide register data, which inherently limits direct patient and public involvement. Data protection regulations further restricted our ability to directly involve patients in the study’s design, conduct, or interpretation. Although we strongly support patient and public engagement, neither the necessary infrastructure nor the specific funding was available to facilitate the involvement of patients in the research process.
Results
After we applied the exclusion criteria, 6778 patients who started treatment with SGLT-2 inhibitors and 14 702 patients who remained on other standard-of-care-drugs for heart failure with reduced ejection fraction and did not use SGLT-2 inhibitors were included (fig 1). Following propensity score estimation (odds ratios from the propensity score model are shown in supplementary table F) and weighting, the study population included 6776 users and 14 686 non-users of SGLT-2 inhibitors. The treatment groups were well balanced on all measured characteristics after weighting (table 1;table 2;table 3), with a mean stabilised weight of 0.99 and a maximum stabilised weight of 2.72 following truncation. Most SGLT-2 inhibitor users were male (70%), the mean age was 71.2 (standard deviation 10.6) years, and 20% had type 2 diabetes. Among the patients in the SGLT-2 inhibitor group, 5366 (79%) started treatment with dapagliflozin and 1410 (21%) with empagliflozin.
In the primary analysis, the median duration of treatment was 0.8 (interquartile range 0.5-1.4) years for the SGLT-2 inhibitor group and 1.1 (0.3-2.3) years for the comparator group.Figure 2shows the weighted cumulative incidence curve for the primary outcome of all cause mortality. During follow-up, 374 deaths occurred among SGLT-2 inhibitor users (incidence rate 5.8 per 100 person years) and 1602 among non-users of SGLT-2 inhibitors (8.5 per 100 person years). Treatment with SGLT-2 inhibitors was associated with significantly lower risk of all cause mortality (inverse probability of treatment weighted hazard ratio 0.75, 95% confidence interval (CI) 0.66 to 0.85). In terms of weighted absolute risk difference, use of SGLT-2 inhibitors was associated with 1.6 (95% CI 0.8 to 2.5) fewer deaths per 100 person years, compared with non-use.
Table 4shows the results for the secondary outcomes; the weighted cumulative incidence curves are shown in supplementary figures A-C. Use of SGLT-2 inhibitors was associated with a significantly lower risk of cardiovascular mortality (weighted hazard ratio 0.77, 95% CI 0.64 to 0.92) but not of the composite of cardiovascular mortality or hospital admission due to heart failure (0.94, 0.85 to 1.04) or hospital admission due to heart failure alone (1.03, 0.92 to 1.15).
Figure 3shows the subgroup analyses for the primary outcome. Although the rate of mortality varied markedly across levels of several subgroups, we observed no statistically significant interaction for any of the subgroups. The weighted hazard ratios for all cause mortality were similar among patients with (0.73, 95% CI 0.58 to 0.91) and without (0.73, 0.63 to 0.85) type 2 diabetes (P=0.99).
In sensitivity analyses of the primary outcome, the results were similar to those of the main analysis when we used a modified intention-to-treat approach (weighted hazard ratio 0.80, 95% CI 0.71 to 0.90; incidence curve in supplementary figure D), when we used inverse probability of censoring weighting (0.78, 0.69 to 0.88), and with additional adjustment for NT-proBNP (0.68, 0.57 to 0.82; incidence curve in supplementary figure E). The results of the sensitivity analyses are shown in supplementary table G.
Discussion
In this large scale database study of patients with heart failure with reduced ejection fraction, use of SGLT-2 inhibitors was associated with a statistically significant 25% lower risk of all cause mortality compared with non-use, showing the effectiveness of SGLT-2 inhibitors in the real world clinical setting. The magnitude of the association was consistent across all investigated subgroups, including those with and without type 2 diabetes. In addition, treatment with SGLT-2 inhibitors was associated with a 23% significantly lower risk of the secondary outcome of cardiovascular mortality, but not of the secondary composite outcome of cardiovascular mortality or hospital admission with heart failure or of hospital admission with heart failure alone.
Comparison with existing evidence
In the two landmark randomised controlled trials investigating dapagliflozin and empagliflozin for heart failure with reduced ejection fraction, both studies showed a significant 25% reduction in the primary composite outcome of cardiovascular death or worsening heart failure compared with placebo. The DAPA-HF trial observed a statistically significant reduction in both all cause mortality (hazard ratio 0.83, 95% CI 0.71 to 0.97) and cardiovascular mortality (0.82, 0.69 to 0.98), and the EMPEROR-Reduced trial showed similarly beneficial, albeit not statistically significant, trends (hazard ratio 0.92 (95% CI 0.77 to 1.10) for all cause mortality and 0.92 (0.75 to 1.12) for cardiovascular mortality). This study corroborates these findings, showing a significantly lower risk of both all cause mortality and cardiovascular mortality with SGLT-2 inhibitors. Furthermore, consistent with observations in the trials, the effectiveness of SGLT-2 inhibitors in this study was similar in patients with and without type 2 diabetes. Conversely, this study did not find an association between use of SGLT-2 inhibitors and reduced risk of hospital admission with heart failure, which contrasts with previous randomised controlled trials. In a post hoc analysis, we observed a similar result when we used the broader outcome of cardiovascular hospital admission (weighted hazard ratio 1.02, 95% CI 0.94 to 1.10). Similar discrepancies between database studies and trial data concerning the risk of hospital admission among patients with heart failure have previously been noted and may be attributed to variations in coding practices.2223Specifically, the strict and rigorous event adjudication typical in clinical trials may contrast with diagnostic coding in routine clinical practice, making direct comparisons difficult. Nevertheless, the positive predictive values for first time hospital admission and readmission for heart failure, as registered in the Danish National Patient Register, were both 76% in a validation study.24
To our knowledge, no previous large scale database studies have investigated the effectiveness of SGLT-2 inhibitors on mortality among patients with heart failure with reduced ejection fraction, with or without type 2 diabetes. The results from our study expand the available evidence through robust analyses of a well characterised set of patients with heart failure, including information on left ventricular ejection fraction, NYHA class, and renal function. This study complements previous randomised controlled trials by extending the evidence to a broad heart failure population in routine clinical practice, offering valuable insights into the effectiveness of SGLT-2 inhibitors across diverse patient groups. In terms of clinical impact, the results support current guidelines recommending SGLT-2 inhibitors for all patients with heart failure with reduced ejection fraction.78
Strengths and limitations of study
This study has several strengths. Firstly, the use of high quality data sources, including a comprehensive, nationwide heart failure registry, allowed for reliable identification of patients with heart failure with reduced ejection fraction and their outcomes. Moreover, these data sources provided key clinical measures of heart failure, such as left ventricular ejection fraction, NYHA classification, and renal function, which we took into account through propensity score weighting. Additional measures to enhance internal validity included the requirement for a recent cardiovascular visit, ensuring that all patients had a recent opportunity to start SGLT-2 inhibitor treatment while also confirming recent clinical assessment of the included patients. Finally, the use of a modified prevalent new user design, comparing patients who started and continued SGLT-2 inhibitors with those who remained on standard-of-care heart failure drugs without the addition of SGLT-2 inhibitors, mimicked the causal contrast of a placebo controlled trial12; this design allowed for a direct assessment of the added benefit of SGLT-2 inhibitors in heart failure with reduced ejection fraction, compared with standard of care alone.
We acknowledge several potential limitations. Firstly, we used prescription fills as a surrogate for actual treatment, but these may not always equate to adherence, potentially leading to exposure misclassification. Assuming that misclassification was non-differential, this would likely bias the results towards the null. Although the main analysis used a per protocol approach, sensitivity analysis using an observational analogue of modified intention to treat led to similar results. Secondly, as patients crossing over from the comparator group to SGLT-2 inhibitors were censored (whereas crossover to the comparator group was not possible), differential censoring could potentially influence the findings. However, the results from sensitivity analyses with inverse probability of censoring weighting suggested no material effect on the results. Thirdly, although the sample size in the main analysis of the primary outcome was sufficient to detect even small differences between the study drugs, the statistical precision for some subgroup analyses was limited, thus restricting the ability to detect less pronounced differences in treatment effects across subgroups. Fourthly, excluding patients without recent specialist care contact may have limited generalisability but ensured that the results are representative of patients in active heart failure management, who could be considered for SGLT-2 inhibitor treatment. Fifthly, not all patients had a recent measurement of NT-proBNP. We explored the effect of NT-proBNP concentrations on treatment outcomes in a sensitivity analysis, which we conducted on a complete case basis; the results of this analysis aligned with the results of the main analysis. Other information not available included blood pressure control and non-drug interventions such as exercise and weight management. Thus, despite multiple measures to maximise internal validity, as for all non-randomised studies, unmeasured confounding cannot be ruled out.
Conclusions
This database study showed that the use of SGLT-2 inhibitors was significantly associated with lower risk of all cause and cardiovascular mortality in patients with heart failure with reduced ejection fraction. These results support the benefits of SGLT-2 inhibitors observed in randomised controlled trials and provide novel and important data on their effectiveness in real world clinical settings and across key clinical subgroups, including patients with and without diabetes.
","Objective: To investigate the association between sodium-glucose cotransporter-2 (SGLT-2) inhibitor use and risk of all cause mortality among patients with heart failure with reduced ejection fraction.
Design: Linked database study.
Setting: National registers in Denmark, July 2020 to June 2023.
Participants: Patients with heart failure, aged ≥45 years, with left ventricular ejection fraction ≤40%.
Main outcome measures: The primary outcome was all cause mortality comparing initiation and continued treatment with SGLT-2 inhibitors versus continued treatment with other standard-of-care heart failure drugs and non-use of SGLT-2 inhibitors; secondary outcomes were the composite of cardiovascular mortality or admission to hospital with heart failure and its individual components. Hazard ratios were estimated using Cox regression adjusted using inverse probability of treatment weighting based on propensity scores.
Results: The study included 6776 patients who started SGLT-2 inhibitors (79% dapagliflozin; 21% empagliflozin) and 14 686 patients who remained on other standard-of-care heart failure drugs and did not use SGLT-2 inhibitors. Most SGLT-2 inhibitor users were male (70%), the mean age was 71.2 (standard deviation 10.6) years, and 20% had type 2 diabetes. During follow-up, 374 deaths occurred among SGLT-2 inhibitor users (incidence rate 5.8 per 100 person years) and 1602 among non-users (8.5 per 100 person years). The weighted hazard ratio for all cause mortality was 0.75 (95% confidence interval 0.66 to 0.85); the weighted incidence rate difference was −1.6 (95% confidence interval −2.5 to −0.8) per 100 person years. Secondary outcomes showed a weighted hazard ratio of 0.94 (0.85 to 1.04) for cardiovascular mortality or hospital admission with heart failure, 0.77 (0.64 to 0.92) for cardiovascular mortality, and 1.03 (0.92 to 1.15) for hospital admission with heart failure. The weighted hazard ratios for all cause mortality were consistent in patients with and without diabetes (0.73 (0.58 to 0.91) and 0.73 (0.63 to 0.85); P=0.99).
Conclusions: In this large database study among patients with heart failure with reduced ejection fraction, SGLT-2 inhibitor use was associated with a 25% lower risk of all cause mortality, supporting their effectiveness in routine clinical practice.
"
SGLT-2 inhibitors for recurrent nephrolithiasis among patients with pre-existing nephrolithiasis or gout,"Introduction
Nephrolithiasis and gout are both common, recurrent, extremely painful conditions with increasing disease burden globally.12345The prevalence of nephrolithiasis in the US, for example, has tripled from 3.2% to 10.9% over several decades, likely as a result of increasing prevalence of obesity, diabetes, and the metabolic syndrome, all key risk factors for nephrolithiasis.67The burden of nephrolithiasis remains considerable, with high recurrence89requiring urgent treatment as well as complications (eg, urosepsis),10hypertension, and permanent kidney damage.11Together with the frequent requirement of surgical interventions to remove impacted stones, the healthcare costs are substantial, with its economic burden in the US projected to reach $4.6bn annually by 2030.12
Gout, which now affects more than 12 million adults in the US (5.1%),13is a common and strong independent risk factor for nephrolithiasis,14151617as is type 2 diabetes.1819Since their initial approval for the treatment of hyperglycemia in type 2 diabetes, sodium-glucose cotransporter-2 (SGLT-2) inhibitors have shown multiple cardiometabolic-kidney benefits.202122Furthermore, randomized controlled trials found that SGLT-2 inhibitors also reduce serum urate levels,23242526by increasing urinary uric acid excretion, which also translated to lower risk of gout flare-ups.2728As such, the capacity of SGLT-2 inhibitors to increase urinary flow293031(osmotic diuresis) and uricosuria could both have an effect on nephrolithiasis risk, although potentially in opposing directions. Support is provided by adverse events data from randomized trials,32including a meta-analysis of empagliflozin trials (n=183 events), which found a 36% lower risk of urolithiasis compared with placebo,33and two larger scale comparative effectiveness analyses.3435Incident nephrolithiasis was the primary outcome in both analyses, however, and data on prevention of recurrent nephrolithiasis (relevant to clinical care) were scarce; those among patients with gout (who are known to have a higher risk of uric acid stones), are lacking.
We used target trial emulation to evaluate the recurrence of nephrolithiasis in people with type 2 diabetes initiating an SGLT-2 inhibitor compared with a glucagon-like peptide-1 (GLP-1) receptor agonist, another second line glucose lowering agent (and dipeptidyl peptidase 4 (DPP-4) inhibitor as an alternative comparator). Our primary target trial emulation was among patients with nephrolithiasis overall. In secondary target trial emulations, we stratified these patients by pre-existing gout at baseline, and we also included those without nephrolithiasis at baseline.
Methods
Target trial
We followed the framework proposed by Hernan and Robins36to emulate two arm randomized clinical trials (the target trials) that would randomize patients with nephrolithiasis and type 2 diabetes to initiate an SGLT-2 inhibitor versus GLP-1 receptor agonist (as well as SGLT-2 inhibitor versus DPP-4 inhibitor as an alternative comparator), in an unblinded fashion. We first specified the framework for the target trial emulation that would address our research questions (see supplementary tables S1-S4), including eligibility criteria, treatment strategies and assignment, start and end of follow-up, outcomes, causal contrasts (eg, intention-to-treat effect or per protocol effect), and analysis plan. We then specified how we would use observational data to emulate components of the protocol and conduct the respective analyses.3637
Source population
The source population for the target trials was the entire population of the province of British Columbia (BC) in Canada. We used Population Data BC, population based linked administrative databases that provide deidentified individual level data for nearly all of British Columbia’s five million residents on provincially funded outpatient medical visits and hospital discharges (since 1990), emergency department encounters (since 2011), and dispensed prescriptions, regardless of age or funding source (since 1996). Vital statistics were available for all individuals. These data have been used in previous studies of diabetes3839and gout.274041424344The current study followed the recommendations of the STROBE (strengthening the reporting of observational studies in epidemiology) initiative.45We used six datasets: BC Medical Services Plan, Consolidation Database, Hospital Separations, PharmaNet, National Ambulatory Care Reporting System, and Vital Statistics Deaths.
Study population and design
For our primary emulated target trial, we performed a new user cohort study with inverse probability weighting to compare the risk of recurrent nephrolithiasis among people with type 2 diabetes and pre-existing nephrolithiasis who initiated an SGLT-2 inhibitor or GLP-1 receptor agonist (an active comparator). In the trials, we included adults aged ≥18 years with type 2 diabetes whose first ever dispensing for one of the comparison drugs was between 1 January 2014 and 30 June 2022, after at least one year of continuous enrolment with the provincial medical services plan (see supplementary figure S1). Diagnoses were based on the presence of at least one ICD-9 or ICD-10 (international classification of diseases, ninth revision (outpatient encounters) and 10th revision (inpatient and emergency department encounters), respectively)2734464748code (see supplementary table S5). Our primary target trial emulation (see supplementary table S1) for recurrent nephrolithiasis included those with a nephrolithiasis diagnosis at baseline (any time before the drug index date) based on ICD-9 and ICD-10 codes, whereas our three secondary target trial emulations included those with pre-existing nephrolithiasis and pre-existing gout (ie, diagnoses of both conditions recorded any time before the index date; see supplementary table S2), those with pre-existing nephrolithiasis but no history of gout (no recorded diagnosis of gout before the index date; see supplementary table S3), and those without a diagnosis of nephrolithiasis at baseline (no recorded diagnosis of nephrolithiasis before the index date; see supplementary table S4). In all analyses, we also excluded individuals who were dispensed an SGLT-2 inhibitor combination pill or an SGLT-2 inhibitor and GLP-1 receptor agonist at the same time (see supplementary figure S1).
Outcome assessment
The primary outcome was recurrent nephrolithiasis events during follow-up, defined by an emergency department visit or hospital admission with a primary discharge diagnosis of nephrolithiasis via ICD-10 codes, or having an ICD-9 code for nephrolithiasis recorded during an outpatient encounter (see supplementary table S5); the latter has been found to have positive predictive value of 91.8% and 95.9% for an active nephrolithiasis diagnosis and any nephrolithiasis diagnosis, respectively.46Recurrent nephrolithiasis was defined as having an active nephrolithiasis event among those with a history of nephrolithiasis. Secondary outcomes included the first recurrent nephrolithiasis event during follow-up and recurrent nephrolithiasis counts resulting in emergency department visits or hospital admission (as the primary discharge diagnosis) and those that required interventions including procedural or surgical removal of stones (see supplementary table S5). In the emulated trial of patients with concurrent gout (see supplementary table S2), recurrent flare-ups were an additional outcome, defined by emergency department visit or hospital admission with a primary discharge diagnosis of gout, or an ICD-9 code for gout recorded during an outpatient encounter together with at least one of the following treatments dispensed within seven days: intra-articular or oral corticosteroids, colchicine, or non-steroidal anti-inflammatory drugs.27A first gout related visit to an emergency department or hospital admission49and the combination of gout related visit and drug dispensing or procedure5051have been found to accurately ascertain gout flare-ups, with a positive predictive value of 95% for that combination in identifying patients with at least one flare-up during a 15 month study period,50closely resembling our ascertainment of flare-up counts over a similar follow-up period.
Covariate assessment
Covariates, selected a priori based on previous literature1952535455and subject and methodologic expertise, included sociodemographic factors (age, sex, area level socioeconomic status, and residence in one of British Columbia’s five geographic health regions), calendar year of first dispensing, time since first recorded diagnosis of diabetes, chronic comorbidities (eg, cardiovascular disease, hypertension, chronic kidney disease), diabetes complications (neuropathy, nephropathy, and retinopathy), risk factors for nephrolithiasis (nephrolithiasis duration, frequency of nephrolithiasis events in the past 12 months, gout diagnosis, obesity, thiazide diuretic use, and use of urate lowering treatment), relevant drugs, and healthcare utilization (table 1). We also included gout duration, baseline flare-up rate, and use of drugs for flare-ups in the secondary target trial emulation among patients with nephrolithiasis and concomitant gout. Medical conditions were assessed from 1990 or enrolment, whichever came later, whereas healthcare utilization and drug use were assessed during the year before the target trial index date.
Cohort follow-up
In our primary analysis, follow-up started the day of first dispensing (target trial index date) and continued until end of the study period (30 June 2022), deregistration from the provincial medical plan, occurrence of nephrolithiasis (except recurrent nephrolithiasis counts), discontinuation of the index drug (>60 days after the expiration date of the last dispensed supply2734535455) (see supplementary tables S1-S4), switching to or addition of the comparator drug, or death.
Statistical analysis
To account for non-random allocation of patients to the treatment groups, we used stabilized inverse probability of treatment weighting for the average treatment effect in the whole population.5657Weights were calculated based on all covariates described above in a multivariable logistic regression model to predict each patient’s probability (ie, propensity score) of filling a prescription for an SGLT-2 inhibitor or GLP-1 receptor agonist before the start of follow-up. Inverse probability of treatment weights based on propensity scores of the cohort result in a weighted cohort where treatment assignment is independent of measured confounders.58Stabilized weights are recommended to account for the otherwise large weights assigned to individuals with a low probability of receiving the treatment of interest, and resultant large variance for the inverse probability of treatment weights estimator.5659We assessed the balance of the distribution of covariates before and after weighting using standardized differences between treatment groups (<0.1 denoting negligible difference).58In a sensitivity analysis we applied overlap weighting of the propensity score instead of inverse probability of treatment weighting to balance baseline characteristics.6061The overlap weighting estimate can be interpreted as the average treatment effect in the overlap population, representing patients with a realistic likelihood of receiving either treatment.57This approach results in exact balance of the mean of every measured covariate.61
We report crude and weighted numbers of events, person time, incidence rates per 1000 person years, and rate differences per 1000 person years for each outcome by treatment group. To facilitate interpretation, we also calculate the number needed to treat (NNT). For each comparison, weighted Poisson or Cox proportional hazards models were used to estimate rate ratios or hazard ratios and corresponding 95% confidence intervals (CIs),62while accounting for competing risk of death.63An E value was calculated to evaluate the robustness of our primary outcome to unmeasured confounders.64We obtained rate differences and corresponding CIs using the methods of Rothman and colleagues.65Missing categories were used for the two variables with missing data: region (0.13%) and area level socioeconomic status (1.0%); no other data were missing. All analyses were performed using SAS statistical software, version 9.4 (SAS Institute), and statistical significance was defined as two tailed P value of ≤0.05.
Subgroup and sensitivity analyses
We conducted prespecified analyses in subgroups based on age, sex, thiazide use, and nephrolithiasis over the previous year (as a proxy for recently active nephrolithiasis). Within each subgroup, propensity scores were recalculated with patients reweighted. We conducted several sensitivity analyses to ensure the robustness of our findings, including analyses: with follow-up truncated after one year; with patients censored one year after the target trial index date unless they had already been censored based on the previously mentioned criteria except drug discontinuation (carrying forward use of the index drug up to one year, akin to an intention-to-treat analysis48); restricted to individuals treated with metformin monotherapy at baseline (ie, using no other glucose lowering drugs, to improve comparability of treatment groups); and comparing initiation of an SGLT-2 inhibitor with a DPP-4 inhibitor instead of a GLP-1 receptor agonist.66Furthermore, to evaluate reproducibility with established relations and the presence of spurious associations, we assessed the risk of a positive control outcome, genital infection (see supplementary table S5), which is positively associated with SGLT-2 inhibitors relative to GLP-1 receptor agonists, per randomized trials,67and for risk of any osteoarthritis encounter and appendicitis (see supplementary table S5), two negative control outcomes for which we expected null associations.68
Patient and public involvement
No patients were involved in setting the research question or the outcome measures, nor were they involved in developing plans for design or implementation of the study. No patients were asked to advise on the interpretation or writing up of results.
Results
The emulated trial for initiation of an SGLT-2 inhibitor versus GLP-1 receptor agonist included 20 146 patients with nephrolithiasis and type 2 diabetes, including those with concomitant gout during the study period, before inverse probability weighting: 14 400 initiating an SGLT-2 inhibitor and 5746 initiating a GLP-1 receptor agonist (see supplementary figure S1). Even before weighting, risk factors for nephrolithiasis, such as frequency of nephrolithiasis events in the past 12 months, duration of nephrolithiasis, gout, thiazide use, and use of urate lowering and alkali treatment, were similar between treatment groups, except for a higher prevalence of obesity among those using GLP-1 receptor agonists (see supplementary table S6). After inverse probability weighting, all baseline covariates (including obesity) were well balanced (standardized differences <0.1), with 14 456 initiators of a SGLT-2 inhibitor and 5877 of a GLP-1 receptor agonist included in the target trial (table 1). Approximately 63% of the study population were men, mean age at index date 63 years, and 22% of the study population had concurrent gout. Twenty two per cent of patients in each group were using thiazide diuretics, and 9% were using urate lowering treatment; <1% were using alkali treatment. Mean follow-up was 1.3 years among initiators of an SGLT-2 inhibitor and 0.93 years among initiators of an GLP-1 receptor agonist, with empagliflozin and semaglutide accounting for most prescriptions (68% and 59%, respectively) (see supplementary table S7). The most common reason for censoring was discontinuation of index treatments, followed by end of the study period (see supplementary table S8).
Nephrolithiasis recurrence among patients with pre-existing nephrolithiasis
Before inverse probability weighting, 1892 recurrent nephrolithiasis events occurred among 14 400 unweighted patients who initiated an SGLT-2 inhibitor (99.7 per 1000 person years), compared with 847 events among 5746 unweighted patients who initiated a GLP-1 receptor agonist (177.9 per 1000 person years). The unweighted rate ratio associated with SGLT-2 inhibitor initiation was 0.57 (95% CI 0.52 to 0.62), and the unadjusted rate difference was −78 (95% CI −91 to −65) per 1000 person years. After inverse probability weighting, 1924 recurrent nephrolithiasis events occurred among the 14 456 weighted patients who initiated an SGLT-2 inhibitor (105.3 per 1000 person years), compared with 853 events among the 5877 weighted patients who initiated a GLP-1 receptor agonist (156.4 per 1000 person years) (table 2). This corresponded to an adjusted rate ratio of 0.67 (95% CI 0.57 to 0.79) and rate difference of −51 (−63 to −40) per 1000 person years, with a corresponding NNT of 20, comparing initiation of SGLT-2 inhibitors with initiation of GLP-1 receptor agonists (table 2). Findings were similar when the SGLT-2 inhibitor group was restricted to those initiating empagliflozin (n=9716), with an adjusted rate ratio of 0.66 (0.60 to 0.72). When we restricted our endpoint to recurrent nephrolithiasis events diagnosed during an emergency department visit or hospital admission, the rate ratio of 0.61 (0.51 to 0.73) and rate difference of −12 (−17 to −7) per 1000 person years; the associations for events defined by requiring a procedure were similar (table 2).
In the sensitivity analysis that applied overlap weighting in place of inverse probability weighting (see supplementary table S9), the adjusted rate ratio was 0.68 (0.60 to 0.78) and the rate difference was −52 (−71 to −34) per 1000 person years (table 2). When use of the index drug was carried forward for up to one year (akin to intention-to-treat analysis), the adjusted rate ratio for recurrent nephrolithiasis was 0.77 (0.71 to 0.84) and the rate difference was −37 (−50 to −24) per 1000 person years (table 2). Restricting to individuals using metformin but no other glucose lowering drugs at baseline, the rate ratio was 0.63 (0.57 to 0.70) and the rate difference was −59 (−75 to −44) per 1000 person years (table 2). The inverse association between initiation of an SGLT-2 inhibitor and recurrent nephrolithiasis also persisted when DPP-4 inhibitors were used as comparator (n=11 911 SGLT-2 inhibitor initiators and n=8843 DPP-4 inhibitor initiators, after inverse probability weighting (table 1and supplementary table S10 and figure S2); the rate ratio associated with SGLT-2 inhibitor initiation was 0.73 (0.68 to 0.78) and the rate difference was −38 (−46 to −29) per 1000 person years (table 2). Results persisted regardless of sex, age, or baseline thiazide diuretic use (table 3). The rate ratio also persisted regardless of the recency of nephrolithiasis at baseline, although the absolute rate difference was higher in patients with recently active nephrolithiasis (−219 (−274 to −164)v−42 (−51 to −33) per 1000 person years) (table 3). The corresponding NNTs were 5 and 24.
As with recurrent nephrolithiasis counts, cumulative incidence of the first recurrent nephrolithiasis event was lower among patients initiating an SGLT-2 inhibitor than among those initiating a GLP-1 receptor agonist (fig 1), with incidence rates of 54.2 and 90.3 per 1000 person years, respectively, resulting in a hazard ratio of 0.68 (95% CI 0.58 to 0.79) and rate difference of −36 (95% CI −45 to −27) per 1000 person years. The risk for a first recurrent nephrolithiasis event was also lower for events resulting in an emergency department visit or hospital admission (see supplementary table S11).
Recurrence of nephrolithiasis and gout among patients with concurrent gout
Overall, 4409 patients with gout at baseline initiated an SGLT-2 inhibitor (n=3159) or GLP-1 receptor agonist (n=1250), before inverse probability weighting. Similar to the primary target trial emulation, even before weighting, risk factors for nephrolithiasis, such as frequency of nephrolithiasis events in the past year, nephrolithiasis duration, gout flare-up rate, gout duration, thiazide use, and use of urate lowering treatment were similar between treatment comparison groups, except for a higher prevalence of obesity among GLP-1 receptor agonist users (see supplementary table S12). After inverse probability weighting, all baseline covariates (including obesity) were well balanced, with 3159 initiators of an SGLT-2 inhibitor and 1272 initiators of a GLP-1 receptor agonist; the mean age was 65 years and 73% were men (see supplementary table S12). About 24% of patients in each treatment group were using thiazide diuretics and 34% were using urate lowering treatment. In total, 479 recurrent nephrolithiasis events occurred among the 3159 weighted patients who initiated an SGLT-2 inhibitor (122.4 per 1000 person years), compared with 218 events among the 1272 weighted patients who initiated a GLP-1 receptor agonist (174.1 per 1000 person years), with a rate ratio of 0.67 (95% CI 0.57 to 0.79) and rate difference of −53 (95% CI −78 to −27) per 1000 person years (table 4). When using DPP-4 inhibitors as the comparator group, 2668 weighted initiators of an SGLT-2 inhibitor and 2028 weighted initiators of a DPP-4 inhibitor with concurrent gout at baseline were included (76% men, mean age 66 years). Overall, 418 recurrent nephrolithiasis events occurred among SGLT-2 inhibitor initiators (113.5 per 1000 person years) and 439 among DPP-4 inhibitor initiators (175.0 per 1000 person years), with a rate ratio of 0.63 (0.55 to 0.72) and rate difference of −62 (−81 to −42).
In terms of gout outcome, recurrent flare-up counts were lower among those initiating an SGLT-2 inhibitor. Overall, 161 recurrent gout flare-up events occurred among the 3159 weighted patients with gout initiating an SGLT-2 inhibitor, and 71 flare-up events among the 1272 patients initiating a GLP-1 receptor agonist, corresponding to a rate ratio of 0.72 (0.54 to 0.95) and absolute risk reduction of 16 fewer flare-up events per 1000 person years (table 4). Initiation of an SGLT-2 inhibitor was also associated with a lower rate of recurrent gout flare-up events compared with initiation of a DPP-4 inhibitor, with a rate ratio of 0.65 (0.52 to 0.82) and absolute risk reduction of 21 fewer recurrent gout flare-up events per 1000 person years (table 4).
Rate ratios in our target trial of patients with nephrolithiasis without concurrent gout (see supplementary table S3) were similar to those of patients with concurrent gout, but the rate difference tended to be smaller (see supplementary table S13), with a more pronounced difference when comparing initiation of an SGLT-2 inhibitor with initiation of a DPP-4 inhibitor, with 62 versus 31 fewer recurrent nephrolithiasis events per 1000 person years among initiators of an SGLT-2 inhibitor with and without gout (see supplementary table S13).
Incident nephrolithiasis
We also identified 153 445 initiators of an SGLT-2 inhibitor or GLP-1 receptor agonist without a history of nephrolithiasis at baseline (since 1990 or enrolment); 112 771 initiating an SGLT-2 inhibitor and 40 674 initiating a GLP-1 receptor agonist (see supplementary table S14). After inverse probability weighting, 1072 new nephrolithiasis events occurred among 112 728 weighted initiators of an SGLT-2 inhibitor (7.4 per 1000 person years) and 499 new events among 41 623 weighted initiators of a GLP-1 receptor agonist (12.8 per 1000 person years). This corresponded to an adjusted rate ratio of 0.58 (0.52 to 0.64) and rate difference of −5 (−7 to −4) per 1000 person years (see supplementary table S15). Results were consistent when applying overlap weighting of the propensity score, with a rate ratio of 0.56 (0.49 to 0.68) and rate difference of −6 (−8 to −4) per 1000 person years.
Potential impact of unmeasured confounding
The E value for our primary target trial outcome comparing rates of recurrent nephrolithiasis events between initiators of an SGLT-2 inhibitor versus a GLP-1 receptor agonist was 2.35 overall (and 2.66 for nephrolithiasis events requiring emergency department visits or hospital admission); the corresponding E value among patients with gout was also 2.35. This finding indicates that a hypothetical unmeasured confounder would need to have a rate ratio of 2.35 (or 2.66) for both initiation of an SGLT-2 inhibitor and risk of nephrolithiasis to nullify the findings.
Control outcomes
In the positive control outcome analyses, initiation of an SGLT-2 inhibitor versus GLP-1 receptor agonist was associated with a higher risk of genital infection (incidence rates 24.1 and 10.7 per 1000 person years, respectively), with a hazard ratio of 2.21 (95% CI 1.68 to 2.90) and rate difference of 13 (95% CI 10 to 17) per 1000 person years (table 5). For our negative control outcome of osteoarthritis encounter, comparing an SGLT-2 inhibitor with a GLP-1 receptor agonist, the hazard ratio was 0.87 (0.68 to 1.14) and the rate difference was −2 (−6 to 2) per 1000 person years, respectively. Similarly, for the negative control outcome of appendicitis, the corresponding hazard ratio and rate difference were 1.07 (0.69 to 1.67) and 1 (−2 to 3) per 1000 person years (table 5). These findings for all control outcomes persisted when comparing with a DPP-4 inhibitor (table 5) and when using overlap weighting instead of inverse probability of treatment weighting to emulate randomization (see supplementary table S16).
Discussion
In these emulated trials based on general population cohorts of patients with nephrolithiasis and type 2 diabetes, rates of recurrent nephrolithiasis were 33% lower among new initiators of an SGLT-2 inhibitor compared with GLP-1 receptor agonist, a key alternative second line glucose lowering drug. This benefit corresponded to an absolute risk reduction of 51 fewer active nephrolithiasis events per 1000 person years overall (NNT of 20), and 219 fewer nephrolithiasis events among those with recently active nephrolithiasis (NNT of 5). A similar rate ratio reduction was observed among those with concurrent gout, with a simultaneous reduction in recurrent gout (ie, dual added benefits beyond diabetes). The benefits for nephrolithiasis persisted in secondary, sensitivity, and subgroup analyses, including among those taking thiazide diuretics (a therapeutic agent for nephrolithiasis), as well as for nephrolithiasis events requiring a visit to the emergency department or hospital admission, or procedures, and when comparing use of a SGLT-2 inhibitor with a DPP-4 inhibitor, another contemporary glucose lowering agent. The absolute risk reduction tended to be larger among those with concurrent gout at baseline (compared with DPP-4 inhibitor) and was substantially greater among patients with a recent nephrolithiasis event. In contrast, SGLT-2 inhibitor use was associated with a higher risk of genital infection but was not associated with risk of an osteoarthritis encounter or appendicitis compared with GLP-1 receptor agonist, as expected.68697071The nephrolithiasis benefits associated with SGLT-2 inhibitor in these target trial emulations suggest that, particularly for patients with an existing indication (eg, type 2 diabetes, heart failure, chronic kidney disease), this class of drugs may be a useful addition to current nephrolithiasis treatments to simultaneously tackle nephrolithiasis recurrence and comorbidities, including gout.
Biological mechanisms
The mechanism underlying the lower risk of recurrent nephrolithiasis with SGLT-2 inhibitor use could partly be attributed to increased urinary output from osmotic diuresis.293031Randomized controlled trials have shown increasing fluid intake lowers the risk for recurrent nephrolithiasis72by increasing urinary volume and reducing the concentration of stone forming solutes. Indeed, SGLT-2 inhibitors can increase urinary volume up to twofold, although this effect may be most pronounced at the start of treatment.73Furthermore, SGLT-2 inhibitors also lower serum urate levels2347and have been associated with lower risk of incident and recurrent gout.27477475Glycosuria is thought to be a central mechanism for the urate lowering effect, competing with soluble urate for transporter mediated reabsorption in the proximal tubule76and enhancing the excretion of uric acid. This uricosuria could raise the potential to transiently increase the risk of uric acid stones, as do prototypic uricosuric urate lowering drugs such as probenecid or benzbromarone7778; however, our results suggest that the net effect for nephrolithiasis recurrence is beneficial. Indeed, urine pH levels, rather than hyperuricosuria itself, are thought to be the main determinant of uric acid crystallization and precipitation (and potentially calcium oxalate stone formation) since a more alkaline environment will enhance solubility of uric acid crystals and prevent stone formation.3379To that end, empagliflozin can raise urine pH by inhibiting the renal sodium-hydrogen exchanger and increasing urinary bicarbonate excretion,8081whereas dapagliflozin can increase urinary citrate excretion, which can also increase urine pH and inhibit crystallization of calcium salts.3382At the same time, canagliflozin has been shown to decrease urine pH.83These apparently conflicting effects among SGLT-2 inhibitors could be explained by differences in mineral metabolism between agents, but this warrants further investigation, as does the association between SGLT-2 inhibitors and different compositions of stones. More information about the potential mechanisms of action may come from the SWEETSTONE trial,84an exploratory, phase 2 randomized trial (n=50) assessing the treatment potential of empagliflozin to prevent the recurrence of nephrolithiasis, with a primary outcome of urinary supersaturations.
Type 2 diabetes is associated with a higher risk of nephrolithiasis, which is thought to be due in part to impaired kidney ammoniagenesis, leading to low urinary pH, a primary factor in the formation of uric acid stones. Acidic urine increases the concentration of the insoluble undissociated uric acid, facilitating precipitation at levels of urine uric acid within or even below the normal range.8586For example, in a study involving 2464 patients with nephrolithiasis, type 2 diabetes was found to be the strongest predictor of uric acid stones, with an odds ratio of 6.9.87Similarly, gout was associated with a twofold higher risk of kidney stones in a large male health professional prospective cohort study,14which was also thought to be through insulin resistance (a close correlate for gout) interfering with renal ammoniagenesis leading to acidic urine.8586The prevalence of pure and mixed uric acid stones has been found be up to 43-88% in patients with gout,8889although interestingly, a previous randomized trial found allopurinol reduced recurrent events of calcium oxalate stones, suggesting potential benefit of urate reduction for preventing these stones.90To that end, the current study addressed overall risk of kidney stones irrespective of composition.
Clinical implications
Among those with the additional risk factor of concurrent gout, SGLT-2 inhibitor use was associated with simultaneous risk reduction in gout recurrence, with a trend of a larger absolute risk reduction in nephrolithiasis recurrence, given their high background rate (particularly compared with DPP-4 inhibitors). These data suggest that SGLT-2 inhibitors might be useful agents in this multimorbidity clinical context with gout, together with their established cardiovascular-kidney benefits. Furthermore, our study provides important outcome data on emergency department visits and hospital admissions for newly diagnosed nephrolithiasis as well as for patients requiring intervention; all costly and burdensome sources of healthcare utilization.91
Comparison with other studies
The findings for recurrent nephrolithiasis from our emulated trial agree with the few studies that focused on risk of newly diagnosed nephrolithiasis.333435Without prespecified ascertainment, clinical trials of SGLT-2 inhibitors have substantially underreported nephrolithiasis as an adverse event. As such, a previous meta-analysis of such trials with a limited number of nephrolithiasis events (n=106) reported no significant association32; however, a recent meta-analysis of empagliflozin trials with a larger number of reported events (n=183) found a 36% lower risk of urolithiasis compared with placebo.33Nevertheless, the latter meta-analysis had only nine patie","Objective: To emulate target trials comparing recurrence of nephrolithiasis among patients with pre-existing nephrolithiasis (overall and stratified by concomitant gout) initiating sodium-glucose cotransporter-2 (SGLT-2) inhibitors versus an active comparator.
Design: Target trial emulation studies.
Setting: Canadian population database, January 2014 to June 2022.
Participants: 20 146 patients with nephrolithiasis and type 2 diabetes, including those with concomitant gout at baseline, a high risk group.
Interventions: Initiation of an SGLT-2 inhibitor or glucagon-like peptide-1 (GLP-1) receptor agonist, with a dipeptidyl peptidase-4 (DPP-4) inhibitor as alternative comparator.
Main outcome measures: The primary outcome was recurrent nephrolithiasis events ascertained from diagnoses during emergency department visits, hospital admissions, or outpatient visits. Secondary outcomes included nephrolithiasis resulting in hospital admission or emergency department visits and flare-up of gout, as well as a positive control outcome (genital infection) and negative control outcomes (osteoarthritis encounter and appendicitis). Poisson and Cox proportional hazards regression models were used (primary analyses), as well as overlap weighting.
Results: After inverse probability of treatment weighting, 1924 recurrent nephrolithiasis events occurred among the 14 456 weighted patients who used an SGLT-2 inhibitor (105.3 per 1000 person years), compared with 853 events among the 5877 weighted patients who used a GLP-1 receptor agonist (156.4 per 1000 person years). The adjusted rate ratio was 0.67 (95% confidence interval (CI) 0.57 to 0.79) and rate difference was −51 (95% CI −63 to −40) per 1000 person years, with a number needed to treat (NNT) of 20. Among those with recently active nephrolithiasis, the absolute rate difference was 219 per 1000 person years (NNT of 5). Protective associations persisted for nephrolithiasis events that required emergency department visits, hospital admissions, or procedures, and when an SGLT-2 inhibitor was compared with a DPP-4 inhibitor (rate ratio 0.73 (0.68 to 0.78), rate difference −38 (−46 to −29) per 1000 person years (NNT of 26)). Protective associations also persisted among patients with nephrolithiasis and concomitant gout, with a rate ratio of 0.67 (0.57 to 0.79) and rate difference of –53 (95% CI –78 to –27) per 1000 person years versus a GLP-1 receptor agonist (NNT of 19), and 0.63 (0.55 to 0.72) and–62 (–81 to –42) per 1000 person years, respectively, versus a DPP-4 inhibitor (NNT of 16). Furthermore, SGLT-2 inhibitor use was associated with a lower rate of gout flare-ups (rate ratio 0.72, 0.54 to 0.95, rate difference –16, –31 to –1 per 1000 person years) compared with GLP-1 receptor agonists (0.65, 0.52 to 0.82, and –21, –33 to –9 per 1000 person years) compared with DPP-4 inhibitors. SGLT-2 inhibitor initiators showed higher risk of genital infection (eg, hazard ratio 2.21, 95% CI 1.68 to 2.90, and rate difference 13 per 1000 person years), but no altered risk of osteoarthritis encounter (0.87, 0.68 to 1.1, and –2 per 1000 person years) or appendicitis (1.07, 0.69 to 1.67, and 1 per 1000 person years). Results:  were similar when propensity score overlap weighting was applied.
Conclusions: The benefits associated with SGLT-2 inhibitor for patients with nephrolithiasis in these target trial emulations suggest they may be a useful addition to current treatments to simultaneously manage nephrolithiasis recurrence and comorbidities, including gout.
"
Malnutrition in infants aged 6-23 months in China’s poorest rural counties from 2016 to 2021,"Introduction
Malnutrition accounts for almost half of the deaths of children younger than 5 years globally.1In 2012, the World Health Assembly established three relevant targets to achieve by 2025: reduce stunting in children under 5 by 40%; reduce childhood wasting to less than 5%; and ensure that prevalence of childhood overweight does not increase.2Sustainable development goal 2.2, one of 17 goals, aims to end all forms of malnutrition by 2030. A consensus was reached to prioritise interventions during the first crucial 1000 days of a child’s life—from conception to the age of 2 years.34Disaggregated data by geographical areas and subpopulation groups are essential to support subnational efforts and address inequities.56
China has made progress in reducing child undernutrition. A 2014 study assessing trends of growth among children and adolescents from 1975 to 2010 (covering a 35 year period) reported decreasing levels of underweight and stunting.7Much of this success can be attributed to rapid economic development and reduced urban-rural disparities; however, these events have been accompanied by an increase in obesity.8910Moreover, previous analyses have focused only on the height and body mass index of school aged children and adolescents. Data for children under the age of 5 years and on specific sustainable development goal indicators are lacking. Additionally, little is known about differences in child malnutrition between small geographical areas and subpopulations.
In 2013, China prioritised poverty alleviation by targeting its 832 poorest rural counties, with government investment reaching USD $246 billion (€219 billion, £184 billion) by 2021 (supplementary S1).111213A broad programme was adopted to promote health equity and alleviate poverty by enhancing compulsory education, ensuring safe housing and basic healthcare, guaranteeing safe drinking water and sanitation, and improving child nutrition. Importantly, the government augmented the national fortified complementary food supplement programme, or Ying Yang Bao. This programme distributed complementary soy based food supplements, rich in macronutrients and micronutrients, to all children aged 6-23 months living in these counties. However, despite these efforts, limited research exists on China’s progress in addressing child malnutrition in rural poor populations.78910
This study focuses on analysing trends in stunting, wasting, overweight, and anaemia among children aged 6-23 months in China’s poorest rural populations. By incorporating socioeconomic factors and children’s age in months, we aimed to identify the underlying factors contributing to disparities in health outcomes in rural contexts.
Methods
Study design and data sources
This study used six rounds of cross sectional survey data collected annually from 2016 to 2021 from the monitoring and evaluation project of the national fortified complementary food supplementation programme of China in poverty stricken counties (ie, the Ying Yang Bao programme). For all surveys, a four stage cluster sampling procedure was used to select participants. In the first stage, all 832 poverty stricken counties were stratified into each of the 19 provinces and ranked by GDP and population size; then, seven counties were randomly selected from each province (for one province where only five counties were in poverty, all were selected). All following surveys used the same counties that were sampled in 2016. During these surveys, some provinces expanded their sample size, and eight counties from the original sampling were not followed up because of uncontrolled factors such as natural disasters. In the second stage, townships within each county were ranked by per capita net income and population size, with five townships randomly selected from each county. In the third stage, three to five villages were randomly selected from each township. Finally, 12-20 children aged 6-23 months were randomly selected from each village. Considering the baseline stunting prevalence of 10% and an expected reduction proportion of 20%, a minimum of 300 children per township and 2100 per province were predetermined.
For each county, doctors from the county maternal and child health hospital were recruited for the programme. Four doctors were responsible for growth monitoring, five to eight conducted the interviews, and one managed on-site quality control. After two days of training, county doctors travelled to the selected townships to conduct the survey from July to August. Village doctors notified and escorted the selected children and their caregivers to the township hospital where the four trained doctors conducted anthropometric measurements and collected finger blood samples. All procedures, including training, were standardised by a predefined protocol (supplementary text S2). Face-to-face interviews were then held with the children’s caregivers to collect basic information about the children and their feeding habits, and parents, using a 24 h recall form (supplementary text S3). All participants were required to sign an informed consent form (supplementary text S4). The non-response rate was less than 2%. Overall, 73.9% (189 617 out of 256 505) of the respondents were mothers, 21.4% (54 906 of 256 505) were grandparents, 4.1% (10 569 of 256 505) were fathers, and 0.5% (1413 of 256  505) did not provide this information. All completed forms underwent a final on-site review by the head of the county maternal and child health hospital for logical inconsistencies and missing items. The Chinese Centre for Disease Control and Prevention managed this survey.
Definition of outcomes and adjustments variables
The outcomes of interest were stunting, wasting, overweight, and anaemia. Length-for-age z scores and weight-for-length z scores were analysed with the World Health Organization’s (WHO’s) Anthro software (version 3.2.2); a length-for-age z scores of less than −2 was defined as stunting, a weight-for-length z scores of less than −2 was defined as wasting, and a weight-for-length z scores more than 2 was defined as overweight. WHO adjusted their 2024 criteria of haemoglobin concentration for altitude to sea level, and anaemia was defined as a haemoglobin concentration below 105 g/L.14Children were classified into quarters according to county annual GDP, adjusting the monetary values of per capita GDP to those of 2016. This approach showed the change in the proportion of children according to the counties’ economic development. Mothers’ education levels were determined through survey questions. These levels were classified into four groups: up to primary education (6-12 years old), middle school (13-15 years), high school (15-17 years), and college or above (18 years and older). Additionally, the mothers’ ethnic group and the child’s sex, age in months, health status, and feeding habits (obtained from the questionnaire) were included as covariates. The self-reported incidences of diarrhoea and acute respiratory infection in the preceding two weeks were used to determine the children’s health status. Age was divided into three equal groups. Feeding habits included whether the child was breastfed and whether they had followed the recommended minimum acceptable diet recommended by WHO.15
Statistical analysis
Yearly trends for each form of malnutrition and all possible comorbidities were described. To explore the importance of double burdens, comorbidities were first excluded from each single form (ie, not counting stunting and overweight in stunting), and then comorbidities were included (ie, double counted) in each single form to compare the trends and prevalence. Once done, Varghese and Stein’s approach were adopted to test whether two malnutrition forms coincided non-randomly.16
The prevalence of the four individual malnutrition forms and the concentration of haemoglobin were described by age (in months) to explore the trajectory of child nutrition by sex and socioeconomic status, using cubic or quadratic splines to fit the age patterns.17Socioeconomic inequalities were visualised by determining annual trends in prevalence stratified by four GDP groups and the mother’s education level.
Multilevel Poisson regression, which accounts for the sampling stratification and clustering at the county, township, and village level, was used to assess crude and adjusted rate ratios (RRs) of each malnutrition form over time and in various groups. Rate ratios were adjusted for years, GDP groups, the mothers’ education level, and ethnic group, and for the children’s sex, age in months, and health status and feeding habits. The year was modelled as a linear variable to estimate the average annual rate of reduction (1-rate ratios for the variable year).18Yearly trend interactions were explored with GDP quarters, mother’s education, and children’s sex and age in months to examine whether the prevalence changed at the same rate over time in different subgroups.
The SVY command in STATA was used to adjust the survey design. The data were weighed against the probability that a child would be selected, using the number of live births in 2016 to determine the population size. All missing values were excluded from the analysis.
Patient and public involvement
Despite distributing complementary food supplements, county level maternal and child health doctors provided health education and consultation services to the caregivers of participating children. Specific advice was given on breastfeeding and complementary feeding. The Ying Yang Bao programme was shared with the local government (health commission), and disseminated through newspapers, television broadcasts, radio, websites, and social media to raise public awareness. The public and caregivers supported the programme by providing input, especially for knowledge sharing and peer health education on breastfeeding and complementary feeding. They contributed to the design and production of multiple health education methods, including village broadcasts, posters, and slogans painted on village central squares, and online chat groups where caregivers shared their feeding experiences. Moreover, children’s healthy appearance and positive feedback from caregivers on child health facilitated peer-to-peer sharing of health information and enhanced adherence to recommended practices.
Results
We included 210 088 children aged 6-23 months from 116 (74%) of 157 counties who had completed the six surveys (supplementary figure S1, and tables S1 and S2 for the sensitivity analysis for the 157 counties). Missing measurements were noted in 95 participants (0.1%) for haemoglobin, 224 (0.1%) for length, and 292 (0.1%) for weight. We excluded children with absolute values of z scores more than 5 for length for age (n=1669 (0.79%)) and for weight for length (n=962 (0.46%)). Of 210 088 children, valid measurements were 209 993 (100.0%) for anaemia (haemoglobin), 208 195 (99.1%) for stunting (length for age) and 208 706 (99.3%) for wasting or overweight (weight for age) (fig 1).
Table 1shows the participants’ main characteristics. Children were balanced in sex and age, and missing values were rare. Income had increased substantially: in 2016, 44.0% of children were in the poorest quarter (per capita GDP USD <$2352) and 11.6% of children were in the richest quarter (>$3909). In 2021, the proportion of children in the poorest quarter in the survey area had decreased to 10.9%, while that of the richest had increased to 43.2%. Mothers’ education levels also improved. In 2016, 77.2% of the mothers had completed education up to the middle school level. In 2021, this percentage had decreased to 61.8%, with 18.4% having an education level of college and above.
Four main malnutrition forms were prevalent in 2016 (fig 2): anaemia (18.3%), stunting (7.5%), wasting (4.7%), and overweight (3.1%). However, the prevalence of any two coexisting malnutrition forms was low (fig 2), which seemed to occur randomly (supplementary table S3). All four malnutrition forms decreased from 2016 to 2021, with particularly fast reductions observed for anaemia and stunting. Anaemia decreased by over half, from 18.3% in 2016 to 8.9% in 2021. Stunting decreased by over a third, from 7.5% to 4.1%. Wasting (4.7% to 3.7%) and overweight (3.1% to 2.8%) also decreased, although at more modest rates. These findings were supported by Poisson regression analysis (table 2,table 3). After adjusting for GDP group, mothers’ education levels, children’s age and sex, and all other covariates, the yearly trends reporting an annual reduction rate of 9.1% (1−rate ratios (95% CI 4.8% to 13.2%)) for anaemia and 10.4% (7.6% to 13.2%) for stunting.
Additional determinants for each of the four malnutrition forms are presented intable 2,table 3. Girls consistently reported lower risks than boys for each outcome. For example, the adjusted rate ratios of stunting comparing girls to boys was 0.65 (95% CI 0.61 to 0.69). Another determinant was children’s age. As months of age increased, rates of stunting increased, while those of anaemia decreased. The prevalence of wasting was the highest for 12-17 months (rate ratio 1.17 (95% CI 1.09 to 1.26)), compared with the age of 6-11 months. Rates of overweight were roughly a third lower for 12-17 months (0.70 (0.64 to 0.77)) and 18-23 months (0.67 (0.60 to 0.74)) compared with 6-11 months.Figure 3shows the measurements and prevalence by sex and age in months. For both girls and boys, anaemia prevalence decreased as age increased. Conversely, length-for-age z scores reduced almost linearly with age, leading to a rapidly increasing prevalence of stunting. Weight-for-length z scores decreased slightly until 16 months, with the prevalence of wasting and overweight plateauing after 1 year of age. Sex differences in wasting and overweight were small, but girls had less anaemia and stunting. Sex differences in anaemia decreased with age but was no different by 24 months. Length-for-age z scores decreased at similar paces for both sexes, with persistent sex differences in stunting in children aged 6-23 months.
In terms of socioeconomic characteristics, mothers’ education levels generally had larger effects on malnutrition than GDP, except for the effects on anaemia (table 2,table 3). For example, for children whose mothers completed education only up to primary school level, the prevalence of stunting was 9.8% and of wasting was 6.2%. In comparison, for children whose mothers attained an education level of college or above, the rates were only 3.1% and 3.2%, respectively (absolute difference was 6.8% for stunting (relative difference, adjusted rate ratio 2.29 (95% CI 1.87 to 2.81)) and 3.0% for wasting (1.73 (1.40 to 2.13)), compared with primary school. However, the effects of GDP were small and largely attenuated by other factors (adjusted rate ratio for stunting was 1.02 (95% CI 0.81 to 1.28); for wasting was 1.26 (0.84 to 1.88)), comparing between the poorest quarter and the richest).
Figure 4presents the yearly trends stratified by county GDP quarter and mothers’ education for each malnutrition form. Although differences in stunting and wasting by county GDP quarter were small, they persisted across mothers’ education levels. As indicated by the Poisson regression (table 4), the annual rates of anaemia reduction were similar across population subgroups. By contrast, stunting declined faster in the poorest counties and in children whose mothers had lower education levels compared to those in wealthier counties and with more educated mothers (supplementary table S4 for trends in wasting and overweight).Figure 5presents the education related differences by age. Differences in anaemia, stunting, and wasting already existed at 6 months of age and changed only slightly between 6-23 months.
Discussion
Main findings
Using large scale survey data, this study provides a comprehensive analysis of the prevalence of malnutrition among children aged 6-23 months in China’s poorest rural counties during 2016-21. We found little evidence for a double burden of malnutrition, but our data show that four forms of malnutrition (anaemia, stunting, wasting, and overweight) were present. The prevalence of each form reduced faster than the WHO global target. Changes occurred alongside a large scale reduction in poverty and substantial improvements in mothers’ education levels. Gaps in growth among children in richer and poorer counties decreased, although differences in undernutrition related to educational level were common, with a larger effect on children whose mothers only completed education up to primary school level. As children’s age increased, length-for-age z scores reduced faster than weight-for-length z scores. Boys had higher rates of undernutrition than girls. For all outcomes, differences related to sex and education were largest at 6 months of age.
Comparison with other studies
We appraised all combinations of concurrent malnutrition that were drawing substantial attention.1920Our data suggest that, within China’s poorest rural counties, the double burden of malnutrition may not be a priority. Previous studies from China have primarily focused on school aged children and adolescents, reporting declining rates of undernutrition but increasing overweight and obesity.78910Our data corroborate the evidence that child undernutrition has decreased rapidly; with an annual rate of reduction exceeding 10%, stunting has decreased at a rate much faster than the global target of 3.9%.21In 2021, the prevalence of anaemia was lower than 13%, and the prevalence of stunting and wasting was also a low burden. Table S5 presents the estimated prevalence of malnutrition among the under 5s from selected countries, showing that the prevalence of each form of malnutrition in our population was near (or even lower than) that of the best-performing countries globally, such as Singapore, Japan, and Australia.22Since global data indicate that all four forms of child malnutrition are more severe during 6-23 months compared with 24-59 months,232425our evidence suggests that China is making commendable progress towards sustainable development goal 2.2. Importantly, the prevalence decreased during the study period and with the children’s age, suggesting that overweight was well controlled for these infants and young children.
Our study showed several important findings on child malnutrition regarding age and sex. Firstly, length-for-age z scores decreased from 6 to 23 months, and did so faster than weight-for-length z scores. This finding concurs with the results of a recent cohort study conducted in 15 low and middle income countries,3which supports the global consensus that managing height and length is more important than managing weight.262728Secondly, child anaemia is difficult to mitigate in low and middle income countries212930; children aged 6-11 months have the highest prevalence of anaemia among the under 5s.313233In our population, haemoglobin concentrations increased steadily in line with age; this decline in anaemia prevalence indicates an encouraging change. Thirdly, we observed poorer nutrition among boys than girls, which aligns with prior research.34Boys may be more susceptible to infectious diseases and have higher incidences of inflammation.34However, we found that in the poorest rural counties, sex differences in anaemia decreased as age increased, and no differences were noted at about 24 months. Although boys’ linear growth lagged that of girls at 6 months of age, the sex differences did not increase from 6 to 23 months. In addition to providing new data for sex differences in infant malnutrition under the age of 2 years, these data, taken together, suggest that the poorest rural communities could effectively manage undernutrition within 6-23 months.
Interpretations
We found that the large average annual rate of reduction in stunting and anaemia could not be explained by county GDP quarter, mothers’ education and ethnic group, or the children’s sex, age, feeding and health status, and preterm birth and weight at birth. According to global evidence, child undernutrition is determined by the complex interplay of social and economic factors, with high quality foods, improved hygiene, and adequate healthcare playing crucial roles.3536Infant nutrition at 6-23 months is not only affected by feeding habits and health conditions, but mounting evidence suggests that maternal health and family conditions are also important.317The rapid increase in haemoglobin concentrations may be explained by the complementary food supplements programme: we found that 85% of the children in the study regularly took the supplements (data not shown). China’s poverty reduction strategy is multisectoral. Despite food supplements, wider social determinants were targets of the so-called three guarantees, which focused on compulsory education for girls, securing jobs and safe housing that has improved hygiene and sanitation, and providing basic healthcare.111213The progress in poverty reduction and improvement in mothers’ education, along with the faster annual reduction rates in stunting in children living in poorer counties and less educated groups compared, support the success of China’s approach to poverty reduction and nutrition interventions.
Focusing on China’s poorest rural counties, this study adds valuable evidence to the discussion regarding the impact of geographical location versus individual socioeconomic status on achieving optimal nutrition for infants.3738For stunting, wasting, and overweight, we found almost no differences by county GDP from 2016 to 2021, suggesting that subnational inequalities across geographical areas can successfully be addressed by large scale programmes. However, disparities remained for children whose mothers completed education up to primary school level. In rural areas, women’s education was positively associated with their income and access to nutritious foods.39In these settings, women with an education at the middle school level (12-15 years old) or higher, compared with the least educated group, were more likely to have a secure job,40were better prepared for their pregnancy,41received higher quality antenatal care and post-partum support,4243and were more likely to provide adequate complementary feeding to their children.44In our population, one in eight mothers completed education only up to primary school level in 2021, suggesting further efforts to make a change. Nevertheless, we found that the persistent inequalities related to education in child stunting and wasting already existed at 6 months of age and did not vary significantly between 6 and 23 months. Future studies are needed to determine whether the so-called three guarantees and food supplements have helped to reduce inequalities related to education during complementary feeding. However, the largest disparities at 6 months of age suggest the importance of interventions during the breastfeeding period and earlier. In low and middle income countries, socioeconomic inequalities in child linear growth are not known to be affected more by maternal status or by interventions during periods of complementary feeding.3Our data indicate that child malnutrition policies and programmes should place a greater focus on care for mothers, and preconception to post-partum care continuum should be prioritised.34546
Limitations
Firstly, all surveys were cross sectional, and we were thus unable to track individual changes over time. Although we attempted to ensure consistency in data collection and processing methods, specific events or conditions unique to each year could have affected the comparability and interpretability of the results. Similarly, although we observed similar trends in child malnutrition by months of age, as in other studies,347causal inferences should be drawn with caution. Secondly, the survey did not collect data for household assets or income; therefore, we used county per capita GDP in the equity analysis. Future research is needed to gain a deeper understanding of income related inequalities at the individual level. Thirdly, the questionnaires did not adequately define breastfeeding and complementary feeding. Recall bias may have also undermined the quality of these data, particularly among the 25% of respondents who were fathers or grandparents. Fourthly, the complementary food supplements were distributed universally in all the counties in poverty when the survey was initiated. Due to only infants aged 6-23 months being eligible to receive government food supplements, whether the same success could be achieved in children aged 2-5 years, as endorsed in sustainable development goal 2.2, is unknown.
Conclusions
Using large scale data, we documented China’s commendable progress in achieving sustainable development goal 2.2 among children aged 6-23 months, showing lessons for other countries seeking solutions for their poorest rural communities. With broad and integrated measures targeting the poorest regions, malnutrition (stunting, wasting, and anaemia) and overweight saw rapid declines, and geographical inequalities related to counties’ economic development also decreased. Gaps mostly remained in education related socioeconomic inequities; children whose mothers only completed education up to primary school level had higher risks of faltering growth. Nevertheless, the inequalities related to education seemed to occur before 6 months of age, and these gaps did not widen by the age of 2 years. These findings suggest that greater attention should be directed towards mothers and interventions in the full maternal and child healthcare continuum.
Sustainable development goal 2.2 involves ending all forms of malnutrition by 2030, prioritising the first 1000 days of a child’s life
Prior studies of child malnutrition from China have focused on school aged children and adolescents, little is known about children under the age of 5
The Chinese government has given poverty alleviation top priority, although data on child nutrition are lacking from the poorest populations, particularly for those disaggregated by small geographical areas and subpopulation groups
China is making progress in eliminating all forms of child malnutrition in its most susceptible populations
Differences by county GDP decreased over time, but inequalities related to education remained
All outcomes showed differences at 6 months of age, suggesting that the optimal time for intervention is during the breastfeeding period and even earlier
","Objectives: To assess trends and differences in child malnutrition by population subgroups among infants aged 6-23 months in China’s poorest rural counties.
Design: Six consecutive cross sectional surveys were conducted annually.
Setting: The study was conducted in 116 counties in 19 provinces from 2016 to 2021, representing China’s 832 poorest counties.
Participants: A total of 210 088 participants were selected through a multistage cluster sampling procedure; all participants were infants aged 6-23 months.
Main outcome measures: Prevalence of anaemia, stunting, wasting, overweight, and growth status in children (measured by length-for-age and weight-for-length z scores).
Results: Four main malnutrition forms were prevalent in 2016: anaemia (prevalence 18.3%), stunting (7.5%), wasting (4.7%), and overweight (3.1%). The prevalence of any two coexisting malnutrition forms was low. All four forms of malnutrition decreased from 2016 to 2021. Anaemia decreased by more than half, with an annual reduction rate of 9.11% (95% confidence interval (CI) 4.83% to 13.20%). Stunting was reduced by over a third, with an annual reduction rate of 10.44% (7.56% to 13.22%), which is faster than the World Health Organization’s target of 3.9%. Differences in child growth by county gross domestic product quarters were small and decreased over time, but growth differences related to education persisted. Infants whose mothers completed education up to primary school level had approximately twice the risk of stunting (adjusted rate ratio 2.29 (95% CI 1.87 to 2.81)) and wasting (1.73 (1.40 to 2.13)) compared with children whose mothers had an education level of a college degree or above. Boys had poorer growth and higher anaemia than did girls. For all outcomes, differences related to sex and education were greatest at 6 months of age.
Conclusions: Education related inequalities in growth of infants persists, with these differences particularly affecting children whose mothers completed education only up to primary school level.
"
Association of holidays and the day of the week with suicide risk,"Introduction
Suicide is an important global public health concern. According to the World Health Organization, more than 700 000 people died due to suicide in 2019, accounting for approximately 1.3% of deaths, which was higher than the number of deaths by malaria, HIV/AIDS, and breast cancer.1Suicide has been shown to be a leading cause of premature deaths and particularly the fourth leading cause of death among young people aged 15-29 years.1Studies have attempted to theoretically explain suicidal behaviour in sociology since Durkheim’s work,2suggesting that suicide may be associated with social factors, as well as individual characteristics.34These social or individual factors might contribute to developing short or long term suicidal thoughts and behaviours.
Suicide is associated with time varying factors.56Suicides have historically and consistently peaked in the spring and early summer in multiple countries.7Several studies have reported other shorter term variabilities in suicide risk, particularly in reference to the day of the week and holidays.58The suicide risk peaks on Mondays and decreases during weekends.59However, previous results for holiday effects regarding suicide were mixed. Multiple studies in European countries and the United States reported that end-of-year holidays such as Thanksgiving, Advent, and Christmas were associated with lower suicide or suicidal intent, whereas suicidal events seemed to increase on New Year’s day.51011Public holidays seem to provide a protective association with suicidal events, however, several studies have reported an increase in risk in the immediately following days.812However, a few other studies reported no strong linkage between suicide and general holidays.1314
The broken-promise effect theory has been widely adapted to explain short term temporal variations in suicide. It states that individuals may postpone committing suicide due to the hope of a “new beginning” when the cycle ends (eg, weekends and the end of the year),810while people may be prone to suicidal reactions when they encounter a sense of hopelessness from a new cycle (eg, Monday and New Year).515In accordance with the broken-promise effect, plausible explanations for the protective effects on suicidal events may also include improved family or social support during weekends and holidays.9Previous studies have been conducted on this theory; however, they have several limitations. The existing findings were primarily based on Western cultures, and no multiregional study could provide comparative results across different lifestyles and cultures with unified analytical frameworks. Second, the day of the week and holiday effects on suicide using time series data should be estimated considering the season and ambient temperature,71617potentially confounding the results. Previous studies in the UK reported a peak in suicide in January along with a strong New Year’s day effect on suicide,516which appears contradictory to the peak in spring and early summer in other countries. Applying the unified approach to multiple countries can provide comparable results and clarify the changes in risk at different timescales by adjusting the season and temperatures under the same conditions.
Therefore, to address these limitations, we performed a multicountry time series study that included suicide deaths across 26 countries from 1971 to 2019. We investigated the association of the day of the week and major holidays with suicide risk and differences by culture and country using a cutting-edge standardised two stage time series analysis. Based on the large sample size and geographical scope, this study aimed to provide international results regarding the effects of the day of the week and major holidays on suicide risk.
Methods
Data
We collected suicide data for 740 locations in 26 countries from the database of the Multi-country Multi-city Collaborative Research Network (https://mccstudy.lshtm.ac.uk/). The dataset we used for this study included location specific daily suicide counts and daily mean temperatures (°C) in overlapping periods between 1 January 1971 and 31 December 2019. Detailed information regarding the country name, number of locations, and country specific study period are displayed intable 1. In this study, suicide was defined as intentional self-poisoning and self-harm using the 8th, 9th, and 10th revisions for the International Statistical Classification of Diseases and Related Health Problems (E950.0-E958.9 for ICD-8 and ICD-9, and X60-X84 for ICD-10).717Details regarding the data collection and data sources are described in the “data collection details” section of the supplementary materials. Table S1 displays the summary of statistics regarding suicides for each location.
The holiday information for each country during the study period was collected from a calendar website (https://www.timeanddate.com/calendar/). We collected data for all the national holidays for each country; however, because this study addressed multicountry data, we were limited in covering the entire holidays that are heterogeneous by country and have different meanings (eg, festivals or memorial days). Additionally, we conjectured that the practical impacts of holidays are substantially heterogeneous; for example, it is difficult to assume that all holidays identically affect the general life. Therefore, we classified the holidays into the following three categories: Christmas (25th December), New Year’s day (1 January), and other national holidays. The other national holidays included all variable holidays, such as temporary holidays (eg, election days) and observed holidays: a list of holidays we collected is displayed in supplementary table S2. Moreover, for comparability, we allocated Christmas and New Year’s day as national holidays for all countries based on the assumption that these holidays have similar implications across culturally diverse countries; although some countries did not designate them as national holidays. For countries with Lunar New Year holidays, we considered them as other holidays for comparability with other countries.
The daily suicide counts from all locations within each country were summed up to create the daily suicide counts by country. To address the conditions in more densely populated areas of each country, we calculated the suicide count-weighted daily mean temperatures by country using the location specific daily mean temperature and suicide count data.
Statistical analysis
We performed a two stage analysis. In the first stage, we fitted a quasi-Poisson regression model to estimate the associations between suicide and the day of the week, as well as three different types of holidays (New Year’s day, Christmas, and other national holidays), with the corresponding 95% confidence intervals. We used an indicator variable for the day of the week and several binary indicators representing the different types of national holidays. We incorporated all those variables into the model to ensure that they were mutually adjusted. We also considered the possible effects before and after the holidays on the risk of suicide that may be attributed to behavioural changes, expectations of the event, or lagged effects observed after the holidays. We used lagged indicator variables for a five day period from two days before until two days after the holidays for each holiday type, which is statistically equivalent to an unconstrained distributed lag model.18Thus, the implicit reference days were the days other than the five day period for each holiday type. Seasonal and long term trends were controlled using a natural cubic spline of time (date) with four degrees of freedom per year. In addition, we adjusted for the weighted daily mean temperature using a cross-basis function with a quadratic B-spline, with three internal knots placed at the 25th, 50th, and 75th percentiles of the country specific temperature distribution. For the association of the lag response for temperature, we used dummy intervals defined by parameters lag 0 and lag 1-2. The modelling choices for temperature were based on the previous Multi-country Multi-city study for the short term temperature and suicide association that used relevant statistical tests.17The relative risk estimates of 1 do not indicate that no suicide event occurred, and therefore, should be carefully interpreted.
In the second stage, we conducted a meta-regression model with a random intercept to pool the country specific estimates for the day of the week, Christmas, New Year’s day, and other national holidays from the first stage analysis and obtained the best linear unbiased predictor for each country.19To address the heterogeneity across the countries, we incorporated a region indicator, including North America, Central America, South America, Europe, Africa and Oceania, and Asia (table 1) as a meta predictor.
We repeated the same analyses for subpopulations, stratified by sex and age group (aged 0-64 and 65 or older). Limited data were available for the subpopulation analysis: three countries (Costa Rica, Guatemala, and Paraguay) were excluded from the subpopulation analysis. More detailed information on the statistical analysis is reported in the supplementary materials. To examine the potential bias due to misclassification from the mortality registration system during the weekends, we did a simulation study that considers the situations in which the number of suicides would have been over-counted (supplementary materials, in the section: simulation study for the misclassification on Monday). Lastly, to consider the most recently available suicide data for the United States (1999-2019), we did a sensitivity analysis (supplementary materials, in the section: sensitivity analysis using the recent US mortality data).
Patient and public involvement
This study used deidentified suicide mortality data at the aggregated level, and because of the sensitive and emotional nature of this topic we were without the expertise in our team to involve patients or members of the public in this study.
Results
A total of 1 701  286 suicides in 740 locations across 26 countries were included in this study. During the study period, the suicide rate was highest in South Korea and Japan, South Africa, and Estonia, and lowest in the Philippines, Brazil, Mexico, and Paraguay (table 1). Across all countries, higher suicide counts were shown for men (vwomen) and people aged 0-64 years (v≥65 years). Average temperatures were generally higher in low latitude countries (Vietnam, Philippines, Taiwan, and Costa Rica) than in high latitude countries (Finland, Estonia, and Canada). Monday accounted for approximately 15-18% of total suicides (fig 1). Country specific distributions of other national holidays by day of the week are displayed in table S3. Although the distributions were heterogeneous by country, most countries showed the highest proportion of suicides on Monday.
The risks of suicide were higher on Mondays compared with Wednesdays (reference) and other weekdays in the total population, with relative risks ranging from 1.02 (95% CI 0.95 to 1.10) in Costa Rica to 1.17 (1.09 to 1.25) in Chile (fig 2). However, suicide risk on weekends varied by country. Suicide risks for the countries in South and Central America, South Africa, and Finland were generally higher on weekends than on other weekdays. By contrast, suicide risks in most countries in North America, Europe, and Asia were lowest during weekends. These patterns were broadly similar between sexes and age groups in most countries (figures S1-S2). Table S4 shows country specific average suicide counts by the day of the week.
Suicide risk marginally increased on Christmas day and for two days after in the total and male populations, but not in the female group (fig 3). However, this pattern was heterogeneous among the regions: relative risks ranged from 0.54 (95 CI 0.39 to 0.76) in Switzerland to 1.89 (1.01 to 3.56) in South Africa. The suicide risk marginally increased on Christmas day for countries in Central and South America, and South Africa. However, countries in North America and Europe generally showed a decreased suicide risk on Christmas day. These patterns were more evident in men across the countries (figure S3).
Risk of suicide peaked on New Year’s day across all countries: ranging in relative risk from 0.93 (95% CI 0.75 to 1.14) in Japan to 1.93 (1.31 to 2.85) in Chile. The only exception was in Asian countries, where peak risk remained for one or two days after New Year’s day (fig 4). Furthermore, men generally had a more pronounced increasing pattern of suicide on New Year’s day (fig 4and figure S4). We also examined the effect of the Lunar New Year’s day for three East Asian countries and regions where people celebrate New Year’s day on the Lunar calendar (China, South Korea, and Taiwan). Only South Korea showed a decreased suicide risk during the Lunar New Year (figure S5).
In many countries, suicide risk decreased on other national holidays (relative risks ranging from 0.80 (95% CI 0.71 to 0.89) in the UK to 1.14 (0.97 to 1.35) in Chile) and one or two days before the holidays. Conversely, risk was reversed and increased one or two days after the holidays (fig 5). This trend was generally more evident in men than in women and was generally observed in European countries and Asian countries (figure S6).
Patterns of suicide risk for individuals aged 0-64 years on Christmas (figure S3), New Year’s day (figure S4), and other national holidays (figure S6) were broadly similar to those in the total population. Numerical data corresponding tofigures 2-5(and figure S1-S4, and S6) can be found in tables S5-S8. We were unable to assess the associations with holidays among older individuals owing to the insufficient number of suicidal events.
The heterogeneity statistics from our meta-regression analyses are reported in tables S9-S13. Although the Cochran Q test provides evidence for residual heterogeneity in all the models, the I2statistics substantially decreased after incorporating the region indicator as a meta-predictor, suggesting that the heterogeneity was largely explained by between-region differences (table S9). Another measure of heterogeneity, tau (τ), is reported in supplementary tables S10-S13 with the corresponding correlation matrix of random effects. Consistent with the results on the I2statistics, the τ generally decreased when the region indicator was included in the meta-regression model.
Finally, the simulation study examining potential bias due to misclassification arising from the administrative system showed that our main results remained consistent when the misclassification of suicide on Monday was less than 10%. However, when the misclassification on Monday reached 20%, the peak observed on Monday diminished in the total population, and in several countries in Central America and Asia, which showed relatively low Monday risks (relative risks <1.1) (supplementary materials, in the section: Simulation study for the misclassification).
Discussion
Principal findings
We investigated the short term variation in suicide risk with respect to the day of the week and national holidays, including New Year’s day and Christmas across 26 countries. We found that Monday had the highest suicide risk during weekdays across all countries; however, the effect of the weekend on suicide was mixed. Suicide risk increased on New Year’s day in all countries, whereas the pattern on Christmas was heterogeneous. We also found a decreasing pattern on other national holidays; however, suicide risk increased after other national holidays in most countries.
Our findings provide empirical evidence of the temporal variations in suicide,20considering possible interactions between underlying vulnerable states with suicidal thoughts and behaviours as well as short term psychological fluctuations derived from changing environments before, during, and after holidays and weekends. Our results provide epidemiological evidence to establish more effective action plans for suicide prevention and administrative support for enhancing the mental health of workers and younger generations (eg, pre-emptive screening, and psychological support programmes), which might be important for reducing suicide events related to weekdays and holidays.
Our findings on Mondays and New Year’s day were broadly consistent with previous studies that could be explained by the “broken-promise effect theory”.51011“Blue Monday” was also used as an explanation, indicating that the beginning of a week makes individuals distressed by pressure from work.21In addition, increased alcohol consumption before and on New Year’s day and weekends was considered as one of the major risk factors for higher suicide risk on that day.222324Regardless, we found different patterns of suicide risk by day of the week in several countries, where the risk peaked during the weekends, but not necessarily on Monday. A few previous studies have reported consistent results: studies in Colombia and Brazil showed that the suicide risk increased on the weekends and holidays in relation to alcohol consumption.252627Another previous study in northern Finland reported an increased risk of suicide during the weekends,28which might be linked to their drinking culture in which drinking alcohol is concentrated on weekends, as highlighted in the previous review.29Higher rates of alcohol consumption on the weekends in comparison to weekdays might be common across various countries,30therefore, additional underlying factors, such as religion and working conditions might be at play. Our results support the necessity of in-depth studies to investigate these factors.
Notably, our study showed that men who died by suicide were more affected by the day of the week and New Year’s day compared with women. These sex differences may also be associated with disparities in the social capital and susceptibility to isolation by gender. Previous studies have reported that men—particularly male older people—are more susceptible to isolation, stress, and insufficient social capital. Conversely, women derive more health benefits from social capital than men and generally have a bigger and more diverse social support system, actively participating in social networks.3132We also hypothesise that the higher economic activity by men in general globally might be associated with the higher vulnerability on Mondays, New Year’s day, and days after major holidays, which are the beginning of the new economic activity cycle.233334Higher labour force participation in younger individuals might also support our findings of the stronger effect by day of the week on suicide in individuals aged 0-64 compared with people aged 65 or older.
We noted that suicide risks decreased on Christmas and other national holidays among men in North American and European countries, although the statistical significance was weak. Better family and social relationships may be associated with a lower suicide risk on holidays.3536However, the results for Christmas were heterogeneous among the countries studied. This suggests that the existing plausible explanations for the results should be addressed with caution, and future research is needed to identify the related factors.
Limitations and strengths
This study has several limitations. Firstly, we were unable to use data at the location level because of a problem with model convergence. Although the location level analysis could provide more information regarding spatial heterogeneity, we believe that our results using a standardised method can provide more valuable information to establish relevant interventions on a national scale. Secondly, our estimates should not be considered generalisable for each country because this study included a sample of locations. Some countries were limited to one or two cities or could not cover recent periods because of the data availability. Due to privacy concerns, the National Center for Health Statistics in the US has discontinued providing the date of death on their mortality files to the public since the 2010s. Thus, we used the US data from 2001 to 2006 in the main analysis. Thirdly, most of the countries analysed in our study were generally based on urban populations, therefore, our results are limited in addressing patterns of suicide in rural areas. Previous studies reported that rural areas showed higher suicide rates than urban areas,37thus the underlying differences related to urbanisation should be addressed in future studies. Fourthly, the lower quality of suicide data (under-reported or misclassified) in less industrialised countries may be another limitation.117Finally, we could not address the potentially heterogeneous impacts of different types of holidays (eg, festivals or memorial days) on suicide risk by country.
Despite these limitations, our results have important implications for suicide studies and relevant public health policies. This study was a large epidemiological investigation into the association of the day of the week and national holidays with suicide. Based on the large sample, we have provided statistically reliable results on the effects of the day of the week and major holidays on suicide with their heterogeneities among the countries. These results can be used to establish more elaborate international and national interventions. In particular, our findings suggest benefits for expediting prioritisation in response to the different suicide risks during a week and national holidays and we provide evidence to establish more effective and targeted action plans for enhancing the mental health of workers and younger generations (eg, pre-emptive screening). Furthermore, our results including multiple countries provide evidence for more targeted suicide preventions to address country specific suicidal patterns. Finally, by applying state-of-the-science analytical frameworks, this study considerably alleviates potential estimation biases for the effects of the day of the week and holidays, which were attributable to the less developed seasonal decomposition methods.38
Conclusion
In conclusion, this study provides evidence regarding the association of the day of the week and national holidays with suicide risk using multicountry data with 1.7 million suicide cases. Suicide risk peaked on Mondays during weekdays across all countries, and the effect of weekends was heterogeneous among the countries. New Year’s and the consecutive days were universally associated with an increased risk of suicide. Our findings contribute to the implications for national and global suicide prevention strategies in both industrialized and less-industrialized countries considering public health resource allocation and mobilisation.
Previous studies reported that suicide risk differed by the day of the week
However, the generalisability of the previous findings was limited due to the restricted study locations and heterogeneous methodological frameworks
Mixed results have been reported on the association between holidays and suicide risk
This study presents the association as the relative risk of suicide by days within a week or a ratio of risk for national holidays compared with non-national holidays
Mondays and New Year’s day were both associated with increased suicide risk in most countries; however, suicide risks related to weekends and other national holidays were heterogeneous
The findings provide novel scientific evidence at a global scale, which can help to establish more targeted suicide prevention and response programmes related to holidays and the day of the week
The results provide significant information about associations between suicide and social and environmental factors
","Objectives: To assess the short term temporal variations in suicide risk related to the day of the week and national holidays in multiple countries.
Design: Multicountry, two stage, time series design.
Setting: Data from 740 locations in 26 countries and territories, with overlapping periods between 1971 and 2019, collected from the Multi-city Multi-country Collaborative Research Network database.
Participants: All suicides were registered in these locations during the study period (overall 1 701  286 cases).
Main outcome measures: Daily suicide mortality.
Results: Mondays had peak suicide risk during weekdays (Monday-Friday) across all countries, with relative risks (reference: Wednesday) ranging from 1.02 (95% confidence interval (CI) 0.95 to 1.10) in Costa Rica to 1.17 (1.09 to 1.25) in Chile. Suicide risks were lowest on Saturdays or Sundays in many countries in North America, Asia, and Europe. However, the risk increased during weekends in South and Central American countries, Finland, and South Africa. Additionally, evidence suggested strong increases in suicide risk on New Year’s day in most countries with relative risks ranging from 0.93 (95% CI 0.75 to 1.14) in Japan to 1.93 (1.31 to 2.85) in Chile, whereas the evidence on Christmas day was weak. Suicide risk was associated with a weak decrease on other national holidays, except for Central and South American countries, where the risk generally increased one or two days after these holidays.
Conclusions: Suicide risk was highest on Mondays and increased on New Year’s day in most countries. However, the risk of suicide on weekends and Christmas varied by country and territory. The results of this study can help to better understand the short term variations in suicide risks and define suicide prevention action plans and awareness campaigns.
"
Chemotherapy among patients with triple negative breast cancer based on integrated mRNA-lncRNA signature,"Introduction
Triple negative breast cancer, characterised by the absence of expression of oestrogen receptor, progesterone receptor, and human epidermal growth factor receptor 2 (HER2), accounts for 15-20% of all invasive breast cancers and is associated with a high risk of early recurrence and mortality.12Adjuvant anthracycline/taxane based chemotherapy is a standard treatment for early stage triple negative breast cancer, but approximately 20-40% of patients experience disease recurrence.345Therefore, an urgent need exists for more effective strategies to optimise adjuvant therapy for triple negative breast cancer. Treatment of triple negative breast cancer has been challenging owing to the molecular heterogeneity of the disease, which leads to inconsistent responses and outcomes following current treatment approaches. Advances in high throughput technologies have led to the development of multigene signatures, including Oncotype DX, Mammaprint, and HER2DX, for predicting prognosis and guiding adjuvant therapy.6789Nevertheless, signatures specific to triple negative breast cancer are scarce and lack validation in prospective clinical trials.101112
In our previous research, we developed an integrated mRNA-lncRNA signature (three mRNAs: FCGR1A, RSAD2, and CHRDL1; two lncRNAs: HIF1A-AS2 and AK124454) that could effectively classify patients with triple negative breast cancer into groups at high risk or low risk for disease recurrence.13Importantly, our preliminary data suggest that the patients at high risk derived less benefit from taxane based chemotherapy. This finding supports the notion of integrating non-cross resistant platinum containing agents into standard adjuvant chemotherapy for patients at high risk. Given the strong pre-clinical evidence for a synergistic effect of cisplatin with gemcitabine,1415and the high objective response rate observed with this doublet as first line treatment in a phase 3 trial of metastatic triple negative breast cancer,16we hypothesised that patients with high risk, early stage triple negative breast cancer identified using the multigene signature would benefit from the addition of gemcitabine and cisplatin to the standard anthracycline/taxane based regimen in the adjuvant setting.
In this context, we did a phase 3 trial (BCTOP-T-A01) to compare an intensive adjuvant regimen—four cycles of docetaxel, epirubicin, and cyclophosphamide followed by four cycles of gemcitabine and cisplatin—with the anthracycline/taxane containing standard regimen (epirubicin and cyclophosphamide followed by docetaxel), in patients with high risk, early stage triple negative breast cancer identified using the multigene signature. Additionally, the trial aimed to prospectively validate the prognostic value of the signature in patients treated with a uniform adjuvant chemotherapy regimen of epirubicin and cyclophosphamide followed by docetaxel. Here, we present the results of this trial after a median follow-up time of 45.1 months.
Methods
Study design and patients
This prospective, multicentre, open label, phase 3 trial was conducted across seven cancer centres in China (supplementary table A). Eligible participants were women aged 18-70 years with newly diagnosed, operable, unilateral invasive triple negative breast cancer with clear margins after primary surgery and pathologically confirmed regional node positive disease or node negative disease with a primary tumour diameter >10 mm. The status of oestrogen receptor, progesterone receptor, and HER2 was confirmed locally through immunohistochemical analysis, with oestrogen receptor/progesterone receptor negative status defined as <1% nuclear staining and HER2 negative status defined as an immunohistochemical score of 0 or 1 or an immunohistochemical score of 2 without HER2 amplification.1718Other inclusion criteria were Eastern Cooperative Oncology Group (ECOG) performance score of 0 or 1, normal organ function, and ability to start study treatments within eight weeks after surgery. We excluded patients if they had received preoperative anticancer therapy or had distant metastases. Detailed inclusion and exclusion criteria are provided in the protocol (supplementary material). Written informed consent was obtained from all patients.
Sample preparation
Samples from the resected tumours were obtained from every woman who participated in the trial. The fresh resected tumour samples were preserved in RNAlater (Thermo Fisher Scientific) and shipped on ice to a central laboratory for further analysis. Total RNA was isolated from the samples by using the RNeasy Plus Mini Kit (Qiagen). The purity and quantity of total RNA were estimated by measuring the absorbance at 260 nm (A260) and 280 nm (A280) using a NanoDrop 2000 spectrophotometer (Thermo Fisher Scientific). RNase-free water was used as a blank control. When the A260/A280 ratio was between 1.9 and 2.1, the extracted RNA was determined to be pure and was used in subsequent experiments.
Quantitative real time polymerase chain reaction assay
The expression of mRNAs and lncRNAs constituting the multigene signature (mRNAs: FCGR1A, RSAD2, and CHRDL1; lncRNAs: HIF1A-AS2 and AK124454) was measured using quantitative real time polymerase chain reaction, as previously reported.13cDNA was synthesised using the PrimeScript RT reagent kit (Takara Bio Inc, Otsu, Japan) and SYBR Premix Ex Taq kit (Takara Bio Inc, Otsu, Japan). The ABI PRISM 7900HT Sequence Detection System (Applied Biosystems, Foster City, CA, USA) was used for quantitative real time polymerase chain reaction analysis. All experiments were conducted by following the standard protocol provided by the manufacturer. U6 was used as the reference gene. The expressions of the five RNAs were normalised to U6 expression for the calculation of the risk score for recurrence, as previously described.13
Randomisation and masking
On the basis of the recurrence score, patients at high risk were randomised (1:1) to receive either intensive adjuvant chemotherapy (four cycles of docetaxel, epirubicin, and cyclophosphamide followed by four cycles of gemcitabine and cisplatin; arm A) or standard adjuvant chemotherapy (four cycles of epirubicin and cyclophosphamide followed by docetaxel; arm B). Patients at low risk (arm C) were assigned to receive the same chemotherapy regimen as arm B. Randomisation was generated centrally with a block size of four. This process was carried out with a computer generated random allocation sequence prepared by an independent statistician. The investigators sent the random assignment forms by fax to the Clinical Research Coordination Office at Fudan University Shanghai Cancer Centre (Shanghai, China). After applying the inclusion and exclusion criteria to each patient, the study coordinator sent the details for the allocated treatment group to the investigator by fax. The study was open label, and allocation was unmasked to patients and investigators. However, all outcome assessors (for example, radiologists and laboratory personnel), data collectors, and analysts were blinded to treatment assignments.
Procedures
The patients in arm A received four cycles of epirubicin 75 mg/m2, cyclophosphamide 500 mg/m2, and docetaxel 75 mg/m2on day 1 every three weeks, followed by four cycles of gemcitabine 1250 mg/m2on days 1 and 8 and cisplatin 75 mg/m2on day 1 every three weeks. In both arms B and C, chemotherapy consisted of four cycles of epirubicin 90 mg/m2and cyclophosphamide 600 mg/m2on day 1 every three weeks, followed by four cycles of docetaxel 100 mg/m2on day 1 every three weeks). Treatment was continued until the maximum number of cycles was reached, the disease progressed, the patient withdrew, or unacceptable toxicity occurred or at the discretion of the investigator. Post-chemotherapy radiotherapy was administered according to guidelines from the National Comprehensive Cancer Network, St Gallen International Consensus, and Chinese Breast Cancer Society and implemented according to institutional protocols.1920
All patients received primary prophylaxis with pegylated recombinant human granulocyte colony stimulating factor. Patients were permitted up to two dose reductions (set at 75% and 50% of the initial dosage, respectively) and a dose delay of up to 14 days for managing adverse events. The dose was reduced in the event of grade 4 neutropenia lasting for three or more days, febrile neutropenia, grade 4 thrombocytopenia or bleeding related to thrombocytopenia, grade 3 anaemia, grade 3/4 non-haematological toxicities, and other toxicities deemed by the investigator to necessitate dose reduction. If the whole blood count was low, day 8 gemcitabine could be given at a reduced dose or postponed for up to seven days to allow recovery; otherwise it was discontinued. Patients could discontinue a single chemotherapy agent in the combination if a severe adverse event was judged to be related to that specific agent (for example, grade ≥2 pneumonitis or haemolytic uraemic syndrome for gemcitabine; creatinine clearance <30 mL/min or grade 3/4 neurotoxicity for cisplatin). Additional details on dose modification, pre-medication, and supportive care are specified in the protocol.
Clinical and laboratory assessments were required before each cycle and within four weeks after completion of chemotherapy. Physical examination, breast ultrasonography, and abdominal ultrasonography were performed every three months during years 1 and 2, every six months during years 3-5, and yearly thereafter. Mammography and computed tomography of the chest were performed yearly.
Outcomes
The primary endpoint was disease-free survival for arm A versus arm B. The secondary endpoints included disease-free survival for arm B versus arm C, recurrence-free survival, overall survival, and safety. Events used for the analysis of the endpoint of disease-free survival included locoregional or distant recurrence, invasive contralateral cancer, second primary malignancy, or death from any cause, whichever occurred first. Events used for the analysis of the endpoint of recurrence-free survival included first instance of invasive breast cancer recurrence or death. Events used for the analysis of the endpoint of overall survival were death from any cause. All events were measured from the date of random assignment.
The study was originally designed with recurrence-free survival for arm A versus arm B as the primary endpoint (version 1.0; 14 May 2015). In August 2023, after completion of patient enrolment but without any comparative analysis, our data monitoring committee recommended that the primary endpoint be changed from recurrence-free survival to disease-free survival (originally a secondary endpoint), owing to a lower than expected event number for recurrence-free survival, making observation of the required number of events within a reasonable timeframe unlikely; in addition, disease-free survival serves as a more widely adopted primary endpoint in most large scale trials of adjuvant breast cancer treatment.2122
The protocol (version 1.2; 16 August 2023) and statistical analysis plan (version 1.1; 16 August 2023) were amended accordingly. The updated documents, including a protocol amendment list, are provided in the supplementary material. No results from the BCTOP-T-A01 trial were available at the time of this amendment; we did no data analyses until the data cut-off for this report.
Safety assessments included evaluations of the incidence and severity of adverse events as per National Cancer Institute Common Toxicity Criteria version 4.0. These were conducted up to 30 days from the last dose of chemotherapy.
Statistical analysis
We based the sample size calculation for the original primary endpoint of recurrence-free survival on the requirement for sufficient patients at high risk to test the superiority of intensive over standard adjuvant chemotherapy. We needed a total of 106 recurrence-free survival events for patients at high risk to detect an improvement in the three year recurrence-free survival rate of 12%; the detailed sample size calculation is available in the study protocol. For the sample size calculation for disease-free survival, we updated the hypothesis to reflect an 11% difference in the three year disease-free survival rate from 79% to 90% (hazard ratio 0.45) with the intensive regimen versus the standard regimen.2324We estimated the period of enrolment and follow-up needed at 36 and 24 months, respectively. On the basis of a 1:1 randomisation ratio and an assumed 9% drop-out rate, 335 patients at high risk with 50 disease-free survival events would provide 80% power at a significance level of 5% for the two sided log-rank test. We also enrolled 168 patients at low risk who received standard treatment at a 1:1 ratio alongside patients at high risk on the same regimen, totalling 503 participants required. We used the intention-to-treat population for efficacy analyses. We assessed safety and toxicity in patients who received at least one dose of study treatment. We estimated survival outcomes by using the Kaplan-Meier method and compared them with log-rank tests. We built Cox models to control for intergroup prognostic variables and estimate hazard ratios and 95% confidence intervals (CIs). We also tested interaction of treatment with clinicopathological factors and analysed subgroups by menopausal status, histological grade, T stage, nodal status, lymphovascular invasion, and Ki-67 index. We plotted time dependent receiver operating characteristic curves and calculated areas under the curve to assess the efficacy of the signature compared with conventional clinicopathological factors in patients receiving standard chemotherapy.
For the primary endpoint, we considered P values to be significant at a two sided significance level of 5%. We present nominal P values for other endpoints without adjustment for multiplicity. We used SPSS version 22.0 for statistical analyses.
Patient and public involvement
Patients were not involved in the design or implementation of the trial as this was not customary in China at the time of study design. In addition, to keep the confidentiality of clinical data, the patients were not involved in data analysis, interpretation, or writing up of the results. Although patients and the public were not directly involved in this trial, mainly owing to training restrictions, we informed patients about the trial and ensured their awareness of its purpose and content during recruitment. We also invited a member of the public to review our manuscript after submission and communicated the results to patients who expressed an interest during clinic visits.
Results
Patients’ characteristics
Between 3 January 2016 and 17 July 2023, 504 patients were enrolled in the study, of whom 336 were classified as being at high risk and 168 as at low risk (fig 1). The baseline characteristics of the patients at high risk were well balanced between arms A and B (table 1). In the high risk cohort, the overall median age was 52 (range 27-69) years; 279 (83%) patients had a primary tumour size of greater than 2 cm, and 105 (31%) patients had four or more involved lymph nodes (44 (13%) patients had ≥10 positive nodes). Most patients in both arms had undergone a mastectomy (99%; 333/336) and axillary lymph node dissection (80%; 270/336) and had received radiation therapy (69%; 231/336). Compared with patients at low risk in arm C, a numerically higher proportion of patients at high risk had poor prognostic features (that is, larger tumour sizes and greater axillary node involvement). For example, among women at high risk, 70% (116/166) in arm A and 69% (118/170) in arm B had lymph node involvement compared with 17% (29/168) in arm C.
Approximately 99% (498/504) of patients received at least one cycle of assigned chemotherapy (fig 1), with most completing chemotherapy treatment as specified in the protocol (91% (149/163) of patients in arm A, 93% (158/169) in arm B, and 95% (157/166) in arm C). The proportion of patients needing dose reductions was 31% (51/163), 15% (25/169), and 14% (23/166) respectively. Cycle delays occurred in 29% (47/163), 20% (33/169), and 22% (37/166), respectively. Haematological toxicity was the most frequent reason for cycle delay.
Efficacy outcomes in patients at high risk
As of the data cut-off date of 25 December 2023, the median follow-up time was 45.1 months. A total of 47 (14%) disease-free survival events occurred among 336 patients at high risk, including 36 locoregional or distant relapses, eight second primary malignancies, one contralateral breast cancer, and two deaths.Table 2shows the distribution of the first disease-free survival events by treatment arm. The primary endpoint of three year disease-free survival rate (originally a secondary endpoint) was 90.9% for arm A and 80.6% for arm B (a 10.3 percentage point difference;fig 2, top). Disease-free survival was significantly higher among patients receiving intensive chemotherapy than in those receiving standard therapy (hazard ratio 0.51, 95% CI 0.28 to 0.95; P=0.03). The three year recurrence-free survival rate (the original primary endpoint) was higher in arm A than in arm B (92.6%v83.2%; hazard ratio 0.50, 95% CI 0.25 to 0.98; P=0.04;fig 2, middle). As of data cut-off, overall survival events were recorded in a total of 18 (5%) patients in arm A and B. Preliminary data at this early time point indicated a trend favouring arm A over arm B in overall survival (three year overall survival rate 98.2%v91.3%; hazard ratio 0.58, 95% CI 0.22 to 1.54; P=0.27;fig 2, bottom); adequate assessment of the efficacy of intensive chemotherapy in terms of overall survival will require long term follow-up and more events. Exploratory forest plot analyses for disease-free survival in the intention-to-treat population showed hazard ratios that consistently favoured the intensive chemotherapy regimen (fig 3).
Validation of signature
Among the 168 patients classified as having low risk triple negative breast cancer, 22 (13%) disease-free survival events occurred (table 2). The three year disease-free survival, recurrence-free survival, and overall survival rates for patients in arm C were 90.1%, 94.5%, and 100%, respectively. Patients classified as being at low risk had significantly higher rates of disease-free survival (hazard ratio 0.57, 95% CI 0.33 to 0.98; P=0.04), recurrence-free survival (0.42, 0.22 to 0.81; P=0.007), and overall survival (0.14, 0.03 to 0.61; P=0.002) than did patients at high risk receiving the same standard chemotherapy (supplementary figure A). Analysis of areas under the receiver operating characteristic curves indicated that the integrated signature was more effective than the traditional clinicopathological factors, including TNM stage, tumour grade, Ki-67, and age in predicting disease-free survival, recurrence-free survival, and overall survival at three years (supplementary figure B).
Safety
All 498 patients in the safety population had at least one treatment related adverse event (table 3). The overall incidence of grade 3 or 4 treatment related adverse events was 105 (64%) of 163 patients in the arm A, 86 (51%) of 169 patients in arm B, and 90 (54%) of 166 patients in arm C. The most common treatment related adverse events of grade 3 or 4 were haematological toxicities in all arms; grade 3 or 4 events occurring more frequently in arm A (percentage point difference ≥2% versus either arm B or arm C) included neutropenia, thrombocytopenia, febrile neutropenia, anaemia, nausea, and vomiting. Overall, 22 treatment related serious adverse events occurred: nine (6%) in arm A (febrile neutropenia and infection (n=3 each); thrombosis, thrombocytopenia, and diarrhoea (n=1 each)), seven (4%) in arm B (febrile neutropenia (n=2); liver dysfunction, vomiting, rash, infection, and severe neutropenia n=1 each)), and six (4%) in arm C (febrile neutropenia (n=2); allergic reaction, thrombocytopenia, liver dysfunction, and vomiting (n=1 each)). Discontinuation of study treatment due to toxicity was recorded in eight (5%), five (3%), and four (2%) patients, respectively, in arms A, B, and C. In randomised patients who received study treatment, incidence of grade ≥3 treatment related adverse events was significantly higher in arm A than in arm B; however, we found no significant difference in the incidence of treatment related serious adverse events or treatment related adverse events leading to dose discontinuation between arms A and B(supplementary table B). No treatment related deaths occurred in the study.
Discussion
The assessment of the risk of recurrence of breast cancer after therapy of curative intent has traditionally relied on clinical and histological evaluations. However, given the disease heterogeneity and the absence of well defined molecular targets, optimised adjuvant strategies are needed. This phase 3 trial marks a pivotal advance, showing for the first time the feasibility of using multigene signatures to tailor individualised adjuvant therapy for patients with operable triple negative breast cancer. The results showed that the addition of gemcitabine and cisplatin to anthracycline/taxane based adjuvant chemotherapy led to significantly improved disease-free survival (hazard ratio 0.51) compared with standard anthracycline/taxane based chemotherapy in patients with high risk triple negative breast cancer identified using the integrated mRNA-lncRNA signature. The benefits for disease-free survival with the intensive chemotherapy were consistent across all patient subgroups. In addition, this study provides independent external validation of the prognostic value of the integrated signature in a uniformly treated population.
Comparison with other studies
After we had designed and initiated our trial, the KEYNOTE-522 study showed that adding pembrolizumab to neoadjuvant chemotherapy, followed by adjuvant pembrolizumab, significantly improved pathological complete response rates and event-free survival for high risk, early stage triple negative breast cancer.25Nevertheless, controversies remain regarding the optimal chemotherapy partners and treatment duration with immunotherapy. In addition, the US Food and Drug Administration approved olaparib as adjuvant treatment for patients with high riskBRCA1/2mutated triple negative breast cancer, on the basis of the findings of the OlympiA study. However, the reported prevalence ofBRCA1/2variants in unselected patients with triple negative breast cancer is low at 11.2%.26These data highlight the urgent need for more effective treatment strategies to optimise adjuvant therapy for the broad population of patients with triple negative breast cancer. Our multigene signature might serve as a prognostic tool to guide clinical decisions. For example, patients at low risk may benefit from de-escalated chemotherapy (such as a platinum-free regimen) in combination with immunotherapy during the neoadjuvant phase. Moreover, those with low risk disease who have achieved a pathological complete response following neoadjuvant chemoimmunotherapy might be exempted from adjuvant immunotherapy. Further research is needed to explore the applicability of this multigene signature across various clinical settings.
Identifying molecular characteristics specific to triple negative breast cancer subtypes to distinguish patients with different prognoses is crucial for tailoring treatment. Various multigene signatures have been proposed for triple negative breast cancer, such as the 44 gene DNA damage immune response signature, exploiting an RNA based signature to differentiate patients with different prognoses.12Although these models have enhanced our understanding of the heterogeneity of triple negative breast cancer and facilitated clinical research, prospective evidence on their performance is scarce. This study applied the integrated mRNA-lncRNA signature to patients prospectively and tailored adjuvant treatment strategies on the basis of the risk classification indicated by the signature. The results independently validated the prognostic value of the signature, confirming its clinical feasibility in guiding precision treatment. Importantly, the stratification schema based on the multigene signature could be iteratively updated with advances in drug development, highlighting its potential for widespread application.
A major focus of this trial was to explore the efficacy of intensive chemotherapy in patients with high risk triple negative breast cancer. We hypothesised that augmenting standard anthracycline/taxane based therapy with non-cross resistant agents would further improve patients’ outcomes. In this trial, intensive chemotherapy significantly improved disease-free survival compared with standard chemotherapy (90.9%v80.6%; P=0.03), particularly reducing the risk of distant metastases (10v20 events). These results are remarkable, considering that the control arm received the standard eight cycles of anthracycline/taxane based chemotherapy. Previous attempts to improve outcomes by adding additional chemotherapeutic agents to anthracyclines, taxanes, and cyclophosphamide in unselected patients have not been successful.272829Distinct from previous intensive treatment strategies mentioned above, the intensive chemotherapy regimen used in our trial was specifically tailored to patients at high risk identified using our integrated signature, in contrast to the inclusion of “all-comers” in other trials. This study provides evidence that the multigene signature could be used to effectively identify patients for intensive treatment.
This BCTOP-T-A01 trial tested the hypothesis that the gemcitabine and cisplatin doublet regimen would improve the prognosis of patients with high risk triple negative breast cancer. Our results indicated a 10.3% absolute improvement in disease-free survival compared with standard chemotherapy. Although platinum containing regimens have shown clinical benefits in both metastatic and preoperative settings,16242526their value as adjuvant treatment remains debatable. A recent meta-analysis showed that platinum based chemotherapy using carboplatin in the adjuvant or neoadjuvant setting improved disease-free survival and overall survival in patients with triple negative breast cancer.30Nevertheless, none of the studies included in the meta-analysis assessed the benefits of incorporating platinum in a standard anthracycline containing regimen in the adjuvant setting, which could potentially optimise outcomes for patients at high risk. Two ongoing large scale randomised trials, NRG BR-003 (ClinicalTrials.gov:NCT02488967) and CITRINE (Carboplatin Intensified Chemotherapy for TRIple NEgative Breast Cancer;NCT04296175), will shed further light regarding the benefits of platinum in the adjuvant setting for patients with triple negative breast cancer.
In addition to defining a subgroup of patients with a high risk of recurrence who will benefit from intensive adjuvant therapy, defining a subgroup in which treatment can be safely de-escalated with a minimal risk of recurrence is equally crucial. To this end, our results indicated an overall promising prognosis in patients who were classified as being at low risk, with survival rates (three year disease-free survival 90.1%; three year overall survival 100%) substantially higher than those previously reported in the ECOG 1199 trial for a general triple negative breast cancer population (three year disease-free survival of 73% and three year overall survival of 82%) treated with the same schedule and dosage of epirubicin and cyclophosphamide followed by docetaxel.31Notably, 15% of patients in arm C were lymph node positive and 65% had T2-3 disease. Our findings might underscore the need for a cautious approach to de-escalating adjuvant therapy for patients in the low risk category, even if they have high risk clinicopathological factors. This observation is of clinical importance because adjuvant chemotherapy is uniformly administered in routine clinical practice despite the existence of distinct biological subgroups. Identifying patients destined for favourable outcomes opens the door to the exploration of de-escalation treatment strategies, such as shorter chemotherapy regimens or non-anthracycline based approaches.
The spectrum of adverse events associated with the intensive chemotherapy regimen were consistent with those reported for gemcitabine plus cisplatin and anthracycline/taxane respectively, with no new safety concerns identified.1632As expected, an increased incidence of grade 3 or 4 treatment related adverse events was seen among patients in the intensive chemotherapy group compared with patients in the standard chemotherapy group (64%v51%; P=0.01), primarily driven by haematological toxicities including neutropenia, febrile neutropenia, and thrombocytopenia. Neutropenia and febrile neutropenia were effectively managed with standard supportive measures. Although the incidence of febrile neutropenia was higher among patients treated with intensive chemotherapy (despite the administration of primary prophylaxis with granulocyte colony stimulating factor) than among those treated with standard chemotherapy (10%v4%; P=0.04), grade 3 or 4 infection with neutropenia was comparable in the two arms (4%v2%). In addition, thrombocytopenia was effectively managed through dose modifications or the use of thrombopoietin or interleukin 11. No patient in our study needed transfusion. Importantly, the addition of gemcitabine plus cisplatin did not increase the incidence of serious adverse events or compromise the patients’ ability to receive chemotherapy, with a comparable proportion of patients in the two arms completing the full number of cycles per protocol (91%v94%; P=0.47). The generally manageable safety profile with intensive treatment facilitated the maximisation of exposure to treatment to achieve favourable long term cancer related outcomes in a potentially curable disease setting,3334and it supported a clinically significant improvement in disease-free survival in patients at high risk, compared with standard treatment.
Limitations of study
This study has several limitations. Firstly, the primary endpoint was amended after completion of enrolment. This amendment aimed to expedite the trial’s primary completion within a reasonable and relevant timeframe, following the data monitoring committee’s recommendation under blinding. Importantly, the required event number for a fully powered analysis of disease-free survival was achievable without affecting the sample size, on the basis of assumptions of similar between group differences (arm A versus arm B) in rates of three year recurrence-free survival and disease-free survival. The results showed significant differences in both disease-free survival and recurrence-free survival, with the degree of absolute and relative benefit being highly consistent. Secondly, despite the finding that dose dense chemotherapy can reduce both recurrence of and mortality from breast cancer,35it is not widely used, and a treatment schedule of once every three weeks was considered standard when the trial was first designed in 2015. Thirdly, updated data from the ECOG 1199 trial suggest that epirubicin and cyclophosphamide, followed by weekly paclitaxel, may be the optimal regimen for treating triple negative breast cancer.31However, evidence directly comparing epirubicin and cyclophosphamide followed by docetaxel against epirubicin and cyclophosphamide followed by weekly paclitaxel is lacking, and the former regimen remains the recommended choice for triple negative breast cancer. Fourthly, the study was confined to Chinese patients, and validation trials for extrapolation to other ethnic groups are warranted. Finally, we used an open label design in our study owing to the nature of the interventions. However, all outcome assessors as well as data collectors and analysts were masked to treatment assignment to reduce the potential for open label bias.
Conclusions
The results of our study indicate that the integrated mRNA-lncRNA signature had potential to tailor adjuvant chemotherapy for patients with operable triple negative breast cancer. Intensive regimens incorporating gemcitabine and cisplatin led to significantly improved disease-free survival compared with standard anthracycline/taxane based therapy in a well defined subgroup of patients with operable triple negative breast cancer—namely, those classified as being at high risk on the basis of the signature. Despite a higher incidence of adverse events, primarily haematological, the safety profile of intensiv","Objective: To evaluate the feasibility of using a multigene signature to tailor individualised adjuvant therapy for patients with operable triple negative breast cancer.
Design: Randomised, multicentre, open label, phase 3 trial.
Setting: 7 cancer centres in China between 3 January 2016 and 17 July 2023.
Participants: Female patients aged 18-70 years with early triple negative breast cancer after definitive surgery.
Interventions: After risk stratification using the integrated signature, patients at high risk were randomised (1:1) to receive an intensive adjuvant treatment comprising four cycles of docetaxel, epirubicin, and cyclophosphamide followed by four cycles of gemcitabine and cisplatin (arm A; n=166) or a standard treatment of four cycles of epirubicin and cyclophosphamide followed by four cycles of docetaxel (arm B; n=170). Patients at low risk received the same adjuvant chemotherapy as arm B (arm C; n=168).
Main outcome measures: The primary endpoint was disease-free survival in the intention-to-treat analysis for arm A versus arm B. Secondary endpoints included disease-free survival for arm C versus arm B, recurrence-free survival, overall survival, and safety.
Results: Among the 504 enrolled patients, 498 received study treatment. At a median follow-up of 45.1 months, the three year disease-free survival rate was 90.9% for patients in arm A and 80.6% for patients in arm B (hazard ratio 0.51, 95% confidence interval (CI) 0.28 to 0.95; P=0.03). The three year recurrence-free survival rate was 92.6% in arm A and 83.2% in arm B (hazard ratio 0.50, 95% CI 0.25 to 0.98; P=0.04). The three year overall survival rate was 98.2% in arm A and 91.3% in arm B (hazard ratio 0.58, 95% CI 0.22 to 1.54; P=0.27). The rates of disease-free survival (three year disease-free survival 90.1%v80.6%; hazard ratio 0.57, 95% CI 0.33 to 0.98; P=0.04), recurrence-free survival (three year recurrence-free survival 94.5%v83.2%; 0.42, 0.22 to 0.81; P=0.007), and overall survival (three year overall survival 100%v91.3%; 0.14, 0.03 to 0.61; P=0.002) were significantly higher in patients in arm C than in those in arm B with the same chemotherapy regimen. The incidence of grade 3-4 treatment related adverse events were 64% (105/163), 51% (86/169), and 54% (90/166) for arms A, B, and C, respectively. No treatment related deaths occurred.
Conclusions: The multigene signature showed potential for tailoring adjuvant chemotherapy for patients with operable triple negative breast cancer. Intensive regimens incorporating gemcitabine and cisplatin into anthracycline/taxane based therapy significantly improved disease-free survival with manageable toxicity.
Trial registration: ClinicalTrials.govNCT02641847.
"
GLP-1 receptor agonists before upper gastrointestinal endoscopy and risk of pulmonary aspiration or discontinuation of procedure,"Introduction
The use of glucagon-like peptide-1 (GLP-1) receptor agonists, a class of drugs used to treat type 2 diabetes and obesity,12has increased rapidly over the past decade owing to evidence of cardiovascular benefit in these populations.34567This rapid uptake has been further driven by evidence showing that high dose semaglutide and the dual glucose dependent insulinotropic polypeptide-GLP-1 receptor agonist tirzepatide reduce weight by 10-20% in people with obesity with or without diabetes.28910GLP-1 receptor agonists exert glycemic and weight lowering effects through mechanisms that include delayed gastric emptying, which could have important implications in the perioperative setting.111213
Recently, several published case reports have described pulmonary aspiration or retained gastric contents during elective procedures in patients using semaglutide despite adherence to appropriate fasting protocols.141516In small observational studies, use of GLP-1 receptor agonists in patients undergoing upper gastrointestinal endoscopy or elective procedures requiring anesthesia was associated with retained gastric content compared with patients not using GLP-1 receptor agonists, which can increase the risk of pulmonary aspiration.17181920
In light of these safety signals and in an effort to minimize this potential risk, the American Society of Anesthesiologists published consensus based guidance to withhold GLP-1 receptor agonists in all patients for one dose before an elective procedure (one day for drugs administered daily and one week for those administered weekly).21In response, concerns about the potential harms of discontinuing GLP-1 receptor agonists, particularly among people with diabetes, have been raised,22with the American Gastroenterological Association issuing a clinical practice update urging that well designed studies should investigate the risk of pulmonary aspiration in GLP-1 receptor agonist users who require endoscopy.23More recently, a multisociety statement was published emphasizing the lack of data to support stopping GLP-1 receptor agonists in all patients before elective endoscopy procedures.24
Additional evidence from robust pharmacoepidemiologic studies on the association between GLP-1 receptor agonist use and risk of pulmonary aspiration during upper endoscopy is needed. Pharmacoepidemiologic studies can be useful for evaluating rare safety outcomes by using large claims databases while implementing design and analysis techniques to minimize bias, such as the active comparator design and propensity score weighting.25In the current study, we assessed the risk of pulmonary aspiration and discontinuation of endoscopy associated with preprocedural use of GLP-1 receptor agonists compared with another antidiabetic drug class with similar indications to minimize confounding, in the real world clinical context of upper endoscopy.
Methods
Data sources
This nationwide cohort study used data from two databases: MarketScan Commercial Claims and Encounters, and Optum Clinformatics Data Mart to identify adults aged 18 years and older with type 2 diabetes who underwent upper endoscopy: between 1 January 2016 and 31 December 2021 in MarketScan and to 31 August 2023 in Optum Clinformatics Data Mart. Both databases provide deidentified longitudinal health claims data, including demographic information, visits to physicians, hospital admissions, diagnoses, procedures, and outpatient dispensations of prescription drugs.
Study design
The cohort entry date was the date of upper endoscopy, which was defined using current procedural terminology codes (see supplemental eTable 1). We did not include endoscopic retrograde cholangiopancreatography procedures owing to their increased complexity and distinct indications relative to routine upper endoscopy. We excluded patients aged <18 years at cohort entry and those without at least 365 days of continuous enrollment in an insurance plan before endoscopy. We also excluded those without a diagnosis code for type 2 diabetes, with a diagnosis code for type 1 diabetes, with any history of bariatric surgery, or admitted to a nursing home facility within 365 days before endoscopy. We also excluded those who underwent lower (colonoscopy) or upper endoscopy within the 365 days before cohort entry as they may have had a chronic gastrointestinal condition, further increasing their risk for pulmonary aspiration or complications during the procedure; additionally, including this population could introduce confounding by indication as these patients might be less likely to receive GLP-1 receptor agonists. Lastly, we excluded patients who had colonoscopy and upper endoscopy on the same day, given the different preprocedural protocol (fig 1).
Drug use
Drug use was assessed within 30 days before upper endoscopy (fig 1). Patients with a dispensed GLP-1 receptor agonist during the 30 days before endoscopy or with a drug supply extending into the 30 day assessment period for drug use were considered to have used the drug; patients meeting the same criteria for sodium-glucose cotransporter-2 (SGLT-2) inhibitors were the referent group. SGLT-2 inhibitors are a class of diabetes drugs used for similar indications to GLP-1 receptor agonists and at a similar disease stage and are not known to affect gastric emptying; we therefore considered SGLT-2 inhibitors to be an appropriate comparator class to GLP-1 receptor agonists in this study. We excluded patients who used both drug classes. The GLP-1 receptor agonist class included dulaglutide, liraglutide, oral and subcutaneous semaglutide, exenatide, lixisenatide, and tirzepatide, and the SGLT-2 inhibitor class included empagliflozin, dapagliflozin, and canagliflozin.
Outcomes
The primary outcome was pulmonary aspiration on the day of or the day after endoscopy, defined using ICD-10 (international classification of diseases, 10th revision) codes (see supplemental eTable 1). We also assessed discontinuation of endoscopy as a secondary outcome. Discontinuation was captured using current procedural terminology modification code 53, which indicates that a surgical or diagnostic procedure was started but discontinued, and procedure codes Z53.X (procedure not carried out), in association with ICD-10 codes for the same endoscopy claim (see supplemental eTable 1).
Covariates
We considered several potential confounders, including personal characteristics (age, biological sex, region of residence, race/ethnicity), lifestyle factors (smoking, alcohol dependence, drug misuse, body mass index (BMI), comorbidities (eg, heart failure, coronary artery disease, gastroesophageal reflux disease, gastrointestinal cancer, peptic ulcer disease, gastroparesis, diabetic neuropathy, diabetic retinopathy), obesity related comorbidities (osteoarthritis, obstructive sleep apnea, non-alcoholic fatty liver disease), and drug use (eg, proton pump inhibitors, histamine receptor 2 antagonists (H2blockers), opioids, antiemetics, benzodiazepines, beta blockers). We also included frailty level as measured by the validated claims based frailty index,2627as well as measures of healthcare utilization, including number of distinct drugs, number of visits to an endocrinologist, and number and length of previous hospital admissions (see supplemental methods for list of covariates). All covariates were assessed within 335 days before the assessment period for drug use (fig 1).
Analyses
To mitigate confounding, we first estimated the propensity score of using a GLP-1 receptor agonist versus an SGLT-2 inhibitor using a multivariable logistic regression that included all the covariates relating to personal characteristics, diabetes severity and drugs, gastrointestinal conditions, other comorbidities, and healthcare utilization, and then used fine stratification weighting to balance the drug groups28(see supplemental methods for details of propensity score model). We trimmed the study population that did not fall within overlapping regions of the propensity score distribution and then created 50 propensity score strata based on the distribution of the GLP-1 receptor agonist group. Patients in the GLP-1 receptor agonist group were assigned a weight of 1 and those in the SGLT-2 inhibitor group were weighted in proportion to the number of GLP-1 receptor agonist users in their corresponding stratum. We assessed for balance of covariates between the two groups using absolute standardized differences, with differences >0.1 considered to indicate substantial imbalance.29For each drug group, we calculated the absolute risk of pulmonary aspiration and discontinuation of endoscopy and estimated the risk ratio and corresponding 95% confidence intervals (CIs) in generalized linear models (PROC GENMOD with weight statement and log-link function and binomial distribution).28
Subgroup and sensitivity analyses
We conducted three subgroup analyses. First, we stratified the analysis by type of sedation used during endoscopy, distinguishing between those who received anesthesia care services (current procedural terminology codes 00731 and 00740) and those who did not. Second, we evaluated BMI subgroups (<30 and ≥30), excluding individuals with missing BMI data. Third, given the differences in the half-life and frequency with which GLP-1 receptor agonists are administered, we stratified the analysis based on the specific GLP-1 receptor agonist (subcutaneous semaglutide, dulaglutide, liraglutide, exenatide-lixisenatide, and tirzepatide) used within 30 days before endoscopy. Exenatide and lixisenatide were combined in the agent specific analysis owing to small sample sizes and similar short half-life.
In sensitivity analyses, we first restricted the cohort to patients who received a prescription dispensation for a GLP-1 receptor agonist or SGLT-2 inhibitor within 30 days before endoscopy, excluding those who had drug supply from a previously filled prescription but did not receive a dispensing during the 30 day assessment period for drug use. Second, we restricted the cohort to short term new users of GLP-1 receptor agonists or SGLT-2 inhibitors, defined as receiving a prescription dispensation within 12 weeks before endoscopy after a washout (no drug use) period of six months. Third, we restricted the cohort to persistent users of GLP-1 receptor agonists or SGLT-2 inhibitors, defined as receiving at least two prescriptions dispensations within six months before endoscopy. Fourth, owing to different preparations required for procedures, we repeated the study in two additional cohorts: a cohort of patients who had colonoscopy only, and a cohort of patients who had colonoscopy and upper endoscopy on the same day. Fifth, we excluded patients who were intubated during endoscopy as the decision to use endotracheal intubation is rare and affected by the duration and complexity of a procedure as well as the presence of respiratory comorbidities, making it difficult to ascertain if intubation was an a priori decision or related to a complication of the procedure. Finally, since delayed recording of aspiration pneumonia is possible, we increased the follow-up period to assess pulmonary aspiration as an outcome from one day to three days. For each of these analyses, we re-estimated propensity scores and reweighted study participants.
Missing data
Given the nature of claims data, where the presence or absence of a specific billing code is interpreted as the presence or absence of a specific drug dispensation or clinical event, no data on drug use or outcomes were missing, and imputation was not required. BMI level was missing for >50% of patients, and given the high percentage of missingness, we used a missing indicator rather than imputation in the primary analysis. To deal with potential residual confounding from under-reporting of information on BMI, we included several proxies for obesity in the propensity score model. Additionally, we conducted a sensitivity analysis wherein we stratified patients based on the availability of BMI data and reconducted analyses to test the robustness of the primary findings. No other data were missing.
We conducted analyses separately for each database and then pooled the results using a Mantel-Haenszel fixed effect model. All analyses were conducted with Aetion Evidence Platform version 4.4830and SAS version 9.4 (SAS Institute, Cary, NC). The study protocol was registered at institutional level. This manuscript is reported according to STROBE (strengthening the reporting of observational studies in epidemiology) guidelines.31
Patient and public involvement
Our study was based on analysis of secondary data. No patients were involved in setting the research question or the outcome measures, nor were they involved in the design and implementation of the study. As the study received no specific funding, it would not have been possible to engage patients or members of the public.
Results
A total of 5 212 871 individuals underwent upper endoscopy during the study period, of whom 24 824 used a GLP-1 receptor agonist and 18 541 used an SGLT-2 inhibitor within 30 days before the procedure and met the eligibility criteria (fig 2). Before weighting, both study groups were well balanced on many covariates; however, GLP-1 receptor agonist users were slightly younger and more likely to be female patients, had a higher BMI, and received antiemetics and opioids (table 1). After weighting, the groups were well balanced for all included covariates, with standardized differences <0.1 (see supplemental eTables 2-4 for list of characteristics overall and by database).
Pulmonary aspiration
Overall, GLP-1 receptor agonist use in the 30 days before upper endoscopy was not associated with an increased risk of pulmonary aspiration compared with SGLT-2 inhibitor use (weighted risk 4.15v4.26 per 1000 people, respectively; pooled risk ratio 0.98, 95% CI 0.73 to 1.31) (table 2).
No difference in risk of pulmonary aspiration was observed between GLP-1 receptor agonists and SGLT-2 inhibitors in subgroup analyses based on type of anesthesia care used for endoscopy or obesity (fig 3). Neither was a difference in risk of pulmonary aspiration found between specific individual GLP-1 receptor agonists and SGLT-2 inhibitors, except for a higher risk of pulmonary aspiration with exenatide-lixisenatide (pooled risk ratio 2.49, 1.36 to 4.59), albeit with low precision owing to the small number of patients who used these agents (fig 3).
Results were consistent in the sensitivity analysis that redefined drug use as a prescription dispensation of GLP-1 receptor agonist or SGLT-2 inhibitor within 30 days before endoscopy, and when the cohort was restricted to new users of either GLP-1 receptor agonists or SGLT-2 inhibitors within 12 weeks before endoscopy, or to persistent users (table 2). Lastly, no increased risk of pulmonary aspiration was observed among GLP-1 receptor agonist users who underwent colonoscopy only (pooled risk ratio 1.57, 0.76 to 3.24) or colonoscopy and upper endoscopy on the same day (0.74, 0.35 to 1.56), compared with users of SGLT-2 inhibitors who underwent the same procedures (table 2).
Discontinuation of endoscopy
An increased risk for discontinuation of endoscopy was observed among GLP-1 receptor agonist users compared with SGLT-2 inhibitor users (9.79v4.91 per 1000 people; pooled risk ratio 1.99, 1.56 to 2.53) who underwent upper endoscopy (table 2). This increase did not appear to differ based on type of anesthesia care (fig 3). The risk of discontinuation among GLP-1 receptor agonist users compared with SGLT-2 inhibitor users did, however, appear to differ on the basis of obesity, with an increased risk observed among those with a BMI ≥30 (pooled risk ratio 2.60, 1.65 to 4.06) who used a GLP-1 receptor agonist compared with an SGLT-2 inhibitor, but not among those with a BMI <30 (0.99, 0.51 to 1.94) (fig 3). Additionally, we consistently observed a higher risk for discontinuation of endoscopy associated with use of specific GLP-1 receptor agonists, subcutaneous semaglutide, dulaglutide, exenatide-lixisenatide, and tirzepatide compared with SGLT-2 inhibitors (fig 3). The risk of discontinuation was attenuated in the cohort that underwent colonoscopy and upper endoscopy on the same day (pooled risk ratio 1.09, 0.95 to 1.24) and in the colonoscopy only cohort (1.16, 1.08 to 1.26). Results were consistent in all sensitivity analyses (table 2). Results were also consistent when we stratified by the presence or absence of data on BMI (see supplemental eTable5). Supplemental eTables 6 and 7 show the results of all analyses by database.
Discussion
This cohort study of people with type 2 diabetes found no association between use of GLP-1 receptor agonists before upper gastrointestinal endoscopy and increased risk of pulmonary aspiration compared with use of SGLT-2 inhibitors. Our findings provide real world evidence that despite delayed gastric emptying and concern about retained gastric contents with use of GLP-1 receptor agonists before endoscopy, pulmonary aspiration appeared to be rare, and the risk was not higher compared with use of SGLT-2 inhibitors.
Comparison with previous studies
Our findings are consistent with a previous chart review study that estimated the incidence rate of pulmonary aspiration in patients receiving GLP-1 receptor agonists, using electronic medical records from several Mayo Clinic sites, and reported a low rate of pulmonary aspiration similar to that of the general population.3233Additionally, the rarity of pulmonary aspiration in our type 2 diabetes cohort is in line with the available evidence.34Although data on the incidence of pulmonary aspiration in people with type 2 diabetes are not available, in previous studies conducted in the general population, the estimates ranged between 0.05% and 1.1%.3536373839Although a 33% increased risk of pulmonary aspiration during endoscopy among GLP-1 receptor agonist users versus non-users was reported in a cohort study40the lack of an active comparator design makes these findings prone to confounding by indication since diabetes and its related comorbidities can affect the risk of pulmonary aspiration.41In our study, we minimized this risk by using an active comparator design with a clinically appropriate comparator at a similar disease stage.41Indeed, recent findings showed the risk of pulmonary aspiration in an endoscopy cohort was no higher with use of GLP-1 receptor agonists than with use of DPP-4 inhibitors,42although the latter might not be an ideal comparator class given their closely related incretin based mechanism of action, which theoretically could also affect gastric emptying.4344
Clinical implications
Importantly, we observed an increased risk for discontinuation of endoscopy among GLP-1 receptor agonist users compared with SGLT-2 inhibitor users (9.60v5.10 per 1000 people). The reason for discontinuation was not available in our data. Nonetheless this relative increase in risk among GLP-1 receptor agonist users suggests that to minimize the risk of pulmonary aspiration, endoscopists might be inclined to reschedule procedures when retained gastric contents are detected or suspected. Indeed, in a recent case report of retained gastric contents during elective endoscopy in preparation for bariatric surgery, the procedure was stopped owing to concerns about pulmonary aspiration.45Although not a life threatening event, discontinuation of endoscopy can have several clinical and administrative implications, including delays in diagnosis and increased healthcare costs, as well as personal and societal economic costs such as missed workdays.23
We also found that the increased risk for discontinuation of endoscopy might be modified by obesity. A higher risk of discontinuation was observed among those with a BMI ≥30 compared with a BMI <30. GLP-1 receptor agonists have been on the market for nearly two decades, and concerns about the associated risk of pulmonary aspiration and retained gastric contents have surfaced only recently. Owing to their efficacy in lowering weight and their additional indication for the treatment of obesity, higher potency and higher dose GLP-1 receptor agonists are now being used specifically in patients with obesity. Given the inaccuracies in correctly identifying the dose regimen of GLP-1 receptor agonists from pharmacy dispensations, however, we were unable to conduct a subgroup analysis by dose of GLP-1 receptor agonist. Notably, the subgroup analyses suggested a higher risk for discontinuation of endoscopy associated with the higher potency GLP-1 receptor agonists subcutaneous semaglutide and tirzepatide. The risk of pulmonary aspiration and discontinuation of endoscopy should be assessed in future studies, with a focus on high potency GLP-1 receptor agonists.
Although most of the available evidence thus far has been restricted to semaglutide,14151617the analysis stratified by specific GLP-1 receptor agonist showed a potentially increased risk of pulmonary aspiration associated only with the less commonly used exendin based short acting GLP-1 receptor agonist exenatide-lixisenatide; shorter acting GLP-1 receptor agonists are reported to be associated with a greater incidence of gastric side effects attributed to delayed gastric emptying, which could explain this phenomenon.46
It has been suggested that the potential increased risk of pulmonary aspiration is strictly associated with short term use of GLP-1 receptor agonists (ie, initiating the drug within 8-12 weeks before endoscopy), and that this risk might be mitigated with longer term use as gastric emptying might normalize as a result of tachyphylaxis; thus caution has been recommended only with short term use of GLP-1 receptor agonists.47When we restricted the cohort to those initiating GLP-1 receptor agonists within 12 weeks before endoscopy, however, we did not detect a higher risk of pulmonary aspiration.
Our study focused on the risk of pulmonary aspiration during upper endoscopy. Importantly, the two concerns about the increased risk of pulmonary aspiration with GLP-1 receptor agonist use are interrelated. The first concern is among patients undergoing upper endoscopy, in which the risk of pulmonary aspiration is independent and the procedure often not performed under general anesthesia.4849The second is among patients who undergo general anesthesia, regardless of procedure, as postulated by the American Society of Anesthesiologists consensus guidance and several other recent editorials.2150Our findings need to be interpreted in the context of endoscopy only; this study did not investigate the association between GLP-1 receptor agonist use and risk of pulmonary aspiration during general anesthesia for other procedures, although a subgroup analysis by anesthesia care did not identify an increased risk among GLP-1 receptor agonist users compared with SGLT-2 inhibitor users even among those receiving anesthesia care. Specifically, we did not assess the risk of regurgitation or pulmonary aspiration in other elective procedures that require general anesthesia in which there is no direct inspection of gastric contents during the procedure, as is the case with upper endoscopy. Additional evidence is needed to understand the risk of pulmonary aspiration in non-endoscopic procedures that require general anesthesia.
Limitations of this study
Although the clinical findings were consistent across a range of subgroup and sensitivity analyses, our study does have several limitations. First, the primary and secondary outcome definitions were based on diagnostic codes from claims data, which have not been previously validated. Additionally, both outcomes may be under-reported in claims data. We do not expect this misclassification of measurement to differ between GLP-1 receptor agonists and SGLT-2 inhibitors because this study was conducted using data before the publication of any information on the potential safety signal of GLP-1RA and pulmonary aspiration. If more severe pulmonary aspiration events are likely to occur among GLP-1RA users and be recorded, however, this might have led to a biased higher risk of pulmonary aspiration in the GLP-1RA group versus SGLT-2 inhibitor group, which was not observed. Additionally, although we inferred that discontinuation of endoscopy might be related to concerns about retained gastric contents, the exact reason for discontinuation was unknown. Second, misclassification of drug use was possible given the use of claims data. Specifically, drug use before endoscopy was defined based on drug possession and supply of the dispensed prescription; whereas a sensitivity analysis restricting the cohort to patients dispensed a prescription within 30 days before endoscopy confirmed the primary findings, it is not possible to ascertain if patients prescribed GLP-1 receptor agonists were taking the drugs and whether they were instructed to stop treatment before endoscopy. Patients may have discontinued using GLP-1 receptor agonists before endoscopy, although the American Society of Anesthesiologists’ specific recommendation was only recently published (29 June 2023), and before this no specific recommendations were available on withholding GLP-1 receptor agonists before procedures. We were also unable to account for the specific dose of GLP-1 receptor agonist or any recent titration. Third, given the rarity of pulmonary aspiration, effect estimates comparing GLP-1 receptor agonists with SGLT-2 inhibitors were not precise owing to wide confidence intervals. Although our findings showing no difference in risk between use of GLP-1 receptor agonists and use of SGLT-2 inhibitors are consistent with those of other studies, with a 95% upper confidence limit of 1.31 we cannot rule out the possibility that GLP-1 receptor agonists may be associated with up to a 31% increase in relative risk of pulmonary aspiration, compared with SGLT-2 inhibitors. Fourth, despite the inclusion of a wide range of potential confounders, the restriction of the study population to people with type 2 diabetes using similar diabetes drugs, and the use of advanced adjustment techniques to balance covariates across drug use groups, residual confounding cannot be fully ruled out. Data on laboratory measures, including hemoglobin A1Cand glomerular filtration rate were only available in a subset of the study population and thus were not included in the propensity score model. Additionally, the covariate assessment period was limited to one year before endoscopy and thus conditions not brought to medical attention during this time may not have been captured and accounted for in the analyses. We also did not have data on the specific indication for endoscopy, although we included a list of potential indications, such as gastroesophageal reflux disease and gastrointestinal cancer, in the propensity score model. Fifth, we did not have information on BMI for all patients (>50% missingness), therefore residual confounding by BMI is possible. Additionally, patients with missing information on BMI were excluded from the subgroup analysis exploring the potential modification of the GLP-1 receptor agonist effect by BMI, which could reduce precision of the estimates in this analysis or introduce bias if information on BMI was not missing at random. However, clinical findings were consistent in a sensitivity analysis stratified based on the presence of information on BMI. Similarly, our subgroup and sensitivity analyses might have had limited statistical power to detect small but clinically meaningful differences owing to the small number of events. Sixth, we did not have data on the specific healthcare provider or medical facility, therefore we were unable to account for potential clustering by these factors.51Finally, to minimize threats to internal validity, we restricted the cohort to people with type 2 diabetes. We did not include patients receiving GLP-1 receptor agonists for weight loss without diabetes given that these patients have different characteristics and would not have a valid propensity score weighted cohort of SGLT-2 inhibitor users. We also excluded those who had undergone a previous endoscopy procedure. Therefore, the findings of this study might not be generalizable to all users of GLP-1 receptor agonists. A randomized controlled trial could potentially overcome several of these limitations that are inherent to the data and observational nature of the study; however, given the rarity of pulmonary aspiration, the feasibility of such a trial might be challenging (requiring >30 000 participants for 90% power and >23 000 for 80% power). Thus, while we wait for such trials to be conducted, real world evidence provides important information to guide clinical decision making in the setting of an urgent research question.
These limitations were balanced by several strengths. By using two national databases, we were able to go beyond case reports and small observational studies to estimate the risk of pulmonary aspiration as well as discontinuation of endoscopy. In addition, by using an active comparator design and rigorous pharmacoepidemiologic methods, we were able to reduce several of the biases inherent in many of the smaller observational reports.
Conclusions
Among adults with type 2 diabetes using GLP-1 receptor agonists before upper endoscopy, pulmonary aspiration was rare (4.2 per 1000 patients), and we did not observe an increased risk compared with those taking SGLT-2 inhibitors. Discontinuation of endoscopy occurred in less than 1% of procedures but was about twice as common among those using GLP-1 receptor agonists. Although the exact reason for discontinuation is unknown, it could indicate a higher risk of retained gastric content and concern about pulmonary aspiration. These findings, and those of future pharmacoepidemiologic analyses, may inform future practice recommendations on the preprocedural protocol for elective endoscopy in the absence of evidence from randomized trials.
Pulmonary aspiration or retained gastric contents during upper gastrointestinal endoscopy have been reported among patients taking glucagon-like peptide-1 (GLP-1) receptor agonists before the procedure
Emerging data from cohort studies have been inconsistent about whether GLP-1 receptor agonist use before endoscopy is associated with a higher risk of pulmonary aspiration
The risk for discontinuation of endoscopy, which might indicate concern about retained gastric contents, is unknown
In this study of >43 000 adults with type 2 diabetes, GLP-1 receptor agonist use within 30 days before upper endoscopy was not associated with an increased risk of pulmonary aspiration compared with use of sodium-glucose cotransporter-2 (SGLT-2) inhibitors
Use of GLP-1 receptor agonists was, however, associated with a higher risk for discontinuation of endoscopy compared with SGLT-2 inhibitors
These findings may inform practice recommendations on the preprocedural protocol for patients requiring endoscopy
","Objective: To assess whether use of glucagon-like peptide-1 (GLP-1) receptor agonists before upper gastrointestinal endoscopy is associated with increased risk of pulmonary aspiration or discontinuation of the procedure compared with sodium-glucose cotransporter-2 (SGLT-2) inhibitors.
Design: Cohort study.
Setting: Two deidentified US commercial healthcare databases.
Participants: 43 365 adults (≥18 years) with type 2 diabetes who used a GLP-1 receptor agonist or SGLT-2 inhibitor within 30 days before upper gastrointestinal endoscopy.
Main outcome measures: The primary outcome was pulmonary aspiration on the day of or the day after endoscopy, defined using diagnostic codes. The secondary outcome was discontinuation of endoscopy. Risk ratios and corresponding 95% confidence intervals (CIs) were estimated after fine stratification weighting based on propensity score.
Results: After weighting, 24 817 adults used a GLP-1 receptor agonist (mean age 59.9 years; 63.6% female) and 18 537 used an SGLT-2 inhibitor (59.8 years; 63.7% female). Among users of GLP-1 receptor agonists and SGLT-2 inhibitors, the weighted risk per 1000 people was, respectively, 4.15 and 4.26 for pulmonary aspiration and 9.79 and 4.91 for discontinuation of endoscopy. Compared with SGLT-2 inhibitor use, GLP-1 receptor agonist use was not associated with an increased risk of pulmonary aspiration (pooled risk ratio 0.98, 95% CI 0.73 to 1.31), although it was associated with a higher risk for discontinuation of endoscopy (1.99, 1.56 to 2.53).
Conclusions: In this comparative cohort study, no increased risk of pulmonary aspiration during upper gastrointestinal endoscopy was observed among adults with type 2 diabetes using GLP-1 receptor agonists compared with SGLT-2 inhibitors within 30 days of the procedure; however, GLP-1 receptor agonists were associated with a higher risk of discontinuation of endoscopy, possibly owing to a higher risk of retained gastric content. In the absence of evidence from randomized trials, these findings could inform future practice recommendations on the preprocedural protocol for patients requiring endoscopy.
"
Prioritising primary care patients with unexpected weight loss for cancer investigation,"Introduction
Unexpected weight loss is associated with both early and late stage cancers in adults attending primary care.123The likelihood of a cancer diagnosis in people with unexpected weight loss is increased in the three to six months after the first record of the weight change compared with people without unexpected weight loss: men with unexpected weight loss are three times as likely as men without unexpected weight loss to have a diagnosis of cancer within three months and are twice as likely to receive a diagnosis within six months; women with unexpected weight loss are twice as likely to have a diagnosis of cancer within three months.1The greatest risks are from lymphoma, cancer of unknown primary, or cancers of the pancreas, gastro-oesophageal tract, lung, bowel, or renal tract.14A cancer diagnosis is less likely than in people without recorded unexpected weight loss after the initial three to six month period.1
Unexpected weight loss can also be caused by a wide range of benign and serious conditions associated with various bodily systems, lifestyle choices, and socioeconomic factors.2Differential diagnoses include advanced heart failure, chronic obstructive pulmonary disease, renal disease, pancreatic insufficiency, malabsorption, and endocrine disease, with up to 25% of patients without a diagnosis to explain their weight loss after extended follow-up.25The non-specific nature of unexpected weight loss creates the clinical problem of who should be investigated further for cancer—and possibly using invasive methods—and who could be spared investigation. Several clinical reviews have proposed plausible approaches assessing the risk of cancer, but evidence generally has been from studies of older people admitted to hospital for investigation.2Such research does not directly help general practitioners to plan investigations in primary care because of spectrum bias.6Given the absence of appropriate clinical guidelines, or standardised practice, doctors have been reported to take diverse action, from doing nothing to ordering “extensive blind investigations” to avoid missing underlying cancer.78
Most research on the predictive value of cancer related unexpected weight loss in primary care has included patients on the basis of their final cancer diagnosis rather than on weight loss.4The evidence base informed the National Institute for Health and Care Excellence guidance on suspected cancer, which recommended further investigations for patients with a positive predictive value (PPV) for cancer that exceeded 3%.9Studies have investigated unexpected weight loss together with other symptoms and signs occurring over a one to two year period preceding the cancer diagnosis without acknowledging that the predictive value of individual symptoms will vary during different periods.41011In this context, predictive values could have been reported for pairs of clinical features that occurred months or years apart, potentially with different causes unrelated to the eventual cancer diagnosis.
Although simple blood tests are often used to investigate non-specific symptoms in primary care patients,121314the role of such tests in selecting those with unexpected weight loss for further investigation of cancer is poorly understood. Abnormal test results might facilitate patient triage,1516be poor predictors of cancer,1718or be predictive across several cancer sites.19Triage testing in primary care is important to avoid unnecessary urgent referrals of patients for invasive investigation.
We conducted a diagnostic accuracy study using routinely collected electronic health records in primary care to establish the predictive value of unexpected weight loss for cancer, given the patient’s age, sex, smoking status, concurrent symptoms or signs, and blood test results. To identify malignancies that might be prioritised for further investigation after referral, we considered the predictive value for cancer overall and by cancer site.
This article is an updated version of a previously published BMJ paper, which has since been retracted (doi:10.1136/bmj.m2651). The authors identified a selection bias in the previous paper, which meant that some patients were excluded from the study because their healthcare records contained a code that was not associated with unexpected weight loss. Sometime later, however, some of these patients might have had a code included in their healthcare record that was associated with unexpected weight loss and cancer. This resulted in the previous study underestimating the prevalence of cancer in patients with unexpected weight loss attending primary care, which modified the study’s key results and messages.
Methods
Study design and population
We performed a retrospective diagnostic accuracy study using electronic health records from Clinical Practice Research Datalink (CPRD), a representative database of anonymised primary care records covering 13% of the UK population,20linked to the National Cancer Registrations and Analysis Service (NCRAS) cancer register. The “Performance of diagnostic strategies” section of the published protocol pertains to this analysis.8We followed the RECORD (reporting of studies conducted using observational routinely collected data) reporting guidelines.21Study entry was from 1 January 2000 to 31 December 2019 to allow two years or more to accommodate the time it takes for NCRAS to release validated data.
Patients were included if they were aged ≥18 years; registered with a general practice contributing data to CPRD and eligible for linkage to NCRAS, Hospital Episode Statistics, and Office for National Statistics data; and had at least one code for unexpected weight loss and at least 12 months of data before the first recorded unexpected weight loss code (the index date). These unexpected weight loss codes equated to a mean weight loss of ≥5% within a six month period in our previous internal validation study of weight related coding in CPRD.22Unexpected weight loss could be coded according to a range of clinical scenarios, including unexpected weight loss reported as the patient’s presenting condition with or without objective evidence, after targeted history taking by the clinician, and after weight measurement as part of the clinical examination or as part of a routine health check or chronic disease review.
We excluded patients if they had a prescription of weight reducing treatment (orlistat) or a code for bariatric surgery in the previous six months, or if they had a cancer diagnosis before the index date.
Cancer (reference standard)
To identify cancers, we updated an existing library of codes to include all high level ICD-O (international classification of diseases for oncology) categories.1We excluded cancers classified as non-melanoma skin, in situ, benign, ill defined, or uncertain. Furthermore, we grouped cancer sites that would usually be investigated using the same test or by the same specialty—for example, renal, ureteric, and bladder as renal tract cancers, and liver, gallbladder, and biliary tree as hepatobiliary cancers. All cancers diagnosed in the six months after the index date were identified in CPRD and linked NCRAS data. We used the first site specific cancer code after the index date to define cancer site. Cancer of unknown primary was defined if a code identifying a secondary cancer (such as lymph node metastasis or cerebral metastasis) was found but there was no code for a primary cancer.
Sociodemographic and clinical features
Sociodemographic details coded on or before the index date were extracted from CPRD records. We identified codes related to signs and symptoms and blood test results in the three months before the index date to one month after. A long list of symptoms and signs shown to have an independent association with undiagnosed cancer were selected either through their inclusion in the NICE NG12 guidance for suspected cancer or based on studies published after the NICE guidance—that is, central nervous system malignancies and head and neck cancers (see supplementary appendix 1). For blood tests, we identified those most commonly requested within the four month period and dropped outliers and dichotomised continuous test results as abnormal or normal using standard laboratory ranges (see supplementary appendix 2).
Statistical analysis
Box 1shows the definitions of true and false positive and negative test results for clinical features. For combinations of unexpected weight loss and age group, sex, smoking status, clinical features, and abnormal blood test results, we estimated diagnostic accuracy statistics for the cancer outcome using 2×2 tables with the DIAGT Stata module: positive likelihood ratios, negative likelihood ratios, PPVs, and diagnostic odds ratios along with 95% confidence intervals (CIs). A rule of thumb is that a test with a positive likelihood ratio ≥5 is reliable for ruling in disease, and a test with a negative likelihood ratio ≤0.2 is reliable for ruling out disease.2324The analysis was conducted for cancer overall and by cancer site. To select symptoms and signs, we used multivariable backwards stepwise logistic regression starting with all symptoms, signs, and sociodemographic factors as independent covariates, using a value of P≤0.01 for retention (see supplementary appendix 1). We elected to use an indicator variable over multiple imputation to replace missing data on lifestyle factors, as the main purpose of including the covariates was to reduce confounding in variable selection rather than to identify the association between the lifestyle covariate with missing data and cancer.
Classification of true and false positive and negative test results
Presence of a clinical feature recorded in CPRD in the three months before to one month after the index date of unexpected weight loss in patients with a cancer diagnosis (recorded in CPRD or NCRAS cancer registry) in the six months after the index date
Presence of a clinical feature recorded in CPRD in the three months before to one month after the index date of unexpected weight loss in patients with no cancer diagnosis (recorded in CPRD or NCRAS cancer registry) in the six months after the index date
Absence of a clinical feature recorded in CPRD in the three months before to one month after the index date of unexpected weight loss in patients with a cancer diagnosis (recorded in CPRD or NCRAS cancer registry) in the six months after the index date
Absence of a clinical feature recorded in CPRD in the three months before to one month after the index date of unexpected weight loss in patients with no cancer diagnosis (recorded in CPRD or NCRAS cancer registry) in the six months after the index date
CPRD=Clinical Practice Research Datalink; NCRAS=National Cancer Registrations and Analysis Service
In discrete analyses we also calculated diagnostic accuracy statistics for each of the 11 most frequently recorded blood tests, and we included only patients with a test result for each. When tests were components of another test, we chose the quantum—for example, using total white cell count rather than white cell subtypes.
Sensitivity analysis
We repeated the selection process for clinical features using an interval of three months before the index date to the first date of either three months after or the cancer diagnosis to explore whether broadening the window for inclusion of clinical features changed our findings.
Patient and public involvement
Patients and members of the public were involved in an advisory capacity in the application for funding to support this research. An advisory panel of patients and members of the public provided comments on a related article that informed the current analysis. Patients were not directly involved in the conduct or analysis of the study.
Results
Figure 1shows the flow of participants through the study. Of 326 240 patients with a code for unexpected weight loss, 184 270 (56.5%) were women, 176 508 (54.1%) were aged ≥60 years, 142 654 (43.7%) had a body mass index (BMI) within normal range, and 176 053 (54.0%) were ever smokers (table 1). The most commonly recorded features alongside unexpected weight loss were cough (6.7%), abdominal pain (5.4%), back pain (5.3%), shortness of breath (5.3%), and chest infection (4.7%) (see supplementary appendix 3). The most frequently recorded tests were full blood count (predominantly for haemoglobin (68.4%), platelets (67.7%), and total white cell count (67.8%)), and liver function tests—namely, bilirubin (63.8%), albumin (61.2%), alkaline phosphatase (64.0%), and alanine transaminase/aspartate transaminase (63.7%) (see supplementary appendix 2).
Age, sex, and smoking status
The PPV for a cancer diagnosis was higher in people who were older and those who smoked (fig 2)—it was >2% in non-smokers aged ≥50 years. Analysis by sex, however, showed that the PPV was >3% for men aged ≥50 years and women aged ≥60 years, regardless of smoking status.
Signs and symptoms
In multivariable analysis, clinical features selected to be positively associated with cancer in people with unexpected weight loss were abdominal mass, abdominal pain, loss of appetite, back pain, chest signs, dysphagia, fatigue, iron deficiency anaemia, jaundice, lymphadenopathy, rectal mass, and venous thromboembolism (table 2andtable 3). Constipation, abnormal prostate examination, dyspepsia, haemoptysis, hoarseness, and itch were associated with cancer only in men with unexpected weight loss, and nausea, bloating, reflux, and pelvic mass only in women with unexpected weight loss. Positive likelihood ratios in men ranged from 1.43 (95% CI 1.30 to 1.58) for fatigue to 21.00 (8.59 to 51.37) for rectal mass, and in women from 1.28 (1.16 to 1.41) for back pain to 19.46 (12.69 to 29.85) for pelvic mass. Although six symptoms and signs had positive likelihood ratios >5, two of these were relatively uncommon, occurring in 10 to 35 people with a diagnosis of cancer, depending on sex. Negative likelihood ratios ranged from 0.93 to 1.00, with none <0.20 (table 2,table 3, andtable 4).
For men and women aged ≥50 years, unexpected weight loss and the co-occurrence of the selected symptoms and signs were associated with a ≥3% increase in the PPV, except for fatigue and back pain in women, which reached this threshold at ages ≥60 years (fig 3andfig 4). For men aged 40-49 years with unexpected weight loss, the co-occurrence of 10 of the selected symptoms and signs was associated with a ≥3% increase in the PPV, whereas only three reached this level for women in this age group. For men aged 18-39 years with unexpected weight loss, the co-occurrence of lymphadenopathy was also associated with a ≥3% increase in the PPV.
Blood tests
Several abnormal blood test results in combination with unexpected weight loss had the highest positive likelihood ratio values: low albumin (3.24, 3.13 to 3.35), raised platelets (3.48, 3.35 to 3.62), total white cell count (3.01, 2.89 to 3.14), and raised C reactive protein (3.13, 3.05 to 3.20) (table 4). Normal inflammatory markers had the lowest negative likelihood ratios: C reactive protein (0.48, 95% CI 0.46 to 0.49) and erythrocyte sedimentation rate (0.51, 0.49 to 0.52). Individual blood test results therefore did not reach the ideal threshold of 5 to rule in a cancer diagnosis, or the threshold of 0.2 to rule out a diagnosis.
In men and women aged ≥50 years, however, all individual abnormal blood test results showed a PPV ≥3%, above the underlying PPV for all people with unexpected weight loss (fig 5andfig 6), except for raised alanine transaminase/aspartate transaminase and creatinine in women (fig 5). For patients aged 40-49 years, PPVs ≥3% were observed for thrombocytosis, low albumin and haemoglobin, and raised alkaline phosphatase and C reactive protein, and in men for raised erythrocyte sedimentation rate and white cell count. Thrombocytosis, low albumin and haemoglobin, and raised C reactive protein in younger men also had PPVs ≥3%. Some combinations of abnormal test results in younger age groups had PPVs ≥3%, but confidence in these estimates was less because not all patients had all tests (fig 7).
Cancer diagnoses
Cancer was diagnosed in 15 624 (4.8%) patients within six months of the index date, 15 051 (96.3%) of whom were aged ≥50 years and 15 455 (98.9%) ≥40 years. The most frequently diagnosed malignancies were cancers of the lung (n=3569, 22.8%), bowel (n=2434, 15.6%), gastro-oesophagus (n=1936, 12.4%), prostate (n=1373, 8.8%), and pancreas (n=1329, 8.5%). Individually, some clinical features are generally considered to be associated with a single cancer site; however, when they co-occurred with unexpected weight loss, they were associated with several cancer types. For example, men with dyspepsia and unexpected weight loss were diagnosed as having cancers of the following types or site (in rank order): stomach or oesophagus, bowel, lung, prostate, pancreas, renal tract, lymphoma, other, hepatobiliary, myeloma, head and neck, melanoma of skin, bone and connective tissue, central nervous system, and leukaemia (fig 4). Similarly, abnormal test results in patients with unexpected weight loss were also associated with multiple cancer sites. For example, women aged 50-59 years with low albumin levels were diagnosed as having cancers of the following types or site (in rank order): lung, bowel, breast, renal tract, stomach or oesophagus, lymphoma, pancreas, ovarian, other, hepatobiliary, myeloma, uterine, head and neck, melanoma of skin, bone and connective tissue, central nervous system, and leukaemia (fig 5).
Isolated unexpected weight loss
PPVs for patients without any of the selected clinical features were lower across every age range compared with the full cohort (fig 2). In addition, 81 538 (25.0%) patients had no record of a blood test, 2449 (3.0%) of whom had a diagnosis of cancer. Overall, 867 patients with unexpected weight loss and cancer had neither a clinical feature nor blood test on record: 407 out of 482 men and 334 of 385 women were aged ≥60 years.
Sensitivity analyses
Appendix 3 shows the results of the sensitivity analysis. Widening the time window made almost no difference to the clinical features selected for inclusion in the final analysis.
Clinical guideline
Appendix 4 summarises the current NICE guideline recommendations for suspected cancer in patients with unexpected weight loss.Table 5outlines an updated clinical guideline based on the results of the current analysis.
Discussion
The risk of undiagnosed cancer in adults aged <50 years with recorded unexpected weight loss alone is below the UK’s current 3% threshold that warrants investigation. However, in men aged ≥50 years, women aged ≥60 years, and younger patients with concurrent clinical features (certain symptoms and signs or abnormal results for simple blood tests) the risk of undiagnosed cancer rises above 3%. For women aged 50-59 years these features were low haemoglobin, abdominal mass, abdominal pain, loss of appetite, chest signs, dysphagia, iron deficiency anaemia, jaundice, leucocytosis, lymphadenopathy, low albumin, nausea, pelvic mass, raised alkaline phosphatase, raised C reactive protein, raised erythrocyte sedimentation rate, reflux, thrombocytosis, venous thromboembolism, and vomiting. A subset of 17 clinical features in men and eight in women increased the risk of cancer >3% for patients aged 40-49 years. Low haemoglobin, lymphadenopathy, thrombocytosis, low albumin, and raised C reactive protein in younger men also had PPVs of ≥3%. The absence of individual clinical features in the three months before and one month after the index date, or the presence of individual normal blood test results in this time window, does not reliably rule out cancer in patients with unexpected weight loss.
Strengths and limitations of this study
We took several steps to improve the likelihood that the unexpected weight loss cohort was accurately defined. Firstly, we confirmed that insufficient weight measurements were recorded in UK primary care to define unexpected weight loss, with clustering of weight recording noted in women with higher BMI and in those with comorbidity.25We then conducted an internal validation study to identify which codes most consistently defined unexpected weight loss, and investigated whether weight measurements could be used in preference to codes.25We included each patient once in the analysis by choosing the first unexpected weight loss code, and excluded patients with a history of cancer to ensure we were investigating unexpected weight loss associated with a first diagnosis of cancer.6We excluded patients with objective evidence of deliberate weight loss (ie, prescription records and coding for bariatric surgery). The presence of advanced disease states might be more likely to be associated with unexpected weight loss and could modify the association of unexpected weight loss and cancer. As it is problematic to identify disease severity using CPRD data, however, we did not exclude patients with these conditions.
We analysed clinical features as occurring with unexpected weight loss if they were coded in the three months before and one month after the index date. This was a clinical decision, as the epidemiology on this topic is limited, based on consideration that a general practitioner is likely to look back at recent notes and investigate unexpected weight loss within a month of presentation. Symptoms occurring more than three months before the index date might well be unlinked to the unexpected weight loss. Some studies have reported the frequency of individual clinical features for people with cancer and controls before a cancer diagnosis, with few symptoms more common in people with cancer than controls earlier than six months before the diagnosis.162627None of these studies, however, reported the timing of multiple symptoms leading up to a cancer diagnosis. We have shown previously that cancers are likely to be diagnosed in patients within three months of presentation with unexpected weight loss, and our findings did not change significantly in sensitivity analysis extending the period to capture co-occurring symptoms and signs up to the day of cancer diagnosis.1The high number of false negatives observed for individual symptoms and signs indicates that many people with cancer do not develop unexpected weight loss and that symptoms and signs are only recorded if patients experience them, patients remember to report them, or they are uncovered by the doctor during a clinical examination.
We also conducted individual analyses for each blood test, including only patients with a result for that blood test. Previous studies have replaced missing blood tests with negative results to allow full case multivariable analysis. We decided against this for two reasons. Firstly, patients who have been tested represent a higher risk population than those who have not been tested,1828and therefore people with a normal test result might not have the same likelihood of undiagnosed cancer as people who have not been tested. It is unclear how this testing bias relates to the study participants with unexpected weight loss, for whom the blood test was taken close enough to the index date for us to be confident that the test and the result pertained to it. Secondly, classifying absent tests as negative results inflates the number of “true” negatives and misestimates diagnostic accuracy, making it difficult to interpret negative likelihood ratios and negative predictive values. However, as not all patients had been tested with all blood tests, we could not calculate precise estimates for combinations of multiple blood test results. We also dichotomised continuous test results at thresholds used in clinical practice to signify an abnormal result. By dichotomising we lose information by classifying markedly abnormal test results together with mildly abnormal results. The PPVs presented can therefore be considered conservative estimates of the associated cancer risk. The high number of false positive blood test results represent doctors deciding to use blood tests to investigate unexpected weight loss in most patients, that cancer was associated with unexpected weight loss in fewer than 2% of patients with cancer, and that abnormal results in the blood tests studied are not only found in people with cancer. Finally, we classified people as having cancer if a code was entered within six months of presenting with unexpected weight loss. Previous research has shown that if cancer is not diagnosed within six months, the risk of cancer being the cause of the unexpected weight loss is low.129
Comparison with other studies
A 2018 systematic review reported higher PPVs of unexpected weight loss than we found here.4This could be accounted for by the considerable heterogeneity between studies included in that review. For example, sensitivity was higher in studies at risk of recall bias. PPVs also varied by the method of data collection, and they were higher in case-control studies than in cohort studies reporting on the same tumour site. A recent clinical review reported 17 symptoms, signs, and test results that in combination with unexpected weight loss had a PPV for cancer of >3%.2These estimates were taken from case-control studies using primary care records that included clinical features occurring in the 1-2 years before the diagnosis of a specific cancer. We studied a much shorter interval around the presentation with unexpected weight loss, included all cancer sites, and had a study size sufficient to allow separate estimates to be produced for each age group and by smoking status and sex. We found evidence for clinical features and abnormal blood test results that were predictive of cancer in combination with unexpected weight loss not previously reported: loss of appetite, abdominal mass, back pain, bloating, chest signs, dyspepsia, fatigue, nausea, reflux, pelvic mass, venous thromboembolism, raised alkaline phosphatase, low albumin, and a raised white cell count. In addition, our study confirmed the importance of abdominal pain, constipation, jaundice, lymphadenopathy, haemoptysis, dysphagia, rectal mass, thrombocytosis, and low haemoglobin, but the implication of these features co-occurring with unexpected weight loss differed from when they occurred alone.
Policy implications
The risk of cancer in younger adults (<50 years) presenting with unexpected weight loss alone is below the threshold for referral for invasive cancer investigation set by NICE. However, in men aged >50 years, women aged >60 years, and younger patients with the concurrent clinical features shown infig 3,fig 4,fig 5, andfig 6, the risk of cancer increases such that referral for invasive investigation becomes justified. In the absence of these features, these results might suggest that doctors arrange simple routine blood tests, particularly for full blood count, liver function, erythrocyte sedimentation rate, C reactive protein, and calcium (fig 3andfig 4). Almost any abnormal test result increases the risk of cancer sufficiently to trigger invasive testing. A higher or lower threshold of cancer risk could be chosen to trigger cancer investigation by primary care clinicians practising outside the UK. The PPVs presented will allow clinicians worldwide to use whichever threshold applies locally. However, as negative likelihood ratios were never <0.20, and although normal blood test results might reassure patients, clinicians should be aware that in isolation a normal blood test result does not reduce the probability of cancer downward enough to rule out the disease in patients with unexpected weight loss.24
A pro-inflammatory state underpins cancer cachexia,3031and prognostic scores composed of markers of the systemic inflammatory response are used in patients with cachexia to predict survival and response to treatment in secondary care.323334A potential avenue for future research is to investigate the utility of inflammatory marker scores and combinations of negative test results in selecting who should (and who should not) undergo invasive testing for cancer.
These findings might also have implications for cancer referral pathways. For example, NICE guidelines suggest that patients with unexpected weight loss and abdominal pain should be investigated for colorectal cancer (see supplementary appendix 4).2In the current study, more than 10 additional cancers presented in this way that would be missed by colonoscopy (fig 3andfig 4). Likewise, some non-alarm symptoms, such as loss of appetite and fatigue, indicated a probability of cancer that was above the threshold for invasive investigation in the presence of unexpected weight loss. Throughout Denmark and in an increasing number of centres in the UK, Rapid Diagnostic Centres have started to operate that rapidly investigate non-specific symptoms across a broad range of cancer sites35363738and other associated serious non-cancer conditions.3940
Lastly, women presenting with unexpected weight loss were at markedly lower risk of having cancer than men with unexpected weight loss. Different, but interrelated, mechanisms might underpin this finding. Firstly, women might be more likely to visit their doctor to discuss their weight: they are also more likely to have a weight measurement recorded in UK primary care.25Secondly, women may be more likely to report symptoms of cancer earlier to prompt investigation before weight loss occurs. Thirdly, men may delay presentation until weight loss is noticeable.41This study could not examine these possibilities. Nevertheless, routine weight measurement in primary care could lead to the earlier detection of weight change.
Conclusion
Unexpected weight loss alone in people aged <50 years is unlikely to be due to cancer, and immediate referral for invasive testing might not be justified. In men aged ≥50 years and women aged ≥60 years, onward referral might be justified without additional clinical features recorded in the three months before and one month after the index date. Some additional clinical features recorded for younger patients in this time window increase the risk of cancer substantially over the 3% threshold, justifying further investigation. Clinical features thought to be specific to an individual cancer site are markers of several different types of cancer when they co-occur with unexpected weight loss, which could mean that new, broader investigative approaches are needed for patients with unexpected weight loss.
The likelihood of a diagnosis of early or late stage cancer is increased in the 3-6 months after the first record of unexpected weight loss in primary care
Malignancies most strongly predicted by unexpected weight loss are lymphoma, cancer of unknown primary, or pancreatic, gastro-oesophageal, hepatobiliary, lung, bowel, and renal tract cancers
Studies that have investigated the predictive value of clinical features in combination with unexpected weight loss have not acknowledged that predictive values vary during different periods
The risk of undiagnosed cancer in young adults (<50 years) attending primary care with unexpected weight loss alone is below the UK’s current 3% threshold warranting investigation
In men aged ≥50 years, women aged ≥60 years, and younger patients with other clinical features that could indicate cancer, the risk of undiagnosed cancer across multiple sites rises above the 3% threshold
For women aged 50-59 years these features were low haemoglobin, abdominal mass, abdominal pain, loss of appetite, chest signs, dysphagia, iron deficiency anaemia, jaundice, leucocytosis, lymphadenopathy, low albumin, nausea, pelvic mass, raised alkaline phosphatase, raised C reactive protein, raised erythrocyte sedimentation rate, reflux, thrombocytosis, venous thromboembolism, and vomiting
","Objective: To quantify the predictive value of unexpected weight loss for cancer according to patient’s age, sex, smoking status, and concurrent clinical features (symptoms, signs, and abnormal blood test results).
Design: Diagnostic accuracy study (update).
Setting: Data from Clinical Practice Research Datalink electronic health records linked to the National Cancer Registration and Analysis Service in primary care, England.
Participants: 326 240 adults (≥18 years) with a code for unexpected weight loss from 1 January 2000 to 31 December 2019.
Main outcome measures: Cancer diagnosis in the six months after the earliest weight loss code (index date). Codes for additional clinical features were identified in the three months before to one month after the index date. Diagnostic accuracy measures included positive and negative likelihood ratios, positive predictive values, and diagnostic odds ratios.
Results: Of 326 240 adults with unexpected weight loss, 184 270 (56.5%) were women, 176 508 (54.1%) were aged ≥60 years, and 176 053 (54.0%) were ever smokers. 15 624 (4.8%) had a diagnosis of cancer within six months of the index date, of whom 15 051 (96.3%) were aged ≥50 years. The positive predictive value for cancer was above the 3% threshold recommended by the National Institute for Health and Care Excellence for urgent investigation in men aged ≥50 years and women aged ≥60 years. 17 additional clinical features were associated with cancer in younger men with unexpected weight loss, and eight in women. Positive likelihood ratios in men ranged from 1.43 (95% confidence interval 1.30 to 1.58) for fatigue to 21.00 (8.59 to 51.37) for rectal mass, and in women from 1.28 (1.16 to 1.41) for back pain to 19.46 (12.69 to 29.85) for pelvic mass. Abnormal blood test results associated with cancer included low albumin (positive likelihood ratio 3.24, 3.13 to 3.35) and raised platelets (3.48, 3.35 to 3.62), total white cell count (3.01, 2.89 to 3.14), and C reactive protein (3.13, 3.05 to 3.20). However, no normal blood test result in isolation ruled out cancer. Clinical features co-occurring with unexpected weight loss were associated with multiple cancer sites.
Conclusion: The risk of cancer in younger adults with unexpected weight loss presenting to primary care is <3% and does not merit investigation under current UK guidelines. However, in men aged ≥50 years, women aged ≥60 years, and younger patients with concurrent clinical features, the risk of cancer warrants referral for invasive investigation. Clinical features typically associated with specific cancer sites are markers of several cancer types when they occur with unexpected weight loss.
Readers’ note: This article is an updated version of a previously published BMJ paper that has since been retracted.
"
Paternal metformin use and risk of congenital malformations in offspring,"Introduction
The global prevalence of type 2 diabetes mellitus is escalating among men of reproductive age.123This disease can have an adverse effect on fertility in men at multiple levels, including reduced sperm vitality45and the suppression of gonadal testosterone production.6Obesity, a condition that commonly accompanies type 2 diabetes mellitus, also poses a major risk to fertility in men by impairing spermatogenesis and consequently decreasing fecundability.78As an initial glucose lowering agent, metformin is widely used in the treatment of type 2 diabetes mellitus. Nonetheless, concerns have emerged about the potential harmful effects of metformin on the reproductive health of men.
Exposure to metformin in vitro may lead to decreased cell proliferation and altered secretory functions of testicular Sertoli cells.9Several animal studies have also shown that exposure to metformin results in reduced testicular weight and sperm production.1011In men with type 2 diabetes mellitus, the use of metformin may interfere with testicular steroidogenesis as a result of its antiandrogenic properties,10leading to lower testosterone levels.1213This reduction in testosterone can further diminish sperm quality,14which in turn may negatively affect embryogenesis and the early development of offspring from the time of conception, including an increased potential risk of congenital malformations.1516As metformin is considered non-mutagenic, epigenetic rather than genetic alterations to the sperm DNA have been proposed.17A recent Danish study found preconception metformin use in men to be associated with a 40% increased risk of major congenital malformations in offspring (adjusted odds ratio 1.40, 95% confidence interval (CI) 1.08 to 1.82),18particularly genital birth defects in male infants (3.39, 1.82 to 6.30). A reanalysis of Danish data also indicated a 1.4-fold increase in the risk of major congenital malformations associated with paternal metformin use.19
Although fathers contribute half of their offspring’s DNA, understanding the safety of metformin use in men for offspring is limited. While previous studies have shown an association between paternal metformin use and risk of congenital malformations in offspring,1819the biological plausibility in humans remains unclear. Furthermore, owing to the narrow national focus and inadequate control of confounding factors, including type 2 diabetes mellitus, severity of hyperglycaemia, and other diabetes related conditions, questions about the causality between paternal metformin use and risk of congenital malformations in offspring remain unresolved. To provide further guidance for the treatment of diabetes in men of reproductive age, we conducted a cross national cohort study leveraging national databases from Norway and Taiwan to assess the association between paternal metformin use and risk of congenital malformations in offspring, taking into account potential confounding by underlying indications and associated factors.
Methods
Data source and study population
This cross national cohort study was conducted using population based data from Norway and Taiwan. The Norwegian cohort included data from the Medical Birth Registry of Norway, the Norwegian Prescription Database, the Norwegian Patient Registry, and the Norwegian control and payment of health reimbursements. The Taiwanese cohort used information from the National Birth Certificate Application database, the National Health Insurance database, and the Maternal and Child Health Database. Supplementary appendix 1 provides detailed descriptions of the data sources used in the analysis, and both data sources have been used extensively to study drug safety in pregnancy.202122We performed data linkage deterministically by assigning encrypted and unique identification numbers to newborns and parents to generate data for further analysis. The requirement for informed consent was waived owing to the use of deidentified patient data. This study followed the STROBE (strengthening the reporting of observational studies in epidemiology) reporting guideline.
We identified all pregnancies resulting in liveborn singletons from 2010 to 2021 in the Norwegian cohort and from 2004 to 2018 in the Taiwanese cohort. From both cohorts we excluded pregnancies with missing identification numbers for offspring or either parent, unknown sex of the offspring, missing gestational age, multiple pregnancies, and pregnancies in which the mother had filled a prescription for a known teratogenic drug (see supplementary eTable 1). We determined the start of pregnancy in both cohorts as the date of delivery minus gestational age, which was based on ultrasound examination and registered in databases with a high completeness and validity in both Norway (90-98%)23and Taiwan (88.3%).24
Metformin use
The process of developing fully mature spermatozoa, including spermatogenesis and maturation in the epididymis, spans around three months.25We determined paternal use of metformin during this period of sperm development by utilising the dispensing date and number of days supplied. In Norway, we presumed one defined daily dose to estimate the number of days supplied, thereby calculating the end date of a prescription; in Taiwan, we used the recorded number of days supplied according to the claims databases. We considered fathers to have used metformin when the days supplied overlapped with the three months before pregnancy, the period for sperm development (see supplementary eFigure 1). A dose-effect analysis, which was only possible in the Taiwanese cohort, was estimated by the mean daily use of metformin. Mean daily use was calculated using the defined daily dose, as defined by the World Health Organization Collaborating Center for Drug Statistics Methodology,26and categorised as low dose (defined daily dose <1.0) and high dose (defined daily dose ≥1.0).
Outcomes of interest
The primary outcome was any congenital malformation, and the secondary outcome was organ specific malformations (see supplementary eTable 2), according to the European surveillance of congenital anomalies (EUROCAT) guideline.27In the Norwegian cohort, we retrieved this information from the Medical Birth Registry of Norway, which is a member of EUROCAT. In the Taiwanese cohort, we employed an algorithm based on a previous study that utilised inpatient or outpatient diagnostic codes to identify congenital malformations diagnosed within one year after birth.28Data on organ specific malformations were only available for the Taiwanese cohort, except for the category of congenital heart defects, which were available for both cohorts. Analysis and reporting of data were conducted only for those outcomes that were sufficiently represented in the groups for metformin use and no metformin use.
Covariates
We considered a broad range of confounders using directed acyclic graphs (see supplementary eFigure 2). In both cohorts, we identified the calendar year of the offspring’s birth and paternal characteristics, which included age, proxies for severity of diabetes (number of other glucose lowering drugs used, adaptive diabetes complications severity index),29303132chronic comorbidities (eg, hypertension, hyperlipidaemia, mental illness) (see supplementary eTable 3), and drug use (eg, insulin, sulfonylurea, other antidiabetic drugs, antihypertensive drugs, cardiovascular drugs, psychotropic agents) as confounders (see supplementary eTable 4). Moreover, we also considered maternal characteristics and lifestyle factors as proxies for confounders or risk factors for congenital malformations.
Statistical analyses
We compared the baseline characteristics of fathers who used or did not use metformin using standardised differences to evaluate balance between covariates, with a difference >0.10 considered meaningful. Results are presented from analyses performed at three levels of adjustment: an unadjusted analysis, an analysis restricted to fathers with a diagnosis of type 2 diabetes mellitus to control for the potential effects of the underlying illness or associated factors, and an analysis restricted to fathers with a diagnosis of type 2 diabetes mellitus, utilising overlap propensity score weighting to further control for proxies of diabetes severity and other potential confounders. The propensity score was calculated using a logistic regression model that incorporated all variables listed in the covariates section, followed by the calculation of overlap weights. This method upweighted individuals in the overlapping portion of the propensity score distribution (see supplementary eFigure 3) by assigning each a weight reflective of the probability of being assigned to the opposite group.3334Generalised linear models were used to estimate relative risks with 95% confidence intervals (CIs) for any congenital malformation, employing a robust variance estimator to account for the weighting and data clustering because of multiple offspring for each father. Relative risk estimates for the two countries were pooled using random effects meta-analytical models, with heterogeneity assessed by the I2statistic. Random effects were applied in the meta-analysis owing to the potential heterogeneity in patient characteristics across countries, even though the same protocol was used. This approach assumes a normal distribution of effects, weighted by both within study and between study variances, resulting in a conservative estimate with a wider CI. In the Norwegian cohort, we dealt with missing data using multiple imputation by chained equations with 10 replications, based on the assumption that the data were missing at random (see supplementary appendix 2).3536For the Taiwanese cohort, we used healthcare utilisation databases, which contained complete information on all recorded diagnoses, as well as all procedures and drugs administered. No adjustments were made for multiple testing, so the CI widths should not be interpreted as substitutes for hypothesis testing. Data management and statistical analyses were performed using SAS version 9.4 (SAS Institute, Cary, NC) and Stata/MP version 17.0 (StataCorp).
To control for shared genetic factors, we compared siblings in both cohorts. Sibling comparisons, by design, can account for time fixed confounders shared by siblings.37All sibling pairs of each father were identified in the study, and only those pairs discordant for both metformin use and outcomes contributed to the estimated within pair association. Conditional logistic regression models, adjusted for calendar year of the offspring’s birth and both paternal and maternal characteristics—including age, type 2 diabetes mellitus, proxies for diabetes severity, chronic comorbidities, and drug use—were employed for within family comparisons.
We performed several sensitivity analyses to test the robustness of the findings. Firstly, the definition of type 2 diabetes mellitus was revised to include both diagnosis codes and antidiabetic drug use. Secondly, to reduce the possibility of misclassifying metformin use, we revised the definition of use to require at least one filled prescription for metformin during the period of sperm development. Thirdly, to minimise the effect of maternal risk factors, we excluded mothers who used diabetes drugs, had a diabetes diagnosis, had a hypertension diagnosis, or were prescribed cardiovascular drugs during the six months before the start of pregnancy to the end of the first trimester. Fourthly, we restricted the cohort to both fathers younger than 45 years and mothers younger than 35 years to diminish the effect of advancing age on risk of congenital malformations.3839Fifthly, as the cohorts were initially limited to pregnancies resulting in live births, we included stillbirths, spontaneous abortions, and terminations after pregnancy week 12 in the Norwegian cohort. Sixthly, we used methods that have previously been adopted to evaluate the potential impact of varying frequencies of stillbirths, miscarriages, or terminations between exposure and non-exposure to metformin (see supplementary appendix 3).4041Seventhly, to address potential outcome misclassification in the Taiwanese cohort, we applied a quantitative bias analysis using a probabilistic method to simulate outcome misclassification (see supplementary appendix 4).4142Furthermore, we used probabilistic bias analyses to account for unmeasured paternal overweight or obesity in both cohorts (see supplementary appendix 5).43Finally, we conducted prespecified exploratory analyses to assess the association between sulfonylurea, insulin, and dipeptidyl peptidase-4 inhibitors and risk of any malformations.
Patient and public involvement
Although we support the involvement of patients and the public, no funding was available for such undertakings in this project. As a result, no patients or members of the public were involved in the design, conduct, reporting, or dissemination plans of this research.
Results
Cohort characteristics
We identified 619 389 offspring with paternal data in the Norwegian cohort during 2010-21 and 2 563 812 in the Taiwanese cohort during 2004-18 (see supplementary eFigure 4). Paternal linkage to pregnancies was possible in 91.0% of the Norwegian cohort and 95.6% of the Taiwanese cohort. Among these, fathers of 2075 (0.3%) offspring in Norway and 15 276 (0.6%) offspring in Taiwan used metformin during the period of sperm development. Compared with fathers who did not use metformin, those who used metformin were older and had a higher prevalence of diabetes and other chronic illnesses, notably hypertension, hyperlipidaemia, and mental illness (fig 1, supplementary eTable 5). These fathers were also more likely to use other types of glucose lowering drugs, cardiovascular drugs, and psychotropic agents. Their female partners were also more likely to be older and to have conditions such as diabetes and obesity (fig 2).
In the cohort restricted to fathers with type 2 diabetes mellitus, baseline characteristics were generally balanced between those who used and did not use metformin. However, fathers who used metformin were still observed to be older, have more severe diabetes, and be more likely to use cardiovascular drugs (see supplementary eTable 6). After applying overlap weights, a perfect balance of mean values of covariates included in the propensity score was achieved between fathers who used and did not use metformin in both cohorts (see supplementary eTable 7).
Risk of congenital malformations
In the Norwegian cohort, congenital malformations were observed in 24 041 (3.9%) offspring of fathers who did not use metformin during the period of sperm development, compared with 104 (5.0%) offspring of fathers who used metformin (fig 3). Paternal metformin use was associated with an increased risk of congenital malformations (unadjusted relative risk 1.29, 95% CI 1.07 to 1.55). Similarly, in the Taiwanese cohort, congenital malformations were diagnosed in 79 278 (3.1%) offspring of fathers who did not use metformin, compared with 512 (3.4%) offspring of fathers who used metformin, producing a slightly increased unadjusted relative risk of 1.08 (0.99 to 1.17).
When the cohort was restricted to fathers with type 2 diabetes mellitus, the risk estimates in the Norwegian cohort were not noticeably attenuated, but precision decreased (restricted relative risk 1.20, 95% CI 0.94 to 1.53). In the Taiwanese cohort, the restricted relative risk of 0.93 (0.80 to 1.07) indicated no association between paternal metformin use and congenital malformations when confounding by indication was considered.
After full adjustment for all measured confounders using overlap propensity score weighting in the restricted analysis, the risk estimate shifted towards the null in the Norwegian cohort (weighted relative risk 0.98, 95% CI 0.72 to 1.33). For the Taiwanese cohort, the adjusted risk estimate remained similar to that obtained after restricting the cohort to fathers with type 2 diabetes mellitus (0.87, 0.74 to 1.02). Pooling the adjusted estimates for paternal metformin use from both the Norwegian and the Taiwanese cohorts resulted in a weighted relative risk of 0.89 (0.77 to 1.03; I2=0.0%). When exploring the risk of organ specific malformations associated with paternal metformin use, no increased associations were found (fig 4), and no dose effects were observed across the various types of malformations (see supplementary eTable 8).
Sibling matched comparison
In the sibling matched comparison, 76 sibling pairs were included from the Norwegian cohort and 581 pairs from the Taiwanese cohort (table 1). After adjusting for potential confounders shared between siblings, paternal metformin use during the period of sperm development was not associated with risk of congenital malformations in offspring in either the Norwegian cohort (adjusted odds ratio 0.83, 0.43 to 1.59) or the Taiwanese cohort (0.84, 0.68 to 1.04).
Sensitivity and exploratory analyses
Consistent results were observed in both the Norwegian cohort and the Taiwanese cohort when modifying the definition of type 2 diabetes mellitus, redefining metformin use as at least one prescription dispensed during the period of sperm development, excluding mothers with risk factors, restricting the analysis to fathers younger than 45 years or mothers younger than 35 years, and including stillbirths, miscarriages, or terminations in the Norwegian cohort (fig 5). Under the strongest assumptions tested for potential selection bias owing to restriction to live births, the relative risk estimates remained below 1.30 for any congenital malformations in both the Norwegian cohort and the Taiwanese cohort (see supplementary eFigure 5). Additionally, using probabilistic bias analyses to examine potential outcome misclassification in the Taiwanese cohort (see supplementary eTable 9) and to address unmeasured paternal overweight and obesity in both cohorts (see supplementary eTable 10) yielded similar findings to the main analysis.
In the exploratory analyses, we evaluated the association between paternal use of sulfonylurea, insulin, or dipeptidyl peptidase-4 inhibitor before pregnancy and risk of any congenital malformations in the type 2 diabetes mellitus restricted cohort (see supplementary eTable 11). We found no substantial increase in risk associated with the use of any of these drugs.
Discussion
In this population based, cross national study including about 3.2 million pregnancies with paternal data from Norway and Taiwan, after adjustment for type 2 diabetes mellitus and other potential confounding factors, we found no increase in the risk of any congenital malformations among infants born to fathers who used metformin during the period of sperm development. Additionally, no associations with organ specific malformations were observed, including genital malformations, which were previously reported to be associated with metformin use.18Furthermore, findings were consistent across sibling matched comparisons, accounting for time invariant confounders such as genetic or familial environmental factors, as well as in several prespecified sensitivity analyses.
Comparison with other studies
A previous study conducted in Denmark observed an association between paternal metformin use before pregnancy and congenital malformations.18Additionally, a reanalysis of Danish data revealed a 1.4-fold higher risk of major congenital malformations linked to paternal metformin use.19Our results, however, do not support those findings and point to confounding rather than a causal association. In general, paternal use of medicines can influence the risk of congenital malformations in offspring in two ways: firstly, through a direct effect on sperm DNA, and, secondly, indirectly, through the transmission of agents in the seminal fluid, leading to maternal exposure. Given that metformin has shown no recombinogenic or mutagenic activity at pharmacological concentrations,44the first mechanistic pathway is unlikely, and the drug use window selected in the present study does not allow for assessment of the second pathway. Although the mechanism behind the increased risk of malformations in offspring from paternal metformin use has been proposed to be due to an epigenetic mechanism of action, our findings do not support a biological causal mechanism in humans.
In contrast with analyses in the Danish study, our adjusted analyses restricted the cohort to fathers with type 2 diabetes mellitus to mitigate potential confounding by indication, associated conditions, and lifestyle behaviours. Without this restriction, we observed notable differences in age, prevalence of diabetes, other chronic comorbidities, and drug use between the exposed and non-exposed groups, suggesting lack of exchangeability between the two groups. Notably, advancing father’s age,45as well as paternal metabolic syndrome related diseases,46are well known risk factors for congenital malformations. Although the Danish study reported a high E-value of 2.15,18unobserved confounding in their study, including type 2 diabetes mellitus related conditions, obesity, and codrug use, may explain away the observed association.
A potential concern raised in the previous Danish study was the confounding effect of blood glucose levels on the increased risk of malformations associated with paternal metformin use.18People treated with glucose lowering drugs generally have higher average glucose levels compared with healthy individuals before treatment. Nevertheless, our analysis suggests that the direct effect of mean glucose levels is unlikely to significantly confound these findings. In the Norwegian cohort, when we restricted our analysis to fathers with type 2 diabetes mellitus, we found no substantial shift in the risk estimate towards the null. This finding aligns with the results from earlier studies.184748Specifically, the Danish study found no significant associations in the group that used insulin (adjusted odds ratio 0.98, 95% CI 0.85 to 1.14).18This may highlight the minimal impact of average glucose levels on the risk of malformation. Therefore, the differences between our findings and those of the Danish study may be due to the more extensive control of confounders, including severity of diabetes, metabolic syndrome associated conditions, chronic diseases, codrug use, and maternal characteristics, other than solely blood glucose levels.
In our study, we also observed that offspring of fathers who used metformin more often tend to have spouses with type 2 diabetes mellitus and obesity. Previous research has similarly reported spousal concordance for diabetes,49often attributed to shared lifestyle and environmental factors.50Maternal pre-existing diabetes is associated with several subtypes of congenital anomalies in newborns.51Furthermore, both the overall risk of major congenital malformations and the risk of malformations in specific organ groups increase progressively with severity of maternal overweight and obesity.52Hence, maternal characteristics may also serve as important confounders in studies assessing the safety of paternal drug use on outcomes in their offspring. To address this, we adjusted for a broad range of paternal and maternal potential confounding variables using propensity scores. While this method cannot eliminate all potential confounding, it has resulted in the unexposed and exposed groups with nearly identical measured parental characteristics and has tended to further lower the risk estimates.
An increased risk of genital birth defects in male offspring after paternal metformin use reported in the Danish study18was not observed in our study, which instead showed a reduction in the risk. Hypospadias, the most prevalent major male genital malformation, has a global incidence of 20.9 per 10 000 births.53A recent study indicated a potential link between paternal health, particularly components of metabolic syndrome, and the occurrence of hypospadias in sons.54Notably, between 70% and 80% of people with type 2 diabetes mellitus are estimated to have metabolic syndrome.55In addition to restricting the analyses to fathers with a diagnosis of type 2 diabetes mellitus, we adjusted for metabolic syndrome associated conditions such as hypertension, hyperlipidaemia, and proxies of the severity of diabetes. This comprehensive consideration of metabolic syndrome related confounders offers a possible explanation for the discrepancies between our findings and those of the Danish study regarding male genital malformations.18It is therefore reassuring that metformin was not associated with an altered risk of genital malformations. However, since only the Taiwanese cohort in the present study was available to assess genital malformations, future research with larger sample sizes is warranted to verify these results.
The crude estimates in both the Norwegian cohort (relative risk 1.29, 95% CI 1.07 to 1.55) and the Taiwanese cohort (1.08, 0.99 to 1.17) were lower than the adjusted association reported in the Danish study (adjusted odds ratio 1.40, 95% CI 1.08 to 1.82).18One potential concern is misclassification of either drug use or outcome, as both differential and non-differential misclassification may bias results towards the null.56Similar to the Danish study, however, the Norwegian cohort utilised high quality registry data, and therefore the likelihood of misclassification is considered to be low. We also conducted several sensitivity analyses in both the Norwegian cohort and the Taiwanese cohort to quantify the impact of misclassification, with no substantial alteration to the main findings. On the other hand, the differences in crude relative risks between Norway and Taiwan could be attributed to heterogeneity in patient characteristics, such as body mass index. In probabilistic analyses that accounted for paternal overweight and obesity, we found that paternal overweight and obesity had a greater impact on the Norwegian cohort compared with the Taiwanese cohort. Nonetheless, after full adjustments, we achieved homogeneous findings, which were then pooled using a random effects meta-analysis.
Limitations of this study
The high paternal linkage rate of more than 90% in both cohorts makes these data sources extremely valuable for studying the safety of paternal drug use, and the findings from the present study could inform future drug safety communications about the use of metformin among men before pregnancy. Our study is, however, also subject to certain limitations. Metformin use was determined based on filled prescriptions, and we were unable to confirm actual drug intake by participants, which may lead to misclassification of drug use. To enhance reliability, we revised the definition for metformin use to require at least one filled prescription during the period of sperm development in our sensitivity analyses, which gave similar results. Another potential limitation is residual and unmeasured confounding, as with all observational studies. Although we accounted for a broad range of confounders using multiple statistical methods, including sibling matched comparisons, we cannot completely eliminate the impact of residual and unmeasured confounders, such as lifestyle factors, dietary habits, body mass index, and genetic factors. Moreover, our databases do not include laboratory data, such as haemoglobin A1clevels. As a result, we were constrained to using diagnosis codes to identify type 2 diabetes mellitus and had limited ability to account for glycaemic control. However, we implemented comprehensive measures to deal with this issue, incorporating the adaptive diabetes complications severity index and the use of glucose lowering agents within the propensity score to align diabetes severity across the study comparators to the greatest extent possible. Additionally, the present study excluded women taking teratogenic drugs from the analysis. Many pregnant women may choose to terminate a pregnancy using teratogenic drugs; however, owing to data limitations, we are unable to explore the impact of this situation on our study. Furthermore, despite utilising large national cohorts, estimating the risks for most organ specific malformations remains challenging. Future well designed studies are needed to more accurately assess the risk of paternal metformin use, diabetes, and obesity on specific malformations.
Clinical implications
Metformin is a cornerstone in the drug treatment of type 2 diabetes mellitus. Recently, concerns have emerged about the risk of congenital malformations in offspring associated with paternal use of metformin,18primarily owing to its potential to cause epigenetic alterations to the sperm DNA.17This concern raises questions about the clinical use of metformin for men of reproductive age considering fatherhood. The findings of our cross national study, which includes data from Norway and Taiwan, suggest that metformin keeps its current clinical profile as an initial oral agent for managing glucose levels in men with type 2 diabetes mellitus who are planning a family. The study also underscores the necessity of considering factors beyond the drug itself, such as the underlying indication, related health conditions, and maternal risk factors, when evaluating the association between paternal drug use and risk of congenital malformations in offspring.
Conclusion
Our results indicate that paternal use of metformin during the period of sperm development is not associated with an increased risk of any congenital malformations in offspring. Additionally, we found no notable increases in risk for any specific organ malformations, including genital malformations. These results provide reassurance and can assist clinicians in making informed treatment decisions when selecting metformin in the treatment of type 2 diabetes mellitus among men who are planning a family.
A recent study reported an association between paternal metformin use preconception and an increased risk of major congenital malformations, particularly genital, in male infants
Type 2 diabetes mellitus in fathers, along with related health conditions and maternal factors, is known to be associated with congenital malformations in offspring, potentially providing alternative explanations for the associations
This population based, cross national cohort study found no significant association between paternal metformin use during the period of sperm development and congenital malformations in offspring
When evaluating the association between paternal drug use and risk of congenital malformations in offspring it is important to consider factors beyond the drug itself
","Objective: To evaluate the association between paternal metformin use and risk of congenital malformations in offspring.
Design: Population based, cross national cohort study.
Setting: Norway and Taiwan.
Participants: 619 389 offspring with paternal data during the period of sperm development (three months before pregnancy) in the Norwegian cohort during 2010-21 and 2 563 812 in the Taiwanese cohort during 2004-18.
Main outcome measures: The primary outcome was any congenital malformation, and the secondary outcome was organ specific malformations, classified according to the European surveillance of congenital anomalies guidelines. Relative risks were estimated with an unadjusted analysis and with analyses restricted to the cohort of men with type 2 diabetes mellitus and those using overlap propensity score weighting to control for severity of diabetes and other potential confounders. Sibling matched comparisons were conducted to account for genetic and lifestyle factors. Relative risk estimates for Norwegian and Taiwanese data were pooled using a random effects meta-analytical approach.
Results: Paternal data on metformin use during the period of sperm development was available for 2075 (0.3%) offspring in Norway and 15 276 (0.6%) offspring in Taiwan. Among these offspring, 104 (5.0%) in Norway and 512 (3.4%) in Taiwan had congenital malformations. Increased risks of any congenital malformation associated with paternal metformin use were observed in the unadjusted analysis and attenuated with increasing control of confounding. The relative risks of any malformations with paternal metformin use were 1.29 (95% confidence interval 1.07 to 1.55) in Norway and 1.08 (0.99 to 1.17) in Taiwan in the unadjusted analysis and 1.20 (0.94 to 1.53) and 0.93 (0.80 to 1.07), respectively, in the analysis restricted to fathers with type 2 diabetes mellitus. In the overlap propensity score weighting analysis restricted to fathers with type 2 diabetes mellitus, the relative risks were 0.98 (0.72 to 1.33) in Norway and 0.87 (0.74 to 1.02) in Taiwan, resulting in a pooled estimate of 0.89 (0.77 to 1.03). No associations were observed between paternal metformin use and any organ specific malformations. These findings were consistent in sibling matched comparisons and sensitivity analyses.
Conclusions: The findings suggest that paternal use of metformin during the period of sperm development is not associated with congenital malformations in offspring, including organ specific malformations. Metformin can therefore continue to be considered a suitable initial oral agent for managing glucose levels in men with type 2 diabetes mellitus who plan on having children.
"
Intense simplified strategy for newly diagnosed type 2 diabetes in patients with severe hyperglycaemia,"Introduction
As estimated by the International Diabetes Federation, the global prevalence of diabetes has reached 537 million.1Exposure to hyperglycaemia, even in the first year after diagnosis, is a primary risk factor for chronic complications of diabetes and even death. These effects can persist for decades and are known as the legacy effect.234As discovered recently by the 24 year follow-up of the UK Prospective Diabetes Study, early intensive glycaemic control provides a near lifelong reduction in the risk of death and of microvascular and macrovascular complications.5However, despite various glucose lowering treatments, more than half of the patients had inadequate glycaemic control,6indicating that traditional treatment strategies that escalate treatment intensity in a stepwise manner to overcome deteriorating hyperglycaemia are unable to delay β cell failure and progression of diabetes.
Significant hyperglycaemia is common in patients with newly diagnosed type 2 diabetes mellitus. Owing to the limited efficacy of monotherapy, the American Diabetes Association and the European Association for the Study of Diabetes recommend that combination therapy should be used when the glycated haemoglobin A1c(HbA1c) levels exceed treatment targets by 1.5% (≥8.5% for most patients), with insulin therapy needed if HbA1cexceeds 10% (evidence level E; expert consensus or clinical experience).7Nevertheless, standardised treatment pathways for these patients remain unclear.
Emerging evidence has shown that the progression of type 2 diabetes is not irreversible, especially in the early stages.89Short term intensive insulin therapy (SIIT) has shown immediate and substantial benefits in early type 2 diabetes. Two to three weeks of SIIT significantly improves β cell function and insulin sensitivity by eliminating glucotoxicity, inducing remission of diabetes for longer than a year in more than 50% of patients with newly diagnosed type 2 diabetes.10111213In most studies of SIIT, benefits were observed in patients with significant hyperglycaemia (mean HbA1c≥10%) and mild overweight (mean body mass index approximately 25).10121314Nevertheless, the remission rate declined over time, from 70% immediately after SIIT to roughly 50% at the end of the first year and around 40% at the end of the second year.15This provides a rationale for investigating whether simplified oral regimens subsequent to SIIT could help to maintain the good glycaemic control induced by SIIT by persistently avoiding the detrimental effects of glucotoxicity and circumventing the disadvantages of intricate glucose lowering regimens.
In this context, we proposed an intense simplified strategy, which uses SIIT as the initial therapy followed by simplified oral anti-diabetes regimens as maintenance therapy. We hypothesised that this strategy could improve glycaemic control and preserve β cell function in patients with newly diagnosed type 2 diabetes and severe hyperglycaemia. We thus conducted this nationwide, multicentre, randomised controlled trial to evaluate the effect of different subsequent regimens (metformin, linagliptin, or their combination) after SIIT on glycaemic outcomes over 48 weeks, with lifestyle modification alone as the control.
Methods
Study design and participants
This is an open label, nationwide, multicentre, randomised controlled trial conducted across 15 centres in China. The participants, whose diagnosis of type 2 diabetes was newly made according to the World Health Organization’s criteria in the research centres, had to be aged between 20 and 70 years, be naïve to antihyperglycaemic drugs and had not previously received any systematic diabetes related medical advice and interventions, had body mass index of 22.0-35.0, had fasting plasma glucose between 7.0 mmol/L and 16.7 mmol/L, and had HbA1c≥8.5% at screening. Disease duration was estimated on the basis of the onset of clinical symptoms or the period of elevated blood glucose reported by the participants. Major exclusion criteria included the presence of glutamic acid decarboxylase antibodies, use of drugs that affect glucose metabolism, acute complications of diabetes (for example, diabetic ketoacidosis or hyperosmolar hyperglycaemic state), diabetic proliferative retinopathy, albumin excretion >300 mg/day or cardiovascular events within the preceding six months, alanine transferase ≥2.5 times the upper limit of normal, an estimated glomerular filtration rate of ≤60 mL/min, or any significant systemic diseases as determined by physical examination or medical history. The trial followed the principles of the Declaration of Helsinki and Good Clinical Practice guidelines of the International Conference for Harmonization. All participants provided signed informed consent.
Randomisation and masking
After enrolment, eligible patients were randomised 1:1:1:1 into four subsequent treatment arms, stratified by the participating centres. A statistician not involved in the study encoded and prepared random envelopes, which were then distributed to each centre and stored by the file administrator. After obtaining informed consent from the participant, the file administrator retrieved and opened the envelope in the presence of the investigators. Afterwards, the assignment was verified and archived in the participant's file for future reference. Both the researchers and patients were unaware of the allocation before randomisation but were informed afterwards.
Procedures
At baseline, we assessed anthropometric parameters, took standard biochemistry measurements, and screened for complications of diabetes according to the current guideline16(urinary albumin-to-creatinine ratio and serum creatinine concentrations for diabetic kidney disease, retinal photography for diabetic retinopathy, and physical examination and 10 g monofilament examination for diabetic neuropathy). After a two to three day run-in period, participants were admitted to the hospital and received SIIT, as previously described.13Briefly, SIIT was administered using continuous subcutaneous insulin infusion (MiniMed Paradigm 722 insulin pump) to deliver insulin aspart or insulin lispro with an initial total daily insulin dose of 0.5 IU/kg, split 50:50 for basal insulin and pre-meal boluses. Insulin doses were titrated according to capillary blood glucose concentrations, targeting fasting/pre-meal blood glucose of <6.1 mmol/L and two hour postprandial blood glucose of <8.0 mmol/L. On achievement of these targets, insulin infusion was sustained for two weeks. Throughout the hospital stay, participants were supplied with daily diets that followed the current nutritional guidelines and were encouraged to walk or jog for 30-60 minutes after each meal. A nutritionist designed the diet, ensuring that carbohydrates, proteins, and fat accounted for 50-60%, 10-15%, and 20-30%, respectively, of total energy intake. Patients consumed three meals a day, with breakfast, lunch, and dinner accounting for 20%, 40%, and 40% of the total caloric intake, respectively. From the beginning of the hospital stay, participants were provided with guidance on exercise, including recommended speed and duration, and written management tips on a card. Participants walked or jogged in the ward hallway. Healthcare staff monitored and supervised the exercise during daily rounds by discussing participants’ activities, providing further guidance, and recording exercise adherence.
After discontinuation of insulin, baseline measurements were repeated the next morning (at least 15 hours after insulin was stopped). Afterwards, participants received one of the following subsequent therapies according to their assignment: participants randomised to the linagliptin plus metformin group received both linagliptin (5 mg/day; Trajenta, Boehringer-Ingelheim, Germany) and metformin (1000 mg/day; Glucophage, Merck Serono, Switzerland) for 48 weeks; participants in the linagliptin group were treated with linagliptin (5 mg/day) for 48 weeks; participants in the metformin group were treated with metformin (1000 mg/day) for 48 weeks; and participants assigned to the control group received no anti-diabetes medicine after SIIT.
Participants were encouraged to adhere to lifestyle changes and met with study staff every 12 weeks for repeat measurement of the parameters assessed at baseline. During each follow-up visit, we assessed adherence to drug treatment by calculating the number of returned tablets; researchers provided lifestyle change consultations consistent with routine diabetes management practices.16The consultations included a review of the participants’ dietary habits, exercise duration and intensity, weight changes, and glycaemic control, as well as guidance to improve their nutritional habits and maintain at least 150 minutes of moderate intensity physical activity per week. Oral anti-diabetes drugs were held for at least 48 hours before visits at which blood sample were taken. In addition, participants were advised to measure fasting capillary blood glucose at least twice a week and document these values in their patient diaries. Participants were asked to contact study staff if fasting capillary blood glucose exceeded 9.0 mmol/L. If HbA1cexceeded 8.0% after week 12 or fasting blood glucose was above 9.0 mmol/L continuously for two weeks as confirmed by venous blood glucose, the participant was returned to standard clinical care according to the Chinese diabetes guideline.16
Biochemical analyses
Plasma samples were centrally processed at the Kingmed Center for Clinical Laboratory (Guangzhou). HbA1cwas measured by high performance liquid chromatography (Bio-Rad, Hercules, CA, USA). Insulin concentrations were determined using chemiluminescent immunoassay (Roche Diagnostics GmbH, Germany). Lipid profiles, transaminase, and serum creatinine were measured at designated visits. A mixed meal tolerance test was accomplished by participants consuming 100 g of commercial instant noodles, with blood samples collected at fasting and 30, 60, and 120 minutes after ingestion of the mixed meal.
We measured insulin sensitivity by using the homoeostasis model assessment (HOMA-IR) and the Matsuda index. We assessed β cell function with the homoeostasis model assessment (HOMA-β) and the insulin secretion-sensitivity index-2 (ISSI-2), which we calculated as the product of the insulin-to-glucose area under the curve ratio (AUCins/AUCgluc) and the Matsuda index.1718
Outcomes
The study’s primary outcome was optimal glycaemic control, predefined as HbA1c<7.0% at 48 weeks after SIIT. Secondary outcomes included the percentage of participants achieving HbA1c<6.5%, changes in HbA1c, fasting and two hour postprandial plasma glucose in the mixed meal tolerance test, β cell function indices, and insulin sensitivity indices from baseline. We used capillary blood glucose recorded by patients during follow-up only for monitoring hyperglycaemia relapse and did not include it in the outcome analysis. Frequencies of adverse events were recorded. We classified episodes of hypoglycaemia as level 1 (blood glucose <3.9 mmol/L but ≥3.0 mmol/L), level 2 (blood glucose <3.0 mmol/L), and level 3 (altered mental and/or physical status requiring assistance for correction of hypoglycaemia).
Statistical analysis
Sample size estimation suggested that 89 participants per group could provide 80% power to detect a 20% higher probability of achieving HbA1c<7.0% in the linagliptin plus metformin group versus the control group at week 48. This calculation assumed that 75% of participants in the intervention group and 55% in the control group would achieve this endpoint with a type I error of 2.5% in the one sided test. Anticipating a 15% dropout rate, we needed 103 per arm (412 in total).
We did analyses by following the intention-to-treat principle. We did multiple imputations within each group for the missing endpoints (HbA1c<7.0% and <6.5% at week 48) by using a logistic model with group, gender, age, baseline body mass index, and HbA1clevels as predictors. We imputed five complete datasets and combined the estimates from each imputed dataset into one overall estimate in SPSS. We did per protocol analyses, as well as sensitivity analyses treating missing 48 week HbA1cdata as not achieving the primary endpoint, to assess the robustness of the results.
We expressed continuous variables as mean (standard deviation) for normally distributed data and compared them via one way analysis of variance or as median (interquartile range) for skewed data and compared them via Kruskal-Wallis H tests. For comparisons of trial outcomes among treatment groups, we applied generalised linear models for continuous variables and used the χ2test or Fisher’s exact test for categorical variables. We evaluated longitudinal changes over time in outcome variables by using a generalised estimating equation model, with goodness of fit optimised using the quasi-likelihood under the independence model criterion. We compared Kaplan-Meier curves with log-rank tests and used a logistic regression model to calculate the risk ratio in the primary endpoint between the treatment groups. We considered P values of <0.05 to be statistically significant. To reduce the risk of type I errors in multiple comparisons between the treatment and control groups, we set the significance level at P<0.0167 in pairwise comparison. We used SPSS version 19.0 and GraphPad Prism version 9.0 for statistical procedures.
Patient and public involvement
Patients or members of the public were not directly involved in the design, conduct, reporting, or analysis of the trial. This trial was started before patient and public involvement gained widespread adoption in research practice, and the necessary funding to facilitate such involvement was not allocated. Nonetheless, the study protocol and treatment strategies underwent extensive consultation with clinical endocrinologists and were rigorously reviewed by the ethics board. Furthermore, clinical endocrinologists will play a key role in communicating the results to patients, the public, and healthcare professionals to encourage the translation of research into practice. Once our paper is published, we plan to publish a press release. We further plan to present the study outcomes at annual meetings of endocrinology societies in China and internationally, hoping the strategy will be adopted by more centres in China and worldwide, which will require further input from the public, including patients.
Results
Baseline characteristics and SIIT
A total of 464 patients were screened between December 2017 and December 2020, of which 412 were eligible and were subsequently randomised and treated with SIIT. Thirty nine participants who did not return for any follow-up after SIIT were excluded from further efficacy analysis. Among the remaining 373 patients (full analysis set), 321 completed the 48 week follow-up (per protocol set), with 12 from the linagliptin plus metformin group, 13 from the linagliptin group, 10 from the metformin group, and 17 from the control group dropping out during follow-up (fig 1).
The baseline characteristics were well balanced across the four groups (intention-to-treat population, n=412;table 1). The participants had an average age of 46.8 (standard deviation (SD) 11.2) years, a mean body mass index of 25.8 (2.9), and an estimated duration of diabetes of 1.0 (interquartile range 0.5-5.0) months. Among the randomised participants, 75% (311/412) had a disease duration of less than six months and 25% (101/412) had a duration of six months or more (up to three years), including 16% (64/412) with a duration of more than one year. All participants had significantly elevated HbA1c(11.0% (SD 1.9%)), fasting plasma glucose (11.4 (2.9) mmol/L), and two hour postprandial glucose (20.5 (4.0) mmol/L) concentrations, with 76% (312/412) of the participants showing symptoms of hyperglycaemia (that is, polyuria, polydipsia, or unexpected weight loss). The initial insulin dose was 37.6 (SD 25.4) IU/day (0.50 (0.35) IU/kg/day). During SIIT, near-normoglycaemia was achieved in 3.8 (SD 2.3) days, with a mean maximum daily insulin dose of 53.4 (18.7) IU/day (0.74 (0.22) IU/kg/day). Afterwards, the insulin dose gradually decreased to 32.0 (SD 15.0) IU/day (0.45 (0.21) IU/kg/day) at the end of SIIT, with no significant differences among the treatment groups. As anticipated, in the overall cohort, fasting glucose decreased from 11.4 (SD 2.9) mmol/L to 5.9 (1.1) mmol/L, two hour postprandial glucose decreased from 20.5 (4.0) mmol/L to 13.8 (3.0) mmol/L, and HbA1cdecreased from 11.0% (1.9%) to 9.2% (1.3%) after SIIT (all P<0.001). We observed improvements in lipid profiles and recovery in both β cell function and insulin sensitivity, with no significant differences among the groups.
HbA1ccontrol
The interaction between linagliptin and metformin treatment was not significant (P=0.63). We therefore assessed the treatment effects of different subsequent regimens independently. In the full analysis set, we generated five datasets by using multiple imputation and pooled them for HbA1cendpoint analyses (supplementary tables S3 and S4). Overall, 71% (266/373) of participants achieved the primary endpoint of HbA1c<7.0% at week 48. Participants with a disease duration of less than six months tended to have a higher likelihood of achieving this HbA1ctarget (supplementary table S9). The proportions of participants achieving HbA1c<7.0% at week 48 were 80% (78/97) in the linagliptin plus metformin group, 72% (63/88) in the linagliptin group, 73% (69/95) in the metformin group, and 60% (56/93) in the control group (χ2test: overall P=0.02). After adjustment for multiple comparisons, the difference between the linagliptin plus metformin group and the control group reached statistical significance (P=0.003), whereas the linagliptin group versus the control group and the metformin group versus the control group did not (P=0.12 and P=0.09, respectively) (fig 2). Additionally, 70% (68/97), 68% (60/88), and 68% (65/95) of participants in the linagliptin plus metformin, linagliptin, and metformin groups achieved HbA1c<6.5%, compared with 48% (45/93) in the control group (χ2test: P=0.005 overall; P=0.005 for linagliptin plus metformin versus control; P=0.01 for linagliptin versus control; P=0.008 for metformin versus control; all were significant after adjustment for multiple comparisons) (fig 2). Logistic analysis, with trial centres set as a random effect, indicated that compared with the control group, the linagliptin plus metformin group (odds ratio 2.78, 95% confidence interval (CI) 1.37 to 5.65; P=0.005) was more likely to achieve the HbA1c<7.0% endpoint after adjustment for age, gender, body mass index, and baseline HbA1c, whereas the relative effects in the linagliptin group (1.70, 0.84 to 3.42; P=0.17) and the metformin group (1.77, 0.91 to 3.42; P=0.07) were not significant. We found similar results in the sensitivity analysis and the per protocol analysis (supplementary figure S1). In the per protocol analysis, the proportions of participants achieving HbA1c<7% at 48 weeks were 86% (73/85) in the linagliptin plus metformin group, 79% (59/75) in the linagliptin group, 76% (65/85) in the metformin group, and 63% (48/76) in the control group (χ2test: overall P=0.008). The difference was significant between the linagliptin plus metformin group and the control group (P<0.001) but not significant for the linagliptin group or metformin group versus the control group after adjustment for multiple comparisons (P=0.05 and P=0.08, respectively).
As shown infigure 3, the HbA1clevel was significantly lower in the linagliptin plus metformin group across the follow-up period. At week 48, the HbA1clevels were 6.2% (95% CI 6.0% to 6.4%) in the linagliptin plus metformin group, 6.5% (6.3% to 6.7%) in the linagliptin group, 6.4% (6.2% to 6.6%) in the metformin group, and 6.7% (6.5% to 6.9%) in the control group, after adjustment for age, gender, body mass index, and baseline HbA1c(P=0.004 overall; P<0.001 for linagliptin plus metformin versus control; P=0.02 for linagliptin plus metformin versus linagliptin; P=0.05 for metformin versus control group) (table 2). The mean change in HbA1cfrom baseline was −4.8% (95% CI −5.0% to −4.6%) in the linagliptin plus metformin group, −4.4% (−4.6% to −4.2%) in the linagliptin group, −4.5% (−4.8% to −4.3%) in the metformin group, and −4.2% (−4.5% to −4.0%) in the control group (overall P=0.003) (fig 4). We noted significant differences between the linagliptin plus metformin group and the control group (P<0.001) and between the metformin group and the control group (adjusted P=0.045) (table 2).
We did a survival analysis to explore the time to loss of optimal glycaemic control, defined as HbA1c≥7.0% since week 12 or fasting plasma glucose ≥7.0 mmol/L between the cessation of SIIT and week 12. The linagliptin plus metformin group had a significantly higher probability of maintaining optimal glycaemic control than the control group (log-rank test: P=0.001). After adjustment for multiple comparisons, the comparison between the linagliptin plus metformin group and the control group did not reach statistical significance (log-rank test: P=0.09 and P=0.04, respectively) (fig 5).
Other outcomes
The changes in other outcomes are summarised infigure 6andtable 2. As shown infig 6(top left), the subsequent treatment groups maintained stable fasting plasma glucose concentrations throughout the follow-up period, particularly in the linagliptin plus metformin group. At week 48, the linagliptin plus metformin group had the lowest fasting plasma glucose concentrations and the greatest reduction in fasting plasma glucose from baseline. The mean two hour postprandial glucose concentrations did not differ among treatment groups (fig 6, top right). The linagliptin plus metformin group had better preservation of ISSI-2 than did other groups throughout the follow-up period (generalised estimating equation model analyses: P=0.04 overall; P=0.001 for the linagliptin plus metformin group versus the control group) (fig 6, second graph on left). At week 48, mean ISSI-2 values were 345.7 (95% CI 310.4 to 379.0), 287.8 (252.5 to 323.0), 280.1 (241.7 to 318.4), and 265.4 (231.4 to 299.5) in the linagliptin plus metformin, linagliptin, metformin, and control groups, respectively (P=0.009 for linagliptin plus metformin versus control) (table 2). Similarly, the three subsequent treatment groups showed significantly higher HOMA-β values than did the control group throughout the follow-up until the end of the study (fig 6, middle right). Insulin sensitivity variables—HOMA-IR and Matsuda index—showed similar patterns of change over time (fig 6, third left and bottom right). In generalised estimating equation model analyses, the between group differences in HOMA-IR and Matsuda index were both significant (P=0.02 and 0.03, respectively). However, after adjustment for multiple comparisons, none of the comparisons between the three subsequent treatment groups and the control group was significant (all P>0.0167). We observed a mild reduction in body weight in all treatment groups (−2.0 (interquartile range −4.7-0.5 kg); −3.5% (SD 0.7%) of baseline body weight), with no significant inter-group difference (P=0.23) (fig 6, bottom left). The medication adherence rates for the linagliptin plus metformin, linagliptin, and metformin groups were 87.7% (SD 5.4%), 92.2% (4.4%), and 91.0% (3.9%), respectively.
Safety
During the SIIT phase, an average of 2.0 (SD 2.6) episodes of level 1 hypoglycaemia and 0.15 (0.46) episodes of level 2 hypoglycaemia occurred in each patient, neither with significant differences between groups. No episodes of severe hypoglycaemia occurred during either SIIT or subsequent therapies. A total of 41 participants needed additional glucose lowering interventions during the follow-up period owing to relapse of hyperglycaemia reaching the previously described thresholds (fasting plasma glucose >9.0 mmol/L or HbA1c>8.0%; seven in the linagliptin plus metformin group, 10 in the linagliptin group, nine in the metformin group, and 15 in the control group; supplementary table S8). At the time of additional intervention, the mean HbA1cwas 8.0% (SD 0.6%) and the mean fasting plasma glucose was 9.3 (1.1) mmol/L. Of these hyperglycaemic relapse events, 73% (30/41) occurred within 24 weeks after SIIT treatment, and 22% (9/41) of the patients with a hyperglycaemic relapse received injectable therapy (insulin or glucagon-like peptide 1 receptor agonists). Adverse events during the subsequent therapy period are summarised in supplementary table S5. Overall, the subsequent oral regimens were well tolerated. The most common adverse events were gastrointestinal side effects, more prevalent in the metformin group (16/103; 16%) and the linagliptin plus metformin group (18/103; 17%) than in the linagliptin group (7/103; 7%) and the control group (4/103; 4%). Withdrawal due to gastrointestinal side effects occurred in three participants from the linagliptin plus metformin group and five from the metformin group. No participant died.
Discussion
In this proof-of-principle study, the implementation of the intense simplified strategy significantly improved glycaemic outcomes. SIIT rapidly eliminated glucotoxicity (mean fasting glucose decreased from 11.4 mmol/L to 5.9 mmol/L) and improved β cell function, and these glycaemic benefits were well maintained with simple oral medication regimens. Over 48 weeks, 80.4%, 71.6%, and 72.6% of patients in the linagliptin plus metformin, linagliptin, and metformin groups, respectively, achieved HbA1c<7.0%. These rates were even higher in the per protocol analysis (85.9%, 78.7%, and 76.5%, respectively; supplementary figure S1). These favourable outcomes were accompanied by more stable β cell recovery. The findings suggest that this intense simplified strategy merits further consideration in managing early diabetes in which significant hyperglycaemia is noted.
Potential rationale
As previously stated, SIIT exerts its effect uniquely by rapidly clearing glucotoxicity and lipotoxicity. Previous studies have shown that rigorous glycaemic control during SIIT, characterised by lower mean blood glucose and higher time in range within predefined glycaemic targets, is crucial for favourable outcomes.1920This is supported by basic experimental data, as stringent glycaemic control restored the response pattern to glucose in human β cells and induced β cell redifferentiation.2122However, maintaining these benefits over time is challenging. Hyperglycaemic relapse may occur as a result of ageing, poor adherence to diabetes management, and insufficient intervention for underlying mechanisms of type 2 diabetes mellitus, such as impaired incretin action.2324Notably, even mild hyperglycaemia can result in β cell impairment,2526potentially exacerbating a vicious cycle. Therefore, sequentially applying therapies targeting multiple mechanisms of type 2 diabetes after SIIT may be essential for maintaining durable glycaemic control and preventing functional β cell loss.
Comparison with other studies
This study of the intense simplified strategy substantially differs from several previous studies investigating the effect of early initiation of combination therapy on reducing treatment failure in newly diagnosed type 2 diabetes. In the VERIFY study, vildagliptin plus metformin reduced the risk of initial treatment failure (hazard ratio 0.51). However, the VERIFY study enrolled only patients with mild hyperglycaemia (HbA1c6.5-7.5%).27In a randomised controlled trial, linagliptin plus high dose metformin (up to 2000 mg/day) reduced HbA1cby 2.8% (SD 0.1%) and achieved HbA1c<7.0% in 60.3% of participants with a baseline HbA1cof 10%, but in a shorter observation period (24 weeks).28Another analysis combining data from two randomised controlled trials found that linagliptin plus metformin (1000 mg/day) reduced HbA1cby 1.67% in patients with early type 2 diabetes and moderate hyperglycaemia (mean baseline HbA1c8.7%), with only 51.4% achieving HbA1c<7.0% and 40.1% achieving HbA1c<6.5%.29Compared with these studies, our study achieved more prominent (mean reduction of HbA1cranging from 4.2% to 4.8% in the subsequent treatment groups) and durable (48 weeks) optimal glycaemic control in patients with more marked hyperglycaemia (mean HbA1c11.0%). In the EDICT study in patients with new onset of type 2 diabetes and significant hyperglycaemia (average HbA1c~11.0%), initial triple therapy, comprising metformin, pioglitazone, and exenatide, achieved HbA1c<6.5% in 78% of participants over three years.30However, long term triple therapy, particularly involving injectable agents, may encounter challenges of adherence, tolerance, and economic pressure.31The first step of intensive treatment lays the foundation for subsequent management by rescuing damaged β cells from persistent metabolic distress (disease modifying property), which allows for achievement of sustained glycaemic control with a convenient medication regimen (linagliptin once daily, moderate dose metformin (1000 mg/day), or a combination). Use of simplified regimens contributed to good drug adherence and tolerability, with withdrawals due to side effects occurring in less than 5% of participants. Moreover, the benefits of the intense simplified strategy were not dependent on weight loss, with participants losing only around 2 kg of body weight over the treatment period. Taken together, the intense simplified strategy provides an effective, safe, and affordable approach for patients with newly diagnosed type 2 diabetes and significant hyperglycaemia, avoiding the disadvantages of complex regimens.3233This finding is supported by an observational study from Shambo and colleagues, which showed long term glycaemic control and reduced need for anti-diabetes agents after early insulin initiation in type 2 diabetes.34
Limitations of study
This study has some limitations. The intervention was not blinded. Nevertheless, the fact that the primary and secondary endpoints were generated from biochemical measurements by blinded technical staff reduces potential bias. Furthermore, the attrition rate was higher than expected, largely owing to the covid-19 pandemic in China during the later stages of the study, which caused some patients to drop out owing to strict social control measures and travel restrictions. However, on the basis of the number of participants in the full analysis set (97 in the linagliptin plus metformin group and 93 in the control group) and primary endpoint data (80.4% and 60.2%, respectively), the statistical power exceeded 85%. Additionally, sensitivity analyses and per protocol analyses confirmed the robustness of the conclusions. Lastly, although the participants in this study were recruited from 15 centres across China, the findings need to be validated in more diverse national and ethnic populations.
Conclusions
The intense simplified strategy using subsequent oral therapies after SIIT, especially the linagliptin plus metformin combination, sustainably improved glycaemic control and β cell function in patients with newly diagnosed type 2 diabetes and severe hyperglycaemia. This proof-of-principle study also provides a new area for future exploration. More practical and feasible intensive approaches that can be applied in outpatient settings, such as multiple daily insulin injections or even regimens based on basal insulin, could be explored for the “intense” module, and more convenient drugs such as fixed dose combination preparations or sodium-glucose co-transporter 2 inhibitors with benefits for complications could also be considered in the “simplified” module. Further data are needed to evaluate the durability of treatment effects, the strategy’s impact on reducing long term complications, and its potential economic advantages.
","Objective: To evaluate whether the intense simplified strategy, which comprises short term intensive insulin therapy (SIIT) followed by subsequent oral antihyperglycaemic regimens, could improve long term glycaemic outcomes in patients with newly diagnosed type 2 diabetes mellitus and severe hyperglycaemia.
Design: Multicentre, open label, randomised trial.
Setting: 15 hospitals in China between December 2017 and December 2020.
Participants: 412 patients with newly diagnosed type 2 diabetes and significant hyperglycaemia (HbA1c≥8.5%).
Interventions: All randomised participants initially received SIIT for 2-3 weeks, followed by linagliptin 5 mg/day, metformin 1000 mg/day, combination linagliptin plus metformin, or lifestyle modification alone (control) for 48 weeks.
Main outcome measures: The primary outcome was the percentage of participants achieving HbA1c<7.0% at week 48 after SIIT. Secondary outcomes included glycaemic control, β cell function, and variations in insulin sensitivity.
Results: 412 participants were randomised. At baseline, the mean age was 46.8 (standard deviation 11.2) years, mean body mass index was 25.8 (2.9), and mean HbA1cwas 11.0% (1.9%). At week 48, 80% (78/97), 72% (63/88), and 73% (69/95) of patients in the linagliptin plus metformin, linagliptin, and metformin groups, respectively, achieved HbA1c<7.0%, compared with 60% (56/93) in the control group (P=0.02 overall; P=0.003 for linagliptin plus metformin versus control; P=0.12 for linagliptin versus control; P=0.09 for metformin versus control). Additionally, 70% (68/97), 68% (60/88), and 68% (65/95) of patients in the linagliptin plus metformin, linagliptin, and metformin group, respectively, achieved HbA1c<6.5% compared with 48% (45/93) in the control group (P=0.005 overall; P=0.005 for linagliptin plus metformin versus control; P=0.01 for linagliptin versus control; P=0.008 for metformin versus control; all were significant after adjustment for multiple comparisons). Thus, compared with the control group, participants in the linagliptin plus metformin group were more likely to achieve HbA1c<7.0% at week 48 (odds ratio 2.78, 95% confidence interval 1.37 to 5.65; P=0.005). Moreover, the linagliptin plus metformin group showed the most significant improvement in fasting plasma glucose and β cell function indices. All treatments were well tolerated.
Conclusions: The intense simplified strategy using subsequent oral therapies post-SIIT, especially the linagliptin plus metformin combination, sustainably improved glycaemic control and β cell function in patients with newly diagnosed type 2 diabetes mellitus and severe hyperglycaemia. This approach offers a promising direction for decision making in the clinical management of type 2 diabetes mellitus.
Trial registration: ClinicalTrials.govNCT03194945
"
"Progress of nations in the organisation of, and structures for, kidney care delivery","Introduction
As the burden of kidney disease continues to grow worldwide,12significant inequities remain the availability and accessibility of care for patients with kidney disease.3Chronic kidney disease affects an estimated one in 10 people globally, with a higher prevalence in low-income and lower-middle income countries where more than 50% of the population with chronic kidney disease resides.45In the United States, approximately one in seven adults (35.5 million people) have chronic kidney disease, and a third of people with diabetes mellitus and a fifth of people with hypertension may also have chronic kidney disease.6A large systematic review identified large gaps in access to kidney replacement therapies, especially in sub-Saharan Africa and Asia, where up to 91% and 83% of people who needed kidney replacement treatment, respectively, were unable to access it.7
In response to the global challenges posed by the growing burden of chronic kidney disease, the global nephrology community developed a cohesive 10 point roadmap in 2017 to address gaps in care, research, and policy.8The International Society of Nephrology Global Kidney Health Atlas (ISN-GKHA) was set up as part of that plan to routinely provide relevant updates on access to care, health infrastructure, national and regional policies, and research capacity as a way of assessing what has been achieved for chronic kidney disease.8The first iteration was reported in 2017 and showed that globally, 95% of countries had facilities for haemodialysis, 76% had facilities for peritoneal dialysis, and 75% had facilities for kidney transplantation.9Subsequently in 2019, a second iteration showed that the numbers were100%, 76%, and 74%, respectively; however, the numbers of centres providing these services were much lower in low income countries and lower middle income countries.3The third iteration in 2023 used a similar survey instrument as the second, and also showed large gaps in availability of, and access to kidney replacement treatment services between high income and lower income countries.10
The covid-19 pandemic presented important challenges for healthcare systems globally, including interruptions in the delivery of kidney care (predialysis, dialysis, and kidney transplantation services) across countries, especially in low resource nations.1112Challenges were compounded by workforce availability,12the supply chain for the delivery of dialysis products,13trainee education, and the emotional wellbeing of the nephrology workforce and patients,14along with increased mortality risk in people on dialysis.15Even though this study was not specifically designed to assess the effect of the pandemic on availability of care, we used serial ISN-GKHA data to assess changes in key domains of kidney care delivery (ie, health financing, workforce, availability of essential medicines and health technologies, access to kidney replacement treatment, health information systems, and policy and advocacy for kidney care) over a four year period that spanned the pandemic.
Methods
The rationale and methods used for the ISN-GKHA project have been previously reported,16including specific methods and survey instruments adopted in each iteration of the work.3910Briefly, the work consisted of two components: (1) a global literature review to assess the incidence and prevalence of treated kidney failure and kidney replacement treatment (ie, haemodialysis, peritoneal dialysis, and kidney transplantation) and (2) a multinational survey to evaluate availability, access, funding, health information systems, and advocacy and policy for chronic kidney disease and kidney failure. This survey engaged opinion leaders with knowledge of the status and extent of national kidney care practices in each country. Three leaders were identified from each country to participate in the survey, including a nephrology society leader, a policy maker, and the lead of a national or regional patient representative organisation. In some instances, the country stakeholders had multiple roles (eg, both nephrology leader and policy maker). The survey framework was based on the building blocks for health systems laid out by the World Health Organization (WHO): access to essential medicines and technologies, health systems financing, health service delivery, health workforce, health information systems, and leadership and governance.17The survey was conducted in English, French, and Spanish. Data from non-English language surveys were later translated in English.
Survey development, validation, and administration
In both surveys, key stakeholders with societies affiliated with the International Society of Nephrology were invited to participate using an electronic link to the survey online portal via REDCap (https://www.redcapcloud.com). Each survey was conducted over a three month period from July to September before the year of publication, ie, 2018 and 2022, respectively. During this period, members of the ISN-GKHA working group and regional and national leaders of the International Society of Nephrology intensively followed-up via email and telephone to ensure complete and timely responses. The surveys were coordinated through the society’s 10 regional boards: Africa, Eastern and Central Europe, Latin America, the Middle East, Newly Independent States (ie, Armenia, Azerbaijan, Belarus, Georgia, Kazakhstan, Kyrgyzstan, Russia, Tajikistan, Turkmenistan, Ukraine, Uzbekistan) and Russia, North America and the Caribbean, North and East Asia, Oceania and South East Asia, South Asia, and Western Europe.
Data handling and reporting
Responses to the French and Spanish surveys were converted to English by certified translators. Data from all individual surveys were subsequently extracted to Microsoft Excel, checked for inconsistencies, missing data, duplicates, cleaned, merged, and then combined into a single file to create the global database for analysis using Stata 17 software (Stata Corporation, 2017). Regional leaders from the society reviewed all collated data, ensuring that they were consistent with their understanding of availability of services in countries in their region and were of high quality. Inconsistencies between respondents from the same country were flagged for the regional board to review. Each regional board from the society clarified any ambiguities or inconsistencies and where these could not be resolved, the respondents were contacted to clarify their responses. Findings were further validated by triangulating with published literature and grey sources of information (eg, government reports and other sources provided by the survey respondents).
Data analysis and definition of key variables
In each survey, the country was used as the unit of analysis using a descriptive statistical approach to summarise responses, which were reported as counts and percentages. The results were then stratified by region as defined by the society and by country income groups as defined by the World Bank: low income countries, lower middle income countries, upper middle income, and high income countries. Estimates at the global, regional, and country income level for continuous variables were presented as median (interquartile range). Country populations in 2021 were used as denominators to assess prevalence data that were reported as per million population (pmp). Standard definitions applied to the components of kidney replacement treatment (haemodialysis, peritoneal dialysis, and kidney transplantation) as well as elements of care delivery based on established frameworks.18Availability was assessed in terms of whether a service was “generally available” (offered in ≥50% of centres, hospitals, or clinics) or “generally not available” (offered in <50%). Accessibility referred to the proportion of people able to access a given service (eg, dialysis). Quality was defined as the proportion of centres that routinely measured a given indicator (eg, solute clearance) to assess the quality of the service provided. Affordability was determined by the proportion of the treatment cost directly covered by the individual.
We evaluated the results with a focus on identifying key gaps in, and challenges to, the delivery of kidney care according to the Guidelines for Accurate and Transparent Health Estimates Reporting (GATHER) statement.19The Checklist for Reporting Results of Internet E-Surveys (CHERRIES)20was used to inform survey development and administration, analysis, and reporting. Respondents were asked to report the number of centres in their countries providing kidney replacement treatment. The overall concentration, by World Bank income group and country, was then computed by dividing the total number of centres by the overall population in millions. Population estimates were obtained from the CIA World Factbook in June 2022. A similar approach was used to measure the prevalence of nephrologists and nephrology trainees, incidences and prevalence of kidney replacement treatment, and availability of kidney replacement treatment centres. Notably, our assessments were confined to countries that participated in both surveys (supplementary table 1). We reported ISN-GKHA 2019 data as before the pandemic and the 2023 data as after the pandemic. Comparisons were limited to survey responses only; literature review data from both iterations were not included in the comparisons. Relative changes were presented, and we used McNemar's test for comparison of dichotomous variables and Wilcoxon matched pairs signed-rank test for comparison of continuous variables. To evaluate the agreement between responses for continuous variables, we calculated the intraclass correlation coefficient, which indicates the level of similarity between responses obtained from both surveys. The level of agreement, as indicated by intraclass correlation coefficient, was categorised as poor (<0.5), moderate (0.51-0.75), good (0.76-0.90), and excellent (0.91-1.00). For categorical variables, we used the (Cohen’s) kappa statistic as a measure of agreement between non-unique raters due to the distinct respondents from country to country. The agreement, as indicated by kappa, was categorised as poor (< 0), slight (0-0.20), fair (0.21-0.4), moderate (0.41-0.60), substantial (0.61-0.80), and almost perfect (0.81-1.00).21A P value of less than 0.05 was taken as significant.
Patient and public involvement
The design of the project was led by nephrologists and patient care organisations were involved in the development of the survey instrument. Individual patient data were not used in both iterations of the ISN-GKHA from which data for this study were obtained. Furthermore, healthcare policy makers (nephrology society leaders and kidney care administrators) participated in providing relevant data for their countries.
Results
Demographic features of countries included in the analysis
Overall, data for 148 countries that participated in both surveys were available for analysis. Countries that participated in 2019 but not 2023 (n=12), or new countries that participated in 2023 that did not participate in 2019 (n=19), were excluded from the analysis (supplementary table 1). The same participants provided responses to both surveys in 82 (55%) countries with mostly significant levels of agreement identified for responses obtained across assessed survey domains (supplementary table 2).
Changes in funding for kidney care
Data for changes on funding for kidney replacement treatment were available from 141 countries (table 1). Overall, the proportion of countries where haemodialysis was publicly funded and free at point of delivery increased from 27% in 2019 to 28% in 2023. This proportion reduced from 21% to 18% in upper middle income countries and from 19% to 0% in low income countries but increased in high income countries from 40% to 55% (P=0.046). However, countries where haemodialysis was reimbursed through private funds and solely out-of-pocket did not change globally, but did increase in Africa from 11% to 17% and also increased in low income countries from 13% to 19% (table 1). Similarly, the proportion of countries that covered peritoneal dialysis through public funding that was free at the point of delivery increased from 23% to 28% globally. However, this decreased in some regions, including Africa (11% to 9%), the Middle East (36% to 27%), Newly Independent States and Russia (43% to 29%), and Oceania and South East Asia (20% to 7%). The proportions of countries where solely out-of-pocket payment method was used for peritoneal dialysis stayed the same across all country income groups. However, this proportion decreased from 13% to 6% in Eastern and Central Europe (table 1).
The proportion of countries where reimbursement for the costs of kidney transplantation and medications were through public funds and free at point of delivery increased globally (31% to 36%). This proportion did not change in Africa, the Middle East, and the Oceania and South East Asia region. Globally, countries where payments for kidney transplantation and medications were through out-of-pocket methods came down from 8% to 6%; this proportion reduced from 6% to 0% in Latin America and from 36% to 0% in the Middle East region (P=0.046) (table 1). Funding for non-dialysis chronic kidney disease care, vascular access surgery creation and insertion, peritoneal catheter insertion, and kidney transplantation surgeries are shown in supplementary tables 3-7.
Changes in the availability of health technologies for kidney care
Globally, the prevalence of haemodialysis centres increased, rising from 4.4 pmp to 4.8 pmp (P<0.001) (fig 1). Increases were observed across most regions, except the Middle East where there was a decrease from 3.8 pmp to 3.3 pmp. This proportion also decreased in high income countries (8.7 pmp to 8.6 pmp) but increased across other income levels: 0.18 pmp to 0.31 pmp in low income countries, 1.3 pmp to 1.7 pmp (P<0.001) in lower middle income countries, and 4.7 pmp to 5.4 pmp (P<0.001) in upper middle income countries. The overall prevalence of peritoneal dialysis centres increased from 1.4 pmp to 1.6 pmp. Peritoneal dialysis centre prevalences decreased in Eastern and Central Europe (2.3 pmp to 2.0 pmp), the Middle East (0.8 pmp to 0.7 pmp), North and East Asia (1.9 pmp to 1.3 pmp), and Oceania and South East Asia (2.2 pmp to 1.5 pmp). With respect to country income groups, peritoneal dialysis centres only decreased in low income countries (0.09 pmp to 0.06 pmp) but increased across other income levels: 0.17 pmp to 0.22 pmp in lower middle income countries, 0.8 pmp to 1.1 pmp in upper middle income countries, and 2.3 pmp to 2.5 pmp in high income countries (fig 1). Globally, centres where kidney transplantation was available increased from 0.4 pmp to 0.5 pmp. Regionally, these centres only increased in Western Europe (0.5 pmp to 0.6 pmp) (fig 1).
Globally, availability of a functioning vascular access (arteriovenous fistula or arteriovenous graft) increased from 17% to 23% as did tunnelled catheters for dialysis initiation, from 13% to 17%, (supplementary table 8). Availability of arteriovenous fistula reduced in Eastern and Central Europe (44% to 25%), Latin America (12% to 6%), and upper middle income countries (18% to 12%). However, the proportion of countries in which tunnelled catheters were available to initiate dialysis decreased in Africa (18% to 6%) and in low income countries (13% to 0%). Availability of temporary catheters for dialysis initiation decreased globally from 46% to 43% and also decreased in the Middle East (64% to 36%), Newly Independent States and Russia (43% to 29%), North America and Caribbean (43% to 14%), Oceania and South East Asia (40% to 27%), and South Asia (80% to 40%) regions (supplementary table 8).
Changes in quality and availability of diagnoses and treatment
Overall, the proportion of countries where haemodialysis was offered three times weekly (three to four hours per session) increased from 77% to 83%. This proportion increased in Africa (43% to 54%), Latin America (89% to 100%), Newly Independent States and Russia (86% to 100%), North America and the Caribbean (57% to 100%), and South Asia (43% to 57%) (fig 2). Overall, the proportion of countries in which adequate peritoneal dialysis (ie, three to four exchanges per day or equivalent cycles on automated peritoneal dialysis) was offered increased from 60% to 61%. Regions that had an increase in the proportion of countries that offered adequate peritoneal dialysis included North America and the Caribbean (43% to 71%), Latin America (67% to 83%), and Africa (14% to 17%). Overall, countries with capacity for provision of adequate kidney transplantation (ie, provision of appropriate immunosuppression and anti-rejection treatments) did not change and only increased in Latin America (83% to 94%), North America and the Caribbean (57% to 71%), and the North and East Asia region (83% to 100%) (fig 2).
We show the quality of outcomes reporting of haemodialysis, peritoneal dialysis, and kidney transplantation, including reporting of blood pressure, small solute clearance, haemoglobin concentrations, bone mineral markers, technique survival (ie, how long a patient continues peritoneal dialysis before switching to haemodialysis or dying), patient survival, and allograft survival, are shown in supplementary tables 9-11. We evaluated changes in diagnostic and treatment capacity for kidney failure complications, including anaemia, mineral bone disease, and electrolyte disorders (fig 3). Regarding diagnosis and treatment of anaemia, overall increases were reported for measurement of iron parameters (75% to 80%), treatment with oral iron (98% to 100%), treatment with parenteral iron (83% to 85%), and availability of erythropoietin stimulating drugs (84% to 88%) (fig 3).
Globally, the capacity to measure serum calcium concentrations increased from 92% to 96% while capacity to measure serum phosphorus increased from 87% to 92% (P=0.046) (fig 3). Overall, availability of calcium based phosphate binders increased from 88% to 92%. Treatment with non-calcium based phosphate binders increased from 49% to 55% and remained available in all countries in North and East Asia and Western Europe in both periods (fig 3). Overall availability of cinacalcet increased from 39% to 50% (P=0.001), did not change in low income countries, and increased in lower middle income countries (5% to 16%; P=0.046), upper middle income countries (15% to 35%; P=0.035), and high income countries (86% to 93%; P=0.046).
Overall, capacity to measure serum electrolytes (eg, sodium, potassium, and bicarbonate) increased from 95% to 97% and increased in low income countries (69% to 75%) and in lower middle income countries (95% to 97%). Availability of potassium exchange resins increased from 64% to 70% and oral sodium bicarbonate increased from 75% to 82% (fig 3). Availability of potassium exchange resins only increased significantly in low income countries (25% to 50%; P=0.046).
Changes in access to kidney replacement treatment
Globally, the proportions of countries in which a majority (>50%) of their national populations were able to access kidney treatment increased from 72% to 74% for haemodialysis and from 4% to 6% for peritoneal dialysis but decreased from 30% to 29% for kidney transplantation (fig 4). Countries where more than 50% of people with kidney failure were able to access haemodialysis remained the same in the North America and the Caribbean, Oceania and South East Asia, and Western Europe regions, and increased in the Africa (34% to 40%), Eastern and Central Europe (94% to 100%), the Middle East (82% to 100%), and North and East Asia (83% to 100%) regions. Similarly, countries where more than 50% of people with kidney failure were able to access peritoneal dialysis (ie, start dialysis with peritoneal dialysis) increased in Africa (0% to 6%), Oceania and South East Asia (0% to 7%), and Western Europe (0% to 10%), decreased in Newly Independent States and Russia (20% to 0%), and remained the same across the remaining regions. Although the proportion of countries reduced in which the majority of people with kidney failure were able to access kidney transplantation, increases were reported in Africa (3% to 6%), Latin America (17% to 22%), and Western Europe (60% to 85%). Only high income countries reported an increase in the proportion of countries with access to kidney transplantation (55% to 56%); this proportion was reduced in lower middle income countries (11% to 3%) and remained the same in low income countries and upper middle income countries (fig 4).
Changes in availability of workforce for kidney care
Overall, the median prevalence of nephrologists increased from 9.5 pmp to 12.4 pmp (P<0.001). The prevalence of nephrologists reduced in Eastern and Central Europe (26.0 pmp to 24.8 pmp) and in North America and the Caribbean region (18.1 pmp to 18.0 pmp) (fig 5). Nephrologist prevalence increased across all income levels: in low income countries (0.26 pmp to 0.30 pmp), lower middle income countries (1.3 pmp to 1.8 pmp; P<0.001), upper middle income countries (9.3 pmp to 12.5 pmp; P=0.008), and high income countries (25.2 pmp to 25.6 pmp; P=0.019).
The percentage of countries that reported a shortage of nephrologists decreased overall from 68% to 62% (supplementary table 12). Although Africa did not have a change in number of countries that reported shortages of nephrologists, it still had the highest proportion of countries with shortages of nephrologists (86%). No significant changes were noted in reporting of shortages of nephrologists across all regions and country income levels (supplementary table 12).
Changes in health information systems for kidney diseases
Overall, availability of all types of registries for kidney diseases increased (supplementary table 13). The number of countries with registries for non-dialysis chronic kidney disease increased from 13% to 19%. All the countries in Newly Independent States and Russia and Oceania and South East Asia that had a non-dialysis chronic kidney disease registry in 2019, reported not having them in 2023. Dialysis registries increased from 67% to 68%. Regionally, only Africa recorded a reduction of dialysis registries (44% to 32%) with low income countries and lower middle income countries also recording reductions (supplementary table 13). While kidney transplantation registries increased globally from 60% to 62%, reductions were reported in Africa (15% to 12%), in low income countries (7% to 0%), and high income countries (91% to 89%). Variability in acute kidney injury registries was also noted across regions and country income levels, although none statistically significantly.
Changes in national policies and strategies for chronic kidney disease care
Overall, the proportion of countries with a national strategy for non-communicable diseases increased from 47% to 58% (P=0.016). Significant increases were only reported in Africa (35% to 62%; P=0.003) and in lower middle income countries (36% to 61%; P=0.013) (supplementary table 14). The proportion of countries with strategies for improving the care of people with chronic kidney disease increased overall from 22% to 25% while those with a strategy specific for chronic kidney disease increased from 36% to 37%. Overall, the proportion of countries in which chronic kidney disease was recognised as a health priority reduced from 52% to 50% while countries where kidney failure and kidney replacement treatment were recognised as a health priority increased from 55% to 65% (supplementary table 15). Variabilities in regional and country income level were also noted.
Barriers to optimal kidney care delivery
Overall, we noted increases in the proportion of countries that identified geographical factors (55% to 59%), availability of a nephrologist (58% to 68%; P=0.043), and lack of political will and enabling policies (46% to 52%) as specific barriers to optimal delivery of kidney care with significant variations noted across regions and country income levels (supplementary table 16).Of note, the proportion of countries that identified economic factors (eg, limited funding or poor reimbursement systems) as a barrier to optimal kidney care delivery reduced (64% to 59%).
Discussion
The primary focus of the ISN-GKHA is to routinely update the status, at a global, regional, and country level, of the capacity for kidney care delivery across domains of health systems building blocks. All iterations have described the extent of available services and important gaps, especially in low resource countries, in accessing kidney care services. This study leveraged 2019 and 2023 ISN-GKHA data to describe changes across core domains.
Principal findings of this study
Our study documents change in kidney care availability, accessibility, and funding in countries and regions over a four year period (in 2019 and 2023). To our knowledge, this study is the first to provide a comprehensive report of such changes in the organisation, structures, and care delivery processes for people living with kidney disease across different metrics of kidney care in countries in all major regions of the world. On a global scale, this study identified the following: (1) increased proportions of countries in which haemodialysis, peritoneal dialysis, and kidney transplantation were publicly funded and free at the point of delivery, (2) increased prevalences of haemodialysis and peritoneal dialysis centres, (3) more countries with improved quality of delivered haemodialysis and peritoneal dialysis with no change in quality of kidney transplantation, (4) increased access to haemodialysis and peritoneal dialysis for populations with kidney failure, (5) an overall increase in the prevalence of nephrologists, (6) an increase across all types of kidney registries, and (7) an increase in strategies specific to chronic kidney disease for improving kidney care.
Comparisons with previous studies
No studies have provided global trends across WHO’s six health systems building blocks for kidney care (ie, service delivery; health workforce; health information systems; medical products, vaccines, and technologies; health financing; leadership and governance). However, deductions can be made from studies and reports that have projected trends in availability and access to treatment and capacity for funding kidney care. For example, in 2010, 2.6 million people accessed kidney replacement treatment worldwide out of a possible 4.9-9.7 million who were in need, suggesting at least 2.3 million premature deaths due to limited access to kidney replacement treatment.7Projections show that fewer people (5.4 million of an estimated 14.5 million) will be able to access kidney replacement treatment by 2030, especially in low income countries and countries in Asia and Africa. This disparity in access to treatment shows the need for implementation of cost effective strategies. Even though our study showed increased access to dialysis, the proportion of countries in low income countries where dialysis can be readily accessed remained too low to be able to meet the burden of kidney diseases in these regions (from 13% to 31%v98% to 100% in high income countries).
An estimated 8-37% of total domestic health expenditure would be required to address universal access to dialysis in three countries in Africa.22This figure suggests that the opportunity costs of wholesale dialysis expansion in Africa are too great to be easily justifiable from a population health perspective, a region in which more than 50% of countries use public funds for dialysis reimbursement.23Another study from Africa reported mortality exceeding 90% within 12 months in people with incident kidney failure who were treated with haemodialysis, mainly due to the inability to pay for the cost of care.24Unfortunately, our study only showed a small increase in number of countries in which haemodialysis was fully funded through public funds (27% to 28%) with major reductions noted in low income countries (19% to 0%). Additionally, countries that used only an out-of-pocket payment system increased in low income countries (13% to 19%) (table 1). While our study showed a marginal increase in global government funding for dialysis, public funding reduced or was completely removed in low resource settings. Healthcare funding reforms are urgently needed in many countries, especially in poorer countries, to improve and allow equitable access to kidney care. Such reforms will likely involve processes to increase government funding for health, strategies that lead to an overall reduction of the cost of treatments, disease prevention policies, and an evaluation of economic and societal return on such investments, particularly in low income countries.25
Interpretation and policy implications
Improved healthcare funding is an important component of a well functioning healthcare system for optimal care delivery and better outcomes.26Kidney replacement treatment is costly and its high cost and excessive out-of-pocket payment methods in many countries represent major hurdles to accessing high quality kidney care, leading to poor outcomes in people with kidney failure.2427Even though our study showed an increase in the proportion of countries that offer kidney replacement treatment that is publicly funded and free at point of delivery, the categories of countries in which these increases were reported were mainly high income countries. In low and lower middle income countries where the burden of kidney disease is high, the proportion using public funds to reimburse kidney replacement treatment was reduced and countries relying on private payment methods increased.
Although sustained pressure should be put on governments to increase public funding for kidney replacement treatment, the focus should be on public funding of haemodialysis because this modality of kidney replacement treatment is most common,28and is often the only modality in low or lower middle income countries.29By contrast, the funding for peritoneal dialysis is often government funded and free at the point of delivery. Specific countries, including Hong Kong, Taiwan, China, Mexico, and Guatemala, have adopted a policy to use peritoneal dialysis first or to favour peritoneal dialysis.30This strategic choice could account for the higher percentage of the national population with access to peritoneal dialysis, particularly in North and East Asia, and an increasing number of countries in North and East Asia, Latin America, and Western Europe that provide peritoneal dialysis with government funding.
Furthermore, pragmatic approaches need to address the areas in which low availability of public funding exists, particularly in low income countries and lower middle income countries. Such approaches might include mechanisms for early disease identification, treatment of chronic kidney disease and risk factors by increasing awareness through education, early referral to nephrology, and education on avoidance of nephrotoxins. Additionally, approaches include initiation of medications that slow disease progression and reduce excess morbidity and mortality.31Efforts should not only be geared towards increasing availability of, and access to, kidney replacement treatment, but also towards the quality of delivered kidney replacement treatment and availability of shared decision making for conservative kidney management.32Africa, Oceania and South East Asia, and South Asia had an increase in the proportion of countries able to provide adequate haemodialysis, however, the goal should be to ensure that almost all countries in every region have capacity to deliver high quality care. With this improvement, patients would have an improved quality of life, reduced complications, and reduced morbidity and mortality.33Availability, although increased, were still limited, particularly in low income countries and lower middle income countries. The overall rate of access to kidney transplantation slightly decreased, especially in lower middle income countries. This aligns with earlier reports highlighting a decline in the rate of kidney transplantation during the pandemic period in 2020, particularly for living donor kidney transplantation. Enhancing access to kidney transplantation is essential because this treatment modality is the most effective.
Availability of essential medicines is crucial to promote health, slow disease progression, treat complications of kidney failure, and successfully implement preventive measures. Several medicati","Objective: To assess changes in key measures of kidney care using data reported in 2019 and 2023.
Design: Cross sectional survey in 148 countries.
Setting: Surveys from International Society of Nephrology Global Kidney Health Atlas between 2019 and 2023 that included participants from countries in Africa (n=36), Eastern and Central Europe (n=16), Latin America (n=18), the Middle East (n=11), Newly Independent States and Russia (n=10), North America and the Caribbean (n=8), North and East Asia (n=6), Oceania and South East Asia (n=15), South Asia (n=7), and Western Europe (n=21).
Participants: Countries that participated in both surveys (2019 and 2023).
Main outcome measures: Comparison of 2019 and 2023 data for availability of kidney replacement treatment services, access, health financing, workforce, registries, and policies for kidney care. Data for countries that participated in both surveys (2019 and 2023) were included in our analysis. Country data were aggregated by International Society of Nephrology regions and World Bank income levels. Proportionate changes in the status of these measures across both periods were reported.
Results: Data for 148 countries that participated in both surveys were available for analysis. The proportions of countries that provided public funding (free at point of delivery) increased from 27% in 2019 to 28% in 2023 for haemodialysis, 23% to 28% for peritoneal dialysis, and 31% to 36% for kidney transplantation services. Centres for these treatments increased from 4.4 per million population (pmp) to 4.8 pmp (P<0.001) for haemodialysis, 1.4 pmp to 1.6 pmp for peritoneal dialysis, and 0.43 pmp to 0.46 pmp for kidney transplantation services. Overall, access to haemodialysis and peritoneal dialysis improved, however, access to kidney transplantation decreased from 30 pmp to 29 pmp. The global median prevalence of nephrologists increased from 9.5 pmp to 12.4 pmp (P<0.001). Changes in the availability of kidney registries and in national policies and strategies for kidney care were variable across regions and country income levels. The reporting of specific barriers to optimal kidney care by countries increased from 55% to 59% for geographical factors, 58% to 68% (P=0.043) for availability of nephrologists, and 46% to 52% for political factors.
Conclusions: Important changes in key areas of kidney care delivery were noted across both periods globally. These changes effected the availability of, and access to, kidney transplantation services. Countries and regions need to enact enabling strategies for preserving access to kidney care services, particularly kidney transplantation.
"
Rainfall events and daily mortality across 645 global locations,"Introduction
Emerging evidence from epidemiological studies suggests a compelling association between rainfall events and adverse health outcomes,12particularly the transmission of infectious diseases.3The influence of rainfall events on non-communicable diseases, however, remains understudied.45Current evidence, derived mainly from specific locations,67has failed to capture the diverse health effects of regional variations in rainfall characteristics. Extreme rainfall events can result in rapid meteorological changes that potentially lead to an interaction with harmful aerosols,89which can contribute to an increased risk for various cardiovascular610and respiratory conditions.10
Several challenges hinder comprehensive understanding of the association between rainfall events and health outcomes. Firstly, most studies defined rainfall events using traditional centile methods,111213and the variation in centile thresholds used across studies has led to mixed results. This problem, compounded by limitations such as restricted geographical locations and short study durations,6has prevented consensus from being reached. Secondly, these traditional centile methods failed to reflect local rainfall patterns and did not follow the World Meteorological Organization criteria for identifying important events14: High intensity (substantial rainfall within a brief period), low frequency, and short duration.14Thirdly, the association between rainfall intensity and health outcomes was likely non-linear. Although moderate rainfall can mitigate summer heat and help reduce air pollution, which in turn might reduce some environmental health risks,15high intensity rainfall events of low frequency and short duration may have a particularly harmful effect on health, as such events can result in rapid meteorological changes, increased run-off, overwhelmed infrastructures,16increased breeding of pathogens, and increased risk of different kinds of pollutants, including particulate matter and chemicals,1317which can potentially exacerbate health conditions.
Climate change is intensifying the variability in precipitation1819and extreme rainfall events both daily and overall.2021An awareness of the effects of these extreme events is crucial for understanding the complex health consequences of climate change. Using an intensity-duration-frequency model to combine the three rainfall indices (high intensity, low frequency, short duration) with mortality data from 34 countries or regions, we estimated associations between mortality (all cause, cardiovascular, and respiratory) and rainfall events with different return periods (the expected average time between occurrences of an extreme event of a certain magnitude) and crucial effect modifiers, including climatic, socioeconomic, and urban environmental conditions. The findings should help improve our understanding of the effect of rainfall on health and guide health services to better respond to rainfall related health risks in the context of climate change.
Methods
Study population
We obtained data on mortality from the Multi-Country Multi-City (MCC) Collaborative Research Network database (https://mccstudy.lshtm.ac.uk/), which is described elsewhere.222324Daily mortality records, provided by local statistical authorities within each participating country or region, were categorised using ICD-9 and ICD-10 (international classification of diseases, ninth and 10th revisions, respectively) codes. We obtained records for non-external mortality (ICD-10 codes: A00-R99; ICD-9 codes: 001-799) or all cause mortality depending on data availability, as well as specific data for cardiovascular diseases (ICD-10 codes: I00-I99; ICD-9 codes: 390-459) and respiratory diseases (ICD-10 codes: J00-J99; ICD-9 codes: 460-519) across a total of 743 locations from 1980 to 2020. These locations were selected based on the representativeness of the mortality data in relation to demographic characteristics of populations across diverse conditions in different countries and regions.2526The appendix in our previous study provides a list of these locations and details on the sources of mortality data for each.24
Exposure data
Our primary climate data consisted of surface precipitation rates on an hourly scale (hourly rainfall amount) sourced from the land component of the fifth generation of the European reanalysis (ERA5-Land) dataset.27This dataset integrates satellite and ground observations through advanced assimilation and modelling to estimate climate variables globally on land. We obtained data on a regular 0.1°×0.1° grid for 1980 to 2020 for each location, although the available years differed by location. The data were aggregated to daily scales, accounting for different time zones. Numerous studies have validated the precision of this precipitation data across various regions.2829Furthermore, considering the distinct effects of rainfall and snowfall,30we also collected data on snow depth. To differentiate between rain and snow in subsequent analyses, we treated rainfall days with a daily mean temperature <0°C and a snow depth >0 as non-precipitation days, assuming such conditions likely indicated snowfall or mixed precipitation rather than pure rainfall.30
In addition, we sourced precipitation data from two additional datasets for sensitivity tests. The MSWEP (multi-source weighted-ensemble precipitation) dataset31provided data with a three hour temporal resolution, matching ERA5-Land’s spatial resolution for 1980-2020. We also utilised the IMERG (integrated multi-satellite retrievals for global precipitation measurement) dataset, which offers half hourly precipitation measurements from 2000 to 2021.32IMERG uses >10 satellites with passive microwave radiometers to deliver calibrated global estimates of precipitation, a method widely recognised for its reliability.33From these datasets, for each city in our study, we extracted data from the grid point closest to the city centre’s coordinates. We also obtained daily mean temperature and relative humidity data from the MCC database, originally sourced from local monitoring stations. Further details on data collection and assessment procedures are provided elsewhere.26Finally, we collected daily (24 hour average) pollutant concentrations, specifically particulate matter with aerodynamic diameters ≤2.5 μm (PM2.5) and eight hour maximum ozone, from local monitoring stations. These air pollution data were available only for 372 locations across 19 countries or regions, as described previously.23
Definitions of rainfall events
Rainfall patterns often do not follow a normal distribution owing to periods of no rain leading to many zero values and extreme weather events causing skewness.3435Geographical factors also contribute to uneven distributions across regions.36Therefore, centile methods that are suitable for defining heat waves or cold spells based on daily temperature data may be inadequate for rainfall analysis. Impactful events are usually intense and rare, necessitating a definition that accounts for both intensity and frequency.
Following the World Meteorological Organization’s guideline,14we used the extreme value theorem to assess the threshold of extreme rainfall events at daily scale with a specific return period. Return periods are the expected average times between occurrences of an extreme event of a certain magnitude. This approach involves considering the frequency, duration, and intensity of extreme events at the same time.37Specifically, we used the regional frequency analysis framework38with the generalised extreme value function to model the distribution of annual maximum daily cumulative rainfall across all locations.39Using this fitted generalised extreme value model, we established thresholds for rainfall events with return periods of one year, two years, and five years for each group of cities with similar climatic and precipitation conditions. For example, if the cumulative probability of daily cumulative rainfall reached 50% based on the fitted distribution, this level of rainfall was then associated with a two year return period. This does not imply that such events happen regularly every two years but rather describes their probability of occurrence over a long term horizon. The supplementary methods provide details on the methodological and validation approaches. The methodology is commonly employed in research examining the health effects of extreme climatic conditions40and is extensively used in hydrological studies.37414243
Furthermore, to illustrate the gradation of relative risk associated with increasing rainfall intensity from relatively moderate to extreme, we calculated the percentage of daily accumulated rainfall exceeding the two year return period threshold as a continuous variable, denoted as PE2(fig 1). This measure provides a continuous index for quantifying the relative intensity of extreme rainfall events.
As an example, if the threshold for a two year return period was 50 mm and a city experienced 70 mm in a day, the index value would be calculated as 40%, meaning that on that day the city experienced rainfall that was 40% greater than expected for a two year return period. We selected this threshold based on our preliminary research findings, where no significant effect was observed for rainfall events with a one year and two year return period.43
Statistical analysis
To investigate the association between extreme rainfall events and daily mortality, we employed an established two stage analytical method.264445This approach follows the methods detailed in previous studies,4345ensuring consistency and comparability in our analysis.
In the first stage of our analysis, we utilised quasi-Poisson regression models to assess the city specific associations between extreme rainfall events and daily mortality. The quasi-Poisson regression model was adopted as it has been found to fit data for daily mortality count well in many related studies.4546For the main model, we employed distributed lag models to evaluate the cumulative impact of exposure to extreme rainfall as a binary variable. Based on return period analysis, we identified a rainfall threshold for three different return periods (one year, two years, and five years). Days with rainfall below certain threshold were assigned a value of 0, indicating either no rainfall or precipitation not meeting the criteria. Conversely, days exceeding this threshold were assigned a value of 1. We modelled the lag structure using a natural cubic B spline with four degrees of freedom and two internal knots, which were evenly spaced in the log scale.47The lag period was set to 14 days, which was informed by our preliminary findings in east Asia.43To control the potential confounding effects of non-optimal temperatures, we used a distributed lag non-linear model to introduce a cross basis function of daily mean temperature in the main model. This distributed lag non-linear model comprised a quadratic B spline with three internal knots placed at the 10th, 75th, and 90th centiles of location specific temperature distributions.26Additionally, the model included a lag response curve characterised by a natural cubic spline with three internal knots evenly spaced on the log scale with the same 14 day lag setting.26We also included a natural cubic B spline of time with seven degrees of freedom per year26and an indicator of the day of the week to control for the long term and seasonal trends and weekly variations.26In the second stage, we pooled the city specific estimates to obtain overall relative risk based on coefficients and covariance matrices of the results from each city using random effects meta-analysis with restricted maximum likelihood estimation.48Specifically, these associations were pooled to generate an overall level and were based on several key factors related to the rainfall effect to explore potential effect modifications that have not been adjusted for, including local climate types (according to the Köppen climate classification, a widely used global climate zoning method49), annual cumulative precipitation, the average annual standard deviation of daily cumulative precipitation for all rainfall days, economic conditions (reflected in the country’s income class, 2010 version), population density, and the average percentage of vegetation coverage (2010 version). The supplementary file provides detailed information on the data sources for each factor. We examined heterogeneity using Cochran’s Q test and the I2statistic.50
For the analysis of PE2, we employed a similar process, with one key distinction. In the first step, we replaced the distributed lag model function of extreme rainfall events (treated as a binary variable) with a distributed lag non-linear model cross basis function of PE2, treating it as a continuous variable. For this function, we utilised the same distributed lag non-linear model settings as those applied for daily mean temperature. This modification in the model allowed us to capture the cumulative effects of varying rainfall intensities while maintaining methodological consistency.
To strengthen the reliability of our analysis, we focused on locations that had matched a minimum of two days of rainfall events with a five year return period during the study period to exclude locations lacking adequate exposure events. Additionally, we excluded rainfall events if during event days no hourly total rainfall exceeded 4 mm, according to the World Meteorological Organization’s criteria.51Finally, to ensure the robustness of our findings, we conducted a series of sensitivity analyses (see supplementary methods), including adjustments to some key model settings, the incorporation of alternative precipitation datasets, and controlling for air pollution and flood events.
Effect modifications
To investigate how various factors might modify the health effects of extreme rainfall events across different populations, we applied random effects meta-regressions with maximum likelihood estimation. This approach allowed us to compare the cumulative relative risks across different levels of potential effect modifiers.45The potential effect modifiers examined include the classification of local climate types, the amount of total annual precipitation, the yearly average standard deviation of precipitation on rainfall days, economic status, population density, and average vegetation coverage.
Patient and public involvement
This study utilised data from worldwide death registries, which inherently restricts direct involvement of patients or their relatives owing to data privacy and security regulations across different counties. In addition, no funding was allocated for patient or public involvement, and given the nature of the data, such interaction was not feasible. At the same time, given the nature of this study it was not appropriate or feasible to involve members of the public directly. Moreover, the research was primarily data driven, with no clear mechanism for public participation in the analysis process. Importantly, this study was partially inspired by observed respiratory health effects after extreme rainfall events, which reflect the issues patients face.
Results
After excluding 98 locations owing to insufficient exposure events, where the number of matched extreme rainfall days with a five year return period was less than two days, this study encompassed 645 locations across 34 countries or regions. Results of the Kolmogorov-Smirnov test, a method used to verify the reliability of the results (see supplementary methods for details), showed that the generalised extreme value distributions were accepted at a 5% significance level for all included locations (see supplementary figure S1). Our analysis included 109 954 744 all cause, 31 164 161 cardiovascular, and 11 817 278 respiratory deaths. During the study period, we identified a total of 50 913 rainfall events with a one year return period, 8362 events with a two year return period, and 3301 events with a five year return period.Figure 2illustrates the distribution patterns of the threshold of rainfall events with the three levels of return periods. Locations that experienced higher daily cumulative rainfall within the same return period were predominantly in South America, east Asia, and South East Asia. Supplementary table S1 provides a statistical overview of the included locations, detailing the economic classification, number of included locations, and number of matched extreme rainfall events during the study period for each country or region.
We identified a global positive association between all cause mortality and extreme rainfall events with a five year return period (fig 3). However, shorter return periods of one year or two years were not associated with increased risks of all cause mortality. Overall, across all locations, a day of extreme rainfall with a five year return period was associated with a cumulative relative risk across 1-14 lag days of 1.08 (95% confidence interval (CI) 1.05 to 1.11) for daily all cause mortality. Cause specific analyses showed that rainfall events with a two year return period were associated with increased daily respiratory mortality (1.14, 1.05 to 1.23), whereas no significant effect was observed for the same period for cardiovascular mortality (1.01, 0.98 to 1.04). Furthermore, five year return period events were associated with increased risk for both cardiovascular (1.05, 1.02 to 1.08) and respiratory mortality (1.29, 1.19 to 1.39), with the risk for respiratory mortality notably higher than for cardiovascular mortality (P<0.05). Conversely, rainfall events with a one year return period showed no effect on either cardiovascular or respiratory mortality. The lag-response function results (see supplementary figure S2) indicate that the peak relative risk occurred on the day of the event (lag 0), with a substantial decrease observed by lag 3 to 4 days. Sensitivity analyses (see supplementary file) further confirmed the robustness of these findings.
Figure 4illustrates the distinct intensity-response associations between PE2and all cause mortality. The association displayed a negative trend when PE2was below a critical threshold. However, the curve showed a consistent upward trajectory once PE2surpassed 50%, with a substantial positive effect emerging as PE2surpassed about 100%, indicative of rainfall intensities more than twice the threshold for the two year return period. The pattern for cardiovascular mortality closely mirrored that of all cause mortality, which initially presented a negative association but shifted to a positive association at higher PE2values. The association for respiratory mortality escalated steeply and more pronouncedly with increasing PE2values.
Regional specific analyses revealed variations in the relative risk of daily mortality associated with different rainfall events (fig 3). For events with a two year return period, the relative risk ranged from 0.85 (95% CI 0.74 to 0.99) in Australia to 1.23 (1.08 to 1.38) in South America. This variation became more pronounced for events with a five year return period, with the relative risk in South America reaching 1.43 (1.19 to 1.67). When regions were compared, the estimated impact of extreme rainfall events on mortality in South America was notably higher than the global average for both two year and five year return periods. North America’s estimates were close to the global average. Additionally, increases in mortality risk (P<0.05) were observed with five year return period events in east Asia, North America, and South America. When health outcomes were compared, risk ratios for respiratory mortality exceeded those for cardiovascular mortality across most regions.
The analyses of effect modification (seetable 1and supplementary figures S3-S7) revealed that climate types modified the mortality risk associated with extreme rainfall events with a five year return period. The relative risk of all cause mortality was found to be higher in locations characterised by scarce vegetation coverage or lower variability in daily precipitation (P<0.05). Additionally, a statistically significant association with all cause mortality was observed predominantly in lower income countries. The association with respiratory mortality was also more pronounced in locations with scarce vegetation coverage and lower variability in daily precipitation. Evidence that average annual precipitation and population density modified these associations is, however, limited.
Discussion
In this study, we performed an analysis of mortality risks associated with extreme rainfall using a large dataset covering 34 countries or regions and employing a uniform methodology to define rainfall events based on intensity, frequency, and duration. Our analysis identified associations between extreme rainfall events and daily mortality from all causes, as well as specifically from cardiovascular and respiratory diseases. Furthermore, the intensity-response functions revealed a protective effect of daily rainfall at moderate to heavy intensities, which transitioned to a marked increase in mortality risk at extreme rainfall intensities. Moreover, the substantial health effects of these extreme rainfall events were found to vary by climate types. Extreme rainfall events were more severe in locations with lower variability of precipitation or scarce vegetation coverage. Our study utilised a comprehensive database encompassing information from multiple countries, territories, and continents. The analysis incorporated death records from numerous locations from 1980 to 2020, providing a global perspective on the effect of extreme rainfall events on health.
Only a few studies have explored the association between exposure to rainfall events and risk of mortality from non-external causes, with results showing notable inconsistencies.525354This variation might be attributed to the lack of a consistent definition of rainfall events and the limited geographical scope of most previous investigations. Similar to our results, many studies have found harmful effects of extreme precipitation. A study in rural Bangladesh observed a 0.6% increase in daily mortality for every additional millimetre of rainfall beyond 100 mm per day in the 5-19 age group.53A study in Burkina Faso found an increased risk of malaria related death associated with precipitation levels ≥453 mm.1Additionally, a notable connection has been observed between thunderstorms accompanying extreme rainfall and an increase in respiratory morbidity and mortality, especially in people with asthma.5556However, limited studies have shown either insignificant effects or protective effects of rainfall, aligning with certain aspects of our findings. A recent study in tropical regions of Africa found no significant association between extreme rainfalls and incidence of respiratory diseases among children.11A study in India observed a significant reduction in risk of all cause mortality during the first week after extreme rainfall.2Similarly, research conducted in rural areas of Ecuador found that extreme rainfall events were associated with a decreased incidence of diarrhoea after wet periods.57These findings align with our observations that some levels of precipitation may not only be non-detrimental but could also offer certain protective effects.
Biological mechanisms
Although the biological mechanisms linking rainfall intensity with health outcomes are not fully elucidated, several plausible explanations may be helpful to clarify the complex association observed in our study. Firstly, moderate to high intensity rainfall may exert protective effects through two primary mechanisms: Improvement in air quality (rainfall can decrease the concentration of PM2.5particles in the atmosphere,58potentially mitigating its adverse effects on all cause, respiratory, and cardiovascular mortality), and behavioural changes (rainfall may alter people’s daily patterns, leading to more time spent indoors).59This reduction in direct exposure to outdoor air pollution and non-optimal temperatures could explain the protective effect observed on various health outcomes when intensity is not extreme. Secondly, as rainfall intensity increases, the initial protective effects may be affected by a cascade of negative impacts: Critical disruptions to resources, physiological impacts, and indirect effects.
Critical resource disruptions—Intense rainfall can cause major disruptions to healthcare access. It might damage infrastructure, leading to power outages and hindering essential medical services, both of which can particularly affect older people and those with chronic illnesses, who often require consistent access to drugs and healthcare.60Intense rainfall can compromise water and food quality by allowing pathogenic microorganisms to reproduce and spread,61leading to diarrhoeal diseases, infections, and associated mortality.6263
Physiological effects—Increased humidity levels facilitate the growth of airborne pathogens,64potentially triggering allergic reactions and respiratory problems, particularly in vulnerable individuals.65Rapid shifts in atmospheric pressure can reduce the partial pressure of oxygen in the body,6667leading to cardiovascular and respiratory complications.60Furthermore, major temperature fluctuations often accompanying rainfall days can affect both respiratory and cardiovascular systems.68
Indirect effects—Extreme rainfall can have profound effects on mental health, inducing stress and anxiety that may exacerbate pre-existing mental health conditions69and indirectly contributing to increased total mortality from non-external causes. Overall, these pathways elucidate the complex associations between rainfall intensity and health outcomes observed in our study. Moderate rainfall (PE2<100%) shows protective effects, whereas intense rainfall (PE2>100%) leads to important harmful effects as negative factors accumulate. Supplementary analysis of the five year return period threshold also corroborated these findings, showing a weaker protective effect and a rapid transition to harmful impacts beyond 50% of the threshold rather than 100% as seen with PE2(see supplementary figure S7). Moreover, the rapid onset of these negative factors during extreme rainfall days explained the observed lag pattern (see supplementary figure S2), with health effects most pronounced on the first day and then gradually diminishing.
We found several factors that could influence the variability in the health effects of extreme rainfalls. Notably, the associations were modified by climate types and baseline variations in rainfall intensity. Many studies have identified an association between a region’s resilience to extreme climate events and its natural conditions.7071Areas with stable rainfall patterns, such as arid regions or areas with low variability in rainfall, often have weaker adaptive capacities for heavy rainfall. A recent study suggested that the risk of rainfall associated infectious diseases is more pronounced in arid areas, likely due to the more major environmental changes caused by brief heavy downpours.72Furthermore, many studies have highlighted a negative correlation between per capita gross domestic product and mortality resulting from extreme climate events.7374This finding suggests that socioeconomic development enhances urban resilience and the ability to respond rapidly to extreme rainfall related disasters. Conversely, unemployment and social inequality can exacerbate health problems during such events. It has also been noted that the effect of extreme rainfall is influenced by vegetation coverage, with higher vegetation coverage correlating with greater adaptive capacity for managing the consequences of extreme rainfall.75In contrast, we found no significant effect modifications linked to annual average precipitation and population density. Greater cumulative rainfall does not necessarily equate to a higher frequency of extreme rain events. Adaptation to extreme rainfall is most noticeable in areas with more frequent and variable rainfalls.76Additionally, population density does not indicate socioeconomic conditions or residential environments,77and thus it does not serve as a reliable predictor of resilience to disasters. Regional variations in the effects of extreme rainfall can be attributed to several factors, in addition to the varying adequacy of identified total events. In east Asia, higher intensity of extreme rainfall events (fig 2) coupled with rapid urbanisation and reduced green space coverage (see supplementary figure S5) could increase overall vulnerability. In North America, varying green space coverage (see supplementary figure S5) combined with ageing infrastructure, inadequate updates to drainage and water management systems, and uneven distribution of resources, are important explanatory factors for the observed effects.78In South America, major effects likely arise from limited access to medical resources.79
Policy implications
Our findings have some practical implications. Policy makers should be aware of the implications of rainfall related risks on public health, encompassing both non-communicable and infectious diseases. Hospitals and clinicians need vigilance when monitoring and managing patients during the rainy season, and researchers need to investigate the exact mechanisms that could then be targeted by systematic protective action. Considering the health risks associated with the modification of rainfall by local climatic conditions and vegetation coverage, localised or health education initiatives centred on communities would be imperative to increase awareness about the risks to health of severe rainfalls, and increasing vegetation coverage is a reasonable adaptation strategy. A multidisciplinary approach and integrated strategies are essential to navigate public health interventions to cope with extreme rainfall. This necessitates not only the fortification of meteorological observation and early warning systems but also the implementation of robust measures across urban planning and social security sectors to bolster the resilience of communities and individuals against disasters.
Limitations of this study
This study has some limitations. Firstly, although the analysis included 34 major counties or regions on six continents, the analysed locations were mainly located in east Asia, Europe, and North America, with fewer in Latin America and Africa. This limitation restricts the global representativeness of the estimated risks. Furthermore, the study areas were all urban centres, therefore the associations between extreme rainfall and mortality might differ in rural areas, and we only used mortality statistics from nearly 700 cities globally. Although our results provide a broad, population level perspective, they do not capture granular details of individual factors such as age, sex, race, urban/rural residence, or specific clinical settings. In addition, this study could be limited by its ecological study design as with most previous epidemiological studies on the impact of climate related events.2680As such, this study was unable to analyse specific personal factors, interventions, or clinic based practices. A more granular study focusing on individual features or clinical settings would be a valuable next step in this line of research. Secondly, the accuracy of our assessment outcomes may be influenced by potential exposure misclassification arising from our reliance on modelled output precipitation data.81This issue might further be compounded by the limited spatial resolution inherent in the rainfall databases employed. We incorporated a diverse array of rainfall databases to sensitively test our findings. Thirdly, in a global study spanning several decades, diagnostic or coding errors in health data may have occurred. The effect of these errors on our results is likely to be minimal, however, given most data used in this study were from urban centres and from government data sources. Finally, our analysis captured only a part of the health effects of heavy rainfalls. It notably lacked data such as hospital admission records and health outcome data from rural areas and excluded data on external causes of mortality. These limitations highlight the need for further comprehensive studies to evaluate a broader range of health effects associated with extreme rainfall events.
Conclusions
Our multicountry analysis provided epidemiological evidence for associations between heavy rainfall events and mortality","Objective: To examine the associations between characteristics of daily rainfall (intensity, duration, and frequency) and all cause, cardiovascular, and respiratory mortality.
Design: Two stage time series analysis.
Setting: 645 locations across 34 countries or regions.
Population: Daily mortality data, comprising a total of 109 954 744 all cause, 31 164 161 cardiovascular, and 11 817 278 respiratory deaths from 1980 to 2020.
Main outcome measure: Association between daily mortality and rainfall events with return periods (the expected average time between occurrences of an extreme event of a certain magnitude) of one year, two years, and five years, with a 14 day lag period. A continuous relative intensity index was used to generate intensity-response curves to estimate mortality risks at a global scale.
Results: During the study period, a total of 50 913 rainfall events with a one year return period, 8362 events with a two year return period, and 3301 events with a five year return period were identified. A day of extreme rainfall with a five year return period was significantly associated with increased daily all cause, cardiovascular, and respiratory mortality, with cumulative relative risks across 0-14 lag days of 1.08 (95% confidence interval 1.05 to 1.11), 1.05 (1.02 to 1.08), and 1.29 (1.19 to 1.39), respectively. Rainfall events with a two year return period were associated with respiratory mortality only, whereas no significant associations were found for events with a one year return period. Non-linear analysis revealed protective effects (relative risk <1) with moderate-heavy rainfall events, shifting to adverse effects (relative risk >1) with extreme intensities. Additionally, mortality risks from extreme rainfall events appeared to be modified by climate type, baseline variability in rainfall, and vegetation coverage, whereas the moderating effects of population density and income level were not significant. Locations with lower variability of baseline rainfall or scarce vegetation coverage showed higher risks.
Conclusion: Daily rainfall intensity is associated with varying health effects, with extreme events linked to an increasing relative risk for all cause, cardiovascular, and respiratory mortality. The observed associations varied with local climate and urban infrastructure.
"
Strategies and tactics to reduce the impact of healthcare on climate change,"Introduction
The direct and indirect human health impacts of climate change have been well documented over the past two decades. The 2015 Paris Agreement and subsequent research have painstakingly established the association between planetary and human health.1234However, until recently, less attention has been directed towards the impact healthcare systems have on climate change. This issue is gaining momentum, spurred on by the implementation of the Paris Agreement and the introduction of the UK National Health Service (NHS) net zero carbon strategy.56
In essence, healthcare systems occupy a special place; although they are on the frontlines in dealing with climate induced demand for healthcare, they are also major emitters.78Healthcare systems must therefore address a predicament: maintenance of high quality care with the resilience and capacity to respond to escalating climate induced demands; and mitigation of their own, substantial contributions to the climate crisis. The action required to manage the complexities of the environmental and societal costs of delivering health services is not trivial. Healthcare systems, notably hospital facilities, are energy intensive, high consumption organisations that produce considerable quantities of waste.891011Although there is an irrefutable duty of care to patients and a fundamental commitment to do no harm at the point of delivery, healthcare systems have until recently remained largely unrecognised contributors to the climate crisis.12However, healthcare systems are well positioned as environmental stewards to get their own house in order and promote benefits—lowering the collective carbon footprint while simultaneously improving long term health by reducing low value care.
That said, healthcare systems currently lag behind other service sectors in reducing carbon emissions.613To be compliant with the Paris Agreement and contribute to the Intergovernmental Panel on Climate Change’s target of limiting global warming to 1.5°C above preindustrial levels, healthcare must play its part, aiming for net zero carbon emissions by 2050.131415There is a need to act in a swift and decisive manner because humanity has a limited window to achieve significant progress in reducing carbon emissions or irreversible changes will exceed the boundaries of the planet.16
Most estimates place average global emissions from healthcare at about 4.4%, with a country range of 4-8.5%.15As confronting as these numbers are, it is possible they will get worse before they get better.17More frequent large scale events stimulated by climate change (eg, bushfires, floods, cyclones, heatwaves, and other weather sequelae) will require more carbon inducing care, placing additional, often overwhelming loads on overstretched healthcare systems grappling to contend with the current high burden of chronic diseases, non-communicable diseases (such as cardiovascular diseases), communicable diseases, constant introduction of new interventions and technologies as we discover new ways to manage disease, and ageing populations.10111819With slow progress on reduction strategies, increased demand on healthcare systems in turn generates further environmental impacts in a vicious cycle (fig 1).
Against that backdrop, we sought to examine the quality and quantity of global evidence on ways in which healthcare systems contribute to climate change and the proposed and implemented ways of reducing the effects of healthcare systems on the climate.Table 1presents our key terms.
Methods
Search strategy and selection criteria
In this review, we assessed the quality and quantity of evidence on ways in which healthcare systems contribute to climate change and the approaches, models, and tools available to decarbonise healthcare systems. The review was prospectively registered on PROSPERO (CRD42022383719) and reported in accordance with the preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines (supplementary materials 1 and 2).35
The search strategy was designed by the review team in conjunction with a research librarian, and run across eight electronic databases. Seven of these databases were searched from their respective inception dates to November 2023: Business Source Premier, CINAHL, Cochrane Reviews, Embase, Health Business (EBSCO), Medline, and Web of Science. Scopus was searched from 1990 to November 2023. Supplementary material 3 gives more details on the search terms.
Publications were eligible if they were full text articles in peer reviewed journals. All study designs (eg, case studies, reviews) were included. Reports from authoritative international agencies (eg, World Health Organization, World Bank, United Nations, and Organisation for Economic Co-operation and Development) or government reports at a federal or national level were also eligible if assessed to be of suitable quality. A primary focus on the effects of human healthcare systems on climate change was required. Articles were excluded if they were not available in English, were not published as full papers, or did not have a primary focus on healthcare systems’ effects on or contribution to climate change. Supplementary material 4 presents full inclusion and exclusion criteria.
Records identified in the database search were imported into Rayyan, a screening software tool,36and duplicates removed. For inter-rater reliability, pilot screening of titles and abstracts was conducted on a set of 200 randomly selected publications and any resulting discrepancies were resolved by the group. In our original review protocol, it was planned that six investigators would screen records in pairs. However, given the large volume of literature identified in the search, 15 investigators screened records in pairs. The full set of titles and abstracts retrieved from searches were then independently double screened for inclusion by investigators working in teams of two or three (EL, LE, EM, GD, CLS, AC, KB-C, SW, GF, RP, LP, HRA, SS, CR, AP). The full texts of the remaining publications were then duplicate screened against the inclusion criteria to ensure consistency in the final set of included texts. Supplementary material 5 presents reasons for exclusion during the full text review. Regular meetings took place to resolve any disagreements by consensus at each step of the process, with JB available as arbiter.
Data analysis
Data extraction and quality appraisal were conducted independently by investigators, and verified during synthesis processes. Four quality appraisal tools were applied to assess the quality of included publications: Joanna Briggs Institute critical appraisal checklist for systematic reviews and research synthesis37; Mixed-Methods Appraisal Tool (MMAT)38; Scale for the quality Assessment of Narrative Reviews (SANRA)39; and Authority, Accuracy, Coverage, Objectivity, Date, Significance (AACODS) tool for evaluation and critical appraisal of grey literature.40After initial appraisal, the quality of each article was assigned to one of four categories: critically low, low, moderate, high. Articles determined to be of critically low quality were excluded from analysis; supplementary materials 6-10 provide further details.
A purpose built data extraction workbook was developed in Microsoft Excel. Data extraction variables included publication details (author, title, year, country of publication), healthcare system level (micro, meso, macro), healthcare system sector (eg, primary care, surgery, mental health), and measurement of climate impact (eg, carbon footprint). A thematic analysis of the extracted data was performed using an inductive approach to identify themes emerging from the literature.41One reviewer (SW) first familiarised themselves with all extracted data, and then created a set of six initial broad themes and 24 subthemes using inductive thematic analysis.41A subgroup of five reviewers (CLS, KBC, GD, SW, EL) discussed and revised the initial coding of themes to 12 themes. The whole authorship team conducted a review of these broad themes for content and accuracy, and the final nine themes were then reviewed by the senior author (JB). Once the nine broad themes had been developed, a subgroup of reviewers (CLS, KBC, GD, SW, EL, EM) met to code the data within each broad theme into appropriate subthemes, again using an inductive thematic analysis. Two broad categories were also developed to frame the themes as avenues of enacting change (overarching strategies), or specific approaches for reducing GHG emissions in healthcare systems (decarbonisation tactics). The whole authorship team reviewed and approved all categories and subthemes, which were finalised by the senior author (JB). Supplementary materials 11-13 provide details.
Patient and public involvement
This research did not focus on any specific patient population. Although no patients were directly involved in setting the research question or conducting the review, we ensure we have healthcare consumers’ input, expertise and advice on the progress of our research at the intersection of climate change, health, and healthcare, which feeds into and shapes all our work in this area. Healthcare consumers are an integral part of our results dissemination strategy.
Results
Study selection and quality assessment
Electronic database searches identified 33 737 publications, 15 793 of which were duplicates, leaving 17 944 publications for title and abstract screening; 739 studies met the inclusion criteria for full text screening. Of these, 28 were excluded because they could not be retrieved, leaving 711 texts to be screened (fig 2). Two additional papers were identified, screened, and included through backward citation tracking.
Most of the articles in this review were appraised as high (n=112, 54.6%) or moderate (n=80, 39.0%) quality. Only 13 articles (6.3%) were identified as lower quality but were included because they provided valuable information for the review. Seven studies were considered ineligible based on critically low quality appraisal ratings (supplementary materials 6-10). After full text screening and quality appraisal, 205 publications were included (fig 2).
Study characteristics
Included publications were categorised by study design (table 2): empirical studies; reviews including narrative, scoping, and systematic; non-empirical “narrative descriptive” publications (eg, case studies, reports); and those applying multiple methods. Studies were also categorised based on the level of the healthcare system to which they pertained: micro, meso, or macro (table 2; seetable 1for full definitions).
High income countries were over-represented in the included publications (n=101, 69.2%, excluding reviews). The most commonly discussed country was the United States (n=27, 13.2%), followed by the UK (n=21, 10.2%), Australia (n=17, 8.3%), and Canada (n=14, 6.8%).Figure 3depicts the study setting by country. Many publications (n=30, 14.6%) focused on the macro level without discussing a specific country or region.64243444546474849505152535455565758596061626364656667686970
Most included records were published in recent years (fig 4), with 85.4% (n=175) from 2019 onwards, indicating the growing interest in this topic.
A wide range of healthcare related topics were discussed, including surgery (n=36)242751547172737475767778798081828384858687888990919293949596979899100101102; internal medicine specialties (ie, renal services, gynaecological services; n=18)235775103104105106107108109110111112113114115116117; digital health (n=15)596671118119120121122123124125126127128129; primary care (n=15)53566375130131132133134135136137138139140; hospitals (n=14)8292141142143144145146147148149150151152; supply chains (eg, procurement; n=10)4244153154155156157158159160; oncology (n=8)4858161162163164165166; pharmaceuticals (n=7)4547167168169170171; diagnostics (n=6)52172173174175176; chronic condition treatment (n=5)2861177178179; workforce (n=5)180181182183184; pathology services (n=5)75174185186187; dentistry (n=4)60188189190; critical care (n=3)90191192; clinical trials (n=2)67193; emergency medical services (n=2)26194; mental health services (n=2)195196; outpatient care (n=2)197198; screening (n=2)199200; travel related to doctors’ medical education (n=1)201; and palliative care (n=1).202Fifty papers examined the system as a whole, taking a macro perspective.671119254346495055626465686970203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236
Twenty nine (14.1%) publications specifically used terminology that refers to GHG emissions according to three emission scopes defined by the Greenhouse Gas Protocol237(direct emissions, indirect emissions from energy use, indirect supply chain emissions).7274445474850556568698185125142152166172184193197200202207209219224229232Table 3presents definitions of the three scopes.
Forty eight publications developed or applied specific models or tools to measure healthcare emissions; some publications used several models or tools (table 4). Models included life cycle assessment (LCA; n=27),607684104107111119122128130141142155162173174175179186190193200209212213223231economic input-output LCA (n=2),197212LCA multiregion input-output models (n=1),25environmentally extended input-output models (n=6),7112477213232environmentally extended multiregional input-output model (n=4),4344231232and the Bilan Carbone model (n=1).166Tools used in the literature included carbon calculators (n=6),52101115121165176Carbon Trust recommendations (n=1),26the Eyefficiency tool (n=1),89the Pollard model (n=1),188the Ringelmann smoke chart (n=1),157and the US Environmental Protection Agency’s waste reduction model (n=1).144Other studies (n=16) used publicly available datasets (eg, UK Department of Business, Energy, and Industrial Strategy conversion factor data; environmental impact data provided by the German Federal Environment Agency), or previously published literature to quantify GHG emissions in their facilities.74839499110113123126129148161162163168178202
Key themes emerging from the literature
From the thematic analysis, nine themes emerged across the included publications, which were then grouped into one of two categories: overarching strategies (table 5) or decarbonisation tactics (table 6). The most frequently discussed overarching strategy was the need for effective policies and governance surrounding healthcare sustainability (n=97) and the most common decarbonisation tactic was changing clinical and surgical practices (n=107). Papers discussing numerous themes or subthemes were included in several categories.

Overarching strategies
The need for policies and incentives to promote, guide, monitor, or evaluate changes at the country, state, and facility level was discussed in 97 publications671119232542434445464748495051555657585961636465676869707173767988899596103104105109111112114115117120123124125127128130133136138142143151152154159162166168170171173179180184188191192193200201203205206207208209210212213216217218219220221224231232233234; 39.2% of the publications discussing this theme were empirical studies. The most common strategy focused on priority setting and leadership from national and international bodies (n=51), such as integrating sustainability into policy or setting long term sustainability goals across organisations.67111943465051565759636465677076799596104105109111114115120125127128130133138143152154166170184188206209210216217218220221231232234This was followed by publications (n=27) that suggested mandating evaluations, benchmarking, and reporting of the carbon footprint for healthcare organisations to strengthen the evidence base for future decision making.67234555586168707388124136142151159168173179205207208212213216219221An equal number of publications (n=21 each) highlighted using financial policy and incentives for sustainable healthcare processes6424344484970738996123133138143162171180200221224233and the need for a multisectoral approach to mitigate the carbon footprint of healthcare,7235669738996103112117162191192201203205210217220221233such as collaborations between large healthcare organisations. Fifteen publications recommended setting targeted emissions reductions.711192545464758707189136193218219
Organisational change to develop a future ready workforce was dealt with in 76 publications.2425274648495057586365686970717375777980818386919399102103105110112114116127130133135136137138142143150151152154165166171172174178179180181182185187191192195196199202205206207210212216220221228233234235Roughly 30.3% of the publications discussing this theme were empirical studies. Ongoing education and training within healthcare organisations and tertiary institutions was identified in 59 publications as a way of ensuring the workforce is aware of and equipped for sustainability challenges.24254849505763656869737577798183869199102103110112114116127130133135137138142143150151152165166171172174178179180185187192195196202205206207210212220228233235Publications discussing organisational leadership focused on implementing multidisciplinary teams, “green champions,” and other organisational role models to manage sustainability at different levels of the healthcare system and empower employee decision making (n=26).274658637071778093105127133135136152154165181182185191196199216221234
The literature outlined several important roles for individuals and groups in reducing healthcare’s carbon footprint (n=74)7192843484950515356575859616364656667697173818586888990919698102103107108112114116117127130133134135137139142143151160162168171177180181183184192195196210212215216218219220221226229232233236; 20.3% of the publications discussing this theme were empirical studies. Promoting change through advocacy, lobbying, and open support of sustainability initiatives was the focus of 45 publications,74849505153586164656667718185868889909698103107114116117127133134135139142151171180181183184192195196216218232233including the need for community involvement and education when enhancing the sustainability of healthcare practices (n=21).4353568190103112130133151160168192210212218221229232233236Several publications emphasised individual responsibility as a key factor in making change (n=15), particularly through engaging with low carbon healthcare and living an active lifestyle.5759637381899196103143160162192220233Additionally, 15 articles identified benefits of reducing GHGs in the health system, which included financial savings and reducing disease burden.1928506996102107108112137177212215219226
The impetus to develop, use, and standardise tools for measuring and monitoring GHG emissions was a prominent theme (n=70 publications).67111926434445474851585961626668697173757779848896103104105107109112114124125137139142146148149154163164171177188192193194200205207208209211212216217218219221222224225227228230231234Nearly 37.1% of the publications discussing this theme were empirical studies. Evaluative tools, along with existing models such as LCA, can be used to improve decision making on procurement and services, and was a feature of 43 publications.64347585962666873757779848896104105107109112124137139142146149154164192200205208211212216217218224225227230231234Developing tools to measure GHG emissions and monitor the movement of materials was discussed in 25 publications.7111926434469103124125146171177188193194207209211219221222225227228Several publications (n=17) emphasised the need for standardised platforms for reporting and comparing the sustainability of products or processes to enable healthcare organisations to make informed decisions about their current practices.726454748516171107114148154163177207211219
Decarbonisation tactics
Changing clinical and surgical practices as a solution for reducing healthcare’s carbon footprint was advocated in 107 publications67112324252728424348505152545557616366686971737475777879818283848586879091939496979899100101102103106107108109112113114116120126128129130133135138139142143148149151152159161163165166167168169170171173174175177178179183185186189190191192196205212214217224227229230232233234236; 37.4% of the publications discussing this theme were empirical studies. This solution included replacing high GHG practices with lower GHG options (n=80),1123272843485457616368717374757778798182838485868790919394969798100102103106107112113114120126128129133135138139148149152159161163165166167168170171173174177178179183189190192196214217224227229230232233234236such as choosing anaesthetic gas types and systems with lower carbon footprints.5471738183858696100229This advocation was also seen in primary care settings (n=6).63130133135138139As a way of minimising healthcare usage, and thus reducing healthcare’s carbon footprint, 41 publications reinforced the need to assess clinical practices for low value care.67242842515255666869757779939799101109116130142149151152169174175177179185186190191192205212214224229233This included identifying and reducing unnecessary processes or procedures, minimising drug overprescription, or encouraging a preventative care approach to reduce the need for health services. Reducing water usage during procedures, disinfection and sterilisations, laundry, recycling water, and practising water efficiency were also suggested in six publications.232550108143189
Dealing with waste was a focus of 83 publications112543474849505154555760616367727377788081828485868889919395969798100101102104105107110112114116135138142143144149150151152153154155157159160169171177183189190191192198199206210211217218219222223224228229232233234236; 32.5% of the publications discussing this theme were empirical studies. This theme included waste minimisation methods (n=71),1125434748495051545557606163677273777880828485889195969798100101102104105107110112116135138142143144149151152154155159160169171177183189190191192198206211217218219222224228229232233234such as reducing, reusing, and recycling medical equipment, and using reusable alternatives over disposable items. Several publications highlighted the need for effective segregation and disposal of waste (n=29),505773778081828688899395107114143144150151153157160177183199206210223232236including choosing disposal methods such as incineration that help minimise carbon emissions.
Patient or clinical travel and transportation was emphasised as an important and addressable source of carbon emissions in 70 publications.67244347485155596163697173747984858693949596105109114117119120121122124125126127128131132135137140142148152156161163165166171172174176188190191192193195196197201202217224226228229232233Some 42.9% of the publications discussing this theme were empirical studies. Using carbon efficient models of care was seen as a method to reduce emissions (n=41), for example, telemedicine or home based treatment options.6244347485159637173747984949596105109119121122124125126128132135148161163165171172176188191195196217228229A further 25 publications discussed alternative forms of transportation (eg, car sharing, buses, trains, and active travel such as cycling) for patients and clinicians.747556979105114121131137140142152161166174176190192197202224226232233Streamlining medical services, such as emergency service coordination, redesigning schedules for clinicians to enhance ride share and closing sites with lower use, was discussed in 14 publications.61798693127131132148156161166176188193Several papers (n=13) also highlighted reducing professionals’ travel (eg, clinicians, researchers, industry partners) for conferences and education as a decarbonisation method.4347516385117120171191195201224228
This theme had the most implementation evaluations. These studies often highlighted reductions in carbon emissions resulting from teleconferencing or telemedicine,122163176and they also tended to be of high quality.
Sixty three papers recommended improvements to healthcare infrastructure2627434750555758596163697273808184889293969798108109112114117118135141142143145146147151152160161171174175184192193194195202206207210211212218221224226228229232234235; 33.3% of the publications discussing this theme were empirical studies. Improving the energy efficiency of healthcare facilities such as hospitals was a feature of 40 publications.475055576163697273808192939798109112135141142143146147152160171192193194207210211212218221224229232234235For example, consideration of design features such as insulation and passive shading options can help reduce reliance on electrical heating and cooling, and therefore reduce GHG emissions. Thirty four publications advocated for using energy conservation measures—such as turning off appliances when not in use—to help minimise carbon emissions at a facility level.26274357585961637384889697108114117118135143145151152161174175184194195202206221226228229
Forty eight papers proposed decarbonising the supply chains as a way of reducing GHG emissions.7252642434447505455565965738485878998104105110112114127136139142143151152158171175183193194197204210212215218224229232234235Roughly 39.6% of the publications discussing this theme were empirical studies. Numerous publications (n=43) focused on ways to reduce the carbon footprint by deploying sustainable production and procurement mechanisms within supply chains, such as using sustainable or local sources of food supply, or using supply chains with low emissions.7254243444750545556596573848598104105110112114127136139142143151152158171175183193197210212215218224229232234235Decarbonising the sources of energy for healthcare by using renewable sources rather than fossil fuel sources and monitoring energy grid consumption was noted in 10 publications.726508789158194204212232
Discussion
Key areas for policy and practice
From our findings we developed a rich picture of the strategies and tactics for reducing the emissions produced by healthcare systems (fig 5). This schematic figure is not an exhaustive example, but rather illustrates how these strategies and tactics to reduce emissions produced by healthcare systems are interconnected and affect flow within and outside of the healthcare system. Authorisation to proceed starts at the top, with effective policies, governance, and high level leadership. Measurement of GHGs through recognised and increasingly sophisticated tools is necessary, and now more pronounced, as is the use of tools and frameworks to report on progress, and make decisions about alternative, greener options. Tools and frameworks for use at micro, meso, and macro levels are now more widely available. Several options for individuals and groups to orchestrate change and improvement, typically targeted at organisations such as hospitals, care facilities for older people, and general practices, have been generated.
On decarbonisation specifically, attention was focused on modifying medical, surgical, and other clinical practices, reducing physical waste, and minimising transportation costs or eliminating travel altogether through new clinical models such as virtual care, telehealth, or telemedicine.245963819497120121126165172196When enacting decarbonisation tactics that affect patient care, it is vital that lower carbon alternatives provide equal or improved patient outcomes compared with current practices and be acceptable to and supported by patients.105Other specific activities for reducing carbon emissions and greening healthcare, directly or in those services indirectly controlled by healthcare systems, include building new or remediating existing healthcare infrastructure and reducing the carbon footprint along healthcare supply chains. Given the large contribution of supply chain emissions and the high potential for reducing emissions,238it is surprising that supply chain decarbonisation approaches, including those related to food supply, were less frequently discussed than others. Another surprising lack of attention was discussion at the intersection of universal health coverage and healthcare system decarbonisation because reducing the need for care through primary prevention would have a direct impact on carbon emissions by lowering demand.49140
A way forward
Overall, a framework of useful activities to substantially reduce the carbon budget of healthcare is now available, and represented infigure 5. Although this framework provides a platform to assess these activities, determining which elements would be most effective, and under what conditions, will partly depend on the characteristics of the individual healthcare system.96Barriers to implementing sustainable practice into healthcare systems might also exist, including patients accepting changes, cost and funding mechanisms, attitudes towards change, and workforce capacity in an already overburdened system. As these decarbonisation approaches are put into practice, any barriers and potential enablers should be explored further.
The included literature was mainly from high income countries. However, we found some examples of lower-middle income country healthcare systems and organisations taking the lead in advancing the overarching strategies identified, such as developing tools to measure emissions (eg, the Aga Khan Health Services (AKHS) freely available tool),44and the decarbonisation tactics, such as reducing physical waste and emissions from waste disposal (eg, Snigdha and colleagues)158and improving the environmental sustainability of clinical practices.111156199Some of the decarbonisation tactics might be more challenging to implement in lower-middle income country healthcare systems, such as strengthening infrastructure.96
Regardless of the setting, having an overarching set of mechanisms and approaches might be useful and highlights the need for a multipronged approach, including how we measure emissions and progress, especially given the range of measuring tools that have emerged. The actions include effective governance and supportive policies, appropriate financing, and strengthening infrastructure and service delivery.20Collaborative, international, multisectoral leadership is needed to learn from progress elsewhere and to localise and prioritise strategies and support direct actions.
Achieving a step change reduction in emissions is a socially and ethically responsible imperative. In principle, it is feasible and beneficial to roll out these strategies and tactics at scale to achieve systemic change—this is central to the mission of healthcare providers: to do no or less harm, to alleviate suffering, and to improve the health status of the population. Healthcare can set an example for other industries and sectors as responsible environmental stewards. This review suggests that healthcare systems and professionals have recognised their duty and are embracing the necessary changes, but there is a long way to go.
Strengths and limitations
The comprehensive analysis of 18 years of studies, frameworks, and tools that assess and quantify the carbon footprint of healthcare is a key strength of the paper, as is the development of a new framework. Previous reviews did not appear to focus sufficiently on mitigation strategies or use the systematic review methodology.
Our review was limited to papers and authoritative reports written in English, and so some papers might have been missed. However, eight databases were searched, therefore the likelihood of new themes being identified is low. Most of the included literature discussed healthcare systems in high income countries, which is a well known bias. This bias can create challenges when extrapolating identified strategies to low-middle income countries. However, the included papers revealed that healthcare systems in low-middle income countries are undertaking similar strategies to high income countries. A review specifically focused on healthcare systems in low-middle income countries, and the ethical implications of climate change and decarbonisation tactics, is warranted. Inductive approaches, as in this study, allow flexibility and deeper understanding of the data, but introduce issues of inter-rater reliability and bias. Our review used 19 investigators for screening, extraction, synthesis, and interpretation to minimise bias.
Conclusions
We reviewed the literature to generate strategies and tactics for reducing the impact of healthcare systems on climate change. Implementing these strategies should enable substantial progress towards reducing healthcare’s carbon footprint and support a greener health sector powered by renewable sources. However, healthcare systems would need to adapt these approaches to their context and needs to maximise their effects. Healthcare can lead the way in shiftin","Objective: To review the international literature and assess the ways healthcare systems are mitigating and can mitigate their carbon footprint, which is currently estimated to be more than 4.4% of global emissions.
Design: Systematic review of empirical studies and grey literature to examine how healthcare services and institutions are limiting their greenhouse gas (GHG) emissions.
Data sources: Eight databases and authoritative reports were searched from inception dates to November 2023.
Eligibility criteria for selecting studies: Teams of investigators screened relevant publications against the inclusion criteria (eg, in English; discussed impact of healthcare systems on climate change), applying four quality appraisal tools, and results are reported in accordance with PRISMA (preferred reporting items for systematic reviews and meta-analyses).
Results: Of 33 737 publications identified, 32 998 (97.8%) were excluded after title and abstract screening; 536 (72.5%) of the remaining publications were excluded after full text review. Two additional papers were identified, screened, and included through backward citation tracking. The 205 included studies applied empirical (n=88, 42.9%), review (n=60, 29.3%), narrative descriptive (n=53, 25.9%), and multiple (n=4, 2.0%) methods. More than half of the publications (51.5%) addressed the macro level of the healthcare system. Nine themes were identified using inductive analysis: changing clinical and surgical practices (n=107); enacting policies and governance (n=97); managing physical waste (n=83); changing organisational behaviour (n=76); actions of individuals and groups (eg, advocacy, community involvement; n=74); minimising travel and transportation (n=70); using tools for measuring GHG emissions (n=70); reducing emissions related to infrastructure (n=63); and decarbonising the supply chain (n=48).
Conclusions: Publications presented various strategies and tactics to reduce GHG emissions. These included changing clinical and surgical practices; using policies such as benchmarking and reporting at a facility level, and financial levers to reduce emissions from procurement; reducing physical waste; changing organisational culture through workforce training; supporting education on the benefits of decarbonisation; and involving patients in care planning. Numerous tools and frameworks were presented for measuring GHG emissions, but implementation and evaluation of the sustainability of initiatives were largely missing. At the macro level, decarbonisation approaches focused on energy grid emissions, infrastructure efficiency, and reducing supply chain emissions, including those from agriculture and supply of food products. Decarbonisation mechanisms at the micro and meso system levels ranged from reducing low value care, to choosing lower GHG options (eg, anaesthetic gases, rescue inhalers), to reducing travel. Based on these strategies and tactics, this study provides a framework to support the decarbonisation of healthcare systems.
Systematic review registration: PROSPERO: CRD42022383719.
"
Integrated mental health video consultations for people with depression or anxiety in primary care,"Introduction
Globally, depression and anxiety disorders are among the top leading causes of years lived with disability in both sexes.1Primary care physicians can effectively treat many patients with depression and anxiety, but some individuals require specialised mental health care. Despite the availability of effective treatment options, most people with depression and anxiety disorders do not have access to specialised mental health care.23This issue is primarily linked to the fact that services are not available, do not have capacity, or are unaffordable.4Especially in rural and remote areas, people are hindered by transportation barriers and refrain from seeking help because of widespread stigma.5Moreover, the ageing population entails an increasing number of individuals dealing with multiple health conditions, including mental disorders. These challenges require increased coordination of long term integrated care for complex multimorbidity.
According to the World Health Organization, primary care settings are considered the most suitable setting for treating patients with mental disorders.6Most patients, even those with severe and chronic conditions, are treated solely by general practitioners (GPs) who provide effective care.7However, a large number of individuals affected by depression and anxiety disorders are not identified, do not receive adequate treatment, or are simply in urgent need of specialised treatment.8Most GPs prioritise providing holistic care for both physical symptoms and psychological distress. However, the so-called somatising effect, whereby mental or emotional factors can manifest in physical conditions, can lead healthcare professionals within primary care settings to focus more on evaluating and addressing physical symptoms.910
In recent years, the integration of mental health care into primary care settings has been pursued.11One major approach features a referral model that involves the colocation of on-site mental health specialists in primary care settings. In this approach, GPs refer distressed patients to mental health specialists who assume the primary responsibility for the psychosocial management of the presenting problem.12In 2020, the results from a systematic review of 15 studies showed that co-located specialty care was associated with mental health benefits, and concluded that more rigorous randomised controlled trials are needed.13Recently, virtual colocation (ie, live interactive videoconferencing) has been proposed to enrich integrated primary care.14The limited number of published randomised controlled trials to date were conducted in highly regulated environments, such as the US Veterans Health Care Administration, or involved patients from inpatient facilities.1516The potential scalability of these models to primary care settings, particularly in countries where smaller, single handed, or rural and remote practices dominate, remains uncertain.
To explore this potential, we previously developed and successfully piloted a scalable integrated mental health video consultation model, designed to implemented in primary care settings. This model, which targets patients with depression and anxiety, allows mental health specialists to be virtually embedded in primary care through co-located specialty care.17The aim of this assessor masked randomised controlled trial was to investigate the effectiveness of this new mental health service model for treating people with depression or anxiety, or both, in primary care settings. We hypothesised that people randomly assigned to receive the intervention would lead to greater reductions in symptoms of depression and anxiety at six months compared with those assigned to receive treatment as usual.
Methods
Trial design and setting
This study was a multicentre, stratified, assessor masked, parallel group, randomised controlled trial (the PROVIDE-C trial) with 1:1 allocation of patients and conducted in Germany. We recruited patients from 29 primary care practices in the federal states of Baden-Wuerttemberg and Rhineland-Palatinate (overall population 15.2 million; overall area size 55 605 km2). In Germany, GPs receive regionally negotiated fee-for-service payments from sickness funds up to a maximum number of services per quarter. Typically, no gatekeeping process exists, meaning that patients can directly access the services of a GP without prior registration (ie, a free access system). Although covered by all sickness funds, video consultations are not common, however, they increased during and after the covid-19 pandemic, that is, after commencement of the trial. The trial was approved by the Medical Faculty of the University of Heidelberg Ethics Committee (S-923/2019), and its protocol is available online.18We reported the PROVIDE-C trial in accordance with the CONSORT 2010 statement.
Participants and recruitment
Patients were eligible if they had at least one of the following mental health conditions: (1) at least moderately severe depression, defined as a patient health questionnaire-9 (PHQ-9) score of 10 points or greater with item one or two being endorsed (5-9 mild, 10-14 moderate, 15-19 moderately severe, and 20-27 severe depressive symptoms); (2) at least moderately severe general anxiety, defined as a generalised anxiety disorder scale (GAD-7) score of 10 points or greater (5-9 mild, 10-14 moderate, 15-21 severe anxiety symptoms); or (3) a combined anxiety and depression score (patient health questionnaire anxiety and depression scale, PHQ-ADS) of 12 points or greater, had received no or insufficient treatment (psychotherapy, psychopharmacotherapy, or both); agreed to participate in the trial by providing written informed consent; were capable of providing consent, and were aged 18 years or older.
Patients were excluded if they (1) had substance misuse or dependence that was likely to compromise intervention adherence (identified during an unstructured assessment as part of the screening), (2) were acutely suicidal or put others at risk (PHQ-9 Item 9 endorsed and positive structured suicide screening result), (3) needed emergency medical treatment, such as, hospital admission (as assessed by the referring GP), (4) had acute psychotic symptoms, such as, persecutory delusions or thought insertion (identified during an unstructured assessment as part of the screening), (5) had severe cognitive impairment or dementia (as assessed by the referring GP), (6) had substantial hearing or visual impairment (as assessed by the referring GP), (7) were pregnant and in the second or third trimester (as assessed by the referring GP), (8) showed insufficient German language proficiency (identified during an unstructured assessment as part of the screening), (9) or had prior experience with video consultations through participation in the PROVIDE-B feasibility trial.17
GPs recruited patients during their regular clinic hours or by calling them at home. Based on their clinical judgement, the GPs selected individuals suspected of having depression or anxiety, introduced the trial to them, obtained consent from them for screening, and referred them to the study team for screening. We also instructed GPs via weekly reminders to review their electronic health records to recruit potentially eligible patients. Moreover, we shifted to the research staff in the trial centre all time consuming tasks (eg, assessing eligibility, explaining the trial in detail, addressing patients’ questions, obtaining consent, randomising patients, and collecting baseline data) that could have deterred GPs from recruiting patients with the routine consultations during busy times (eg, Monday mornings, holidays, and peak phases of the covid-19 pandemic).19We obtained signed informed consent forms from all participants before performing the baseline assessment.
Randomisation and masking
We collected baseline measurements immediately before randomisation. Eligible participants were then randomly assigned (1:1) to the intervention or control group via a secure web based randomisation system (Randomiser V.2.0.2) operated by a data manager who was not involved in patient recruitment, centrally at the Institute of Medical Biometry, Heidelberg University. The treatment sequence was a computer generated sequence of random numbers and was stratified by centre (primary care practice) and symptom severity at baseline as measured by the PHQ-ADS (three levels of symptom severity with scores of 10, 20, and 30 points indicating mild, moderate, and severe depression and (10-19 mild, 20-29 moderate, 30-48 severe)) using randomly permuted block sizes of 2 and 4. While the patients, GPs, and mental health specialists were aware of the intervention assignment after allocation, the data analysts were masked to the allocation. While the patients reported outcomes in assessments after baseline through computer assisted telephone interviews, we masked the interviewers who questioned the patients and completed the questionnaire on their behalf. Specifically, we ensured that these interviewers were not present when discussing individual patients and avoided mentioning any patient names or assigned treatments. When scheduling the interviews with the patients, research assistants who were not involved in conducting the interviews instructed patients not to mention to the interviewers which group they belonged to.
Procedures
The PROVIDE intervention aimed to reduce severity of depressive and anxiety symptoms by integrating specialised mental health care into primary care practices via video consultations.
We hypothesised that the PROVIDE intervention would increase in the (virtual) availability of mental health specialists, leverage patients’ familiarity with the primary care practice for treatment engagement, and draw on a transdiagnostic treatment approach that combined elements from problem solving treatment. Transdiagnostic treatment has been shown to yield moderate effects in alleviating depression and anxiety in primary care patients with a focus on building a strong working alliance. This approach has been promoted as a crucial element of manuals achieving high acceptability for both patients and clinicians.122021
The five core components of the PROVIDE intervention were as follows: video consultations for primary care patients conducted by mental health specialists; five consultations over a period of approximately eight weeks; diagnostic clarification and case formulation plus stepped care, based on interim symptom monitoring using the PHQ-ADS after the third consultation; brief psychotherapy focused on therapeutic alliance and affect expression and regulation; and case supervision in a biweekly group format, led by a senior consultant specialising in psychiatry and psychosomatic medicine.2223
Following a stage model of psychotherapy manual development, we initially created a stage I intervention manual that outlined the treatment techniques, goals, and format for the PROVIDE-B feasibility trial.2425For the PROVIDE-C trial, we refined this manual into a full stage II intervention manual (supplementary material 1).
We provided the primary care practices with widescreen (12.3 inch) computer tablets and a handbook outlining the trial, its procedures, and feasible contingency plans in case of connectivity failures; mental health specialists were provided with the stage II intervention manual.
In the first consultation, the mental health specialist began by establishing a strong working alliance by inviting the patient to talk freely about the central problem and trying to understand the patient’s concerns and symptoms. To derive a case formulation, the mental health specialist also gathered diagnostic information, such as through probing or applying established psychometric practises. The mental health specialist informed the patient about their condition (depression or anxiety) and the available treatment options. In the second consultation, the mental health specialist facilitated affect experience or expression by introducing the concept of emotional mindfulness and encouraging the patient to practise it between consultations.26In the third consultation, the mental health specialist maximised their effort in supporting the patient in experiencing and expressing their (avoided) affects. Then, the mental health specialist aimed to link the patient’s narrative and the related affects to the central problem and, more importantly, to more adaptive responses. After the third consultation, the study team determined the patient’s PHQ-ADS score to monitor their progress, and the results were sent to the mental health specialist. In the fourth consultation, the mental health specialist and the patient reflected on the symptom severity as indicated by the PHQ-ADS score and the need for prolonged care, that is, the need for referral to more intensive specialised treatment. In the final session, the mental health specialist and the patient reviewed the treatment process and developed the next steps, which, if needed, may have included a recommendation for additional intensive specialised treatment. After the final session, the mental health specialist compiled a one page case summary that comprised recommendations for the GP on how to proceed with the patient’s care.
The intervention was delivered by 22 mental health specialists (21 psychologists in advanced psychotherapy training and one physician with a licence for practising psychotherapy) trained in the PROVIDE model. To standardise the intervention, the physician was not allowed to prescribe medication directly. While all mental health specialists were allowed to discuss psychopharmacological treatment with the GPs, the initiation of psychopharmacological treatment was at the GPs’ discretion. Before their first video consultation, all mental health specialists received a three hour introductory training session on the trial procedures, the intervention manual, and the videoconferencing platform.
The intervention was delivered through individual, synchronous one-to-one video consultations conducted via an encrypted, web based videoconferencing platform on a subscription basis (arztkonsultation ak GmbH, Schwerin, Germany,https://arztkonsultation.de). The patient was in a room of the primary care practice designated for the video consultations to ensure confidentiality, while the mental health specialist was at an offsite location.
The five consultations lasted 50 min each and were held at biweekly intervals over a period of approximately eight weeks. The consultations happened at fixed time slots, on which the primary care practice staff and the mental health specialists agreed.
The mental health specialists were aware of technical limitations such as non-muting microphones, poor visual definition, impaired audio, and speech delay. When the broadband connection was unstable, or the quality of the audio was poor, the mental health specialists switched to a phone call to avoid misinterpretations and frustration with the technology. For acute crises (eg, suicidal ideation, medical emergency, or violence), the mental health specialists had the contact information of the patients’ significant others (those physically close to the patient) on hand and contacted the nearest safe and emergency care locations.
We modified the design after the trial commenced: patients at an increased risk of covid-19 complications in the intervention group received their video consultations during their lockdown period at home.
We evaluated intervention fidelity, that is, the degree to which core components were delivered by providers and understood by participants as planned. Specifically, we computed a predefined intervention integrity score by applying the criteria specified in the statistical analysis plan (supplementary material S2) and the trial protocol.18Moreover, we determined the proportion of video consultations that were conducted as planned.
Treatment as usual arm
For patients allocated to the control group, treatment as usual involved usual care provided by their GP. This may or may not have included interventions such as brief counselling, the prescription of psychotropic medication, and referrals to mental health specialists such as office based psychiatrists and psychotherapists or mental health clinics. Patients in the treatment as usual group needed to make appointments with their GP ad hoc. In both groups, participants were permitted to continue any treatment they had been receiving at the time of enrolment in the trial.
Outcomes
The primary outcome was the absolute change in the mean severity of depressive and anxiety symptoms measured using the PHQ-ADS from the baseline assessment to six months after the baseline assessment. For the sake of comparability, we selected six months because this timepoint is one of the most common for evaluating primary outcomes in integrated mental health care trials.15162728The PHQ-ADS is a 16-item scale (all nine items of the PHQ-9 and all seven items of the GAD-7, scored from 0=not at all to 3=nearly every day). Higher scores indicated more severe symptoms. The PHQ-ADS is a psychometrically validated measure used in primary care settings.29It has shown effectiveness and sensitivity as an outcome measure in treatment trials, with a recommended minimal important difference of 3 to 5 points.30In the multicentre PROVIDE-C trial, outcomes were centrally assessed, including the PHQ-ADS scores.
Secondary outcomes included differences in the absolute changes in mean severity of depressive and anxiety symptoms on the PHQ-ADS at 12 months between the two groups. At six and 12 months, we also investigated: absolute changes in the mean severity of depressive (using PHQ-9) and anxiety (using GAD-7) symptoms; mean score for psychological distress related to somatic symptoms (somatic symptom disorder-B criteria scale, SSD-12); mean score on the personal confidence and hope, goal, and success orientation, willingness to ask for help, reliance on others, and no domination by symptoms subscales (five subscales of the recovery assessment scale, RAS-G); health related quality of life (12 item short-form health survey, SF-12); and quality and patient centredness of chronic illness care score (patient assessment of chronic illness care-short form, PACIC-short form). At six and 12 months, we also evaluated differences in health service use between the groups by applying the questionnaire for the assessment of medical and non-medical resource use in mental disorders. However, findings on health service use and cost effectiveness, based on the scores on the EuroQol 5 dimension 5 level (EQ-5D-L), will be reported in a separate publication. We collected information about harmful outcomes from all randomly assigned participants. Patients, GPs, and mental health specialists in both groups had to immediately report harmful outcomes including severe adverse events, defined as life threatening and fatal events (eg, suicide attempts, death by suicide, and reported violence), to the trial team. For more detailed insights into possible adverse consequences of the intervention, we systematically assessed prespecified harmful outcomes in the intervention group by applying the inventory for the assessment of negative effects of psychotherapy during follow-up visits. Finally, we conducted a cost effectiveness analysis, which we will report in a separate publication that will include the trial registration number and findings on health service use.
Data collection and retention management
We collected participant data from the intervention and control arms at baseline immediately before randomisation. Follow-up measurements were conducted at six and 12 months after the baseline assessment. Specifically, we conducted computer assisted telephone interviews during which patients reported outcomes on validated questionnaires. We captured these outcomes by applying an online survey tool (the Enterprise Feedback Suite Survey, Questback GmbH). Retention management followed a standard operating procedure: we scheduled interviews for follow-up measurements contacting participants up to eight times. We then reminded participants about the scheduled interview via text message (SMS) 24 hours before the anticipated interview. If no interviews could be scheduled or conducted, we sent the questionnaires together with a paid return envelope to the people who had not responded and attempted to call them eight more times.
Sample size
To detect the minimal clinically important difference in the PHQ-ADS score of 3 points (SD 9 points) with a two sided 5% significance level and a power of 80%, a sample size of 160 patients per group was necessary. This size adjusted for the correlation between the baseline value and the change from the baseline value (r=0.35) and given an anticipated dropout rate of 20%. We expected an 18 month inclusion period to recruit these patients.
Statistical analysis
Initially, we compared sociodemographic and medical characteristics of the eligible patients enrolled in the trial to the eligible patients who declined to participate or with whom we lost contact. Specifically, we did a logistic regression using trial participation status (yes/no) as the dependent variable.
The primary analysis used data from the intention-to-treat population, which included all patients in the group to which they were allocated by randomisation. We investigated the missing-at-random assumption using a description of the covariates grouped by missing data on at least one PHQ-ADS item at six months versus no missing data (ie, a second baseline table, a method to identify the potential for bias due to missing data is to compare participants with and without missing values). Missing data for the primary and secondary endpoints were replaced using multiple imputation at the item level (10 imputations with a maximum of 20 iterations per imputation and a fixed seed). Before analysis, assumptions for mixed linear modelling (normality of residuals, linearity, homogeneity, and extreme outliers) were evaluated graphically. We analysed the primary outcome with a mixed linear model, where a random intercept accounted for the primary care practice to which the patient belonged. The patient specific baseline variables, which are established predictors for symptom severity in primary care, were included as fixed effects in the model: age, gender, presence of a chronic physical disease, physical health (SF-12 physical component score), history of depression or anxiety, baseline PHQ-ADS score, trial group, and the number of days between the baseline assessment and randomisation.31The treatment effect was quantified by the parameter estimate of the group together with the respective 95% Wald confidence interval. We supplemented the findings for the primary outcome with analyses of the intention-to-treat complete case dataset (missing data not imputed), the per protocol dataset (data from participants who followed the protocol, excluding their data if they do not adhere), the as treated dataset (considering the treatment actually received by the participant, without regard to adherence to their randomisation assignment). Additionally, we analysed a sensitivity dataset, which was a subset of the as treated dataset for which any form of psychological treatment in the control group did not lead to the exclusion of the patient from the analysis. During the review process for this paper, after the protocol was developed, we decided to report findings for both the primary and the secondary outcomes from the minimally adjusted model adjusting only for primary care practice and the baseline PHQ-ADS score. We decided to perform a responder analysis comparing the proportion of participants within each study arm who had a change at least as large as the minimal clinically important difference as part of the secondary analyses.32Specifically, we calculated the multiple imputation pooled estimator for the difference between the two study arms in terms of the percentage of patients who met the minimal clinically important difference. The analyses of the secondary endpoints were exploratory and were conducted analogously to those of the primary endpoint. To assess the consistency of the observed effects across subgroups of patients defined by baseline characteristics, we performed nine prespecified subgroup analyses based on marital status, education level, employment status, psychiatric treatment or psychotherapy status at baseline, history of psychiatric treatment or psychotherapy, psychopharmacological treatment status at baseline, history of psychopharmacological treatment, willingness to accept psychotherapy, and willingness to accept psychopharmacological treatment. For all subgroup analyses, we incorporated an interaction term between group and subgroup in the mixed linear regression models to analyse the primary endpoint. However, the trial was not powered to detect subgroup differences.
We prespecified the analyses in a statistical analysis plan before database lock (supplementary material S2). The analyses were performed using R 4.4.0 or higher. We registered the trial with ClinicalTrials.gov,NCT04316572, on 20 March 2020, before any participant was recruited.
Patient and public involvement
During the planning phase of the study, we involved two patient representatives (one female, one male) who had participated in video consultations in the PROVIDE-B feasibility trial. Specifically, the patient representatives participated in the conceptualisation of the trial procedures and materials. The selection of the outcome measures was informed by the representatives’ priorities and experiences. They also revised the draft versions of this study protocol and all trial materials including information sheets, consent materials, and questionnaire sets with respect to clarity and understanding from the service user perspective. The patient representatives were not involved in the plans for participant recruitment. We continuously discussed the progress of the trial with these two patient representatives. Both patient representatives were compensated for their expenses. We will report the burden of the intervention on the patients’ quality of life and health in a separate publication on the qualitative process evaluation of the trial.
Results
Sample description
Overall, we invited 3471 GPs to participate in the trial. Fifty three eligible GPs followed up our invitation, while the reasons for non-response among the others is unknown. We included 29 GPs in the trial, while the remaining 24 were not included given that in the meantime the target sample was reached. Between 24 March 2020, and 23 November 2021, GPs referred 620 potential participants to the trial and 536 were identified as potentially eligible. A total of 160 patients were excluded after the screening interview and baseline assessment (fig 1).
The eligibility fraction, which was the proportion of potential participants who underwent screening (including people who were referred but could not be screened) and were eligible for enrolment, was 76% (471/620). The enrolment fraction, which was the proportion of people who were eligible for participation and were enrolled, was 80% (376/471). The recruitment fraction, which was as the proportion of potential participants who enrolled (61% (376/620)), and the number of patients needed to be screened (1.6(620/376)). In total, 376 participants were enrolled from 29 primary care practices and were randomly assigned to receive the integrated mental health video consultation model (n=187) or treatment as usual (n=189). Per general practitioner, a median of 10 patients were enrolled (range 1-58).
Among all 471 eligible patients, patients who were single were less likely to participate in the trial than people in a partnership (P=0.02). Whether eligible patients participated in the trial was not predicted by other sociodemographic and medical characteristics (age, gender, education level, employment status, degree of managing with the available income, chronic physical disease status, patient knowledge of local mental health services, symptom severity score (Patient Health Questionnaire Anxiety and Depression Scale), and the probability of post-traumatic stress disorder (primary care PTSD screen for the Diagnostic and Statistical Manual of Mental Disorders, fifth edition). Given that we were committed to including all eligible patients who were referred to the trial but still waiting to be enrolled when we reached our planned sample size, the final sample size slightly exceeded the planned sample size of 320. Of the 376 participants, 238 (63%) participants were female. The mean age was 45 years (SD 14; range 18-81). A total of 220 (59%) participants had at least one chronic physical disease. At baseline, 183 (49%) participants had never received any psychiatric treatment or psychotherapy. A total of 153 (41%) of the 376 participants had a symptom of moderate severity and 139 (37%) had a symptom of severe severity. The sociodemographic and medical characteristics were similar between the intervention group and the control group (table 1).
Intervention fidelity
Concerning intervention fidelity, 906 video consultations were completed in which 169 (90%) of 187 participants in the intervention group completed all five planned consultations. In 82 (9%) of these 906 video consultations, the patient was located at home due to being at an increased risk for covid-19 complications. Participants received a median of five consultations (range 0-5). In the intervention group, 172 (92%) participants were regarded as adherent based on the intervention integrity score. During the intervention period, 18 (10%) participants in the intervention group stopped treatment. Regarding potential contamination bias, we identified nine (5%) participants in the control group who had received some form of video consultation during the intervention period. Based on the description of the covariates grouped by missing data for at least one PHQ-ADS item at six months versus no missing data, we retained the missing at random assumption and proceeded with multiple imputation (supplementary material S3).
Primary outcome
At six months, 155 (83%) of 187 participants in the intervention group and 133 (70%) of 189 participants in the control group were followed up. Specifically, we found a significant difference in the proportion of participants with missing data for at least one PHQ-ADS item at six months between the intervention group and the control group (P=0.01). Unintentional unmasking of outcome assessors occurred during 20% (59/288) of all interviews at six months. For the primary outcome, the mean change in the PHQ-ADS score was −9.2 points (95% CI −10.7 to −7.7) in the intervention group and −7.1 points (−8.8 to −5.4) in the control group (adjusted mean change difference −2.4 points (−4.5 to −0.4), P=0.02). The effect size (Cohen’s d) was 0.21 (95% CI 0.03 to 0.39). No significant association between gender and the primary outcome was noted (P=0.95). Supplementary material S4 presents the parameter estimates calculated with the fully adjusted mixed effect model for the primary outcome based on the intention-to-treat dataset. Findings for the analyses based on the intention-to-treat complete case dataset (missing data not imputed), the per protocol dataset, the as treated dataset, and the sensitivity dataset did not indicate that the treatment effect was due to selection because they did not yield any significant differences between the trial groups.Table 2presents findings for all analysis types applied to the primary outcome. The responder analysis compared the proportion of participants within each study arm who had a change at least as large as the minimal clinically important difference at six months. Our results showed that the proportion of participants in the intervention group who had an improvement at least as large as the minimal clinically important difference (median of 70.1%, range 69.0-72.7% for all 10 imputed datasets) was 6.8% ((95% CI −4.5 to 18.1), P=0.23), which was greater than that in the control group (62.7%, range 60.8-67.2%). The intraclass correlation coefficient for the primary care practice as centre was 0.01.
Secondary outcomes
Follow-up results at 12 months were available for 146 (78%) of the 187 participants in the intervention group and 135 (71%) of the 189 participants in the control group. At 12 months, the mean change in the PHQ-ADS score (minimally adjusted model) was −11.0 points (95% CI −12.6 to −9.4) in the intervention grou","Objective: To evaluate whether an integrated mental health video consultation approach (PROVIDE model) can improve symptoms compared with usual care in adults with depression and anxiety disorders attending primary care.
Design: Assessor masked, multicentre, randomised controlled trial (PROVIDE-C).
Setting: In 29 primary care practices in Germany, working remotely online from one trial hub.
Participants: 376 adults (18-81 years) who presented to their general practitioner (GP) with depression or anxiety, or both.
Intervention: Participants were randomised (1:1) to receive the PROVIDE model (n=187) or usual care (n=189). Usual care was provided by GPs through interventions such as brief counselling and psychotropic medication prescriptions and may or may not have included referrals to mental health specialists. The PROVIDE model comprised transdiagnostic treatment provided through five real-time video sessions between the patient at the primary care practice and a mental health specialist at an offsite location.
Main outcome measures: The primary outcome was the absolute change in the mean severity of depressive and anxiety symptoms measured using the patient health questionnaire anxiety and depression scale (PHQ-ADS) at six months, in the intention-to-treat population. Secondary outcomes, measured at six and 12 months, included PHQ-ADS subscores, psychological distress related to somatic symptoms, recovery, health related quality of life, quality and patient centredness of chronic illness care, and adverse events.
Results: Between 24 March 2020 and 23 November 2021, 376 patients were randomised into treatment groups. Mean age was 45 years (standard deviation (SD) 14), 63% of the participants were female, and mean PHQ-ADS-score was 26 points (SD 7.6). Compared with usual care, the PROVIDE intervention led to improvements in severity of depressive and anxiety symptom (adjusted mean change difference in the PHQ-ADS score −2.4 points (95% confidence interval −4.5 to −0.4), P=0.02) at six months. The effects were sustained at 12 months (−2.9 (−5.0 to −0.7), P<0.01). No serious adverse events were reported in either group.
Conclusions: Through relatively low intensity treatment, the PROVIDE model led to a decrease in depressive and anxiety symptoms with small effects in the short and long term. Depression and anxiety disorders are prevalent and therefore the small effect might cumulatively impact on population health in this population.
Trial registration: ClinicalTrials.govNCT04316572.
"
Heterogeneous effects of Medicaid coverage on cardiovascular risk factors,"Introduction
Many countries aim to have financially sustainable universal health coverage by expanding public insurance to cover their population.1Ample evidence shows that health insurance coverage improves financial risk protection and mental health, but its effect on physical health is less well understood. Several randomized controlled trials have investigated insurance coverage in the United States of America. The RAND health insurance experiment was conducted in the 1970s-80s with the primary aim of studying the price elasticity of demand for healthcare services and implications for health outcomes, but the effect of having health insurance itself was not studied.2More recently, another study evaluated the effect of health insurance (including Medicaid) on mortality using randomized outreach by the Internal Revenue Service encouraging individuals to take up insurance coverage.3Over the two years of follow-up, they observed a reduction in mortality for people enrolling in health insurance, but used administrative data that did not have a range of individual level health outcomes.
The Oregon health insurance experiment was launched in 2008 and examined the effects of Medicaid (a public health insurance programme for low income individuals) coverage on a wide range of outcomes, including healthcare use, mental and physical health outcomes, and financial strain.45The research design allocated a limited number of Medicaid slots to low income adults using a lottery system. The results showed improvements in access to care and outcomes, including depression, but showed, on average, no evidence that Medicaid coverage improved physical health, including cardiovascular risk factors such as blood pressure and hemoglobin A1c(HbA1c) concentrations.5Some studies using observational or quasi-experimental designs have found that Medicaid coverage is associated with an improved health status, including lower risk of mortality, but such studies are subject to confounding factors and omitted variable bias.678The randomized controlled trial design used in the Oregon health insurance experiment eliminated such biases. However, some subgroups in the Oregon health insurance experiment might have had an improvement in cardiovascular risk factors, while the average treatment effect was diluted by other subgroups who did not benefit from Medicaid coverage. The absence of detectable effects from the average of the results might include clinically meaningful effects that are present only in subgroups of the studied population.
Recent rapid advancements in machine learning techniques have enabled nuanced estimation of how treatment effects vary based on individuals’ observable characteristics, so-called heterogeneous treatment effects.91011Conventional stratified analyses split the sample on the basis of a small set of stratifying variables and test whether the interaction term between the exposure variable and stratifying variable is statistically significant. However, these novel techniques can identify complex heterogeneous effects across many potentially intertwined variables that are not shown by the conventional stratified analysis.12Although prior Oregon health insurance experiment studies did not find heterogeneous treatment effects in the average effects of Medicaid coverage on cardiovascular risk factors based on a limited number of variables,513heterogeneous effects might be identified when the complex interplay of numerous covariates is accounted for. By examining such heterogeneity across subgroups, this study seeks to provide a more nuanced understanding of how Medicaid coverage might influence cardiovascular risk factors.
In this context, this study applies recently developed machine learning based methods to assess whether a subpopulation can be identified for whom Medicaid coverage substantially improves health outcomes. Using data from the Oregon health insurance experiment, we assessed the heterogeneity in the effect of Medicaid coverage on health outcomes, such as systolic blood pressure and HbA1c. By applying the machine learning causal forest algorithm, we delineated the characteristics of individuals with high and low predicted health benefits from Medicaid coverage. We then evaluated the effect of Medicaid coverage on blood pressure and HbA1cfor people predicted to benefit highly, compared with the effect of coverage for the population overall.
Materials and methods
The Medicaid programme in the United States
Health insurance coverage in the United States is available in multiple forms. More than half of Americans are covered by private health insurance, with public programmes such as Medicare and Medicaid covering much of the remaining population, but almost 10 percent of the population remaining uninsured. Medicare is a federal programme that provides health coverage for individuals aged 65 years or older and younger people with disabilities. Medicaid is a joint programme between the federal and state governments that offers health coverage primarily to low income individuals. The programme covers a wide array of healthcare services, including inpatient care, outpatient visits, and prescription medications, although some variations exist across states. As of 2023, nearly 80 million people were enrolled in the Medicaid programme in the United States.
Study sample
We examined data from the Oregon health insurance experiment, which is a randomized controlled study investigating the effect of Medicaid coverage in the state of Oregon, USA.45This study leveraged the random assignment of access to Medicaid insurance coverage in 2008 for low income adults (defined as less than the federal poverty line5) in Oregon who were uninsured. Additional details on the study design are documented elsewhere.45Individuals randomly selected from a waitlist for the programme were permitted to apply for Medicaid. In-person interviews and biometric data were collected from the treatment (selected in the lottery) and control (not selected) groups between 31 August 2009 and 13 October 2010. This survey included questions on medical service usage, health insurance status, and medication details. In addition, blood pressure measurements and blood samples were collected from participants. Across a total of 12 229 participants who responded to the survey (effective response rate, 73%), this study included 12 134 individuals with whose outcome data were available. The protocol for this study was approved by the institutional review board at University of California, Los Angeles, USA (institutional review board number 24-000623). The Oregon health insurance experiment has received approvals from several institutional review boards, and all participants provided written consent during the in-person survey. The Oregon health insurance experiment was registered at the American Economic Association’s registry for randomized controlled trials (registration number AEARCTR-0000028).
Study variables
We used whether an individual was randomly selected to apply for Medicaid coverage as an instrumental variable to estimate the local average treatment effect of Medicaid coverage on blood pressure and HbA1cmeasured at the in-person survey. This local effect corresponded to the average treatment effect for individuals who were able to enrol in Medicaid through the lottery assignment. After a 5 minute sitting period, blood pressures were measured three times, 30 seconds apart, and the average was calculated. HbA1cwas measured from blood samples collected in the in-person survey.14More details are described in the Oregon health insurance experiment protocol.45
The pretreatment variables collected by self-report in the Oregon health insurance experiment included age (years), sex (female or male), race and ethnic group (Hispanic, non-Hispanic black, non-Hispanic white, or other (Asians, Native Hawaiian or Pacific Islander, and other)), education status (less than high school, high school or general educational development, or college or above), and diagnoses before the lottery (hypertension, diabetes, high cholesterol, asthma, heart attack, congestive heart failure, emphysema/chronic obstructive pulmonary disease, kidney failure, cancer, and depression). Data regarding prior healthcare charges (ie, total charges and emergency department charges) were sourced from individual visit records during the period before randomization period from 1 January 2007 to 9 March 2008. More details have previously been published.15Missing data for these covariates at baseline were imputed using a random forest approach.16
Statistical analyses
We built the causal forest algorithm with an instrumental variable regression (ie, instrumental variable forests;instrumental_forestfunction ingrfpackage in R) to evaluate the heterogeneity in the treatment effect of Medicaid coverage on systolic blood pressure and HbA1c.17In the IV forests, we estimated the conditional local average treatment effect for each individualiby taking a ratio of two weighted averages. To calculate the weights, we used a data driven method designed to give more weight to observations with similar treatment effects while avoiding overfitting. We drew 2000 subsamples of the data, and we further divided each subsample into two parts. In the first part of each subsample, we constructed a partition of the data based on observable baseline characteristics. The partition was selected to maximize heterogeneity in the conditional local average treatment effect across elements of the partition. In the second part of each subsample, we recorded which observations were assigned to the same element of the partition that would be assigned to observationi’s baseline characteristics. The weight a given observation received in the final estimation of the conditional local average treatment effect for individualiwas equal to the number of times that the observation fell in the second part of a subsample and was assigned to the same element of the partition as individuali. This sample splitting approach to constructing weights ensured that the outcome and treatment assignments of one unit were not used to determine how much that unit was weighted in estimating the conditional local average treatment effect at a particulari. The conditional local average treatment effect for an individualican be interpreted as the local average treatment effect of Medicaid coverage on systolic blood pressure or HbA1cconditional on baseline characteristics for each individual. In the context of this study, it represented what would be the expected change in an individual's blood pressure and HbA1cafter a year if they enrolled in the Medicaid programme. That is, where the expectation was for individuals withi’s observable baseline characteristics and with (potentially unobservable) characteristics that would lead them to choose to enrol in Medicaid under the expanded eligibility.
To assess the calibration of our instrumental variable forest model, individuals were categorized on the basis of fifths of the predicted conditional local average treatment effect, and then local average treatment effect was estimated for each quintile group. When ranking individuals on the basis of estimated conditional local average treatment effect, we applied a cross fitting approach.18In this approach, we ranked individualiin foldkto a quintile (or decile) on the basis of estimated conditional local average treatment effect for based on data from all folds other thank(ie, τ^{–k(i)}(Xi), where {–k(i)} represents the instrumental variable forest model that was fit with other folds), and then estimated local average treatment effect for the assigned ranking group using data from foldk. By using this approach, we did not use the same data to determine how individuals are ranked and to evaluate the difference across the ranking groups. Ideally, a well calibrated instrumental variable forest model should yield a plot where the group specific local average treatment effects consistently increase in alignment with the conditional local average treatment effect quintiles. We assessed the variable importance by calculating a weighted total of its occurrences at every depth in the instrumental variable forest. More details on the instrumental variable forest are shown in supplementary method and elsewhere.17
Then, we estimated the individualized treatment effect (ie, conditional local average treatment effect) of Medicaid coverage on systolic blood pressure and HbA1cusing the instrumental variable forest model. We used the estimated conditional local average treatment effect to categorize individuals into two groups for each of the two outcomes, systolic blood pressure and HbA1c. People who were estimated to have a high benefit (conditional local average treatment effect <0) and people estimated to have a low benefit (conditional local average treatment effect ≥0). After comparing the demographic characteristics of the high and low benefit groups,19we calculated local average treatment effect among individuals in the high benefit group and compared them with the local average treatment effect among the overall population. In addition, we plotted the local average treatment effect according to the cumulative fraction of the participants according to the predicted conditional local average treatment effect.
Additional analyses
We conducted six additional analyses. (1) To test whether our findings could be affected by individuals who were already treated for hypertension and diabetes before the study began, we reanalysed the data restricting to people with no diagnosis of hypertension or diabetes at baseline. (2) We compared average treatment effect in the high benefit group with average treatment effect in the overall population using intention-to-treat analysis. (3) We calculated the effect of Medicaid coverage on healthcare use (the number of prescription drugs and office visits in the past year) among individuals in the high benefit group and the overall population. (4) We repeated the analysis to assess the heterogeneity in the effect of Medicaid coverage on diastolic blood pressure and total cholesterol concentrations. (5) To understand the characteristics of individuals who enrolled in the Medicaid programme, we fitted a logistic regression model to predict Medicaid enrolment (the “take-up”) among individuals who won the lottery. We then compared the baseline characteristics according to the quintiles of the predicted take-up rates derived from this regression model. (6) We evaluated whether individuals who benefited from Medicaid coverage had limited access to healthcare at baseline, by calculating counterfactual charges (ie, counterfactual charges if individuals were covered by the Medicaid programme and had access to healthcare) for each individual. To do so, we first fitted the zero inflated negative binomial model to predict total charges during the study period using the data restricted to individuals with Medicaid coverage (ie, individuals who won the lottery and enrolled in the Medicaid programme). Using this prediction model, we calculated the counterfactual charges for all individuals at baseline. We then compared the differences in predicted (counterfactual) versus observed total charges between the high benefit group versus others.
Local average treatment effects were estimated with instrumental variable regression, and their robust 95% confidence intervals (CIs) were obtained by repeating the analysis on 1000 bootstrapped samples. We adjusted all analyses for the number of household members on the lottery list as originally conducted because selection for the programme was random and conditional on household size. For each categorical variable (ie, race and ethnic group and education status), we created dummy variables and included them in the model. P values were adjusted for multiple comparisons using the Benjamini-Hochberg procedure.20All statistical analyses were conducted using R, version 4.1.1 (R Project for Statistical Computing).
Patient and public involvement
Our study was a post hoc analysis and did not include patients as study participants. The original Oregon health insurance experiment was done when patient and public involvement in research design was uncommon. Participants did not partake in shaping the research question, defining the outcome measures, or designing the study itself. No direct patient and public involvement was sought because the random assignment of access to Medicaid insurance coverage was not conducted for research purpose and the analysis required specialised training.
Results
A total of 12 134 individuals on low incomes met the inclusion criteria. Of these individuals, 6338 were assigned to the lottery group and 5796 to the control group. Baseline characteristics were balanced between the two groups (table 1). Results of the in-person interview survey showed mean systolic blood pressure was 119 mm Hg (standard deviation 17) and mean HbA1cwas 5.3% (0.6%). Medicaid coverage was not associated with significant changes in systolic blood pressure (−0.62 (95% CI −3.16 to 1.73), P=0.62) and HbA1c(0.00% (−0.10% to 0.10%), P=0.96).
Heterogeneity in the effect of Medicaid coverage on blood pressure and HbA1c
The instrumental variable forest models for systolic blood pressure and HbA1cwere well calibrated and showed significant heterogeneity in the effect of Medicaid coverage (supplementary figure A). In both models, age, sum of total charges, and sum of baseline charges relating to the emergency department were identified as important variables (defined by a weighted sum of the number of splits in the forest models) for heterogeneous treatment effects (supplementary figure B). Individuals predicted to benefit highly (ie, conditional local average treatment effect (<0)) for systolic blood pressure (n=9158) or HbA1c(n=7212) were less likely to have a history of hypertension diagnosis and had lower total and emergency department charges at baseline than those with lower predicted benefit (table 2, supplementary tables A and B). We also observed a higher prevalence of Hispanic individuals in the high benefit group than in the low benefit group. This distribution of racial and ethnic groups was at least partially attributable to the correlation between race and ethnicity and total charges at baseline; Hispanic individuals showed lower total charges at baseline than did other racial and ethnic groups (supplementary table C).
The effect of Medicaid coverage among individuals predicted to benefit highly
Individuals with high predicted benefit for systolic blood pressure showed a greater reduction in systolic blood pressure because of Medicaid coverage compared with the overall population (−4.96 mm Hgv−0.62 mm Hg, adjusted difference −4.34 mm Hg (95% CI −6.04 to −2.74), P<0.001 (table 3)). For this group, Medicaid coverage would also result in a greater average reduction in diastolic blood pressure (−3.91 mm Hgv−1.00 mm Hg, −2.91 mm Hg (−4.10 to −1.79), P<0.001).
Similarly, individuals with high predicted benefit for HbA1cshowed a slight reduction in HbA1cby Medicaid coverage compared with the overall population; however, the effect was not clinically meaningful (−0.12%v0.00%, adjusted difference −0.12% (95% CI −0.22% to−0.04%), P=0.009 (table 3).
When we computed the group specific effects for scenarios among individuals based on the estimated individualized effect (ie, conditional local average treatment effect) from the instrumental variable forest model, we found that Medicaid coverage led to greater reductions in systolic blood pressure and HbA1cfor individuals with a larger conditional local average treatment effect than it did for the overall population (fig, supplementary table D, supplementary figure C).
Additional six analyses
(1) Our findings remained qualitatively unaffected when we restricted our analysis to individuals without hypertension or diabetes diagnosis at baseline (supplementary table E). (2) The results were also consistent when we compared average treatment effect between the high benefit group for conditional local average treatment effect and the overall population using an intention-to-treat analysis (supplementary table F). (3) We found an increase in the number of prescription drugs and office visits in the past year by Medicaid coverage in both the high benefit group for systolic blood pressure and the overall population, but not in the high benefit group for HbA1c(supplementary table G). We noted no evidence that changes in prescription drugs and office visits differ between the overall population versus individuals in the high benefit group. (4) Consistent with our main results, we observed the heterogeneity in the effect of Medicaid coverage on diastolic blood pressure and total cholesterol concentrations (supplementary figure D), and individuals predicted to highly benefit for these outcomes were likely to have lower prior healthcare charges at baseline compared with others (supplementary tables H and I). (5) People with higher take-up rates were more likely to be female, non-Hispanic white, and have greater total and emergency department charges at baseline than those with lower take-up rates (supplementary table J). (6) The difference between the predicted (counterfactual) and observed total charges was larger for the high benefit group than for the low benefit group (US$3837 (£2951, €3464)v−$91; P<0.001), suggesting that individuals who benefit the most from Medicaid coverage were those who did not have access to healthcare before Medicaid coverage.
Discussion
Principal findings
In our post hoc analysis of Oregon health insurance experiment data using machine learning causal forest models, we found heterogeneity in the effects of Medicaid coverage on systolic blood pressure. While effects of Medicaid coverage on average, were not detected, some individuals showed improvements in systolic blood pressure from Medicaid coverage, and these individuals were likely to have no or low prior healthcare charges at baseline. A similar pattern was observed for HbA1c, but the estimated effects were smaller and not of clinical significance. These findings suggest that Medicaid coverage leads to improvement of blood pressure for some people, but the benefit for these people was diluted by individuals who did not benefit from Medicaid coverage in assessments of effects for the Oregon health insurance experiment study population overall.
Policy implications
Our findings suggest that null average effect of Medicaid coverage on cardiovascular risk factors, as observed in the original Oregon health insurance experiment study, may obscure significant benefits for some subgroups. In particular, we observed a clinically meaningful reduction in systolic blood pressure of approximately 5 mm Hg, which is large enough to lower the risk of health outcomes such as cardiovascular diseases and mortality, and equivalent to that achieved through lifestyle interventions.21This effect size is 10 times larger than the point estimate of the average treatment effect (−0.52 mm Hg) observed in the original Oregon health insurance experiment. The experiment’s result was not only statistically insignificant but also too small to be a clinically meaningful change, even if statistical significance was achieved with a larger sample size. A smaller change observed in HbA1cthan in blood pressure may be explained in part by the fact that only a small number of the study participants had increased HbA1cconcentrations (only 5.1% had HbA1cconcentrations of ≥6.5% and 3.3% had levels of ≥7.0%), compared with 16.3% of participants exhibiting elevated blood pressure. It is also important to note that our estimates had a wide confidence interval that might reflect the varied responses of individuals to improved healthcare access through Medicaid coverage, such as differences in medication adherence and follow-up visits.
Possible explanations
Several mechanisms through which Medicaid coverage could improve cardiovascular risk factors are potential. Firstly, insurance coverage facilitates access to healthcare, enabling beneficiaries to consult healthcare professionals, get beneficial care, and more easily adhere to prescribed treatments. This hypothesis is supported by the original Oregon health insurance experiment study findings that Medicaid coverage increased outpatient care use, rates of people admitted to hospital, and prescription medication usage by 15-35%.45Secondly, insurance coverage reduces out-of-pocket healthcare expenses,22which could allow beneficiaries to redirect their financial resources towards other health promoting activities, such as purchasing nutritious foods and engaging in physical exercise. Lastly, a greater sense of security provided by health coverage might reduce stress,5which may in turn improve physical health.2324
Individuals with larger predicted reductions in systolic blood pressure tended to have lower healthcare charges at baseline than did those with lower predicted health benefits. Although the exact underlying mechanisms are unclear, our findings suggest that individuals with low healthcare charges at baseline had limited access to healthcare before receiving Medicaid coverage, and therefore had a large health benefit with the increased access to care that came with Medicaid coverage. By contrast, individuals who had access to healthcare services before Medicaid coverage might not have changed their care patterns, and thereby the outcomes, as much.
Methodological implications
Harnessing recently developed methodological tools, we were able to detect heterogeneity in the effects of Medicaid coverage that had not been previously shown. Traditionally, randomized controlled trials, including the Oregon health insurance experiment, have assessed heterogeneity through stratifying analysis based on a priori hypothesis. However, traditional stratifying analysis does not consider complex functional forms or interaction effects among baseline characteristics when analysing how the effect of Medicaid coverage varies across individuals. Therefore, the original Oregon health insurance experiment did not identify clinically meaningful heterogeneous treatment effect.5By using the causal forest method in this post hoc analysis of Oregon health insurance experiment, our study is the first to identify subgroups (based on multiple characteristics) who had lower blood pressure associated with Medicaid coverage. We showed that these subgroups were likely to have lower healthcare charges before Medicaid coverage. Unfortunately, the sample size in this study is insufficient to answer questions about treatment effect heterogeneity for each baseline characteristic individually; rather, further prospective studies designed to assess treatment effect heterogeneity could elucidate on such questions.
Social perspectives
We found that although Medicaid coverage improved blood pressure across all racial and ethnic groups, the likelihood of enrolling in the Medicaid programme on eligibility (ie, the take-up rate) was lower among Hispanic individuals compared with non-Hispanic white individuals. This discrepancy could be due, for example, to language barriers or information barriers to applying, but further research is warranted to better understand the underlying mechanisms of this difference and to identify interventions that could mitigate potential barriers. It is also important to note that algorithm based approaches have the potential to exacerbate disparities if the training data are biased or used inappropriately.25
Strengths and limitations of this study
Our study has limitations. Firstly, the causal forest model evaluated heterogeneity based on measured covariates, and other unmeasured characteristics may also be important. Since we did not have baseline information on lifestyle factors such as smoking and alcohol intake, obesity status, mental health status, pregnancy, and family history of diseases, we did not assess heterogeneity based on these variables. Secondly, because baseline characteristics were self-reported, our findings might be affected by measurement error and misclassification bias, although these should not have differed between the treatment and control groups. Thirdly, our study participants had an average coverage duration of approximately 17 months, and patterns over a longer follow-up might differ.5Fourthly, we examined only limited health outcomes. Additional examination of heterogeneity in the effect of Medicaid coverage on other clinical outcomes (eg, cardiovascular disease, cancer, infectious disease, Alzheimer’s disease, and mortality) would be informative. Fifthly, although the Oregon health insurance experiment collected data for whether each participant lived in a metropolitan statistical area at baseline, we were not able to assess heterogeneity by such geographical locations because almost all participants in this study had a zip code of residence within a metropolitan statistical area. Sixthly, although we calculated the conditional local average treatment effects for each fold using an algorithm that excluded observations from that specific fold, future research should focus on exploring the uncertainties associated with these estimated conditional local average treatment effects. Our findings need to be validated in external databases, which would provide a more comprehensive understanding and ensure the robustness of our results. Moreover, because instrumental variable methods allow us to estimate the effect among compliers, our findings may not be generalizable to populations with different patterns of compliance with the intervention (ie, people who do not comply). Lastly, the insurance examined in this study was from the Medicaid programme, which is a public health insurance for low income individuals in the US. Our findings may not be generalizable to other types of insurance such as private insurance plans. We conducted this study using data from the state of Oregon in the United States, therefore, our findings may also not be generalizable to other states or countries.
Conclusions
Although Medicaid coverage did not improve cardiovascular risk factors on average, we found substantial heterogeneity in the effects within the study population. Individuals with high predicted benefits were more likely to have no or low prior healthcare charges at baseline, for example. Our findings suggest that expanding Medicaid coverage may lead to important health benefits for some identifiable subpopulations even when there is limited average benefit across the population overall.
Summary box
","Objectives: To investigate whether health insurance generated improvements in cardiovascular risk factors (blood pressure and hemoglobin A1c(HbA1c) levels) for identifiable subpopulations, and using machine learning to identify characteristics of people predicted to benefit highly.
Design: Secondary analysis of randomized controlled trial.
Setting: Medicaid insurance coverage in 2008 for adults on low incomes (defined as lower than the federal-defined poverty line) in Oregon who were uninsured.
Participants: 12 134 participants from the Oregon Health Insurance Experiment with in-person data for health outcomes for both treatment and control groups.
Interventions: Health insurance (Medicaid) coverage.
Main outcomes measures: The conditional local average treatment effects of Medicaid coverage on systolic blood pressure and HbA1cusing a machine learning causal forest algorithm (with instrumental variables). Characteristics of individuals with positive predicted benefits of Medicaid coverage based on the algorithm were compared with the characteristics of others. The effect of Medicaid coverage was calculated on blood pressure and HbA1camong individuals with high predicted benefits.
Results: In the in-person interview survey, mean systolic blood pressure was 119 (standard deviation 17) mm Hg and mean HbA1cconcentrations was 5.3% (standard deviation 0.6%). Our causal forest model showed heterogeneity in the effect of Medicaid coverage on systolic blood pressure and HbA1c. Individuals with lower baseline healthcare charges, for example, had higher predicted benefits from gaining Medicaid coverage. Medicaid coverage significantly lowered systolic blood pressure (−4.96 mm Hg (95% confidence interval −7.80 to −2.48)) for people predicted to benefit highly. HbA1cwas also significantly reduced by Medicaid coverage for people with high predicted benefits, but the size was not clinically meaningful (−0.12% (−0.25% to −0.01%)).
Conclusions: Although Medicaid coverage did not improve cardiovascular risk factors on average, substantial heterogeneity was noted in the effects within that population. Individuals with high predicted benefits were more likely to have no or low prior healthcare charges, for example. Our findings suggest that Medicaid coverage leads to improved cardiovascular risk factors for some, particularly for blood pressure, although those benefits may be diluted by individuals who did not experience benefits.
"
Comparative effects of drug interventions for the acute management of migraine episodes in adults,"Introduction
Migraine is a neurological disorder characterised by disabling, recurrent episodes of moderate to severe headache and accompanying symptoms lasting up to 72 hours.1Migraine affects more than one billion people worldwide and is the leading cause of disability in girls and women aged 15 to 49 years.2The burden of migraine extends to personal welfare, reduced productivity, and poor socioeconomic outcomes.3
The acute management of migraine episodes consists of drug interventions aimed at providing rapid and sustained pain relief, and, ideally, freedom from pain.4Several drugs with different mechanisms of action are available.1International clinical guidelines generally endorse non-steroidal anti-inflammatory drugs (NSAIDs) as initial treatment, whereas triptans are recommended for moderate to severe episodes or when the response to NSAIDs is insufficient.5678In recent years, lasmiditan and gepants have been introduced as further treatment options,1especially for patients with contraindications to triptans owing to potential vasoconstrictive effects and higher risk of ischaemic events.910However, no clear consensus exists as to which specific agents from these drug classes should be selected initially.
Given the wide range of drugs for acute treatment of migraine, clinicians and patients need robust evidence to make the best, individualised choice in routine practice. Network meta-analyses allow for estimation of comparative efficacy, providing a comprehensive summary of the evidence base and understanding of the relative merits of the multiple interventions.11Previous network meta-analyses, however, only compared a subset of available drugs.12131415161718192021As part of the AMADEUS (acute migraine attacks: different effects of individual drugs) “project,” we conducted a systematic review and network meta-analysis to compare licensed oral drugs for the acute treatment of migraine episodes in adults.
Methods
Information sources and eligibility criteria
Full details about the methods are reported in the protocol (see supplementary appendix 1), which has been registered in Open Science Framework (https://osf.io/kq3ys/). Our reporting of the study adhered to the guidelines outlined in the PRISMA (preferred reporting items for systematic reviews and meta-analyses) statement for systematic reviews incorporating network meta-analyses.22
We searched for published and unpublished studies in the Cochrane Central Register of Controlled Trials, Medline, Embase, ClinicalTrials.gov, EU Clinical Trials Register, WHO (World Health Organization) International Clinical Trials Registry Platform, as well as websites of regulatory agencies and pharmaceutical companies without language restrictions until 24 June 2023 (see supplementary appendix 2 for full search strategy).
We included double blind, randomised controlled trials comparing monotherapy using oral drugs with placebo or another eligible active treatment for the acute treatment of migraine episodes in adults (≥18 years). Participants were outpatients with a diagnosis of migraine according to the International Classification of Headache Disorders.23242526Only drugs and treatment dose ranges licensed for migraine or headache were considered eligible if they were recommended by at least one of the regulatory bodies internationally (also see supplementary appendix 3 and table S1): the British National Formulary (UK), the Federal Institute for Drugs and Medical Devices (Germany), the European Medicines Agency, the National Agency for the Safety of Medicines and Health Products (France), the Pharmaceuticals and Medical Devices Agency (Japan), the Therapeutic Goods Administration (Australia), and the US Food and Drug Administration (FDA). We did not include opiates as clinical guidelines discourage their use for migraine owing to limited efficacy, considerable adverse effects, and risk of dependency.46We excluded studies set in emergency departments as people attending these due to migraine usually represent a subgroup with particularly severe or atypical episodes.27
Pairs of researchers independently screened and selected the studies, reviewed published and unpublished reports, extracted data from the included trials, and assessed risk of bias.28Any discrepancies were resolved by discussion with the other members of the team.
Outcomes
We selected outcomes recommended by the International Headache Society.29The primary outcomes were the proportion of participants who were pain-free at two hours post-dose and the proportion of participants with sustained pain freedom from two to 24 hours post-dose, both without the use of rescue drugs.
Secondary outcomes included the proportion of participants with pain relief at two hours post-dose, the proportion with pain relapse within two to 48 hours post-dose, and the proportion using rescue drugs after two hours and up to 24 hours. We also investigated safety and tolerability, assessing the proportion of participants who experienced at least one serious adverse event and the proportion with at least one of 19 specific clinically relevant adverse events predefined in the protocol (see supplementary appendix 1).
Summary measures and synthesis
The intention-to-treat principle was applied by using the number of patients randomised as the denominator in all analyses and assuming that patients with missing information had a negative outcome. We evaluated the assumption of transitivity (ie, that valid indirect comparisons could be made through the network because the distribution of effect modifiers on average was similar between the compared sets of trials)30by comparing the distribution of the several potential effect modifiers across comparisons for our primary outcomes: mean age,31sex (ie, the proportion of female participants),32headache intensity at baseline (ie, the proportion of participants with moderate or severe pain),33and ongoing use of preventive migraine drugs.34Global and local approaches were used to assess the inconsistency between direct and indirect sources of evidence.35To assess the inconsistency globally, we used a design-by-treatment test,36whereas for local inconsistency we used back calculation and separated indirect from direct design evidence methods to compare direct and indirect evidence for each pairwise treatment comparison.37Statistical heterogeneity was assessed for each pairwise and network meta-analysis comparison using τ2and I2statistics.11
We conducted a series of network meta-analyses using a random effects model within a frequentist setting, assuming equal heterogeneity across all comparisons and accounting for correlations induced by multi-arm studies. For studies with rare events (ie, an event rate of <5%), we used a common effect Maentel-Haenszel approach.37We conducted the network meta-analyses using the “netmeta” package in R (version 4.2.2). We estimated effect sizes from pairwise and network meta-analyses by summary odds ratios for dichotomous outcomes with corresponding 95% confidence intervals (CIs).
League tables and vitruvian plots were used to present the findings from the network meta-analyses.37The vitruvian plot is a benefit-harm communication tool to summarise direction, magnitude, and uncertainty of effects over multiple outcomes in network meta-analysis.38For the vitruvian plots, we selected sumatriptan as the reference intervention as it is the most commonly prescribed migraine specific drug and it is included in the WHO Model List of Essential Medicines.39As secondary analyses, we also visualised results using forest plots and vitruvian plots with placebo or ibuprofen as reference treatments.
The risk of bias of individual studies was assessed on each primary outcome with the Cochrane risk of bias tool, version 2.0 (RoB2),28and the certainty of evidence was assessed using the confidence in network meta-analysis (CINeMA) framework.40
Additional analyses
We evaluated possible heterogeneity of treatment effects using bayesian network meta-regressions for sex assigned at birth and presence of aura. To evaluate the robustness of our findings, we carried out the following sensitivity analyses on our primary outcomes: trials only with doses licensed by the FDA, with low risk of bias, with participants experiencing moderate or severe headache, with a diagnosis of menstrual related migraine, splitting nodes with high and low doses, assessed the effect of placebo response, excluding studies with participants with medical comorbidity, or excluding studies that allowed the use of preventive drugs.
Patient and public involvement
We discussed the aims and design of this study with members of the public, including those who had experienced migraine (one patient representative is a coauthor of this paper and has been involved in all stages of the project). We used their feedback to guide the selection of outcomes for the study and inform the interpretation of the results presented in this manuscript. Three members of the research team conducted statistical analyses and presented the results in a blinded fashion (ie, the names of the interventions were masked to reduce bias from previous experience or knowledge) to two independent panels of expert clinicians and patient representatives from international organisations in Argentina, Canada, Europe, and the US.
Results
Study selection and network geometry
Overall, 184 double blind randomised controlled trials published between 1991 and 2023 were identified (fig 1). Supplementary appendix 4 and tables S3 and S4 describe the included studies. Of those studies, 174 (95%) were sponsored by the pharmaceutical industry, 163 (89%) were placebo controlled, and 52 (28%) directly compared at least two eligible active interventions. Seventy six trials were from North America (41%), 47 from Europe (26%), 16 from Asia (9%), and 37 recruited participants from more than one continent (20%). We retrieved unpublished information for 124 (67%) trials. The median study sample size was 378 (interquartile range 132-690) participants, mean age 40.3 (standard deviation 10.9) years, 85.6% of the total sample were female participants, and 32.3% had a history of migraine with aura.
Overall, 137 randomised controlled trials were included in the network meta-analyses, with 62 682 participants allocated to drug treatment and 26 763 to placebo. The 17 individual drugs were divided into five categories: antipyretics (paracetamol), ditans (lasmiditan), gepants (rimegepant and ubrogepant), NSAIDs (acetylsalicylic acid, celecoxib, diclofenac potassium, ibuprofen, naproxen sodium, and phenazone), and triptans (almotriptan, eletriptan, frovatriptan, naratriptan, rizatriptan, sumatriptan, and zolmitriptan). All interventions had at least one placebo controlled trial for one or more outcomes (fig 2andfig 3) and most networks were well connected (see supplementary appendix 5). The full dataset and information for the vitruvian plots are freely available online at GitHub (https://github.com/EGOstinelli/NMA-on-migraine/).
Synthesis of results and certainty of evidence
Figure 4andfigure 5show the results of the network meta-analyses. Further results are available in supplementary appendices 6-9. All active interventions were more efficacious than placebo for pain freedom at two hours (odds ratios from 1.73 (95% CI 1.27 to 2.34) for naratriptan to 5.19 (4.25 to 6.33) for eletriptan) and most were also efficacious for sustained pain freedom from two to 24 hours post-dose, except paracetamol and naratriptan (odds ratio 1.66 (0.68 to 4.04) and 1.57 (0.76 to 3.25), respectively). When the active interventions were compared with each other, eletriptan was superior to almost all the other drugs for achieving pain freedom at two hours, followed by rizatriptan, sumatriptan, and zolmitriptan (odds ratios from 1.35 to 3.01). For sustained pain freedom up to 24 hours, the most efficacious interventions were eletriptan (odds ratios from 1.41 to 2.73) and ibuprofen (odds ratios from 3.16 to 4.82). In terms of secondary efficacy outcomes, all interventions were superior to placebo for pain relief at two hours and for use of rescue drugs from two to 24 hours.
When the drugs were compared head to head, eletriptan was associated with better efficacy than nearly all of the other active interventions for pain relief at two hours (odds ratios from 1.26 to 2.63) and use of rescue drugs (odds ratios from 0.43 to 0.63). Outcome data on pain relapse up to 48 hours were only available for lasmiditan, sumatriptan, and rimegepant: all showed greater efficacy than placebo, with comparable performances for lasmiditan (odds ratio 0.42 (95% CI 0.12 to 1.48)) and rimegepant (0.29 (0.08 to 1.03)) relative to sumatriptan. For adverse events, dizziness was more commonly associated with lasmiditan, eletriptan, sumatriptan, and zolmitriptan (odds ratios from 1.14 to 3.19). Fatigue and sedation occurred more frequently with eletriptan (odds ratios from 1.34 to 2.63) and lasmiditan (odds ratios from 1.33 to 2.50). Paraesthesia was more often associated with lasmiditan (odds ratios from 1.28 to 1.50), sumatriptan (odds ratio versus placebo 1.18 (95% CI 1.04 to 1.32)), and zolmitriptan (odds ratios from 1.18 to 1.50). Nausea was also more likely to be experienced with lasmiditan, sumatriptan, zolmitriptan, and ubrogepant (odds ratios from 1.19 to 2.22). Paracetamol was, conversely, less likely to be associated with nausea (odds ratios from 0.44 to 0.56) but more likely to be associated with hepatic toxicity (odds ratios from 6.40 to 7.69). Eletriptan was the only intervention more frequently associated with chest pain or discomfort (odds ratios from 1.42 to 1.78).
The vitruvian plots show the 10 outcomes deemed the most clinically relevant by the panel of expert clinicians and patient representatives (pain freedom at two hours, sustained pain freedom from two to 24 hours, pain relief at two hours, use of rescue drugs within two to 24 hours, chest pain or discomfort, dizziness, fatigue, nausea, paraesthesia, and sedation) using sumatriptan as the reference drug (fig 6andfig 7). Supplementary appendix 10 shows the vitruvian plots using placebo and ibuprofen as reference interventions.
The certainty of the evidence for the primary outcomes assessed using CINeMA ranged from high to very low. Rimegepant versus placebo was the only comparison rated high certainty for each primary outcome. For pain freedom at two hours, 13 of 153 (8%) comparisons were rated moderate certainty, 26 (17%) were rated low certainty, and 113 (74%) were rated very low certainty. For sustained pain freedom until 24 hours, 4 of 105 (4%) comparisons were rated moderate, 5 (5%) were rated low, and 95 (90%) were rated very low. Supplementary appendix 11 and tables S5, and S6 provide full information about CINeMA. Risk of bias assessed using the Cochrane risk of bias 2 tool (RoB2) was rated low for pain freedom at two hours in 24 of 115 (21%) randomised controlled trials, some concerns in 73 (63%), and high in 18 (16%). For sustained pain freedom, risk of bias was rated low in 16 of 56 (29%) randomised controlled trials, some concerns in 34 (61%), and high in 6 (11%). See supplementary appendix 12 and tables S7 and S8 for further information on risk of bias.
Credibility assessment and sensitivity analyses
Measures of statistical heterogeneity (τ2and I2) and inconsistency for each outcome are shown in supplementary appendix 13 as well as for subgroup and sensitivity analyses in supplementary appendices 14 and 15. No violations of our transitivity assumptions were identified. Inconsistencies were observed among comparisons for the outcomes of pain freedom at two hours (8%), sustained pain freedom (5%), use of rescue drugs (9%), dizziness (8%), chest pain or discomfort (13%), and sedation (5%). We checked the data for potential extraction or entering errors, but no mistakes were identified.
We considered changes in the magnitude of the placebo response as a potential explanation of heterogeneity and inconsistency. To explore this, we did a meta-regression of the log proportion of placebo responders over time for each primary outcome, which showed a structural break corresponding to the year 1997 for pain freedom at two hours. A sensitivity analysis restricted to studies after 1997 resulted in comparable results. Overall, sensitivity analyses on FDA licensed doses only, high versus low doses, risk of bias, and moderate-to-severe headache at baseline confirmed our main findings (see supplementary appendix 15).
Discussion
Compared with previous studies, our systematic review and network meta-analysis provided comprehensive data synthesis on the acute treatment of migraine in adults.2141Our findings showed that some triptans—namely, eletriptan, rizatriptan, sumatriptan, and zolmitriptan—had the most favourable overall profiles in terms of efficacy and tolerability. These four triptans were more efficacious than the most recently marketed drugs lasmiditan, rimegepant, and ubrogepant, which, based on our results, showed efficacy comparable to that of paracetamol and most NSAIDs.
Triptans are selective serotonin (5 hydroxytryptamine)1B/1Dreceptor agonists, exhibiting differences in receptor affinity, lipophilicity, metabolism, and pharmacokinetic profiles within the same class.4Despite their low acquisition costs and balanced efficacy and tolerability profiles, however, triptans remain underused among people with migraine.4243In the US, current use of triptans ranges from 16.8% to 22.7%,43and in Europe from 3.4% to 22.5%.42Triptans are contraindicated in patients with vascular disease, posing an important limitation to their use.4However, concerns about their cardiovascular safety remain difficult to interpret, as cerebrovascular events may present primarily as migraine-like headaches, and misdiagnosis of transient ischaemic attack and minor stroke as migraine is not rare.4445Moreover, studies assessing the response to high dose intravenous eletriptan or subcutaneous sumatriptan found no clinically significant vasoconstriction in patients undergoing diagnostic coronary angiography.9Future studies revisiting the vascular contraindications of triptans are crucial to minimise potentially missed treatment opportunities.
The most recently marketed drugs, such as lasmiditan, rimegepant, and ubrogepant, are not associated with vasoconstrictive effects and have therefore been promoted as alternatives for patients for whom triptans are contraindicated or not tolerated.4While rimegepant was well tolerated based on the results in our study, ubrogepant showed increased risk of nausea compared with placebo. Lasmiditan was associated with a substantial risk of dizziness, along with paraesthesia and sedation. Restrictions raised by the FDA against driving for eight hours after intake of lasmiditan underscore the challenges to its use.1Moreover, the high costs of these new drugs pose a barrier to their widespread use and necessitate trials to ascertain their cost effectiveness for patients with insufficient response to triptans.1Notably, our search identified one ongoing study, with pending results, in participants for whom triptans were unsuitable owing to lack of efficacy, previous intolerance, or contraindications.46
Our results showed wide variation in performance across individual NSAIDs. Diclofenac potassium showed efficacy and tolerability close to that of sumatriptan, but these estimates were imprecise due to the large confidence intervals. For ibuprofen, the high efficacy estimate for sustained pain freedom was driven by a single study with a noticeably low placebo response. Acetylsalicylic acid and naproxen sodium showed moderate efficacy, with tolerability comparable to that of sumatriptan. Celecoxib ranked lowest among NSAIDs, whereas sparse evidence was available for phenazone. Taken together, NSAIDs performed worse than triptans, were comparable to gepants, and were less likely to cause adverse events compared with lasmiditan. Paracetamol, although showing limited effect for pain freedom at two hours, proved to be well tolerated, affirming its role as a viable option for those seeking pain relief with low risk of adverse events.
Strengths and limitations of this study, and future directions
Using the websites of regulatory agencies and international trial registries, and contacting study authors and pharmaceutical companies, we managed to incorporate a large amount of unpublished data in the analysis. Nowadays, online archives exist where trials are prospectively registered, which makes the study search more reliable; however, these registries only collect transparent information about the most recent studies, and we cannot rule out the possibility that some studies were missing or that the same studies were counted twice in our analyses. By making the dataset fully and freely available, we welcome any information that might help clarify mistakes in our meta-analysis.
Our findings have some limitations. Moderate heterogeneity was found for most outcomes and, according to our ratings in CINeMA, confidence in our findings was low or very low for most comparisons. Lower confidence levels were often due to the lack of prespecified analysis plans (within study bias), imprecision of treatment effects, or lack of information about randomisation and allocation concealment. Considering all this, the risk of bias for many studies may largely be a matter of reporting.47To increase the methodological rigour of the contributing evidence, we included only double blind trials, which are similar in design, patient populations, and conduct.4849Available networks were in general adequately connected, with placebo or sumatriptan being the most connected interventions and thus increasing the reliance on indirect evidence. We investigated the impact of study year on our primary efficacy outcomes and found no effect on the results of our network meta-analyses. The temporal trend of the placebo response in trials of acute treatments for migraine episodes warrants further investigations owing to its relevance for planning of sample size in future trials and for network meta-analysis. Although our results enhance the choice of drugs based on personal preferences in relation to efficacy and risk of adverse events, our findings were limited to average treatment effects due to the lack of individual patient data. Since monotherapy drugs are generally preferred for treatment, we did not include combination drugs. To avoid violation of transitivity, we restricted our focus to oral treatments, although the drugs can be administered by alternative routes.4Finally, in the present study we did not consider type of oral formulation, consistency in response across migraine episodes, or cost effectiveness. We also did not cover important clinical issues that might inform treatment decision making in routine clinical practice (eg, drug overuse headache or potential withdrawal symptoms). Additionally, because of the paucity of information reported in the original studies, we were not able to quantify some outcomes, such as global functioning.
Clinical and policy implications
Results on both benefits and harms should inform shared clinical decision making, considering the preferences of patients, caregivers, and healthcare professionals. Our findings should help inform future guidelines and updates to recommendations to ensure that patients receive optimal care. Overall, the results of our network meta-analysis suggest that the best performing triptans should be considered the treatment of choice for migraine episodes owing to their capacity for inducing rapid and sustained pain freedom, which is of key importance for people with migraine.50While the recent introduction of lasmiditan, rimegepant, and ubrogepant has expanded options for the acute treatment of migraine, the high cost of these newer drugs, along with the substantial adverse effects of lasmiditan, suggest their use as third line options, after the less expensive, similarly efficacious, second line options such as ibuprofen, acetylsalicylic acid, diclofenac potassium, almotriptan, and frovatriptan have been considered. However, ranking of treatments in clinical guidelines extends beyond efficacy, tolerability, safety, and acquisition costs alone and must also consider cost effectiveness, of which analyses are warranted, and accessibility. The inclusion of the most effective triptans (available as generic drugs) into the WHO Model List of Essential Medicines should be considered to promote global accessibility and uniform standards of care (currently, sumatriptan is the only triptan included).39Limited access to triptans and their substantial underutilisation represents missed opportunities to offer more effective treatments and deliver better quality of care to people who experience migraine.3
Conclusions
The results of this systematic review and network meta-analysis offer the best available evidence to guide the choice of acute oral drug interventions for migraine episodes. Our results are in line with recent observational evidence.51Careful comparisons between randomised controlled trials and observational evidence represent a productive line of research, as they may complement one another, and both can inform clinical decision making.52Nevertheless, we believe that, making the best use of the available, if limited, randomised evidence, our results and tools are valid and should be used to guide treatment choices, promoting shared, informed decision making between patients and clinicians.
All the statements comparing the performance of one drug with another should be tempered by the potential limitations of the current analyses, the quality of the available evidence, the characteristics of the study population, and the long term management of migraine.53Future network meta-analyses using individual patient data are required to improve personalised guidance for managing acute treatment of migraine episodes.
Migraine is a highly prevalent condition and among the leading causes of disability worldwide
Numerous oral drugs with different mechanisms of action are available for the acute management of migraine, but no clear consensus exists among clinical guidelines about the ranking of these treatments
Previous systematic reviews and network meta-analyses have only included a subset of currently licensed drugs
Considering both efficacy and tolerability, eletriptan, rizatriptan, sumatriptan, and zolmitriptan showed the best overall performance for the acute treatment of migraine
Eletriptan, rizatriptan, sumatriptan, and zolmitriptan were more efficacious than the recently marketed and more expensive drugs lasmiditan, rimegepant, and ubrogepant, which showed efficacy comparable to paracetamol and most non-steroidal anti-inflammatory drugs
Triptans are currently widely underused, and access to the most effective triptans should be promoted globally and international guidelines updated accordingly
","Objective: To compare all licensed drug interventions as oral monotherapy for the acute treatment of migraine episodes in adults.
Design: Systematic review and network meta-analysis.
Data sources: Cochrane Central Register of Controlled Trials, Medline, Embase, ClinicalTrials.gov, EU Clinical Trials Register, WHO International Clinical Trials Registry Platform, as well as websites of regulatory agencies and pharmaceutical companies without language restrictions until 24 June 2023.
Methods: Screening, data extraction, coding, and risk of bias assessment were performed independently and in duplicate. Random effects network meta-analyses were conducted for the primary analyses. The primary outcomes were the proportion of participants who were pain-free at two hours post-dose and the proportion of participants with sustained pain freedom from two to 24 hours post-dose, both without the use of rescue drugs. Certainty of the evidence was graded using the confidence in network meta-analysis (CINeMA) online tool. Vitruvian plots were used to summarise findings. An international panel of clinicians and people with lived experience of migraine co-designed the study and interpreted the findings.
Eligibility criteria for selecting studies: Double blind randomised trials of adults (≥18 years) with a diagnosis of migraine according to the International Classification of Headache Disorders.
Results: 137 randomised controlled trials comprising 89 445 participants allocated to one of 17 active interventions or placebo were included. All active interventions showed superior efficacy compared with placebo for pain freedom at two hours (odds ratios from 1.73 (95% confidence interval (CI) 1.27 to 2.34) for naratriptan to 5.19 (4.25 to 6.33) for eletriptan), and most of them also for sustained pain freedom to 24 hours (odds ratios from 1.71 (1.07 to 2.74) for celecoxib to 7.58 (2.58 to 22.27) for ibuprofen). In head-to-head comparisons between active interventions, eletriptan was the most effective drug for pain freedom at two hours (odds ratios from 1.46 (1.18 to 1.81) to 3.01 (2.13 to 4.25)), followed by rizatriptan (1.59 (1.18 to 2.17) to 2.44 (1.75 to 3.45)), sumatriptan (1.35 (1.03 to 1.75) to 2.04 (1.49 to 2.86)), and zolmitriptan (1.47 (1.04 to 2.08) to 1.96 (1.39 to 2.86)). For sustained pain freedom, the most efficacious interventions were eletriptan and ibuprofen (odds ratios from 1.41 (1.02 to 1.93) to 4.82 (1.31 to 17.67)). Confidence in accordance with CINeMA ranged from high to very low. Sensitivity analyses on Food and Drug Administration licensed doses only, high versus low doses, risk of bias, and moderate to severe headache at baseline confirmed the main findings for both primary and secondary outcomes.
Conclusions: Overall, eletriptan, rizatriptan, sumatriptan, and zolmitriptan had the best profiles and they were more efficacious than the recently marketed drugs lasmiditan, rimegepant, and ubrogepant. Although cost effectiveness analyses are warranted and careful consideration should be given to patients with a high risk cardiovascular profile, the most effective triptans should be considered as preferred acute treatment for migraine and included in the WHO List of Essential Medicines to promote global accessibility and uniform standards of care.
Systematic review registration: Open Science Frameworkhttps://osf.io/kq3ys/.
"
Cumulative live birth rate of a blastocyst versus cleavage stage embryo transfer policy during in vitro fertilisation,"Introduction
In vitro fertilisation (IVF) is a successful treatment for infertility, with more than 10 million children born after assisted reproductive technology since 1978.1Currently around one in 30 children in the US and Europe are born after assisted reproductive technology.23The timing of embryo transfer is an important part of IVF treatment. Traditionally during IVF treatment or intracytoplasmic sperm injection (ICSI), embryos were transferred on day 3 after oocyte retrieval, aligning with the cleavage stage of embryo development. After improvements to in vitro culture conditions and embryo cryopreservation techniques, however, the standard practice has changed towards transferring embryos at the blastocyst stage of embryo development, usually on day 5 or day 6 after oocyte retrieval.45678
The rationale for this change in practice was that as only viable embryos are thought to be able to reach the blastocyst stage in vitro, the selection of embryos for transfer would be enhanced.49Additionally, this timing aligns more closely to the so called implantation window, when the endometrium is more receptive to embryos.4567Theoretically, therefore, a higher treatment efficacy is expected from the transfer of embryos at the blastocyst stage in terms of live birth rate for each (fresh) transfer, risk of pregnancy loss, number of embryos transfers needed, and time to conception leading to live birth. Furthermore, embryo transfers at the blastocyst stage have been proved to improve the live birth rate for each fresh transfer in women with a good prognosis.48Because of those mainly theoretical advantages of the blastocyst stage, worldwide many IVF clinics rapidly changed from using cleavage stage transfer to blastocyst stage transfer without proven effectiveness and safety studies.4567The outcome of a live birth rate for each fresh embryo transfer does not, however, encompass the available supernumerary embryos in IVF, which are frozen for potential future transfers. Transfers and cryopreservation at the cleavage stage generally yield a higher number of embryos for future frozen-thawed embryo transfers, as some embryos will show arrested development during the cleavage to blastocyst stage in vitro. A longer period exposed to an in vitro culture condition could be detrimental compared with exposure to the uterine environment, which would favour cleavage stage embryo transfer.4567
Nevertheless, it remains uncertain which of the two embryo transfers is superior in terms of cumulative live birth rate, encompassing live births from both fresh and frozen-thawed embryo transfers.4567From the patients’ perspective, the cumulative live birth rate is considered the most important outcome, as it summarises the success rate over an entire course of one IVF treatment cycle.10111213Similarly, systematic reviews have concluded that high quality evidence on cumulative pregnancy results is currently insufficient and that well designed randomised controlled trials are needed to assess clinical value.45In light of this, we conducted a multicentre, randomised, superiority controlled trial comparing cumulative live birth rates after IVF or ICSI with blastocyst or cleavage stage embryo transfers in women with a minimum of four embryos available on the day 2 after oocyte retrieval.
Methods
Study design
The Three or Five (ToF) study was designed as a multicentre randomised controlled trial at 21 Dutch hospitals and clinics, with laboratory procedures performed in 11 affiliated IVF laboratories. Trial coordination, study design, data management, and statistical analysis were conducted at Radboud University Medical Centre and Amsterdam UMC. The Netherlands Society of Obstetrics and Gynaecology Consortium provided trial support and performed independent audits and data monitoring. No interim analysis was performed. Details on rationale and design of the trial have been reported previously.14
Participants
Women aged 18-43 years, scheduled for their first, second, or third IVF or ICSI oocyte retrieval cycle were eligible for participation. Each woman could only participate in one treatment cycle. To be eligible, women needed to have four or more embryos available on the second day after oocyte retrieval. The definition of an embryo in the context of the study was all zygotes with two pronuclei, one pronuclei, or no pronuclei at day 1 (observed at 16-18 hours after insemination or injection) and at least one cell division on day 2 (embryos with ≥3 pronuclei were excluded).
Exclusion criteria included preimplantation genetic testing, the use of frozen-thawed oocytes, and the use of donor oocytes. Fertility doctors counselled the women and provided them with a patient information letter during their scheduled visits. Written informed consent was obtained from all women before oocyte retrieval.
Randomisation
Women who had provided consent were randomly assigned in a 1:1 ratio using Castor, an online computerised randomisation software, on day 2 after oocyte retrieval. Randomisation was stratified based on age (<36 yearsv≥36 years). A random permuted block design with block sizes of two, four, or six was used to ensure a balanced allocation of women to both groups. Participants, doctors, embryologists, and laboratory technicians could not be masked owing to the nature of the intervention. Data analysts were masked during data cleaning and preparation of syntaxes, and they were unmasked after the inclusion and treatment phases.
Procedures
A gonadotrophin releasing hormone agonist or a gonadotrophin releasing hormone antagonist protocol was used to control ovarian stimulation, and the use of ICSI was at the discretion of the local investigators. The laboratories adhered to the study protocol, but otherwise applied their own laboratory procedures. Each centre always collaborated with the same laboratory.
In the blastocyst group, fresh embryos were transferred on day 5 (not day 6) after oocyte retrieval, followed by cryopreservation of surplus embryos on day 5 or day 6 using the vitrification method. If no embryo had reached the blastocyst developmental stage, an embryo could be transferred at day 5 from a delayed stage of development, such as morula or cleavage stage. In the cleavage group, fresh embryos were transferred on day 3 after oocyte retrieval, followed by cryopreservation of surplus embryos on day 3 or day 4 using a slow freezing or vitrification method depending on local protocols. In both groups, embryo selection for fresh transfer or cryopreservation was based on embryo morphology using local criteria. For ethical reasons in six centres, if women randomised to the cleavage group had embryos that did not fulfil the local freezing criteria on day 3 or day 4, those embryos were cultured up to day 6. If these remnant embryos developed into blastocysts, cryopreservation was still performed, but the results from these frozen-thawed embryo transfers were excluded from the per protocol analysis. Importantly, available cleavage stage embryos were thawed first.
In the Netherlands, single embryo transfer is commonly performed in both fresh and frozen-thawed cycles following national guidelines and local protocols.15Double embryo transfers are, however, allowed in women aged 38 years or older as well as all women going through a third IVF cycle. For frozen-thawed embryo transfers, endometrial preparation was carried out in either a natural cycle or an artificial cycle with exogenous oestradiol and progesterone, based on characteristics of the women and their preferences.
The follow-up period included the results of the fresh embryo transfer, all frozen-thawed embryo transfers from the initial oocyte retrieval cycle, and natural conceptions within 12 months of randomisation. Owing to the covid-19 pandemic and related restrictions, some treatments were interrupted or postponed. To compensate for delays in treatment, we extended the follow-up period by five months for women with an oocyte retrieval date between 16 March 2019 and 1 September 2020.
Outcome measures
The primary outcome was the cumulative live birth rate of pregnancies arising from fresh or frozen-thawed embryo transfers from the study cycle or natural conceptions within 12 months after randomisation. Secondary outcomes included (cumulative) pregnancy rates, pregnancy loss rate, live birth rate after fresh embryo transfer, number of embryo transfers needed to achieve a live birth, cancelled transfers, number of frozen embryos, embryo utilisation rate, time to conception leading to live birth, multiple pregnancy rate, and obstetric and perinatal outcomes.
Obstetric and perinatal outcomes of gestational age at delivery, mode of delivery, sex, and birth weight were collected from records. More detailed obstetric information on hypertensive disorders of pregnancy, gestational diabetes mellitus, and abnormal placentation were included in a questionnaire administered to participants.
Serious adverse events were reported to the ethical committee and were analysed immediately.
Statistical analysis
The trial was designed as a superiority trial to prove the presence of a two sided difference. At the time of the study design and for determination of the sample size, we expected a cumulative live birth rate of 31% for each oocyte retrieval using cleavage stage embryo transfer based on cumulative results of recorded data in all patient groups in 2015 from the Dutch IVF laboratories.16We hypothesised that the cumulative live birth rate after blastocyst stage transfer would be more than 8 percentage points higher than after cleavage stage transfer. Owing to the absence of high quality data on cumulative results for reference, our sample size was based on earlier findings of increased live birth rates after fresh transfer.4We determined that a sample size of 1200 women would provide 80% power at an α level of 0.05, accounting for an estimated dropout rate of 2%.
We used the χ2test to assess differences in non-continuous variables between the two study groups and the independentttest to assess differences in continuous variables, reported as means. Subsequently, we compared the cumulative and fresh embryo transfer pregnancy outcomes between the groups using log-link binomial generalised linear models, adjusted for age stratified groups. We calculated absolute differences and risk ratios along with corresponding 95% confidence intervals (CIs). A two sided P value <0.05 indicated statistical significance. Women lost to follow-up were considered not to have had a live birth. Cox proportional hazard curves adjusted for age stratified groups were constructed for both groups to summarise time to conception until live birth.17We present hazard rates, and for each group median time to conception leading to live birth. Statistical analysis was based on the intention-to-treat principle.
Planned and prespecified subgroup analyses were performed for two age groups (<36 yearsv≥36 years) to assess whether participants’ age can be used as a treatment selection marker or has prognostic value for cumulative live birth rate and cumulative pregnancy loss, including testing for interaction. We also conducted post hoc subgroup analyses by subdivision of age groups, performance by IVF centre laboratory, only single embryo transfers, fertilisation technique, stimulation protocol, cryopreservation technique, and stage of embryo development. Additionally, we conducted a per protocol analysis adjusted for age stratified groups using log-link binomial generalised linear models. The per protocol analysis excluded cycles that deviated from the protocol for any instances or cycles involving the use of frozen-thawed embryos on day 5 or day 6 in the cleavage group. Statistical analysis was performed with IBM SPSS statistics (version 28).
Patient and public involvement
Patient representatives from the Dutch patient organisation Freya (www.freya.nl) were involved in the design of this research. During the preparatory stage, priority of the research question and choice of outcome measures were chosen after consultation with a focus group from Freya. The board of the Netherlands Society of Obstetrics and Gynaecology and members of the Dutch Society of Clinical Embryologists (KLEM) also pointed out the importance of this research and high priority for clinical research in IVF.
Results
Participants
Between 18 August 2018 and 17 December 2021, a total of 1706 women provided consent and were enrolled in the trial, of whom 504 were ineligible for randomisation. Of the 1202 women who met the inclusion criteria and were eligible for randomisation, 603 (50.2%) were assigned to the blastocyst stage group and 599 (49.8%) to the cleavage stage group. Ten women (1.7%) in the blastocyst group and 43 (7.1%) in the cleavage group either withdrew from the trial or deviated from the protocol after randomisation (fig 1).Table 1shows the baseline characteristics of the two groups. In both study arms no fresh embryo transfers were cancelled because of unsuitable embryo development.
Primary outcome
The intention-to-treat cumulative live birth rate did not differ between the study groups, with live births in 355 (58.9%) of 603 women in the blastocyst group and 350 (58.4%) of 599 in the cleavage group (risk ratio 1.01, 95% CI 0.84 to 1.22), corresponding to an absolute difference of 0.4 percentage points (95% CI −5.1 to−5.9) (table 2). Four women (0.7%) in the blastocyst group and 10 (1.7%) in the cleavage group had a live birth after natural conception within the follow-up period.
Secondary outcomes
The cumulative pregnancy loss rate was lower in the blastocyst group compared with cleavage group (16.3%v24.2%; risk ratio 0.68, 95% CI 0.51 to 0.89) (table 2). The live birth rate after fresh embryo transfer was higher in the blastocyst group, with live births in 223 (37.0%) of 603 women compared with 177 (29.5%) of 599 in the cleavage group (1.26, 1.00 to 1.58) (table 2).
The mean number of embryo transfers needed to result in a live birth was lower in the blastocyst group compared with cleavage group (1.55 (0.99)v1.82 (1.24), P<0.001;table 3).
The time to conception leading to a live birth at any given point within the 12 months’ follow-up period was comparable between the two groups (hazard ratio 1.06, 95% CI 0.92 to 1.23; P=0.35). The median time from oocyte retrieval to conception leading to a live birth was 3.1 months (95% CI 1.9 to 4.2) in the blastocyst group and 4.7 months (3.6 to 5.8) in the cleavage group (see supplementary figure 1).
The incidence of moderate preterm birth (32 to <37 weeks) in singletons was higher in the blastocyst group, with an incidence of 8.9% compared with 4.7% in the cleavage group (risk ratio 1.87, 95% CI 1.05 to 3.34). The obstetric and perinatal outcomes of birth weight, gestational age at delivery, and small or large for gestational age were similar between the two groups (table 4).
Detailed obstetric information, particularly on hypertensive disorders of pregnancy, gestational diabetes mellitus, and abnormal placentation, was collected from administered questionnaires. Some responses were, however, incomplete. Owing to concerns about data accuracy and completeness, we did not analyse these data.
Subgroup and per protocol analyses
In a planned and prespecified subgroup analysis of pregnancy rates by age groups of the women, no interaction was found between age group and treatment group (P=0.20). In women aged 36 years and older (n=431), the cumulative live birth rate was non-significantly higher in the blastocyst group compared with cleavage group (52.1% (112 of 215)v43.1% (93 of 216); risk ratio 1.21, 95% CI 0.99 to 1.48). In women aged <36 years (n=771), the cumulative live birth rate was lower in the blastocyst group (62.6% (243 of 388)v67.1% (257 of 383; 0.93, 0.84 to 1.04) but did not reach a significant difference (table 2, also see supplementary figure 2). Supplementary figure 3 shows the results of a post hoc analysis for investigation of other age categories (<30 years, ≥30-35 years, 36-37 years, and ≥38 years).
The results of the per protocol analysis were generally consistent with those of the intention-to-treat analysis (supplementary table 1). In 34 women in the cleavage group, an embryo was frozen-thawed on day 5 or day 6, which led to four additional live births.
The results of the post hoc subgroup analysis on stimulation method, fertilisation technique, only single embryo transfers, embryo development stage, cryopreservation technique, and laboratory were also consistent with those of the intention-to-treat analysis (supplementary table 2). Six live births occurred in 45 women in the blastocyst group, despite the transferred embryos having not reached the blastocyst stage at day 5 of culture.
Owing to the covid-19 pandemic, an extended follow-up period was applied to 32 women (13 (2.2%) women in the blastocyst group and 19 (3.2%) in the cleavage group. During this extended follow-up period, three conceptions led to live births: one in the blastocyst group and two in the cleavage group.
Adverse events
Three adverse events were reported, none of which were deemed to be related to study procedures.
Discussion
In this randomised controlled trial involving 1202 women undergoing IVF treatment with a good prognosis for live birth, we found no difference in cumulative live birth rate with embryo transfers at the blastocyst stage or cleavage stage.
Comparison with other studies
A 2022 Cochrane review of five randomised controlled trials (632 women) reporting on cumulative live birth rates in relation to day of embryo transfer in IVF concluded that the evidence in favour of the blastocyst stage compared with cleavage stage was uncertain.4We found no difference in outcomes in women with four or more embryos available during IVF treatment.
Our trial reported on the cumulative rate of pregnancy loss, which was lower in the blastocyst group. The results suggest that extended culture to blastocyst stage might benefit the selection of embryos with higher implantation potential and continuation of the pregnancy through the first trimester. These outcomes could possibly be linked to the chromosomal ploidy status of embryos, as the prevalence of aneuploid (abnormal number of chromosomes) embryos is higher at the cleavage stage.18Aneuploid embryos have been regarded as the main reason for implantation failure, pregnancy loss, and recurrent miscarriages.19The risk of pregnancy loss serves as a secondary outcome, for which our study was not specifically designed, thus requiring careful interpretation.
In our study, we observed a higher rate of moderate preterm birth (32 to <37 weeks) in singleton pregnancies in the blastocyst group, whereas studies have not reported an increased risk of preterm birth.2021However, our findings align with two recent cohort studies and two systematic reviews that reported an increased risk of preterm birth with extended culture to the blastocyst stage.22232425This could be explained by the effect of suboptimal conditions of extended in vitro culture compared with in vivo culture for implantation and placentation.22The outcome of preterm birth (<37 weeks’ gestation) was divided into two categories: very preterm birth (<32 weeks) and moderate preterm birth (32 to <37 weeks), because of the significantly higher risks of mortality and morbidity associated with very preterm birth.26We found no significant differences in the risk of very preterm birth. Other suggested differences in obstetric and perinatal outcomes reported in the literature, such as a higher male to female ratio,2021large for gestational age,242527and monozygotic twins after single embryo transfers,2228did not differ between the groups in our study. However, the number of live births with complications in our study was small compared with the numbers in cohort studies and reviews. Additionally, these obstetric and perinatal results were secondary outcomes for which our study was not explicitly designed, necessitating prudent interpretation. As a policy for embryo transfer at blastocyst stage is standard in many IVF centres, future research should focus on differences in obstetric and perinatal outcomes between blastocyst and cleavage stages.
A previous smaller randomised controlled trial suggested that maternal age influences the cumulative ongoing pregnancy rate with embryo transfer during the blastocyst stage versus cleavage stage.29Our findings suggest a similar, although non-significant, effect of age favouring embryo transfer during the cleavage stage in younger women and during the blastocyst stage in older women. A possible explanation might be that among woman of advanced maternal age with four or more embryos, a blastocyst stage policy offers a clearer advantage from selection of viable embryos by extended in vitro culture. Maternal ageing is known to result in reduced oocyte quality and embryo competence, as shown by increased oxidative damage, mitochondrial dysfunction, and number of age related chromosomal aneuploidies.3031Careful interpretation is necessary, however, as these results are subgroup findings and require validation in future trials.
Strengths and limitations of this study
Our trial has some limitations. We only included women with a minimum of four embryos available on day 2 after oocyte retrieval, limiting generalisability to women with fewer than four embryos. When designing the study, we were concerned that women and centres might hesitate to participate in the trial as the likelihood that none of the embryos would develop to blastocyst stage increases when fewer embryos are present. However, it may be justified to invite women to participate in such a trial now.3233
During the design of the study, we expected a cumulative live birth rate of 31% in the cleavage group but found a rate of 58% in both groups. This discrepancy can be explained by the group selection (good prognosis with at least four embryos) and improvement of cryopreservation methods. European data from 1997 to 2019 suggested a higher live birth rate up to 35% after frozen-thawed embryo transfer.34The higher-than-expected cumulative live birth rates slightly decrease the power of the results—consequently uncertainties remain around the risk estimates. With an absolute difference of 0.4 percentage points (95% CI −5.1 to 5.9) in our study, clinically relevant differences are unlikely but small differences cannot be ruled out.
To ensure the generalisability of our findings, we applied broad inclusion criteria and enrolled women from multiple centres. This study encompassed various IVF protocols on ovarian stimulation, embryo culture, endometrial preparation, and embryo cryopreservation methods, reflecting local protocols of participating centres. The results of post hoc analyses were consistent with those of the intention-to-treat analysis, suggesting findings are broadly generalisable. Noticeably, the study was not powered for these specific subgroup analyses, therefore a larger sample size would be needed to prove our findings in specific subgroups. Despite this, the variation in protocols should be considered a strength of our study as it represents the variations in daily clinical IVF practice. Furthermore, the difference in outcome between the 11 IVF laboratories showed no interaction between laboratories and cumulative live birth rate, indicating similar performance of the interventions under different laboratory circumstances.
Our study was designed as a superiority trial. As a protocol for embryo transfer at blastocyst stage is often regarded as a selection-deselection tool, it has been argued that the cumulative live birth rate could theoretically never be higher than that from using a cleavage stage transfer protocol, suggesting that a non-inferiority design should have been considered. However, the timing of a blastocyst stage transfer aligns more closely with the so called implantation window, which suggests the cumulative live birth rate could potentially be higher.4567A superiority trial allows for a more straightforward interpretation of whether the intervention under evaluation offers a meaningful improvement in clinical practice.
Policy implications
In assisted reproduction, many innovations and techniques are introduced into routine clinical practice with little or no evidence of safety and efficacy.35Worldwide, IVF laboratories anticipated improving the efficacy of treatments by prolonging embryo culture to the blastocyst stage. Our results suggest that women and healthcare professionals should consider various other factors alongside live birth rates. The chance of a live birth should be individually balanced against outcomes such as pregnancy loss; risk of preterm birth; emotional and psychological burden, including the burden of time to pregnancy and implantation failure; and financial implications. A cost effectiveness analysis is currently being performed on our data.
Conclusions
This study found that an IVF protocol for transfer of embryos at the blastocyst stage in women with four or more embryos results in cumulative live birth rates comparable to that of embryo transfer at the cleavage stage.
Blastocyst stage embryo transfer was also associated with higher efficiency for secondary outcomes: a lower risk of pregnancy loss, higher live birth rate per fresh transfer, and reduced number of embryo transfers until live birth. A blastocyst stage transfer, however, raises concerns about the safety of preterm birth. Careful interpretation of the secondary outcomes is necessary, since these results are subgroup findings for which the study was not powered. They require validation in future trials.
In vitro fertilisation (IVF) centres expected better treatment efficacy by extending embryo culture to the blastocyst stage, assuming only viable embryos reach this stage
Focus has been on higher live birth rates after the first embryo transfer, with blastocyst transfers showing better outcomes than cleavage stage transfers
The cumulative live birth rate, including both fresh and frozen-thawed embryo transfers, is considered most relevant but is missing in studies
This study found no difference in cumulative live birth rate between blastocyst and cleavage stage transfers in women with at least four embryos
Blastocyst transfers may reduce pregnancy loss and require fewer embryo transfers, but they might increase moderate preterm births (32 to <37 weeks)
","Objectives: To evaluate whether embryo transfers at blastocyst stage improve the cumulative live birth rate after oocyte retrieval, including both fresh and frozen-thawed transfers, and whether the risk of obstetric and perinatal complications is increased compared with cleavage stage embryo transfers during in vitro fertilisation (IVF) treatment.
Design: Multicentre randomised controlled trial.
Setting: 21 hospitals and clinics in the Netherlands, 18 August 2018 to 17 December 2021.
Participants: 1202 women with at least four embryos available on day 2 after oocyte retrieval were randomly assigned to either blastocyst stage embryo transfer (n=603) or cleavage stage embryo transfer (n=599).
Interventions: In the blastocyst group and cleavage group, embryo transfers were performed on day 5 and day 3, respectively, after oocyte retrieval, followed by cryopreservation of surplus embryos. Analysis was on an intention-to-treat basis, with secondary analyses as per protocol.
Main outcome measures: The primary outcome was the cumulative live birth rate per oocyte retrieval, including results of all frozen-thawed embryo transfers within a year after randomisation. Secondary outcomes included cumulative rates of pregnancy, pregnancy loss, and live birth after fresh embryo transfer, number of embryo transfers needed, number of frozen embryos, and obstetric and perinatal outcomes.
Results: The cumulative live birth rate did not differ between the blastocyst group and cleavage group (58.9% (355 of 603)v58.4% (350 of 599; risk ratio 1.01, 95% confidence interval (CI) 0.84 to 1.22). The blastocyst group showed a higher live birth rate after fresh embryo transfer (1.26, 1.00 to 1.58), lower cumulative pregnancy loss rate (0.68, 0.51 to 0.89), and lower mean number of embryo transfers needed to result in a live birth (1.55v1.82; P<0.001). The incidence of moderate preterm birth (32 to <37 weeks) in singletons was higher in the blastocyst group (1.87, 1.05 to 3.34).
Conclusion: Blastocyst stage embryo transfers resulted in a similar cumulative live birth rate to cleavage stage embryo transfers in women with at least four embryos available during IVF treatment.
Trial registration: International Clinical Trial Registry Platform NTR7034.
"
Radiation dose fractionation in breast cancer,"Introduction
Breast cancer is the most prevalent malignancy in women, contributing significantly to the global cancer burden.1Postoperative radiation therapy represents an essential part of the multi-modality treatment for breast cancer, especially after breast conserving therapy, with a primary goal of reducing locoregional recurrence and improving disease-free and overall survival rates.234
Conventional fractionation radiation therapy, typically consisting of a total dose of about 50 Gray (Gy) delivered over five to six weeks in 1.8-2 Gy daily fractions, has been the historical standard of care for treatment of patients with breast cancer in most of the world since the 1970s. In more recent years, hypofractionated radiation therapy has emerged as an increasingly used alternative, primarily owing to studies conducted by researchers in the UK and Canada.567891011This approach delivers a dose greater than 2 Gy per fraction to give a reduced total dose over a shorter overall treatment time than conventional fractionation.12
Hypofractionation regimens can be subdivided into moderate hypofractionation and ultra-hypofractionation, with the second generally defined as using fractions larger than 3.3 Gy.1112Multiple randomised controlled trials conducted since the 1980s have shown that moderate hypofractionation is not inferior to conventional fractionation with regards to key oncological metrics such as overall survival and disease-free survival, with advantages in convenience, cost efficiency, and safety profile.79101113141516More recent trials have compared ultra-hypofractionation with moderate hypofractionation or conventional fractionation.17181920
Despite clinical evidence supporting these hypofractionated approaches, their integration into practice varies considerably across different healthcare settings, primarily owing to concerns about potential side effects and the relative novelty of ultra-hypofractionation, for which the follow-up data are shorter. Financial considerations may also play a role; for example, conventional fractionation may be preferred by practitioners in healthcare systems where reimbursement is calculated on a per fraction basis, whereas moderate hypofractionation or ultra-hypofractionation might be more popular where treatment reimbursement is capitated.2122
Given the imperative to optimise patients’ outcomes and convenience while minimising adverse effects, elucidating the impact of radiation therapy fractionation regimens on patients’ daily lives is crucial, as this can significantly influence their treatment preferences. This systematic review and meta-analysis of randomised controlled trials goes beyond traditional comparisons of survival outcomes to provide a multidimensional perspective on the implications of choice of radiation therapy fractionation. By specifically emphasising side effects, cosmetic outcomes, and quality of life, areas essential for informed clinical decision making yet often under-represented in research, the study aims to offer insights that can guide both clinicians and patients towards treatments that not only extend life but also enhance its quality.
Methods
The analysis was conducted and findings were reported according to the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guideline.23The protocol was registered in the PROSPERO database (CRD42023460249). Two investigators (SFL and SKFK) independently did the literature search and assessed study eligibility following the strategies stated below. Any disagreement between the reviewers was resolved through discussion and consensus or through arbitration by a third investigator (HCYW).
Search strategy
We did a systematic search in Ovid MEDLINE, Embase, and the Cochrane Central Register of Controlled Trials (CENTRAL) to identify eligible articles on the efficacy and safety of postoperative radiation therapy for breast cancer. The initial search on 7 February 2023 was updated on 23 October 2023. Search terms included “breast cancer”, “radiation therapy”, and “hypofractionation”. The detailed search strategies for each database are summarised in online appendix 1. We screened reference lists of relevant studies and reviews and consulted experts to uncover additional studies. We applied no geographical or language filters, but we included for analysis only studies conducted in humans and reported in the English language.
Inclusion and exclusion criteria
We categorised regimens as using conventional fractionation if the intact breast or chest wall/reconstructed breast was treated using daily fractions of 1.8-2 Gy, typically delivering a total dose of 50-50.4 Gy in 25-28 fractions over five to six weeks. We categorised trials as using moderate hypofractionation if they used fraction sizes of 2.65-3.3 Gy, giving 13-16 fractions over three to five weeks (supplementary table S1). Lastly, we categorised regimens as using ultra-hypofractionation when only five fractions were used. Some trials allowed the treating physicians to give an additional boost dose to the tumour bed for patients in the control and experimental arms, at their discretion, or incorporated randomisation to receive a boost or not.
Inclusion criteria were that the study was a randomised controlled trial; postoperative external beam radiation therapy was directed at the whole breast or chest wall, with or without regional nodal irradiation; and the study compared any combination of conventional fractionation, moderate hypofractionation, and ultra-hypofractionation. We excluded studies if they used intraoperative or partial breast radiation therapy or proton therapy, failed to provide quantifiable data or adequate statistical parameters for analysis, or exclusively reported on patients aged under 18 years.
Data extraction
Data extraction in Microsoft Excel (version 2310) was carried out by SFL and checked independently by the second investigator (SKFK). Detailed information on radiation therapy technique in the intervention and control arms, such as radiation doses, scheme, and duration of treatment, was collected. Outcome data were extracted, including primary and secondary outcome measures and follow-up time points.
Quality and risk of bias assessment and certainty of evidence
Risk of bias and quality and certainty of evidence were independently assessed at the outcome levels by SFL and a second investigator (SKFK, HCYW, or AWC) using the Cochrane Collaboration’s risk of bias tool 2.0 for randomised trials.24They typically classified a study as having a high risk of bias in the presence of one or more of the following characteristics: selection from multiple outcome measurements without multiple data analyses; no indication of a pre-specified data analysis plan; or substantial missing data for the primary outcome potentially affecting the results. For the assessment of the overall quality (certainty) of the evidence included in the meta-analysis, we adopted the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) approach, taking into consideration all relevant GRADE domains: methodological limitations, inconsistency, imprecision, indirectness, and publication bias.25In the case of network meta-analysis, we evaluated the confidence in the findings by using the web based application of the Confidence in Network Meta-Analysis (CINeMA) framework. This framework assesses six domains: within study bias, reporting bias, indirectness, imprecision, heterogeneity, and incoherence.26Any disagreements were resolved through consensus or discussion with a third investigator (SC).
Systematic review and statistical analyses
Primary outcomes of interest were grade ≥2 acute radiation dermatitis and late radiation therapy related side effects. Secondary outcomes were cosmesis, quality of life, local recurrence (in the breast for patients treated with breast conserving therapy or in the chest wall for those undergoing mastectomy), locoregional recurrence, disease-free survival, and overall survival. We assessed Common Terminology Criteria for Adverse Events or Radiation Therapy Oncology Group grade ≥2 side effects (or at least moderate if grading was not reported) because of their clinical relevance, which typically require medical management and may affect the treatment course and quality of life. We categorised assessment of acute side effects as up to three months after the completion of radiation therapy and late side effects as occurring later than that. Some but not all trials also reported dosimetric analysis, alterations in pulmonary and cardiac function tests, occurrence of secondary primary cancers, economic implications related to health, and workload; we did not analyse these outcomes here.
For meta-analysis, we used a random effects model to calculate the summary estimate of each risk ratio and hazard ratio, along with 95% confidence intervals (CIs).27We quantified heterogeneity between effect estimates among studies by two statistical tests: the Cochran’s Q statistical test for between study variability and the I2statistic for the proportion of total variation across studies due to statistical heterogeneity instead of chance.28We did a sensitivity analysis to assess whether excluding studies with a high risk of bias influenced the estimated effect or heterogeneity of the outcome.
We also did network meta-analysis to combine all available data and to verify the findings from the main meta-analysis. The analysis used a frequentist random effects model for risk ratios and hazard ratios. We used the P score, which is analogous to the surface under the cumulative ranking curve and estimates the extent of certainty that one treatment is superior to another treatment, to rank the radiation therapy dose fractionation regimens.29
All P values were two tailed, and we considered P values of <0.05 to be statistically significant. The meta-analyses and graphs were generated using Review Manager (RevMan), version 5.4, and we used R Statistical Software (v4.3.2) and netmeta package to do the network meta-analysis.
Patient and public involvement
No patients were involved in setting the research question, outcome measures, study design, or data interpretation. Although members of the public were not directly involved in this study because of funding limitations, the focus of this work is aligned with the research priorities of patients with breast cancer, which include rigorous evaluations of dose fractionation regimens to optimise treatment outcome and quality of life. Plain language messages about the results will be shared with lay audience (for example, via social media feeds).
Results
Search results and characteristics of included studies
The initial literature search identified 1754 articles. After the exclusion of 211 duplicates, 1543 articles were screened for relevance on the basis of titles and abstracts, resulting in 59 studies that met the inclusion criteria. These studies encompassed 35 trials conducted from 1986 to 2023, with a cumulative patient count of 20 237. Patient enrolment per trial ranged from 30 to 4096. Our analysis incorporated data from 29 trials comparing moderate hypofractionation with conventional fractionation (14 395 patients), five trials comparing ultra-hypofractionation with moderate hypofractionation (4927 patients), and one trial comparing ultra-hypofractionation with conventional fractionation (915 patients) (table 1,fig 1, and table S2).56789101115161718192030313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475
Risk of bias and GRADE assessment
Overall, 21.6% of outcomes were rated as having a low risk of bias, and 78.4% were rated as having some concerns or a high risk of bias (fig 2). Specifically, the domain with the poorest reporting was the measurement of outcomes, with 47.4% categorised as having some concerns (seetable 2for a concise summary of results with certainty of evidence). The detailed risk of bias and GRADE assessments for the included studies are shown in tables S3 and S4, respectively. After the evaluation of level of evidence using CINeMA, most pairwise comparison results were deemed to have moderate or high confidence (table S5).
Outcomes
The risk ratio for grade ≥2 acute radiation dermatitis for moderate hypofractionation compared with conventional fractionation was 0.59 (95% CI 0.51 to 0.69; P<0.001;fig 3) in the 20 trials comparing them for all patients, 0.54 (0.49 to 0.61; P<0.001;fig 4) for the eight trials giving results for only patients treated with breast conserving therapy, and 0.68 (0.49 to 0.93; P=0.02;fig 5) for the 10 trials giving results for only those treated with mastectomy. Combining the six trials comparing ultra-hypofractionation with moderate hypofractionation, the risk ratio for grade ≥2 acute radiation dermatitis was 0.85 (95% CI 0.47 to 1.55; P=0.60;fig 6). We found significant heterogeneity between studies (χ2=30.2, df=5; P<0.001; I2=83%). In the two trials that specifically compared ultra-hypofractionation with moderate hypofractionation in patients treated with breast conserving therapy, the risk ratio was 1.72 (95% CI 0.24 to 12.40; P=0.59;fig 7). Of note, grade ≥2 acute dermatitis was particularly low in the two ultra-hypofractionation regimens (28.5 Gy or 30 Gy given in five fractions delivered once weekly over five weeks) of the FAST trial compared with conventional fractionation, with a risk ratio of 0.27 (95% CI 0.19 to 0.40; P<0.001 (figure S1).18In sensitivity analyses assessing the influence of studies at high risk of bias on the cumulative findings, the risk ratio for acute radiation dermatitis comparing moderate hypofractionation with conventional fractionation remained significant at 0.58 (95% CI 0.49 to 0.68; P<0.001; n=11), with no substantial heterogeneity observed (χ2=13.67, df=10; P=0.19; I2=27%) (figure S2).
Criteria for assessment and timing are detailed in table S6. We did subgroup analyses delineating individual fractionation regimens on the basis of the number of fractions (figure S3). Across the compared regimens, the 15 fraction and 16 fraction schedules showed similar decreased risk ratios (0.56, 95% CI 0.46 to 0.68; P<0.001; n=10 and 0.60, 0.47 to 0.76; P<0.001; n=10, respectively) for acute radiation dermatitis, compared with the 50 Gy in 25 fractions approach.
In trials comparing moderate hypofractionation with conventional fractionation, the risk ratio was 0.84 (95% CI 0.66 to 1.06; P=0.14; n=10;fig 8) for grade ≥2 telangiectasia, 0.77 (0.62 to 0.95; P=0.02; n=4;fig 9) for any hyperpigmentation, 0.92 (0.80 to 1.06; P=0.24; n=15;fig 10) for grade ≥2 breast or chest wall induration or fibrosis, 0.92 (0.85 to 0.99; P=0.03; n=7;fig 11) for grade ≥2 breast shrinkage, and 0.82 (0.62 to 1.09; P=0.18; n=8;fig 12) for grade ≥2 breast oedema. However, in the subset of trials that reported results exclusively for patients treated with breast conserving therapy, the outcomes for telangiectasia (fig 13), hyperpigmentation (fig 14), breast induration or fibrosis (fig 15), breast shrinkage (fig 16), breast oedema (fig 17), and breast pain (fig 18) were not found to be statistically significant.
Comparing ultra-hypofractionation with moderate hypofractionation, data mainly from the FAST-Forward trial indicated increased risks associated with ultra-hypofractionation in terms of induration or fibrosis (risk ratio 1.86, 95% CI 1.19 to 2.92; P=0.007; n=3; figure S4B), breast shrinkage (1.38, 1.07 to 1.76; P=0.01; n=2; figure S4C), and breast oedema (2.44, 1.32 to 4.52; P=0.005; n=2; figure S4D). The FAST trial also suggested higher risks of breast shrinkage for ultra-hypofractionation compared with conventional fractionation (risk ratio 1.83, 95% CI 1.09 to 3.07; P=0.02; n=2; figure S5C). Subgroup analyses based on the number of fractions are shown in figures S6 to S8.
Comparing moderate hypofractionation and conventional fractionation, the risk ratio for grade ≥2 arm lymphoedema in the combined breast conserving therapy and mastectomy population was 1.00 (95% CI 0.78 to 1.29; P=0.98; n=13;fig 19). The risk ratio for grade ≥2 pneumonitis or symptomatic lung fibrosis was 1.57 (95% CI 0.81 to 3.02; P=0.18; n=16;fig 20). The risk ratio for ischaemic heart disease for all patients combined (regardless of surgery) was 0.95 (95% CI 0.56 to 1.58; P=0.83; n=8;fig 21). The risk ratio for shoulder stiffness or dysfunction was 1.14 (95% CI 0.69 to 1.89; P=0.62; n=7;fig 22), and the risk ratio for symptomatic rib fracture was 2.82 (0.87 to 9.14; P=0.08; n=8;fig 23). In a study assessing late side effects in the heart and lungs among patients with left breast carcinoma irradiated after mastectomy, pulmonary function tests and echocardiography did not show increased cardiopulmonary side effects of moderate hypofractionation (13 fractions) compared with conventional fractionation over an 18 month follow-up.63Figure S9 shows the forest plots for late non-skin side effects among the breast conserving therapy only and mastectomy only trials comparing moderate hypofractionation and conventional fractionation regimens.
For ultra-hypofractionation versus moderate hypofractionation, the risk ratio for symptomatic rib fractures was found to be statistically significant at 2.07 (95% CI 1.04 to 4.12; P=0.04; n=3; figure S10E), whereas for ultra-hypofractionation versus conventional fractionation, the comparisons did not yield statistically significant results (figure S11).
Severe late side effects were rare and comparable between moderate hypofractionation and conventional fractionation groups across studies. In the subgroup analysis focusing on individual fractionation regimens based on the number of fractions, we detected no significant variations in these late non-skin side effects across the studied regimens (figures S12-S14). Table S7 offers a detailed overview of late side effects when we compared different dose fractionation regimens, categorised by the time elapsed since breast/chest wall radiation therapy.
The DBCG HYPO trial indicated a modest improvement in cosmetic outcomes with a moderate hypofractionation regimen.39The START-PILOT trial reported fewer cases of fair or poor cosmetic results at five and 10 years in patients receiving 39 Gy in 13 fractions compared with those treated with 50 Gy in 25 fractions.11Furthermore, the Germany trial found that cosmetic assessments were significantly better in patients undergoing moderate hypofractionation compared with conventional fractionation.50In the FAST-Forward trial, we observed no significant cosmetic differences when comparing an ultra-hypofractionation regimen at 26 Gy in five fractions with moderate hypofractionation at 40 Gy in 15 fractions. Nevertheless, a higher dose of ultra-hypofractionation at 27 Gy in five fractions was associated with a significant risk of changes in breast appearance at both two and five years, relative to moderate hypofractionation.17The FAST trial, comparing ultra-hypofractionation with conventional fractionation, showed that the incidence of mild or marked changes in photographic breast appearance at two or five years was significantly higher for 30 Gy than for 50 Gy. However, no significant difference was detected between 28.5 Gy and 50 Gy.18Details on the timing and methods of these assessments are available in table S8.
The MD Anderson Cancer Center and BIG 3–07/TROG 07.01 trials showed that moderate hypofractionation improves physical well being at six months and enhances body image at the end of treatment, respectively, in comparison with conventional fractionation.364144Additionally, the OCOG trial showed an improved quality of life relating to breast side effects, attractiveness, fatigue, and convenience at six weeks following moderate hypofractionation compared with conventional fractionation.31In the Belgium trial, moderate hypofractionation was associated with superior quality of life relative to conventional fractionation, highlighting the potential role of treatment delivery methods; specifically, the use of tomotherapy in moderate hypofractionation facilitated better dose homogeneity.6973Conversely, the DBCG HYPO trial observed equivalent levels of satisfaction with breast appearance over a five year period across both moderate hypofractionation and conventional fractionation cohorts, and the START trials similarly noted minimal quality of life discrepancies between moderate hypofractionation and conventional fractionation.3965Lastly, the YO-HAI5 trial, comparing ultra-hypofractionation and moderate hypofractionation, indicated that patients in the ultra-hypofractionation group showed less decline in the physical and social functioning aspects of quality of life two to four weeks after radiation therapy.19
We found no statistically significant differences in the respective hazard ratios between moderate hypofractionation and conventional fractionation for local/locoregional recurrence, disease-free survival, and overall survival. Notably, we detected no significant heterogeneity for these estimates of survival (all P>0.05) (figure S15). In the subgroup analyses focusing on the number of fractions (comparing 13 fraction and 15 fraction regimens with 50 Gy in 25 fractions), most survival outcomes did not show significant differences (figures S16-S18). However, the 15 fraction regimens showed a reduced hazard ratio for disease-free survival of 0.86 (95% CI 0.76 to 0.98; P=0.03; n=6; figure S17C) compared with 50 Gy in 25 fractions. Several randomised controlled trials did not provide enough information to be included in this meta-analysis, but generally the survival outcomes between the regimens show no statistically significant difference.932404447545962686971
The FAST-Forward trial compared two different doses of ultra-hypofractionation with moderate hypofractionation.17Neither 27 Gy in five fractions nor 26 Gy in five fractions resulted in significant differences compared with moderate hypofractionation in several crucial outcomes with a median follow-up of 71.5 months (figure S19).17Finally, in the FAST trial comparing ultra-hypofractionation with conventional fractionation, the hazard ratio for ipsilateral breast recurrence combining both ultra-hypofractionation arms was 1.35 (95% CI 0.47 to 3.94; P=0.58) (figure S20).18
Effects of other factors on side effects
Results related to the effects of tumour bed boost, breast size, and smoking status are provided in the data supplement (appendix 2).
Network meta-analysis
For our network meta-analysis, we included data from 28 trials comparing moderate hypofractionation and conventional fractionation, encompassing 14 344 patients. Additionally, five trials compared ultra-hypofractionation and moderate hypofractionation, involving 4927 patients, and one trial compared ultra-hypofractionation and conventional fractionation, with 915 patients (table S9). On the basis of the network meta-analysis approach, we found that moderate hypofractionation significantly reduced the incidence of grade ≥2 acute radiation dermatitis compared with conventional fractionation, with a risk ratio of 0.56 (95% CI 0.46 to 0.68; P<0.001; figure S21A). Ultra-hypofractionation showed an even more favourable reduction in grade ≥2 acute radiation dermatitis compared with conventional fractionation (risk ratio 0.41, 95% CI 0.29 to 0.58; P<0.001; figure S21A) and a non-significant trend towards a lower incidence than moderate hypofractionation (0.74, 0.53 to 1.02; P=0.07; figure S21B). According to P score rankings, ultra-hypofractionation (98.4%) was the most favourable option, followed by moderate hypofractionation (51.6%) and conventional fractionation (0%), despite significant heterogeneity being observed (I2=65.1%, 95% CI 47.6% to 76.8%; Q test=74.50; P<0.001).
Additionally, moderate hypofractionation was associated with a reduced risk of breast shrinkage compared with conventional fractionation (risk ratio 0.92, 95% CI 0.86 to 0.99; P=0.04; figure S22E). Conversely, ultra-hypofractionation was linked to an increased risk of breast or chest wall induration and fibrosis (risk ratio 1.66, 95% CI 1.12 to 2.48; P=0.01 against conventional fractionation (figure S22C); 1.80, 1.23 to 2.64; P=0.002 against moderate hypofractionation (figure S22D)) and breast shrinkage (risk ratio 1.36, 1.08 to 1.71; P=0.009 against conventional fractionation (figure S22E); 1.47, 1.18 to 1.84; P=0.007 against moderate hypofractionation (figure S22F)) in comparison with both moderate hypofractionation and conventional fractionation. Heterogeneity across these outcomes was not significant. The P score rankings for breast or chest wall induration and fibrosis were highest for moderate hypofractionation at 94.0%, followed by conventional fractionation at 55.7% and ultra-hypofractionation at 0.4%. For grade ≥2 breast shrinkage, the rankings were moderate hypofractionation at 99.1%, conventional fractionation at 50.7%, and ultra-hypofractionation at 0.3%. Ultra-hypofractionation also increased the risk of grade ≥2 breast oedema compared with moderate hypofractionation (risk ratio 2.49, 95% CI 1.21 to 5.10; P=0.01; Figure S22H), with evidence of heterogeneity (I2=58.4%, 95% CI 16.3% to 79.4%; Q test=21.66; P=0.01). The P score rankings for grade ≥2 breast oedema were most favourable for moderate hypofractionation at 96.0%, followed by conventional fractionation at 52.1% and ultra-hypofractionation at 2.0%. Late non-skin side effects and survival outcomes showed no significant differences across the fractionation regimens. Details of the network meta-analysis are in appendix 3. The forest plots, network plots, and league tables, which visualise the pairwise comparisons, can be found in figures S21-S35 and tables S10-S33, respectively.
Discussion
Previous systematic reviews and meta-analyses of trials of breast radiation therapy fractionation primarily focused on disease recurrences and survival metrics.76777879Our study adds a patient centred perspective crucial for informed decision making by examining not only oncological outcomes but also acute and late side effects, cosmetic outcomes, and quality of life associated with moderate hypofractionation and ultra-hypofractionation, compared with conventional fractionation. In our analysis, moderate hypofractionation showed either comparable or reduced acute and late side effects compared with conventional fractionation, and notably, improved cosmetic outcomes were reported in some cases. This suggests that moderate hypofractionation may provide superior quality of life for patients. Ultra-hypofractionation, although less extensively studied, has shown promising results, particularly with its substantial reduction in duration of treatment, which may further improve patients’ convenience and quality of life. These findings have been corroborated by the network meta-analysis, and to our knowledge this is the first study assessing those outcomes by using this approach; the consistency of the results with those of the main analyses lends robust support to our conclusions. Our review also identified several factors influencing treatment outcomes, such as the application of a tumour bed boost, breast size, and smoking status. These factors highlight the need for tailored treatment approaches to optimise outcomes.8081
Comparison with guidelines and other studies
Current guidelines recommend moderate hypofractionation as the standard of care for a broad range of patients with breast cancer, irrespective of the patient’s age, systemic therapy, and disease stage.382However, the need for robust evidence that confirms minimal differences between moderate hypofractionation and conventional fractionation in long term side effects, notably concerning lung and cardiac health,83has slowed the adoption of hypofractionation in the US.84The role of moderate hypofractionation in regional lymph node and chest wall irradiation remains controversial,83but the safety of moderate hypofractionation for these indications is increasingly supported by evidence from randomised controlled trials, including those from the START and Chinese trials.7158586Results from real world data and multiple randomised studies show that moderate hypofractionation also produces equivalent results to conventional fractionation for young patients and those treated with many different systemic therapies.151644628788
The use of ultra-hypofractionation as per the FAST-Forward trial garners differing levels of endorsement across major guidelines, reflecting ongoing debates. The European Society for Radiotherapy and Oncology (ESTRO) and the Italian Association for Radiotherapy and Clinical Oncology endorse 26 Gy in five fractions for both whole breast and chest wall irradiation without reconstruction as standard of care.8289For chest wall irradiation after breast reconstruction, ESTRO advises its use only within clinical trials.82Similarly, in the UK, the National Institute for Health and Care Excellence (NICE) recommends 26 Gy in five fractions over one week for patients with invasive breast cancer undergoing partial breast, whole breast, or chest wall radiation therapy, excluding those requiring regional lymph node irradiation, following breast conserving therapy or mastectomy.90For implant based reconstruction, NICE recommends moderate hypofractionation using 40 Gy in 15 fractions.90By contrast, the National Comprehensive Cancer Network guideline recommends that the FAST-Forward regimen of 26 Gy in five fractions should be offered as an alternative to the FAST regimen (28.5 Gy in five weekly fractions) for selected early stage disease, noting that the efficacy and safety results of the former are not yet available beyond five years.91The German Society of Radiation Oncology (DEGRO) recommends cautious use of postoperative whole breast radiation therapy in five fractions (FAST and FAST-Forward regimens), especially in patients with good long term prognosis.92DEGRO also advises against ultra-hypofractionation in post-mastectomy patients or those needing regional nodal irradiation and urges caution in younger patients.92These guidelines collectively indicate a growing acceptance of ultra-hypofractionation, particularly for early stage breast or chest wall radiation therapy. However, the body of evidence, particularly long term data, for ultra-hypofractionation is less comprehensive compared with that for moderate hypofractionation regimens, and hence uncertainty remains about its long term control and side effects.
Similarly to a previous meta-analysis,79our study found that moderate hypofractionation yields similar rates of local and locoregional recurrence and disease-free and overall survival to conventional fractionation. We observed a slightly improved disease-free survival for the 15 fraction moderate hypofractionation regimens compared with 50 Gy in 25 fractions. However, this subtle difference necessitates cautious interpretation, acknowledging the potential for statistical variance. Specifically, the observed variance may be related to an increased risk of type I errors stemming from multiple hypothesis testing.
Strengths and limitations of study
Our study has several limitations, including the risk of bias due to the lack of blinding of patients and/or outcome evaluators. However, masking is not possible in this kind of intervention, and survival outcomes such as local and locoregional control and disease-free and overall survival are unlikely to have been influenced by the lack of blinding. To ensure a comprehensive assessment, we used a rigorous approach to evaluate the risk of bias and quality of evidence. Sensitivity analysis confirmed robustness even when high risk studies were excluded, reinforcing the strength of evidence despite limitations. Another limitation is that not all outcomes were reported for all trials, especially for side effects and cosmesis, hindering the drawing of definitive conclusions. Our findings nevertheless indicate a generally low heterogeneity between the included studies, enhancing the robustness of our conclusions.
Conclusions and policy implications
In summary, our study corroborates the efficacy of moderate hypofractionation in radiation therapy for breast cancer, highlighting additional benefits including reduced side effects, increased convenience, and potential cost effectiveness. These findings justify the wider adoption of moderate hypofractionation as the preferred approach, given its balance of therapeutic efficacy and improved safety. This approach not only enhances convenience for patients but also improves resource use in healthcare facilities by reducing administrative costs and boosting the ope","Objective: To provide a comprehensive assessment of various fractionation schemes in radiation therapy for breast cancer, with a focus on side effects, cosmesis, quality of life, risks of recurrence, and survival outcomes.
Design: Systematic review and meta-analysis.
Data sources: Ovid MEDLINE, Embase, and Cochrane Central Register of Controlled Trials (from inception to 23 October 2023).
Study selection: Included studies were randomised controlled trials focusing on conventional fractionation (CF; daily fractions of 1.8-2 Gy, reaching a total dose of 50-50.4 Gy over 5-6 weeks), moderate hypofractionation (MHF; fraction sizes of 2.65-3.3 Gy for 13-16 fractions over 3-5 weeks), and/or ultra-hypofractionation (UHF; schedule of only 5 fractions).
Data extraction: Two independent investigators screened studies and extracted data. Risk of bias and quality of evidence were assessed using the Cochrane Collaboration’s tool and the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) approach, respectively.
Data synthesis: Pooled risk ratios (RRs) and hazard ratios (HRs) with 95% confidence intervals (CIs) were calculated using a random effects model. Heterogeneity was analysed using Cochran’s Q test and I2statistic. Network meta-analysis was used to integrate all available evidence.
Main outcome measures: The pre-specified primary outcome was grade ≥2 acute radiation dermatitis and late radiation therapy related side effects; secondary outcomes included cosmesis, quality of life, recurrence, and survival metrics.
Results: From 1754 studies, 59 articles representing 35 trials (20 237 patients) were assessed; 21.6% of outcomes showed low risk of bias, whereas 78.4% had some concerns or high risk, particularly in outcome measurement (47.4%). The RR for grade ≥2 acute radiation dermatitis for MHF compared with CF was 0.54 (95% CI 0.49 to 0.61; P<0.001) and 0.68 (0.49 to 0.93; P=0.02) following breast conserving therapy and mastectomy, respectively. Hyperpigmentation and grade ≥2 breast shrinkage were less frequent after MHF than after CF, with RRs of 0.77 (0.62 to 0.95; P=0.02) and 0.92 (0.85 to 0.99; P=0.03), respectively, in the combined breast conserving therapy and mastectomy population. However, in the breast conserving therapy only trials, these differences in hyperpigmentation (RR 0.79, 0.60 to 1.03; P=0.08) and breast shrinkage (0.94, 0.83 to 1.07; P=0.35) were not statistically significant. The RR for grade ≥2 acute radiation dermatitis for UHF compared with MHF was 0.85 (0.47 to 1.55; P=0.60) for breast conserving therapy and mastectomy patients combined. MHF was associated with improved cosmesis and quality of life compared with CF, whereas data on UHF were less conclusive. Survival and recurrence outcomes were similar between UHF, MHF, and CF.
Conclusions: MHF shows improved safety profile, cosmesis, and quality of life compared with CF while maintaining equivalent oncological outcomes. Fewer randomised controlled trials have compared UHF with other fractionation schedules, but its safety and oncological effectiveness seem to be similar with short term follow-up. Given the advantages of reduced treatment time, enhanced convenience for patients, and potential cost effectiveness, MHF and UHF should be considered as preferred options over CF in appropriate clinical settings, with further research needed to solidify these findings.
Systematic review registration: PROSPERO CRD42023460249.
"
Effectiveness of modified vaccinia Ankara-Bavarian Nordic vaccine against mpox infection,"Introduction
In May 2022, more than 20 countries where mpox had not been previously identified reported infections to the World Health Organization.1On 23 July 2022, the global mpox outbreak was declared a public health emergency of international concern, and targeted use of second or third generation smallpox vaccines was recommended for control of the outbreak.2
Modified vaccinia Ankara-Bavarian Nordic (MVA-BN) vaccine (trade names Imvamune, Jynneos, and Imvanex) is a third generation, live attenuated, non-replicating vaccine against smallpox.3In Ontario, Canada, MVA-BN was introduced in June 2022 as post-exposure prophylaxis for high risk contacts (but few doses were given in this context) and pre-exposure prophylaxis for gay, bisexual, and other men who have sex with men, and sex workers at high risk of exposure to mpox.4Although MVA-BN is approved in Canada as a series of two doses 28 days apart, Ontario initially employed a dose sparing strategy such that vaccine candidates could only receive one dose owing to concerns about limited vaccine supply. A two dose (0.5 mL each, subcutaneously) programme was subsequently implemented on 30 September 2022.
Before the global mpox outbreak, clinical or real world data on the use of MVA-BN to prevent mpox infection were limited.56Estimates of the effectiveness of a single dose of MVA-BN obtained using various observational study designs have since emerged in the literature, ranging from 36% to 86%.789101112131415As with all observational studies, each report discussed the potential for residual confounding and selection biases. Only one study to date emulated a target trial to address these biases, but it was restricted to HIV negative men who had used HIV pre-exposure prophylaxis.10In the current study we estimated the vaccine effectiveness of one dose of MVA-BN against laboratory confirmed mpox infection in a broader population through a target trial emulation to reduce biases.
Methods
Study design, setting, and population
We conducted a target trial emulation to answer the causal question of interest (see supplementary figure S1 and table S1) and to reduce biases, particularly from confounding.16Laboratory, vaccination, reportable diseases, and health administrative data were used from Ontario (population 15.1 million as of July 2022), which has a single payer healthcare system. All datasets included in the analysis (see supplementary methods) were linked using unique encoded identifiers and analysed at ICES.
The study period captured the beginning of the availability of pre-exposure vaccination (12 June 2022 to 26 November 2022, during which time mpox was diagnosed in 691 people in Ontario;fig 1, also see supplementary table S2 for Ontario surveillance definitions of mpox).17The end date was chosen based on several indicators, including weekly percentage positivity <5% and the last individual with outbreak associated mpox reported on 10 November 2022.18Eligibility for single dose, pre-exposure vaccination comprised gay, bisexual, and other men who have sex with men reporting one or more of a diagnosis of bacterial sexually transmitted infection (STI) in the previous two months; currently engaging in or anticipating sex with two or more sexual partners; attending sex-on-premises venues; or engaging in anonymous sex. Eligibility for pre-exposure vaccination also included individuals engaged in sex work, immunocompromised individuals, or pregnant individuals if they were contacts of people at risk, as defined above.19
Because the administrative data do not include information on each of the specific criteria laid out by Ontario’s MVA-BN vaccine programme, we used proxies for potential sexual exposure to mpox to define our eligible population for the trial specification. That is, if a randomised control trial were to be possible but was restricted to using variables available in the health administrative data, our approach to target trial specification would make use of these variables as proxies for sexual activity (see supplementary table S1). Eligibility criteria for the target trial were conceptualised to reduce confounding between vaccination status and risk of subsequent infection. The study population was restricted to men aged ≥18 years as of 12 June 2022 with at least one of the following proxies for risk of exposure to mpox as of the date of matching (ie, time zero, which could occur between 12 June 2022 and 27 October 2022, to ensure each person could have at least 30 days of observation): at least one syphilis test in the previous year and a new diagnosis of one bacterial STI or more (chlamydia, gonorrhoea, or syphilis) in the year before matching; or a filled prescription for HIV pre-exposure prophylaxis in the year before matching (see supplementary table S3 for definitions). We excluded individuals with a documented positive polymerase chain reaction test result for mpox before 12 June 2022.
Intervention and outcome
The intervention of interest was vaccination with a single dose of MVA-BN. We were unable to estimate the effectiveness of a second dose because only a few individuals had received such a dose (13.7% of those who received one dose) by the end of the study period (26 November 2022) and few people received an mpox diagnosis in October and November. The outcome of interest was polymerase chain reaction confirmed mpox infection, based on the specimen collection date. Based on immunogenicity data, an individual was classified as vaccinated >14 days after the first dose.20
Specification and emulation of the target trial
On each day between 12 June 2022 and 27 October 2022, men who had been vaccinated with a single dose of MVA-BN 15 days previously were matched in a 1:1 ratio to unvaccinated controls. We followed individuals until the earliest date of any of the following events: outcome, death, 15 days after receipt of a first vaccine dose (for unvaccinated controls), 15 days after receipt of a second dose, or end of the study period. Individuals who initially contributed observation time as an unvaccinated control were censored (along with their matched vaccinated individual) 15 days after receipt of MVA-BN and were re-matched as a vaccinated individual with a new unvaccinated control (see supplementary table S1).21
To balance the distribution of measured baseline covariates that are associated with the probability of vaccination and mpox infection between vaccine recipients and controls, we matched vaccine recipients and controls on age (within five years), geographical region (since the epidemic trajectory and vaccine uptake varied regionally), proxies for sexual exposures (number of bacterial STIs in the previous three years, HIV status), and a proxy for vaccine confidence (receipt of any non-MVA-BN vaccine in the previous year). These covariates were defined using 12 June 2022. The supplementary methods section provides details of the matching algorithm. For vaccinated individuals, time zero was 15 days after vaccination, whereas unvaccinated controls inherited the time zero of the vaccinated person to whom they were matched.
We conducted three sensitivity analyses. To explore the potential for residual confounding by risk of sexual exposures, we used two negative control outcomes that should not be directly affected by the receipt of MVA-BN but for which the effect of vaccination might be confounded.22Firstly, we measured the risk of mpox during the first 14 days after the first dose, when no difference between vaccinated and unvaccinated groups would be expected (the negative outcome period). Secondly, we used a negative tracer outcome by estimating vaccine effectiveness against bacterial STI >14 days after vaccination; MVA-BN vaccine presumably has no benefit against infection with chlamydia, gonorrhoea, or syphilis. However, an STI diagnosis could be influenced by differential rates of testing after vaccination. Thus we compared syphilis testing among vaccinated and unvaccinated groups over the study period to aid interpretation of the negative tracer outcome. Finally, we examined the potential for residual confounding related to socioeconomic status by adjusting for income at neighbourhood level, given that sexual networks and infection risks are shaped by systemic barriers to engagement in healthcare and access to vaccines.
Statistical analysis
We examined covariate balance after matching using standardised mean differences, and considered a difference of ≥0.1 as potentially clinically meaningful.23Cumulative incidence functions were estimated for the vaccinated and unvaccinated groups and we used a Cox proportional hazards model to estimate the hazard ratio comparing the hazard of mpox between the two groups, using a robust variance estimator to account for the matched design.24We calculated vaccine effectiveness as ((1–hazard ratio)×100). Analyses were performed using SAS software version 9.4 (SAS Institute, Cary, NC).
Patient and public involvement
This work was undertaken in response to questions about the effectiveness of MVA-BN by public health, clinical, and community members. Participants were not involved in the original design of this study. We shared study results with diverse community representatives interested in the mpox response.
Results
A total of 9803 men aged ≥18 years were eligible for the study, of whom 272 received a diagnosis of mpox during the study period, including 15 who required hospital admission with mpox. A total of 3204 men who received the vaccine were matched to 3204 unvaccinated controls (fig 2). The matched population was similar to the eligible population for baseline characteristics (see supplementary table S4). All measured variables were well balanced between the vaccinated and unvaccinated groups (table 1). The median age of matched participants was 35 years (interquartile range (IQR) 29-46 years) and more than half of the participants (66.1%) were residents of Toronto.
During a median follow-up of 85 days (IQR 32-110 days) after the first dose among vaccinated individuals and 86 (31-111) days among unvaccinated individuals, we observed a total of 71 infections, with 21 in the vaccinated group (0.09 per 1000 person days, 95% confidence interval (CI) 0.05 to 0.13) and 50 in the unvaccinated group (0.20 per 1000 person days, 0.15 to 0.27) over the study period of 153 days. We censored 293 (9.1%) individuals owing to receipt of a second dose.Figure 3shows the cumulative incidence functions for the vaccinated and unvaccinated groups during the study period. The hazard ratio for infection in the vaccinated group compared with unvaccinated group was 0.42 (95% CI 0.25 to 0.69), thus the estimated vaccine effectiveness for a single dose of MVA-BN against mpox infection was 58% (95% CI 31% to 75%;fig 4).
Examination for residual confounding using negative outcomes showed a vaccine effectiveness of –15% (–92% to 31%) during the first 14 days post-vaccination, and vaccine effectiveness of –89% (–125% to –58%) against bacterial STI (see cumulative incidence functions in supplementary figure S2). Rates of a first syphilis test post-vaccination were 0.05 per 1000 person days in the vaccinated group and 0.03 per 1000 person days in the unvaccinated group. Finally, we did not identify a meaningful change in vaccine effectiveness against mpox infection after additionally adjusting for neighbourhood level income (vaccine effectiveness 60%, 34% to 76%).
Discussion
Using a target trial emulation, we estimated the effectiveness of a single dose of MVA-BN vaccine to be moderate (58%, 31% to 75%) for preventing mpox infection in the context of a targeted vaccination programme in Ontario, Canada. To confirm the specificity of this association, we determined that MVA-BN was not associated with a reduced rate of mpox infection during the first 14 days post-vaccination (before developing an adequate antibody response) nor bacterial STI diagnoses (against which no protection would be expected).
Comparison with other studies
Our estimate of vaccine effectiveness falls within the range observed across previous studies conducted in jurisdictions with similar epidemic dynamics and targeted vaccination programmes. Our findings are most consistent with studies that restricted the study population to those at greatest risk of exposure to mpox and that reduced time based and risk based confounding.810111415In the Canadian province of Quebec, a test negative study that used health administrative data and adjusted for exposure risks based on similar proxies as in our study (ie, previous bacterial STI), estimated vaccine effectiveness against mpox infection to be 35% (95% CI 2% to 59%).14After further adjusting for self-reported measures of exposure risks (restricting analyses to those who completed a detailed questionnaire), vaccine effectiveness was estimated to be 65% (1% to 87%), similar to our estimate. Our estimate is lower than (but still compatible with) an estimate of 86% (95% CI 59% to 95%) from a retrospective cohort study in Israel that used more restrictive study eligibility criteria (ie, living with HIV and a recent diagnosis of bacterial STI, or receipt of HIV pre-exposure prophylaxis),8and an estimate of 79% (95% CI 33% to 100%) from a target trial emulation conducted in Spain with even more restrictive study eligibility (ie, enrolment restricted to men receiving HIV pre-exposure prophylaxis).10
Strengths and limitations of this study
Our study has several strengths. Firstly, we used linked population based databases within a publicly funded healthcare system to identify all MVA-BN vaccination events and all mpox related laboratory tests in Ontario. Secondly, to address the risks of residual confounding present in any observational study, we conducted rigorous matching across key potential confounders of the causal effect of vaccination on mpox infection. Risk confounding is particularly important when estimating vaccine effectiveness because Ontario, like other jurisdictions, specifically targeted vaccination to individuals at greatest risk of infection. Evidence of exchangeability includes the similarity between groups for proxies of sexual exposure risks, and similar outcomes during the negative control period before the vaccine was expected to confer protection. We examined this negative control period based on immunogenicity data, recognising that some protection may have been conferred in the first 14 days, if the dose was administered deliberately or inadvertently within the window for post-exposure prophylaxis.2526The negative tracer outcome analysis involving bacterial STI suggests that the observed vaccine effect is unlikely to be explained by differential reductions in sexual activity among gay, bisexual, and other men who have sex with men over the study period. In contrast, our finding of higher rates of newly diagnosed bacterial STIs among vaccinated men suggests that the vaccination programme successfully reached those most at risk of mpox and/or vaccinated men engaged in increased sexual activity post-vaccination. This means that vaccinated individuals may have engaged in more sexual activity than their unvaccinated counterparts after vaccination. Indeed, given the focus of vaccine campaign messaging on preventing future exposure risks, individual decisions about the vaccine could be shaped by anticipating future sexual partnerships, irrespective of the past. The higher rates of bacterial STIs post-vaccination could also stem from additional STI testing opportunities and detection after engagement in preventive care with vaccination, as evidenced by higher syphilis testing rates during the post-vaccination period. However, the negative tracer findings suggest that residual confounding could be present due to differential increases in sexual activity, and thus our estimate of vaccine effectiveness may be underestimated. Finally, the study period included a rapidly evolving outbreak, with risks of exposure to mpox declining quickly before a large fraction of the study eligible population was vaccinated, thus the risk for time varying confounding due to differential exposure risks was substantial, which we reduced by emulating a target trial.
Our study also has limitations. Firstly, the rigorous matching meant that our final cohort comprised only 65% of the eligible population, with 71 outcomes for analysis. Thus, the final cohort comprised 8.8% of the 36 312 first dose vaccinations and 10% of all mpox diagnoses in Ontario during the study period. Although the characteristics of included and eligible participants were similar, a reduction in confounding came at the price of decreased sample size and precision. It also meant that subgroup analyses, such as among individuals aged >50 years, who may have received earlier generation smallpox vaccines, were not possible. Secondly, we were limited to routinely collected data, and information on previous smallpox vaccination, sexual exposures, and individual level measures of social determinants of health were not available. Information on neighbourhood level income was available but was not used for matching to limit further loss of sample size, and because area level median income may not capture the ways in which individual level income, or other individual level social determinants, might influence sexual networks.27Furthermore, comparison across groups and the third sensitivity analysis suggested no residual confounding by neighbourhood level income. Thirdly, although we included men with a history of bacterial STIs, our study eligibility population could be missing men who are at risk of mpox infection but have negligible access to healthcare and/or healthcare engagement (thus leading to a selection bias). Fourthly, although data from other studies showed added protective benefit of two vaccine doses,91112we could not evaluate the two dose regimen because of low second dose coverage during the study period, nor could we evaluate duration of protection. Finally, vaccination could also reduce symptoms and signs of mpox and thus result in less testing,28which would mean a higher chance of under-ascertainment of people with subclinical infection among the vaccinated group, which would lead to overestimation of vaccine effectiveness against infection.
Conclusions
Vaccination with a single dose of MVA-BN vaccine was found to be moderately effective against laboratory confirmed mpox infection in this population based study of an evolving outbreak and using a target trial emulation to reduce biases. One implication of our finding is that single dose vaccination may have been a contributing factor in helping to slow transmission in Ontario in 2022. Mpox infections in Canada and across the globe are rising again in 2024, with most diagnoses among individuals who have not yet been vaccinated or have received only a single dose of vaccine.2930Given the moderate effectiveness of a single dose, achieving high coverage with a full course could be important to prevent and manage ongoing transmission globally and prevent a large resurgence.3132In the absence of randomised clinical trials, our findings strengthen the evidence that MVA-BN is effective at preventing mpox infection and should be made available and accessible to communities at risk.
No randomised clinical trials of vaccination against mpox have been conducted
Estimates of vaccine effectiveness of a single dose of vaccination range from 36% to 86%, but these observational designs noted residual confounding as a major concern given vaccine implementation was appropriately prioritised to individuals most at risk of infection
Estimates of vaccine effectiveness, using approaches to minimise biases, are needed
In an emulated target trial to reduce biases, the effectiveness of a single dose of modified vaccinia Ankara-Bavarian Nordic (MVA-BN) vaccine against mpox infection was 58% (95% confidence interval 31% to 75%)—a finding that was robust to further sensitivity analysis for residual confounding
In the absence of data from randomised controlled trials, the study findings strengthen the evidence that MVA-BN is effective at preventing mpox infection and should be made available and accessible to communities at risk
","Objective: To estimate the real world effectiveness of modified vaccinia Ankara-Bavarian Nordic (MVA-BN) vaccine against mpox infection.
Design: Emulation of a target trial.
Setting: Linked databases in Ontario, Canada.
Participants: 9803 men aged ≥18 years with a history of being tested for syphilis and a laboratory confirmed bacterial sexually transmitted infection (STI) in the previous year, or who filled a prescription for HIV pre-exposure prophylaxis in the previous year. On each day between 12 June 2022 and 27 October 2022, those who had been vaccinated 15 days previously were matched 1:1 with unvaccinated men by age, geographical region, past HIV diagnosis, number of bacterial STI diagnoses in the previous three years, and receipt of any non-MVA-BN vaccine in the previous year.
Main outcome measure: The main outcome measure was vaccine effectiveness ((1–hazard ratio)×100) of one dose of subcutaneously administered MVA-BN against laboratory confirmed mpox infection. A Cox proportional hazards model was used to estimate hazard ratios to compare the rate of laboratory confirmed mpox between the two groups.
Results: 3204 men who received the vaccine were matched to 3204 unvaccinated controls. A total of 71 mpox infections were diagnosed, with 0.09 per 1000 person days (95% confidence interval (CI) 0.05 to 0.13) in the vaccinated group and 0.20 per 1000 person days (0.15 to 0.27) in the unvaccinated group over the study period of 153 days. Estimated vaccine effectiveness of one dose of MVA-BN against mpox infection was 58% (95% CI 31% to 75%).
Conclusion: The findings of this study, conducted in the context of a targeted vaccination programme and evolving outbreak of mpox, suggest that one dose of MVA-BN is moderately effective in preventing mpox infection.
"
Effect of laughter exercise versus 0.1% sodium hyaluronic acid on ocular surface discomfort in dry eye disease,"Introduction
Dry eye disease is a widespread, complex ocular surface disease characterised by chronic subjective ocular discomfort, tear film instability, and visual disturbance.123With the population ageing, video display terminal use increasing, and air pollution, the global prevalence of dry eye disease has risen sharply, affecting approximately 360 million individuals.4Use of artificial tears is the mainstay treatment for dry eye disease. Although not fatal, dry eye disease substantially reduces life quality and is a substantial economic burden, particularly among long term users of artificial tears.5The global artificial tears market had a revenue share of USD 2.74 billion in 2022 (£2.12 billion, €2.53 billion), expected to advance to USD 4.40 billion by 2031.6Additionally, the societal costs such as lost work productivity and the psychological and physical impacts of dry eye disease, are estimated at $55 billion in the US.7
Studies across multiple geographies and populations have reported the correlation between dry eye disease and mental health, particularly in negative emotions such as depression and anxiety.89101112In people with depression or anxiety, the link is stronger for dry eye disease symptoms as compared with patient signs.810131415These results suggest that the connection between dry eye disease symptoms and depression or anxiety does not appear to be driven by dry eye disease signs.11
In recognition of the importance of positive psychology, the World Health Organization considers happiness as a crucial component of overall well being.16Individuals with higher levels of subjective happiness report fewer symptoms of dry eye disease,1718however, the impact of positive emotions on dry eye disease remains uncertain.
In accordance with the well known saying “laughter is the best medicine”, laughter therapy was first reported by Cousins in 1979, primarily involving mimicking mouth shapes while enunciating “hahahoho.”19Research findings suggested laughter might be therapeutic to reduce disease symptoms.20At present, laughter therapy comprises various interventions designed to provoke laughter, smiling, and pleasant feelings such as laughter exercises, clowns, comedy movies, games, and puzzles.21Abundant evidence suggests that laughter therapy alleviates depression, anxiety, stress, and chronic pain, while strengthening immune function222324; and therefore, it has been recognised as a complementary and adjunctive approach for chronic conditions like mental health disorders, cancer, diabetes, among others.252627282930Whether laughter therapy has a beneficial effect on dry eye disease, a chronic condition closely related to mental health and lifestyle,31remains unknown. In our pilot studies before the randomised controlled trial, we observed that laughter could immediately improve tear film stability and lipid layer thickness (protocol, pages 12-13 in supplementary file, data unpublished). Additionally, in a small scale study, we found that laughter exercise could alleviate symptoms of dry eye disease (protocol, pages 15-16 in supplementary file, data unpublished).
In this study, we conducted a randomised trial to compare the effectiveness of laughter exercise versus artificial tears in treating symptomatic dry eye disease. The hypothesis was that non-inferiority would be established by observing a between-group difference in the ocular surface disease index score of ≤6 points in eight weeks between the laughter exercise group and the controls, who were using the mainstay treatment of artificial tears. We chose a non-inferiority trial design because laughter exercise is a safe, environmentally friendly, and low cost intervention.
Methods
Study design
In this two arm, non-inferiority randomised controlled trial, the setting was at a single tertiary referral centre, Zhongshan Ophthalmic Center, Sun Yat-sen University, which is the largest ophthalmic centre in southern China. All procedures adhered to the protocol approved by the ethics committee of Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangzhou, China (2020KYPJ010). Written informed consent was obtained from all participants. The trial protocol and statistical analysis plan are available in the supplementary files.
Participants
Participants with symptomatic dry eye disease were recruited from cornea and ocular surface disease clinics and locations surrounding the hospital using community advertisements, flyers, and via social media. All participants passed a screening examination 14 days before inclusion. Screening included a medical history and subjective assessment of dry eye disease symptoms using the ocular surface disease index. The international dry eye workshop in 2007 and 2017, emphasised that dry eye disease is a symptomatic disease, and symptom questionnaires are among the most reliable diagnostic tests available.3233The ocular surface disease index is a 12-item index measuring ocular discomfort, with scores ranging from 0 to 100, with 0 indicating the least discomfort and 100 the greatest.
Participants were required to fulfil the diagnostic criteria of the global consensus tear film and ocular surface association dry eye workshop II for dry eye disease.32Participants were also required to have the ocular surface disease index score of 18 to 80 at both the screening and the eligibility confirmation visits approximately two weeks later, and a fluorescein tear break-up time (time from a blink to the appearance of the first gap in the tear film, with shorter times indicating worse tear film stability) of eight seconds or less at eligibility confirmation visit. Exclusion criteria included a score from the National Eye Institute’s corneal fluorescein staining examination of more than 5 (range 0-15, with higher scores indicating more severe corneal epithelial defects); use of any dry eye disease treatment in the 14 days before enrolment; contact lens usage within 14 days before screening visit; inability to guarantee no contact lens wear for three months after trial participation; any eye surgery or history of ocular trauma within the past 12 months; any history of eye infection or allergy within the past three months; severe ocular surface scarring or conditions that could compromise the integrity of the ocular surface, such as Steven-Johnson syndrome; any previous diagnosis of glaucoma or treatment with glaucoma medications; and the presence of neurological, psychiatric or, sleep disorders. We did not enrol anyone with cognitive or comprehension deficits who could not co-operate with the eye examination, questionnaire completion, or use of the face recognition application study application laughing face, or who had major life changes, such as planning to move to another city within the next three months, or were fearful of the bright light from eye examination equipment for this study. A complete list of the inclusion and exclusion criteria is provided in the protocol (pages 20-22).
All participants were shown the protocol and signed patient consent forms.
Randomisation and masking
Stratified block randomisation was applied with a block size of four. Participants were stratified based on their baseline ocular surface disease index scores (mild ≤18 to <23, moderate ≤23 to <33 and severe ≤33 to <80). Within each stratum, patients were randomly assigned in a 1:1 ratio to receive either laughter exercise or 0.1% sodium hyaluronic acid for eight weeks. The random sequence was generated by an independent statistician who was not involved in project implementation, using an online generator. The randomisation assignment was kept undisclosed to the trained investigators responsible for screening and outcome assessment. Participants were unmasked to group assignment but were masked to trial hypotheses.
Intervention
Drawing from previously established methods of laughter therapy, we adapted and developed the laughter exercise (supplementary video) to optimally engage the ocular muscles of participants. Participants were instructed to perform the laughter exercise four times daily for eight weeks. At the baseline visit, an instructional video showed the execution of the exercise. Specifically, laughter exercise required participants to vocalise and repeat the phrases as “Hee hee hee, hah hah hah, cheese cheese cheese, cheek cheek cheek, hah hah hah hah hah hah” 30 rounds each time, lasting for at least five minutes (video 1,video 2). To standardise the laughter exercise and enhance facial movements throughout the session, participants used a face recognition application on their mobile devices that had been designed by authors, named laughing face, at the start of each session (supplementary figure 1). The app was developed in collaboration with South China University of Technology and Xinhuixing Information Technology, Inc (Guangzhou, China). Following the eight week intervention, participants ceased the exercise and underwent follow-up at the week 10 and 12.
Instructions given to the laughter exercise group translated into English
Instructions given to the laughter exercise group in Mandarin Chinese video
Participants in the control group applied artificial tears, 0.1% sodium hyaluronic acid eyedrops, to both eyes four times daily for eight weeks, tracking their usage frequency via the laughing face app. After the eight week treatment, they were instructed to discontinue the eye drops and returned for follow-up appointments at weeks 10 and 12.
The participants were followed up on weekends for their convenience. Participants in both groups were encouraged to continue their routine activities during the 12 week study period. Adherence was supported through an oral and written commitment from all participants at the baseline evaluation. For daily record keeping, participants in both study groups used the laughing face app, for which they were provided with individual access codes. The app provided daily text reminders to complete the study interventions. Those in the laughter exercise group were requested to practice with the app before each laughter session and upload their exercise videos at least once a day. All participants used the app to log their exercise sessions or eye drops use. If a participant missed an exercise session or eye drop application, they were prompted by the app to make up for the missed session. Throughout the eight week period, we tracked the number of missed exercise sessions and eye drops applications as a measure of compliance.
Outcomes
The primary outcome was the mean change in the ocular surface disease index score from baseline to eight weeks. Prespecified secondary outcomes included: the proportion of patients with a decrease from baseline in the ocular surface disease index score of 10 points or more (ie, the minimal clinically meaningful cut-off34), changes in signs of dry eye disease as assessed by non-invasive tear break up time, corneal fluorescein staining score, tear meniscus height, changes in the scores on the physical and mental health subscales of the 36-item short form health survey35(scores range from 0 to 100, with higher scores indicating better health-related quality of life), changes in the scores of self-rating anxiety scale36and self-rating depression scale37(scores range from 0 to 100, with higher indicating severe self-report anxiety or depression), changes in the scores of Pittsburgh sleep quality index38(scores range from 0 to 21, with higher scores indicating worse sleep quality), changes in the scores of subjective happiness scale39(scores range from 1 to 7, with higher scores indicating a higher level of happiness). Because of the inconsistent correlation between reported symptoms and clinical signs, we chose non-invasive tear break up time as the main secondary outcome. The details and examination procedures of exploratory outcomes including function and structure of meibomian gland were described in the trial protocol (supplementary file).
Participants were scheduled to return on day seven for assessments, after that at two weeks, four weeks, six weeks, and eight weeks (the last day of treatment and the time of assessment for the main study outcome), and again at 10 and 12 weeks. We monitored adverse events throughout the treatment period using a standard adverse event case report form at each visit.
Statistical analysis
The trial was designed to enrol 296 participants (n=148 in each group), reaching 90% statistical power to detect non-inferiority at a one-sided α=0.025 based on the primary outcome using a one sided, two sample t-test. We selected a non-inferiority margin of 6 points, which is 50% of the minimum meaningful clinical difference on the ocular surface disease index,32with the common standard deviation of 15. The true difference between means was assumed to be 0.00, and loss to follow up less than 10%. Non-inferiority would be established by a one sided test of the 97.5% confidence interval (CI) of the difference in trial arms that was less than the non-inferiority margin. This margin would be equivalent to testing whether a two sided 95% CI around the treatment difference falls within the non-inferiority margin. The sample size was calculated using PASS 16.0.
We developed a statistical analysis plan before final analysis (supplementary file). No interim analysis was undertaken. The trial was registered at ClinicalTrials.gov (NCT04421300) on 5 June 2020. The first participant was enrolled on 3 July 2020. The participants’ baseline characteristics were described as mean (standard deviation) or median (interquartile range) for continuous variables, and frequency (percentage) for categorical variables. The normality of continuous data was checked using the Shapiro-Wilk test and histograms.
Both per protocol and the intention-to-treat analyses were used in calculating the adjusted difference and 95% CI for the primary outcome, and intention to treat was used in calculating unadjusted difference and 95% CI for secondary outcomes. In intention-to-treat analysis, all participants undergoing randomisation were included, and all missing data were imputed using multiple imputations, creating 20 copies of the data. Results were obtained by averaging these 20 datasets using Rubin’s rules.40
Line and box plots were drawn to show the outcome’s change across time. Generalised estimated equation model with adjustment for intra-eye correlation was applied for the comparisons of clinical outcomes, which were measured in both right and left eyes. The difference between groups was tested using the two sample t-test for primary and psychological outcomes, and generalised estimated equation model for all clinical outcomes. Pre-post changes were tested with the paired t-test for primary and psychological outcomes and generalised estimated equation models for all clinical outcomes.
Data were cleaned using Stata16.0 and all statistical analyses were performed using SAS 9.4. A two sided P value less than 0.05 was considered statistically significant. For secondary outcomes, P for between group difference was adjusted by the Benjamini-Hochberg method with a false discovery rate of 0.05 for multiple comparisons. P values for exploratory outcomes were not calculated.
Patients and public involvement
Although patients or the public were not involved in the design, or conduct, or reporting, or dissemination plans of our research, they did participate in the preliminary study. Participants provided suggestions for implementing our laughter exercise, including the number of repetitions per section, the frequency of daily interventions, and how to improve communication between the research team and patients. Moreover, the results were communicated to patients who expressed an interest after the completion of the study.
Results
Between 18 June 2020 and 8 January 2021, 1335 volunteers were screened for eligibility (fig 1). Of these, 665 (50%) volunteers met none of the exclusion criteria and were scheduled to attend an eligibility confirmation visit two weeks later. Of the 613 (92%) people presenting for the examination, 299 (49%) were eligible for inclusion in the trial and were randomly assigned (1:1) to the laughter exercise group or the 0.1% sodium hyaluronic acid (control) group. The intention-to-treat population, therefore, included 149 participants in the intervention group and 150 participants in the control group.Table 1shows the baseline data for the 299 participants who were randomly allocated a group. Participants had a mean age of 28.9 (6.30) years, 74% (78/299) were women, 95% (283/299) have more than 12 years of education. The high proportion of female participants is consistent with the general population in that women are more likely to have dry eye disease than men.54142Baseline characteristics were well balanced between the two groups. In the laughter exercise group, 92% (137/149) of participants completed the scheduled follow-up visits at 12 weeks, and that in the control group was 98% (146/150). The median of the cumulative compliance during the eight week treatment was 85% (interquartile range 65-96%) for the laughter exercise group and 81% (68-90%) for the control group.
The primary outcome of mean change in the score of the ocular surface disease index from baseline to eight weeks was −10.5 points (95% CI −13.1 to −7.82) in the laughter exercise group and −8.83 (−11.7 to −6.02) in the control group. The decrease in the ocular surface disease index scores was statistically significant in both groups at eight weeks compared with baseline (both P<0.001). The upper boundary of the confidence interval for the mean between group difference in change between laughter exercise and control was lower than the non-inferiority margin of 6 points (−1.45 points (95% CI −5.08 to 2.19); P=0.43), suggesting that the laughter exercise is not inferior to 0.1% sodium hyaluronic acid eyedrops (fig 2). The intention-to-treat analysis showed similar results (difference between the two groups was −1.32 points, 95% CI −4.90 to 2.26; P=0.47)(table 2). Both treatments were stopped at eight weeks at the trial end, with further evaluations made at 12 week. The laughter exercise group had a persistent and significantly greater decrease in the ocular surface disease index score than did the control group at 12 weeks (table 2,fig 2, supplementary figure 2): the mean between group difference was −4.08 points ((95% CI −7.62 to −0.55); P=0.024).
The secondary outcome of the proportion of participants whose ocular surface disease index score decreased by at least 10 points at eight weeks was 49.3 percentage points (95% CI 41.0 to 57.5) in the laughter exercise group versus 47.3 (39.3 to 55.3) in the 0.1% sodium hyaluronic acid group (mean difference 1.96 (95% CI −9.53 to 13.5); P≥0.05)(table 3). After eight weeks of treatments, the laughter exercise group had a more significant improvement in non-invasive tear break up time than did the control group, with a mean between group difference was 2.30 seconds ((95% CI 1.30 to 3.30); P<0.001) in the change from baseline to eight weeks (supplementary figure 3A). Changes in other secondary outcome measures (supplementary figure 3B-F) were not significant between the groups (all P≥0.05). As for the subjective scale scores, improvements were noted in the self-rating anxiety scale and self-rating depression scale score between baseline and eight weeks in both groups (all P<0.01 for the change in each group, but no significant change between groups), but not in the subjective happiness scale and Pittsburgh sleep quality index scores (table 3). Additionally, the laughter exercise group showed a significant improvement in the mental health subscale of the 36-item short form health survey after eight weeks (table 3).
For the exploratory outcomes (table 3), the laughter exercise group showed amelioration in fluorescein tear break-up time by 1.82 seconds (95% CI 1.60 to 2.03), better than that in 0.1% sodium hyaluronic acid group (mean difference 1.50 seconds (95% CI 1.23 to 1.76)). Moreover, the function of the meibomian gland was improved only in the laughter exercise group, with the meibomian gland secretory capacity improved by −0.24 (95% CI −0.33 to −0.14), and meibomian gland secretion property score improved by −3.78 (−4.45 to −3.12). The above clinical signs for monocular results were consistent with those for binoculars (supplementary table 1).
No adverse events were reported in either study group.
Discussion
Principal findings
Our results showed that laughter exercise was non-inferior to 0.1% sodium hyaluronic acid in alleviating dry eye disease symptoms. Additionally, we found that laughter exercise appeared to improve tear film stability and the meibomian gland function. We observed high cumulative compliance rates in the laughter exercise group.
Dry eye disease imposes an economic burden on both society and individuals because of healthcare use, such as repeated medical visits and long term eyedrops. Artificial tears are the mainstay of treatment for dry eye disease, and 0.1% sodium hyaluronic acid, one of the most widely used artificial tears, has a proven therapeutic effect in alleviating subjective ocular discomfort, and stabilising tear film in patients with dry eye disease. In terms of alleviating symptoms, laughter exercise was non-inferior to 0.1% sodium hyaluronic acid in the per protocol and intention-to-treat analyses in our study. The primary outcome was ocular surface disease index score, a well validated and widely used tool for outcome assessment in clinical trials of dry eye disease.3234Moreover, laughter exercise significantly improved tear film stability and meibomian gland function. These benefits persisted for at least four weeks after discontinuation of the exercise, and such lasting efficacy was not noted in 0.1% sodium hyaluronic acid group.
Possible mechanisms
The biological mechanisms of laughter exercise on dry eye disease are unclear. Lacrimal gland, which secretes aqueous tears, and meibomian gland, which secretes lipid, are both innervated dominantly by parasympathetic nerves.43In the motor system, vocalisations in laughter involve the motoneuronal cell groups innervating the soft palate, pharynx, and larynx as well as the diaphragm, intercostal, abdominal, and pelvic floor muscles.44The contraction of respiratory muscles in laughter, especially abdominal breathing,45stimulates the autonomic nervous system including the activation of sympathetic and parasympathetic nervous system.4647Consequently, laughter exercise stimulates tear secretion via autonomic nervous system activation.48
Notably, the contraction of orbicularis muscle (sphincter muscles of the eyelids) during laughter exercise is another plausible explanation. According to our results, patients in the laughter exercise group had greater improvements in the secretory capacity of the meibomian gland and properties of its secretion, when compared with 0.1% sodium hyaluronic acid. During the laughter exercise, patients were instructed to perform the vocalisations aloud and exaggerate their facial expressions, thus contracting the orbicularis oculi muscle, which function as the sphincter muscles of the eyelids, and are anatomically adjacent to the meibomian glands.49The contraction of the Riolan muscle, a distinct subdivision of striated orbicularis muscle, compresses the ductules of the meibomian gland, resulting in the extrusion of lipids onto the ocular surface.50Lipid tears are essential in maintaining tear film stability by delaying tear evapouration.51
Research has indicated that positive emotions in non-human animals can induce tear secretion through the release of oxytocin,52which could be triggered by laughter.53In our study, we hypothesised that positive emotions may also prompt tear secretion through deep breathing, relaxation, and laughter during the laughter exercise. The biological mechanisms underlying this effect is warranted further study.
Dry eye disease is not only a topical disease but also a systemic disease including brain and lifestyle.1154Evidence indicates that lifestyle factors such as mental health can induce or modulate the severity of symptoms and signs of dry eye disease.115556Symptoms of dry eye disease are closely associated with anxiety and depression.1157Individuals with higher levels of subjective happiness have reported fewer symptoms of dry eye disease.1718Laughter positively affects cognitive behaviour to improve and establish healthy physical, psychological, and social relationships, and thereby quality of life.585960A study published in 2022 indicated that a facial mimicry laughter could both amplify and initiate feelings of happiness, regardless of the presence or absence of emotional stimuli.61We modified our laughter exercise by facial mimicry and voluntary facial action tasks, which might have a similar positive effect to laughter.61Evidence suggests that laughter therapy is a tool in lifestyle medicine that can improve sleep quality, and physical and psychological functions (eg, body weight, subjective stress, subjective well being),62and promote energy expenditure.636465We also observed the improved mental health score in laughter exercise group. More frequent daily laughter is associated with a lower prevalence of lifestyle related diseases, such as hypertension, diabetes mellitus, and heart disease.66Thus, laughter exercise might alleviate dry eye disease indirectly by creating a persisting positive effect on lifestyle. This effect might also explain the continued improvement in ocular surface disease index observed between eight and 12 weeks, following the termination of the trial.
Strengths and limitations of this study
Strengths of this trial include the randomised controlled design, careful compliance monitoring and support, high rates of both compliance and follow-up, and good effort to standardise the laughter exercise intervention with the use of videos.
However, our study had limitations. A double blinded study design was not practical because this would necessitate a sham laughter exercise for which no approach has been validated. To minimise possible placebo effects, participants were informed that the study aimed to compare the effects of two different interventions without alluding to the possible effects of laughter exercise. Additionally, laughter exercise has a greater time investment when compared with the use of eyedrops, albeit minimal.
Future directions
Future research should assess the most effective frequency and duration of laughter exercise. The biological mechanisms warrant further testing by experimental studies. Furthermore, research into laughter exercise for other ocular conditions compared with other artificial tears, with or without lipids, and higher concentrations of sodium hyaluronic acid, would provide a more comprehensive understanding of therapeutic potential. Moreover, the association between dry eye disease and lifestyle disorders is complicated, and a comprehensive approach should be investigated in the future.
Conclusions
The findings of this randomised controlled trial suggest that laughter exercise, four times a day, was non-inferior to 0.1% sodium hyaluronic acid, four times a day, in improving dry eye disease symptoms and clinical signs. As a safe, environmentally friendly, and low cost intervention, laughter exercise could serve as a first-line, home based treatment for people with symptomatic dry eye disease and limited corneal staining.
Dry eye disease is a chronic condition worldwide linked with psychological stress and poses an economic burden of long term use of artificial tears
0.1% sodium hyaluronic acid, a widely used artificial tear, has a proven therapeutic effect for ocular discomfort, and stabilising tear film in patients with dry eye disease
Laughter therapy is recognised as a beneficial complementary and adjunctive treatment for various chronic conditions, including mental health disorders, cancer, and diabetes
Laughter exercise was non-inferior to artificial tears (0.1% sodium hyaluronic acid) in improving dry eye disease symptoms and clinical signs
Laughter exercise is a safe, environmentally friendly, and low cost intervention for patients with symptomatic dry eye disease and limited corneal staining
","Objective: To assess efficacy and safety of laughter exercise in patients with symptomatic dry eye disease.
Design: Non-inferiority randomised controlled trial.
Setting: Recruitment was from clinics and community and the trial took place at Zhongshan Ophthalmic Center, Sun Yat-sen University, the largest ophthalmic centre in China, between 18 June 2020 to 8 January 2021.
Participants: People with symptomatic dry eye disease aged 18-45 years with ocular surface disease index scores ranging from 18 to 80 and tear film break-up time of eight seconds or less.
Interventions: Participants were randomised 1:1 to receive laughter exercise or artificial tears (0.1% sodium hyaluronic acid eyedrop, control group) four times daily for eight weeks. The laughter exercise group viewed an instructional video and participants were requested to vocalise the phrases “Hee hee hee, hah hah hah, cheese cheese cheese, cheek cheek cheek, hah hah hah hah hah hah” 30 times per five minute session. Investigators assessing study outcomes were masked to group assignment but participants were unmasked for practical reasons.
Main outcome measures: The primary outcome was the mean change in the ocular surface disease index (0-100, higher scores indicating worse ocular surface discomfort) from baseline to eight weeks in the per protocol population. The non-inferiority margin was 6 points of this index score. Main secondary outcomes included the proportion of patients with a decrease from baseline in ocular surface disease index score of at least 10 points and changes in dry eye disease signs, for example, non-invasive tear break up time at eight weeks.
Results: 299 participants (mean age 28.9 years; 74% female) were randomly assigned to receive laughter exercise (n=149) or 0.1% sodium hyaluronic acid (n=150). 283 (95%) completed the trial. The mean change in ocular surface disease index score at eight weeks was −10.5 points (95% confidence interval (CI) −13.1 to −7.82) in the laughter exercise group and −8.83 (−11.7 to −6.02) in the control group. The upper boundary of the CI for difference in change between groups was lower than the non-inferiority margin (mean difference −1.45 points (95% CI −5.08 to 2.19); P=0.43), supporting non-inferiority. Among secondary outcomes, the laughter exercise was better in improving non-invasive tear break up time (mean difference 2.30 seconds (95% CI 1.30 to 3.30), P<0.001); other secondary outcomes showed no significant difference. No adverse events were noted in either study group.
Conclusions: The laughter exercise was non-inferior to 0.1% sodium hyaluronic acid in relieving subjective symptoms in patients with dry eye disease with limited corneal staining over eight weeks intervention.
Trial registration: ClinicalTrials.govNCT04421300.
"
Long term exposure to road traffic noise and air pollution and risk of infertility,"Introduction
Infertility is a major global health problem affecting one in seven couples trying to conceive.1Infertility affects all geographical areas of the world, with some of the highest rates observed in south and central Asia, sub-Saharan Africa, the Middle East, north Africa, and central and eastern Europe.2Infertility is defined as lack of conception after one year of regular, unprotected sexual intercourse.3The use of various assisted reproductive technologies has increased noticeably since the 1980s, and more than 10 million children have been conceived using such technologies worldwide.4Infertility in both men and women is associated with various long term adverse health effects, including shorter life expectancy and increased risk of various psychiatric disorders and somatic diseases.56Furthermore, infertility is often a harsh experience, with a high level of physical and psychological strain, including high stress levels, anxiety, and symptoms of depression.78
Many of the established risk factors for infertility are similar for men and women and include advanced age (especially for women, where fertility drops rapidly after the late 30s), tobacco and alcohol use, sexually transmitted infections, various chronic conditions and diseases, obesity, and severe underweight.9In addition, exposure to environmental factors, such as air pollution, pesticides, and ionising radiation, are suspected risk factors for infertility.10Ambient air pollution is a major environmental pollutant causing cardiometabolic and respiratory morbidity and mortality.1112Furthermore, during the past decade, epidemiological studies have found particulate air pollution to be negatively associated with sperm quality, specifically lower sperm motility and count and changes in sperm morphology.131415A growing number of studies have indicated that air pollution is also associated with a reduced success rate after fertility treatment in women,1617181920although results are inconsistent.212223In contrast, only a few studies have studied the effects of air pollution on infertility in women, with inconsistent results.24252627Also, these studies mainly investigated effects on fecundability, thus not capturing infertility in women directly, as fecundability can be influenced by infertility in both men and women.
Road traffic noise is another prevalent environmental pollutant that has been linked with various chronic diseases.282930Health effects of noise are suggested to be mediated through the triggering of a stress response, with activation of the autonomic nervous system and the hypothalamic-pituitary-adrenal axis,31as well as through sleep disturbance.32Both stress and sleep disturbance have been suggested to be associated with impaired reproductive function, including reduced sperm count and quality, menstrual irregularity, and impaired oocyte competence.333435A main suggested biological pathway is activation of the hypothalamic-pituitary-adrenal axis, with release of stress hormones and inhibition of the hypothalamic-pituitary-gonadal axis, resulting in decreased levels of male and female sex hormones.333435Only one study has investigated the effects of noise on fertility, specifically self-reported time to pregnancy in a cohort of ≈65 000 pregnant women, and the results indicated that road traffic noise was associated with an increased time to pregnancy.36
We investigated if long term exposure to road traffic noise and pollution from particulate matter air with a diameter <2.5 µm (PM2.5) in the Danish population was associated with a higher risk of infertility in men and women, using individual level, time varying information on noise, air pollution, and socioeconomic variables and follow-up for infertility in the Danish National Patient Register.
Methods
Study population
Our study was based on all people residing in Denmark. Since 1968, all Danish inhabitants have been assigned a unique identification number, enabling linkage between administrative and health registers.37We used the Civil Registration System with exact address data for people in Denmark, including moving and migration dates, to find the address history from 1995 onwards.37We generated a study population for women and a study population for men and both study populations included people aged 30-45 years who were cohabiting or married, had fewer than two children, and lived in Denmark between 1 January 2000 and 31 December 2017 (n=377 850 women; 526 056 men). These inclusion criteria were implemented to obtain study populations with a high proportion of individuals who were actively trying to become pregnant, and thus under risk of receiving an infertility diagnosis.
Estimation of road traffic noise
We used the Building and Housing Register to obtain geocode and floor (for multistorey buildings) for all addresses in Denmark, and estimated road traffic noise at these addresses for 1995, 2000, 2005, 2010, and 2015 based on the validated Nordic prediction method.3839Main traffic variables for the model were road type (motorway, express road, road wider than 6 m, road 3-6 m wide, and other road) and data on distributions of light and heavy vehicles, travel speed, and annual average daily traffic for all Danish roads.40We accounted for screening effects from all Danish buildings, noise barriers and terrain, reflections, and ground absorption. Noise was calculated as the equivalent A weighted sound pressure level for the day, evening, and night, and expressed as Lden. We estimated noise at the most and the least exposed facades of the residence at each address. Values <35 dB were set to 35 dB because noise below this level is unlikely to be discernible from background noise. We estimated yearly means for all addresses at all years between 1995 and 2017 using linear interpolation.
Estimation of air pollution
We assessed PM2.5at all addresses (ground level) using a validated modelling system comprising the Danish eulerian hemispheric model, the urban background model, and the operational street pollution model.414243This system calculated PM2.5at all Danish addresses as the sum of air pollution at three different spatial scales: the regional background, estimated by a long range chemistry-transport model at 5.6-150 km2resolution (the Danish eulerian hemispheric model)41; local background, estimated in the urban background model covering Denmark in 1 km2resolutions42; and local street, calculated in the operational street pollution model, which takes into account traffic, street configurations, and emission factors.43All models include weather conditions calculated using the weather research and forecasting model.44The model system estimated hourly address specific concentrations of PM2.5during 2000, 2010, and 2015, which were summarised to yearly means for each of the three years. We subsequently calculated yearly means for each address for the period 1995-2017, based on yearly changes in urban background PM2.5estimated using the Danish eulerian hemispheric model and the urban background model.
Covariates
Covariates were selected based on availability in the Danish registers and plausibility to act as potential confounders (see supplementary figure S1). We collected yearly individual level information from 2000 to 2017 using national registers on individual income (sex and year standardised fifths), highest attained education (mandatory, secondary or vocational, or medium or long), occupational status (manual worker, professional, or unemployed or retired), number of children (0 or 1), and country of birth (Denmark or other). We obtained yearly information on five neighbourhood level variables: Proportion of inhabitants in each parish (on average 16.2 km2and 1032 residents) with only mandatory education, low income, manual labour, and a criminal record, and as sole providers. We estimated population density in each parish (0-100, 100-5000, and >5000 individuals/km3) and received information on house type for all addresses (single family house, semidetached house, apartment, or other).
Ascertainment of infertility
To assess infertility, we used personal identification numbers to link the two study populations of men and women with the Danish National Patient Register (valid since 1977), using ICD-8 and ICD-10 (international classification of diseases, eighth and 10th revisions, respectively) codes.45Infertility in women was registered as ICD-8 code 628 and ICD-10 code N97 (excluding N974: infertility in women due to male factors), and infertility in men was registered as ICD-8 code 606 (excluding 606.59, 606.80-89) and ICD-10 code N46 (excluding N469E: infertility in men after sterilisation). We only included the first registered infertility diagnosis. All individuals with a diagnosis of infertility before baseline were excluded. We also excluded women with tubal ligation, bilateral oophorectomy, or hysterectomy before baseline and men who were sterilised before baseline (see supplementary table S1 for operation codes). Furthermore, people undergoing any of these procedures during follow-up were censored at the date of the operation.
For analyses of infertility subtypes, we investigated anovulation (N970), tubal factor (N971), unspecified (N979), and a joint group of other causes of infertility in women (N972, N973, and N978) as subtypes of infertility in women, whereas azoospermia (N469B), oligospermia (N469C), and unspecified (N469) were included as subtypes of infertility in men. Low numbers for other infertility subtypes in men and women precluded meaningful analyses.
Statistical analyses
We analysed data using Cox proportional hazards models, with age as the underlying timescale, to calculate hazard ratios and 95% confidence intervals (CIs) for infertility in men and women (overall and for subtypes of infertility) for each interquartile range as well as for each 10 dB and 5 µg/m3increase in road traffic noise and PM2.5, respectively. Exposure to both pollutants was modelled as time weighted five year running means, taking exposure at all addresses in the period into account (including moving), and entered as time varying variables into the Cox model, thus for each individual with infertility comparing with the five year mean exposure for all people without infertility at the same age as the individual with infertility at the time of diagnosis. Start of follow-up was defined as 30 years of age, 1 January 2000, or date of marriage or cohabiting, whichever came last, and the study populations were followed until date of infertility diagnosis, death, emigration, unknown address, bilateral oophorectomy (women only), tubal ligation (women only), hysterectomy (women only), sterilisation (men only), 45 years of age, divorce or end of cohabitation, birth of second child, or 31 December 2017, whichever came first.
We analysed data using three adjusted models. Model 1 included adjustment for calendar year (two year categories). In model 2, we further adjusted for highest attained education, individual level income, country of origin, occupation, and area level proportion of inhabitants with low income, only mandatory education, manual labour, and a criminal record, and as sole provider. In model 3, we additionally applied mutual adjustment for PM2.5and noise. All individual and area level covariates except country of origin were entered into the Cox models as yearly time varying variables (area level variables also changed with change of address).
We evaluated the assumption of proportional hazards for the three exposures by a correlation test between the scaled Schoenfeld residuals and the rank order of event time. We observed a strong deviation from the assumption for noise (noise at both the most and the least exposed facade) in the men and women study populations. To investigate this further, we calculated associations between the two noise exposures and infertility in men and women in the following age groups: 30-30.9, 31-31.9, 32-32.9, 33-33.9, 34-34.9, 35-35.9, 36-36.9, 37-37.9, 38-38.9, 39-39.9, 40-41.9, and 42-45 years (see supplementary figure S2 and tables S2 and S3). We observed that the hazard ratios differed across age groups, indicating a shift in hazard ratio levels around age 35 years for women and 37 years for men. Subsequent analyses were therefore conducted in the following age groups: 30-34.9 and 35-45 years for women and 30-36.9 and 37-45 years for men.
To investigate the shape of the exposure-response associations, we used natural cubic splines with three degrees of freedom. We furthermore analysed associations (model 3) in categories of noise at the most exposed facade (≤50, 50.01-55, 55.01-60, 60.01-65, and >65 dB) and PM2.5(≤12, 12.01-14, 14.01-16, and >16 µg/m3).
For men aged 37-45 years and women aged 35-45 years we analysed associations separately among people: living at low (<100 people/km2), medium (101-5000 people/km2), or high (≥5000 people/km2) population density; with a low, medium, or high level of education; with a personal income in the first, second, third, and fourth income group; and with 0 or 1 child, by including an interaction term in the model. Also, in analyses of noise at the most exposed facade, we investigated associations among people who had access to a silent facade with substantially lower noise levels than at the most exposed facade compared with people without a more silent facade (defined as a difference between noise at the most and least exposed facades above and below 10.8 dB, corresponding to the median). In sensitivity analyses, we further adjusted for population density and type of residence.
All statistical analyses were performed using SAS 9.4 (SAS Institute, Cary, NC), except tests for proportional hazards and the splines, which were done in R version 4.3.2.
Patient and public involvement
No patients were directly involved in defining the research question, in the study design, or in the analyses and reporting. A main reason was that the present study was conducted without any external funding, thus with limited resources to engage in patient and public involvement. However, discussions with citizens concerned about the effects of the environmental pollutants studied and with patients experiencing infertility and worrying about the causes helped to motivate initiation of the study.
Results
Of 1 133 142 men and 1 090 344 women aged between 30 and 45 years (2000-17) identified, we excluded 12 654 men with an infertility diagnosis or sterilisation before baseline, and 85 700 women with an infertility diagnosis, bilateral oophorectomy, tubal ligation, or hysterectomy before baseline. We further excluded 310 940 men and 458 002 women who had two or more children (or with missing information) at time of enrolment, and 222 796 men and 123 188 women who did not live with a partner at any time during follow-up or were in a same sex registered partnership (or with missing information). We also excluded 30 395 men and 21 872 women with an incomplete address history five years before baseline, and 30 301 men and 23 732 women lacking information on any covariates. This resulted in study populations of 526 056 men and 377 850 women of whom 16 172 men and 22 671 women had an infertility diagnosis during a mean follow-up of 4.3 years and 4.2 years, respectively.
Table 1shows the baseline sociodemographic and exposure characteristics of the two study populations. Distributions of exposure and median levels of noise and PM2.5as well as correlations between exposures were similar among men and women (table 1, also see supplementary figure S3). Noise at the most and least exposed facades were moderately correlated, with Spearman correlation coefficient (Rs) for women of 0.38, whereas correlations between noise and PM2.5were low, with Rs between 0.05 and 0.16 (see supplementary table S4). Median five year mean PM2.5levels decreased in the study population during the follow-up period, from 17.3 µg/m3in 2000 to 12.1 µg/m3in 2017, whereas five year mean noise levels increased slightly from 57.7 dB in 2000 to 59.1 dB in 2017 (see supplementary table S5). Owing to highly non-linear associations between noise at the least exposed facade and infertility in both men and women, we did not conduct further analyses with this noise measure as a continuous variable (see supplementary figure S4).
Among men, we observed that exposure to PM2.5was associated with a higher risk of infertility, with similar sized hazard ratios in the two investigated age groups (30-36.9 and 37-45 years) in the fully adjusted model 3 (table 2). The categorical analyses (table 3) and splines (fig 1) showed that the associations followed linear exposure-response associations throughout the exposure range. For noise, we observed no association with infertility in men in the youngest age group (30-36.9 years) before adjustment for PM2.5(model 2), whereas after adjustment, noise was associated with a hazard ratio of <1 (model 3,table 2). In the oldest age group (37-45 years), noise was associated with a slightly higher risk of infertility in men both before and after adjustment for PM2.5. For both exposures, further adjustment for population density and type of residence resulted in only small changes in hazard ratios (see supplementary table S6). After further adjustment for children (0 or 1) the hazard ratio for noise was reduced in the 37-45 age group, from 1.06 (95% CI 1.02 to 1.11) to 1.02 (0.98 to 1.07), whereas the association with PM2.5remained unchanged (see supplementary table S6).
Among women, noise was associated with a higher risk of infertility in the 35-45 age group, whereas no association was observed in the 30-34.9 age group (table 2). The association with noise in the oldest age group followed a close to linear exposure-response association, although at high exposures (>65 dB) the association levelled off (table 3,fig 1). Exposure to PM2.5was not associated with higher risk of infertility in women in any of the investigated age groups. For both exposures, further adjustment for population density, type of residence, and number of children resulted in only slight changes in hazard ratios (see supplementary table S6).
When investigating the effects of the two exposures on infertility subtypes in women, we found that noise was associated with a higher risk of all three subtypes investigated (anovulation, tubal factor, and unknown cause) in the 35-45 age group but not in the 30-34.9 age group, whereas PM2.5was associated with higher risk of unknown infertility in both age groups (table 4). For subtypes of infertility in men, we observed positive associations between PM2.5and the three subtypes investigated (oligospermia, azoospermia, and unknown infertility) in both age groups. Noise seemed to be associated with a reduced risk of azoospermia (although based on only 273 people) and unknown infertility in men in the 30-36.9 age group and a higher risk of unknown infertility in men in the 37-45 age group.
We found similar hazard ratios between the two exposures and infertility in women and between PM2.5and infertility in men across areas of low, median, and high population density; low, median, and high individual level education; and fourths of personal income, whereas for noise and infertility in men, associations were only observed among men living in low and median population densities, with low or medium educational level, or with income above the lowest fourth, or a combination of these (fig 2, see supplementary table S7). When comparing hazard ratios across people with no children or one child, we observed similar hazard ratios for both exposures in relation to infertility in men, whereas for infertility in women, noise was only associated with higher risk for primary infertility (for secondary infertility we observed a hazard ratio <1) and PM2.5was only associated with a higher risk of secondary infertility (fig 2, see supplementary table S7).
When investigating the association between noise at the most exposed facade and infertility among people with a large versus a small difference between noise level at the most and least exposed facades, we observed stronger associations only when a small difference in noise existed between the two facades, corresponding to having “no silent facade” (fig 2, see supplementary table S7).
Discussion
Based on a large nationwide, prospective cohort, designed to include a high proportion of people actively trying to achieve pregnancy, we found that mean five year exposure to noise was associated with a higher risk of infertility among women aged between 35 and 45 years, whereas no associations were observed between PM2.5and infertility in women. The association between noise and infertility in women seemed confined to those without children (primary infertility). For men, we observed that five year exposure to PM2.5was associated with a higher risk of infertility across the investigated age range (30-45 years), and noise seemed weakly associated with infertility among men aged 37-45 years. The higher risk of noise related infertility in women and PM2.5related infertility in men was consistent across people living in rural, suburban, and urban areas as well as across people with low, medium, and high socioeconomic status. For noise, we observed stronger associations with infertility among people without a silent facade at home.
Strengths and limitations of this study
Strengths of this study include the nationwide design, with low risk of selection bias, together with a high number of people with incident infertility identified from high quality registers during a follow-up period of 18 years. To optimise the likelihood of obtaining valid and unbiased results, we restricted the study population to include a high proportion of people who were at risk of an infertility diagnosis—that is, those who were actively trying to become pregnant. Accordingly, our study population included men and women aged 30-45 years who were married or cohabiting. Furthermore, all study participants were censored at the time they had their second child. This censoring criterion was applied because Danish women on average gave birth to 1.8 children during the study period and therefore it is likely that after the birth of a second child, many couples no longer try for pregnancy. Although applying these restriction criteria increased the probability that a large proportion of our study population were trying to become pregnant, it was inevitable that our cohort also included couples who were not—for example, couples prioritising their career before children. This is a limitation of the study design.
People with infertility were identified using the high quality Danish National Patient Register, which has high validity and completeness of diagnoses in Denmark.46Thus, only infertile couples actively seeking infertility counselling were identified as infertile participants in the study. In Denmark, however, all inhabitants can seek infertility counselling and fertility treatment free of charge, and the procedures are standardised across Denmark, starting with a visit to the general practitioner who, if infertility is suspected, refers individuals to a fertility clinic. As Denmark is a small country, the distance between home and a fertility clinic is not expected to be an obstacle to seeking fertility treatment, and we did not expect major differences in the likelihood of obtaining an infertility diagnosis according to geographical location alone.
Another important strength was that we had access to an exact history of residential address for all participants from five years before baseline until end of follow-up, linked with exposure to both road traffic noise and PM2.5estimated used validated exposure models and high quality input data.4347As these two exposures are correlated and found to be associated with many of the same diseases, mutual adjustment was crucial.
As the present study was based entirely on register data, we did not have information on lifestyle factors, such as alcohol use, smoking, and body mass index, which is a limitation. We did, however, have access to detailed time varying register based information on individual and neighbourhood level sociodemographic variables, enabling us to adjust for key socioeconomic covariates, thereby indirectly adjusting for lifestyle. That our adjustment strategy may sufficiently capture lifestyle confounding is supported by results from our previous studies on noise and air pollution and risk of cardiovascular disease, diabetes, and mortality, which were based on large Danish questionnaire based cohorts with detailed information on lifestyle.484950These studies showed that after adjusting for the socioeconomic variables included in the present study, further adjustment for lifestyle had only a minimal effect on the risk estimates. Another limitation is lack of information on exposure to noise and PM2.5at work and at leisure time activities away from home. This may affect the size and statistical precision of risk estimates owing to a mixture of classic and Berkson error.
Comparison with other studies
In support of our results on PM2.5and infertility in men, particulate air pollution (PM2.5and PM10) has in recent studies been found to be negatively associated with factors defining sperm quality, including sperm motility and count as well as changes in sperm morphology.131415Our study therefore adds to these findings, showing that the effects of air pollution on sperm quality will potentially result in a higher risk of requiring assistance from a fertility clinic to achieve pregnancy. Interestingly, we found that the association between air pollution and infertility in men followed a linear exposure-response association, starting from around ≥8.5 µg/m3in both investigated age groups, indicating that even at the relatively low levels of PM2.5found in Denmark, particulate air pollution can reduce fertility in men.
For women, most previous studies have focused on investigating effects of air pollution on success of fertility treatment among couples referred to fertility clinics.1617181920212223Although most studies found particulate matter air pollution to be associated with, for example, a reduced likelihood of clinical pregnancy, live birth after fertility treatment, and odds of receiving fertility treatment,1617181920others found no association.212223Also, the few studies investigating the effects of short term or long term, or both, exposure to air pollution on fecundability (assessed as time to pregnancy) have provided inconsistent results.252627However, fecundability can be influenced by infertility in both men and women, and therefore results are difficult to interpret in the context of infertility in women, as a positive association can potentially be driven by effects of air pollution on semen quality. The results from these previous studies can therefore not be directly compared with the present study, where we have direct and differentiated measures of infertility in men and women. However, in a study based on 36 000 women from the Nurses’ Health Study II with self-reported follow-up for infertility (defined as attempting conception for ≥12 months), the authors were able to distinguish between infertility in men and women in 27% of women with fertility problems.24In both main analyses (couple based infertility) and analyses restricted to infertility in women, the authors reported that long term exposure to PM2.5(four year mean) was not associated with higher risk of infertility, which agrees with the results of the present study.
A potential explanation as to why we found PM2.5exposure associated with infertility in men and not women is that while female follicle development begins in utero, new sperm cells are produced continuously in the testis (after puberty), with an overall lifespan of three months. Therefore, particulate air pollution may act directly on the sperm cells during the vulnerable spermatogenesis phase—for example, through direct toxic effects of particles translocated from the lungs into the blood, oxidative stress, inflammatory processes, and genotoxicity.5152In contrast, the potential biological mechanisms underlying an association between air pollution and infertility in women are less established but have been hypothesised to involve some of the same pathogenetic mechanisms as described for infertility in men as well as endocrine disrupting properties caused by air pollutants mimicking the effect of androgens and oestrogens.53
The only previous study on traffic noise and a fertility related outcome indicated that among 65 000 pregnant women, road traffic noise was associated with a higher risk of trying for six months or more to achieve pregnancy (self-reported) compared with getting pregnant within six months.36This indicates that noise may impact fecundity, which supports the findings of the present study—although the two studies are not directly comparable, as the previous study focused on self-reported time to pregnancy among pregnant women, whereas the present study investigated risk of receiving a diagnosis of infertility. A potential explanation as to why we only observed an association with noise among women older than 35 years is that many who are trying to become pregnant in this age group are likely to be in a more stressful state than women in a younger age group if pregnancy is not achieved immediately, as it is well known that fertility drops steeply in women in their late 30s.54Therefore, women in this age group may be more susceptible to noise induced stress and sleep disturbance,3132as they are potentially already in a state of distress. In support, we only observed a positive association with noise among women with primary infertility, who are expected to be in a more stressful state than women who already have one child (secondary infertility). It is established that infertility is associated with psychological symptoms, such as depression and distress, especially among women.3355It is still, however, unclear whether stress is a risk factor for infertility.33Our finding of an association between noise and infertility only among women older than 35 years may also be partly explained by different underlying causes of infertility across age groups. For example, somatic disorders known to be important causes of infertility in women, such as endometriosis and polycystic ovary syndrome, are often diagnosed at a relatively early age and people with these disorders are thus more likely to contact fertility clinics for counselling at an earlier age. As we hypothesised that noise would have only a minor or no impact on the risk of infertility among individuals with a definite somatic cause of infertility, this may at least partly explain why we observed no association between noise and infertility in women in the 30-35 age group.
Among men, we observed that noise was associated with a lower risk of infertility in the 30-36.9 age group and a higher risk in the 37-45 age group. The biologically implausible lowering of risk in the youngest age group was, however, driven by adjustment for PM2.5, suggesting that this was an artefact. In the 37-45 age group, the association was robust to adjustment for PM2.5as well as to adjustment for population density, suggesting that noise may be a risk factor for infertility in men. After adjustment for number of children, however, the association was no longer present. More studies are needed to establish whether noise is a risk factor for infertility in men—for example, studies on noise and semen quality.
To investigate the robustness of our results, we examined whether our main findings were consistent across urban, suburban, and rural areas. In Denmark, couples who are considering starting a family are likely to move from apartments in larger cities to single family houses in suburban or rural areas, which in most instances will result in reduction of exposure to air pollution and noise. Although we had detailed information on changes of address (and exposure) for all participants, this could have potentially biased our results. However, the observed associations between PM2.5and infertility in men and noise and infertility in women were present regardless of the degree of urbanisation, suggesting that the high mobility of our population did not affect the results. In Denmark, people of high socioeconomic status are more likely to live in urban areas than in more rural areas, and although infertility treatment is free of charge in Denmark, Danish couples with high education and high income are more likely to seek infertility treatment than couples with l","Objective: To investigate associations between long term residential exposure to road traffic noise and particulate matter with a diameter <2.5 µm (PM2.5) and infertility in men and women.
Design: Nationwide prospective cohort study.
Setting: Denmark.
Participants: 526 056 men and 377 850 women aged 30-45 years, with fewer than two children, cohabiting or married, and residing in Denmark between 2000 and 2017.
Main outcome measure: Incident infertility in men and women during follow-up in the Danish National Patient Register.
Results: Infertility was diagnosed in 16 172 men and 22 672 women during a mean follow-up of 4.3 years and 4.2 years, respectively. Mean exposure to PM2.5over five years was strongly associated with risk of infertility in men, with hazard ratios of 1.24 (95% confidence interval 1.18 to 1.30) among men aged 30-36.9 years and 1.24 (1.15 to 1.33) among men aged 37-45 years for each interquartile (2.9 µg/m3) higher PM2.5after adjustment for sociodemographic variables and road traffic noise. PM2.5was not associated with infertility in women. Road traffic noise (Lden, most exposed facade of residence) was associated with a higher risk of infertility among women aged 35-45 years, with a hazard ratio of 1.14 (1.10 to 1.18) for each interquartile (10.2 dB) higher five year mean exposure. Noise was not associated with infertility among younger women (30-34.9 years). In men, road traffic noise was associated with higher risk of infertility in the 37-45 age group (1.06, 1.02 to 1.11), but not among those aged 30-36.9 years (0.93, 0.91 to 0.96).
Conclusions: PM2.5was associated with a higher risk of an infertility diagnosis in men, whereas road traffic noise was associated with a higher risk of an infertility diagnosis in women older than 35 years, and potentially in men older than 37 years. If these results are confirmed in future studies, higher fertility could be added to the list of health benefits from regulating noise and air pollution.
"
Prostate cancer incidence and mortality in Europe and implications for screening,"Introduction
Prostate cancer is currently the most diagnosed malignancy among men and the third most common cause of death from male specific cancers in EU member states.1In the European Economic Area, which includes the 26 EU member states, Iceland, Lichtenstein, and Norway, and comprises 219 million men, around 341 000 men were diagnosed as having prostate cancer in 2020 (equivalent to 23% of all cancers in men) and about 71 000 men died from the disease (10% of all deaths from male specific cancers) in the same year.1
Screening men to check their prostate specific antigen (PSA) levels aims to reduce mortality from prostate cancer.2The European Randomized Study of Screening for Prostate Cancer found a reduction in deaths from prostate cancer (after around 10 years),34whereas the other large randomised trials—the Prostate, Lung, Colorectal and Ovarian trial,45which reported no reduction in mortality (although likely the results were affected by contamination),6and the CAP (cluster randomised trial of PSA testing for prostate cancer) in the UK—found similarly negative results based on a single screen. In addition, PSA based screening may lead to overdiagnosis through the detection of low risk tumours that are unlikely to progress, with the risk of overtreatment and adverse effects that could lower men’s quality of life.78The potential for overdiagnosis and overtreatment is higher when screening for prostate cancer than when screening for breast, cervix, and colorectal cancers, with autopsy studies reporting that up to one third of men of screening age harbour an indolent prostate cancer.9
Because of the delicate risk-benefit balance, almost all European countries, except Lithuania, have thus far opted against establishing prostate cancer screening programmes in favour of shared decision making about PSA testing between men and their doctors.10Differing individual attitudes and local practices towards PSA testing against a backdrop of on-demand and opportunistic screening unguided by clear protocols (in particular, the testing of older men) are likely to have a less than optimal effect on the population, with a possibly different net balance between the benefits and harms at population level than that observed in randomised clinical trials.1112
The EU Beating Cancer Plan recently released the European Commission’s council recommendations proposing a gradual and well planned implementation of screening programmes for prostate cancer in men younger than 70.1314The suggested approach involves PSA testing initially, followed by magnetic resonance imaging (MRI) or other diagnostic tests for men with raised PSA levels before considering biopsy. The aim of the proposed approach is to maintain the benefit of mortality reduction while reducing overdiagnosis.15Modelling studies have suggested that this could be a cost effective procedure.16
Given that opportunistic PSA testing has largely been carried out in Europe, it is important to assess the effect on prostate cancer incidence and mortality at population level. In addition, baseline data on national levels and trends in prostate cancer outcomes before the possible initiation of screening with new approaches are needed. We therefore carried out a comparative assessment of the main epidemiological features of prostate cancer in 26 European countries, quantifying the range of variability in incidence rates against temporal variations in PSA testing and relative to mortality rates as a contribution to the evaluation of the population level impact of the EU initiative.
Methods
Data sources
We obtained long term data on the annual incidence of prostate cancer (international classification of diseases, 10th revision, ICD-10 code C61) from the International Agency for Research on Cancer’s CI5plus (Cancer Incidence in Five Continents Plus) database and the Global Cancer Observatory.11718From population based cancer registries we retrieved national or subnational recorded incidence data for 26 European countries during 1980-2017. Countries with populations less than 1 000 000 (Iceland and Malta) were not analysed. Coverage and availability of data within this period varied by country, but for most of the countries, the last year with incidence data was 2017 (see supplementary table S1). We obtained mortality data for the 26 European countries for 1980-2020 based on national vital registration from the World Health Organization.19Population coverage of the mortality database was nearly 100% in all selected countries, except Cyprus (86%).19Supplementary table S2 shows data availability and missing data points within the study period. We also extracted the most recent (2020) national incidence and mortality estimates from GLOBOCAN 2020 (with UK countries combined).1
Review on PSA testing
We carried out a review of the literature on PSA testing across European countries. PubMed was searched using keywords (time trendORtrend)AND(prostate-specific antigen)AND(testingORscreeningORtesting rate). The reference lists of relevant articles were also checked to identify additional eligible studies. We selected only studies that provided information on trends in PSA testing in European countries for at least three years. When several studies reported the time trends of PSA testing for one country, we selected the study with the longest periods of data. Overall, information on trends in PSA testing was available for 12 countries (see supplementary table S3), although quality and type of information varied. Therefore we were unable to derive precise characterisation of prevalence, patterns, and trends in PSA testing from the literature, and the available estimates were not directly comparable across countries because they referred to different indicators, age groups, and data sources across populations. Consequently, it was not possible to carry out a quantitative analysis linking levels of PSA testing with incidence of prostate cancer across countries but only to provide a visual assessment of the temporal trajectory of PSA testing against that of incidence by country.
Statistical analysis
We restricted all analyses to the age group 35-84 years, with missing mortality data points removed. Annual age standardised rates of prostate cancer incidence and mortality per 100 000 men were calculated using the world standard population as a reference.20To assess the temporal trends of prostate cancer incidence and mortality by country, we plotted the line chart of annual age standardised rates against calendar years based on all available data points. We assessed trends by country continuously by single year, whereas when emphasis was put on the range of variability in incidence and mortality across the continent, we smoothed trends using Loess regression. The average annual percentage change was calculated as 100×(eβ−1), where β is the regression coefficient in the generalised linear regression models between natural logarithm of annual age standardised rate and year, with a gaussian distribution and identity link function.21
Information on trends in PSA testing was retrieved from the literature for the 12 studied countries and is displayed against the corresponding trend in incidence. To assess the discrepancy between incidence and mortality, we grouped calendar years into four periods of five years each (1998-2002, 2003-07, 2008-12, and 2013-17). We calculated the standardised rate ratios of annual age standardised rates between incidence and mortality and then compared the standardised rate ratio across periods.22Age curves were also plotted for both incidence and mortality over the four periods.
All analyses were performed using R software (version 4.0.3).
Patient and public involvement
This study used deidentified and aggregated registration data provided by patients and collected by staff from local registries in the countries studied. No patients were involved in the development of the research question, outcome measures, study design, or implementation of the study, as it is not possible nor permitted to attempt to identify and engage them. Although no patients were directly involved in this paper, one impetus for this research was the clinical context of the proposed prostate cancer screening programmes in the EU. Results will be disseminated to the public through media and a press release written using layman’s terms.
Results
Time trends of prostate cancer incidence and mortality rates
Figure 1,figure 2,figure 3, and supplementary figure S1 show trends in prostate cancer incidence and mortality by country on an arithmetic scale (more suitable to assess and compare absolute values) and semi-log scale (more suitable to assess and compare relative changes over time), respectively. Supplementary figure S2 shows the trajectory of incidence and mortality over time by country. Increases in incidence were seen in almost every country, although the pace of increase varied greatly across countries. Increases in incidence were highest in northern Europe, France, and the Baltic countries—notably in Lithuania where the rates peaked at 435 per 100 000 men in 2007. In several countries (France, Switzerland, Italy, and Lithuania) the rates showed a parabolic increase, culminating after the mid-2000s and followed by subsequent declines, whereas in other countries the rates stabilised (Denmark, Sweden, Norway, Ireland, Spain, and Slovenia). Increases in incidence were, however, observed in the most recent quinquennium (2013-17) in several countries. In contrast, mortality rates decreased in most countries after the early 2000s, except in the Baltic countries and eastern Europe (eg, Estonia, Latvia, Belarus, Bulgaria, Poland, and Ukraine), where marked increasing trends, from previously low rates, were observed.
Three patterns can be distinguished in trends for prostate cancer incidence and mortality. Among the European countries included, nearly half exhibited upward trends in incidence (generally from the early 1990s to the late 2000s), followed by stable or downward trends, with corresponding mortality rates in uniform decline (such as the Nordic countries, France, Switzerland, and Italy). A second pattern involved increasing incidence rates throughout the study period, accompanied by downward mortality trends, as was observed in Britain (England, Wales, and Scotland) and the Czech Republic. The incidence of prostate cancer increased with stable or increasing mortality in the remaining countries, particularly in eastern and Baltic Europe (including Croatia, Estonia, Latvia, Belarus, Ukraine, Poland, Slovakia, and Bulgaria). Supplementary table S4 shows the average annual percentage changes of prostate cancer incidence and mortality.
Range of geographical and temporal variations in incidence and mortality rates
Figure 4shows the range of variability in the annual age standardised rates for prostate cancer incidence and mortality across European countries and over time, highlighting the contrasting levels and magnitudes of differences in incidence versus mortality. Overall, prostate cancer incidence rates tended to rise during the study period, but with a variable pace and peak incidence in different countries and calendar periods. Consequently, the range varied considerably over time, the lowest rates being at the beginning of the study period in 1980 (from 17.6 in Belarus to 109.4 in Sweden per 100 000), then increasing substantially up until around 2005 (from 46.0 in Ukraine to 335.6 in France) and thereafter somewhat narrowing (from 62.7 in Ukraine to 299.3 in Lithuania) until around 2012, although rising trends were observed thereafter in several countries. The difference between the highest and lowest incidence rates across countries ranged from 89.6 per 100 000 men in 1985 to 385.8 per 100 000 men in 2007.
Compared with incidence, mortality rates were much lower in absolute terms and, despite the declines observed in most countries, presented a smaller range of values, spanning from 12 (Ukraine and Belarus) in 1981 to 53 (Latvia) deaths per 100 000 men in 2006. Considering all countries and periods, the 20-fold maximum variation in prostate cancer incidence contrasts with the fivefold variation in mortality. The difference between the highest and lowest mortality rates across countries ranged from 23.7 per 100 000 men in 1983 to 35.6 per 100 000 men in 2006.
Trends in incidence against trends in PSA testing
Supplementary figure S3 shows the trends in incidence of prostate cancer against trends in PSA testing for the 12 countries where information on both indicators was available. A correlation was evident between the direction and rate of change in incidence relative to PSA testing across all countries assessed, although data on PSA trends are subject to major limitations. Supplementary table S3 provides detailed information on the review of PSA testing in Europe.
Divergence between incidence and mortality
The divergence between incidence and mortality increased in all countries over two decades (fig 5). The standardised rate ratios between incidence and mortality were around 2~4 in most countries during 1998-2002, but higher values (5~7) were observed for several central European countries (Germany, Austria, France, Switzerland, Italy, and Spain). The standardised rate ratios almost doubled by 2013-17 compared with 1998-2012 and reached over 5 for almost all included countries other than Croatia, Latvia, and several countries in eastern Europe. High standardised rate ratios (>10) were found in Ireland, France, Italy, and Spain in 2013-17.
Supplementary figure S4 also shows age standardised incidence rates for incidence and mortality in Europe in 2020, ranked by increasing order of incidence. Higher incidence rates were not consistently associated with the level of mortality rates.
Changes in age curves of prostate cancer incidence and mortality over time
Figure 6,figure 7,figure 8(all on arithmetic scale), and supplementary figure S5 (on logarithmic scale) show the temporal change in age specific incidence and mortality. The age specific profiles changed markedly for incidence, but not for mortality. The incidence curves resembled an inverse U-shape peaking at around 70 years of age during the period 1998-2017, as seen in France, Sweden, Denmark, Norway, Ireland, Estonia, Lithuania, Slovenia, and the Czech Republic. The corresponding age specific curves in the central European countries decreased in 2008-12 after an earlier peak around 2003-07, although increases were observed in the recent quinquennium 2013-17 in some countries in the region. In contrast, the mortality curves remained relatively stable over time, showing a consistent increase with age in all European countries.
Discussion
Our study found noticeable differences in both the magnitude of prostate cancer incidence rates across Europe and the rate of change in the generally upward trends over the past decades. The divergence between countries reached its maximum around the period 2000-10. Thereafter the rates declined in several countries, with somewhat reduced variability in rates, even though they remained high, and even increased in several countries in the most recent years. Such temporal variations in prostate cancer incidence correlated with the national variations in PSA testing. In contrast, mortality rates were substantially lower and showed less variability than incidence, with a more homogeneous pattern over time. Uniform declines in mortality were generally seen across the European continent, although less marked than the increases in incidence. In the Baltic countries and eastern Europe, however, mortality trends remained relatively flat.
The delivery and uptake of PSA testing have been shown to have a rapid effect on the number of new diagnoses of prostate cancer and corresponding incidence rates at the population level. It is widely acknowledged that in the US the large increase (starting in the 1970s and peaking around 2000) and subsequent decline in incidence resulted from the initial increasing use of transurethral resections of the prostate (from the 1970s) and subsequent use of PSA testing (from the mid-1980s)23and was followed by a decline, partly as a result of the US Preventive Services Task Force’s recommendation aimed to discourage the practice.24Our study confirmed this pattern for incidence in Europe yet also found large heterogeneity across countries. Conversely, the extent of the effect of PSA testing on mortality at the population level is less clear. In the US, the decline in mortality from the mid-1990s followed by a period of stability could be attributed to the use of PSA testing as well as to advances in effective treatment for late stage prostate cancer (whereas the cancers are localised at diagnosis). Yet, disentangling the contribution of the two components is challenging. The patterns of prostate cancer in Europe appear to replicate the earlier observations in the US. This suggests the same mechanism and implicates the potential contributions of both PSA testing and improved treatment outcomes.
In this respect, this comparative assessment should help to improve the understanding of the effect of PSA testing on incidence and mortality in Europe by highlighting consistent patterns across countries. Specifically, our results suggest that the intensity and coverage of PSA testing has been a critical driver for the increasing trends in prostate cancer incidence in Europe. Nevertheless, the possible benefits in terms of reduced mortality appeared to be relatively consistent everywhere, regardless of the extent of the increase in incidence as an indicator of PSA testing.25In addition, the magnitude of prostate cancer incidence showed little interdependence with mortality at the national level.
The changes in the age specific incidence curves showed a progressively younger age at peak incidence and increasing resemblance to an inverted U-shape. Older data from the 1960s and 1970s suggest that before the initiation of PSA testing, the incidence of prostate cancer increased strongly with age.1In contrast, in our study the age specific mortality curves did not substantially change over time and increased steadily with age. The decline in mortality rates affected all age groups proportionally, and the trend towards earlier diagnosis in younger men seemed to have only a negligible impact on subsequent mortality in older age groups.
In most Baltic and eastern European countries, mortality rates were relatively stable, in contrast with the declines elsewhere. Explanations may include the limited extent of PSA testing, as well as a slower adoption of therapeutic advances (compared with more affluent areas on the continent). Lithuania was an exception, with minor declines in mortality in the most recent period, possibly because it is the only country in Europe offering population screening using the PSA test (since 2006).26As the national programme has been accompanied by a substantial amount of opportunistic testing, prostate cancer incidence in Lithuania has increased rapidly, reaching the highest levels ever recorded in Europe.
Overall, our findings imply that unregulated and opportunistic PSA testing has had a differential effect at the population level in Europe compared with the results of the randomised screening trials and appear consistent with overdiagnosis. The PSA based screening trials reported a 1.4-fold or smaller increase in incidence,3527whereas national incidence rates in most European countries more than doubled from 1990 to 2017 (and in some countries increased up to eightfold, as in Lithuania). The epidemiological features observed in our study, specifically the rapid inconsistent increase in incidence but not mortality and the progressive change in age specific incidence curves, are difficult to explain for factors other than PSA testing. Our findings have commonalities with what has been previously reported for thyroid cancer, where overdiagnosis is an established driver of rapid increases in incidence.28Opportunistic examinations of the thyroid (often with ultrasound) have spread rapidly in many countries,2930despite the lack of evidence for a mortality benefit from thyroid screening (in contrast with prostate cancer screening) and of current guidelines, which recommend against population screening for thyroid cancer.31
The value of early detection of prostate cancer has been debated extensively, and most of the recent guidelines recommend that asymptomatic men should be offered an informed decision making process about the potential benefits and harms of prostate cancer screening. However, it is still not clear how shared decision making should be implemented, or its possible effect on patient outcomes.323334In countries where such policies have been introduced, the prevalence of PSA testing is disproportionally higher among older men11and among men with a higher socioeconomic position.35This limits the benefits, increases the risk of overdiagnosis, and increases social inequalities.
Overtreatment may be a consequence of overdiagnosis, with harms of overdiagnosis exacerbated by aggressive management. Recent improvements in the de-escalation of treatment for low risk men with prostate cancer are observed in some countries. In Norway, for instance, the proportion of low risk men managed primarily by active surveillance instead of immediate treatment increased from 20% to 80% during 2008-21, with only 7% of such men treated radically in 2021 (eg, with surgery or radiotherapy).36In England, treatment of low risk men was estimated to be 4% in 2018.37Heterogeneity in the management and treatment of low risk and high risk localised prostate cancer is substantial, however, even across high income countries.37International society based treatment guidelines should be enforced to minimise overtreatment. In addition, many men initially treated with active surveillance decided to switch to active treatment within a few years. The process of reducing unnecessary treatment for prostate cancer is multifaceted and involves various aspects of health systems and the attitude of decision makers, medical practitioners, and patients and their families.38It is important to monitor whether opportunistic use of PSA testing, with the consequent cascade of biopsies, aggressive management, and treatment, will continue in the future, especially in settings where the provision of healthcare services is particularly unregulated.
The European Commission has recently recommended that “countries should take a stepwise approach, including piloting and further research to evaluate the feasibility of implementation of organised programmes aimed at assuring appropriate management and quality on the basis of prostate specific antigen (PSA) testing for men up to 70, in combination with additional magnetic resonance imaging (MRI) scanning as a follow-up test.”39The use of pre-biopsy MRI and of targeted prostate biopsies compared with systematic biopsies alone, should reduce the number of men who will receive an unnecessary diagnosis of prostate cancer, and although changes in clinical practice are already occurring in some settings, they are too recent for any potential effect to be observable in our study. To this extent, some proposals have been advanced, including the implementation of systematically designed, risk based national prostate cancer detection programmes aimed at reducing overdiagnosis and overtreatment and increasing equity.1140
Limitations of this study
The present analysis may refer to different age groups, periods in time, and indicators of PSA testing, and therefore the results should be interpreted with caution. The limitations of this study include the lack of data on cancer stage (due to problems with comparability across cancer registries) and on treatment modalities. This is of importance as increases in prostate cancer incidence and mortality at more advanced stages have been observed in the US following the USPSTF recommendations against PSA based screening in 2008 and 2012.4142However, the data used in this analysis include IARC’s GLOBOCAN and CI5, for which the underlying sources are commonly robust and internationally comparable. Although data for Cyprus and Slovakia have been available only since the early 2000s and early 1990s, respectively, for all other countries under study, cancer incidence trends could be analysed up until the relatively recent period of 2017. In some countries, mortality data are missing for a few years, but those are generally scattered throughout the study period, their impact on the trends and on the general conclusions of the study are negligible. Ecological studies in multiple settings, such as the present one, are an appropriate approach for quantifying and monitoring overdiagnosis.43The current review on PSA testing, as noted, does have several limitations. In addition, although we could retrieve data from the literature on the frequency of PSA testing for 12 out of the 26 countries, the information available could not be synthesised or enable quantitative assessments given the lack of comparability of the measures used. A visual inspection of the trend variations in PSA testing showed a strong parallelism with prostate cancer incidence in countries where both indicators were available, but these findings should also be interpreted with caution. In addition to PSA testing, other factors may have affected incidence and mortality rates. The descriptive nature of the data used in the present study, the incompleteness of the data both geographically and temporally, and the lack of information on confounding factors, mean that causality cannot be assumed. The established risk factors for prostate cancer include age (adjusted for in our analysis) as well as family history and genetic predisposition, but these cannot change rapidly within a population. Putative factors include diet, specific drugs, and occupational factors,4445but overall, the cause remains poorly understood. It is, however, unlikely that changes in the prevalence of one or more risk factors could have caused such a surge in incidence, given the variability internationally, and the contrasting mortality trend.
Conclusions
Overall, our results suggest that several of the epidemiological characteristics of prostate cancer in the Europe countries included, particularly the contrast between large heterogeneity in trends for incidence with the more uniform reduction in mortality, are compatible with the highly variable patterns of PSA testing across Europe. The current high incidence of prostate cancer in many countries may be inflated by unregulated and opportunistic PSA testing that serves to mask any variations due to causal factors and may be indicative of overdiagnosis. The importance of these results is further emphasised by the proposed EU guidelines endorsing prostate cancer screening, assuming that resources are available, and that prostate cancer is a public health priority. Careful monitoring and assessment of the benefits and harms, including overdiagnosis, will be essential for the potential implementation of the guidelines and the prospective introduction of population-wide prostate cancer screening.
Unregulated and opportunistic testing of prostate specific antigen has been, and still is, common in Europe
The EU Beating Cancer Plan recently released the European Commission’s council recommendations proposing a new strategy for prostate cancer screening programmes
A baseline assessment of the main epidemiological features of prostate cancer outcomes in Europe is needed before the possible initiation of screening with new approaches
This study found that the magnitude of prostate cancer incidence rates varied markedly across European countries and over time, in parallel with national trends of prostate specific antigen testing. Conversely, the mild and steady declines in mortality rates were at much lower levels and showed a more homogeneous and less variable pattern
The epidemiological features analysed in this study suggest that unregulated and opportunistic screening with prostate specific antigen likely leads to a population effect on prostate cancer outcomes that is less than optimal compared to that observed in randomised clinical trials
The present results are ecological in nature and should be interpreted with caution, but they reinforce the need for prudently planned prostate cancer screening programmes, especially to mitigate harms from overdiagnosis
","Objective: To provide a baseline comparative assessment of the main epidemiological features of prostate cancer in European populations as background for the proposed EU screening initiatives.
Design: Population based study.
Setting: 26 European countries, 19 in the EU, 1980-2017. National or subnational incidence data were extracted from population based cancer registries from the International Agency for Research on Cancer’s Global Cancer Observatory, and mortality data from the World Health Organization.
Population: Men aged 35-84 years from 26 eligible countries.
Results: Over the past decades, incidence rates for prostate cancer varied markedly in both magnitude and rate of change, in parallel with temporal variations in prostate specific antigen testing. The variation in incidence across countries was largest around the mid-2000s, with rates spanning from 46 (Ukraine) to 336 (France) per 100 000 men. Thereafter, incidence started to decline in several countries, but with the latest rates nevertheless remaining raised and increasing again in the most recent quinquennium in several countries. Mortality rates during 1980-2020 were much lower and less variable than incidence rates, with steady declines in most countries and lesser temporal differences between countries. Overall, the up to 20-fold variation in prostate cancer incidence contrasts with a corresponding fivefold variation in mortality. Also, the inverse U-shape of the age specific curves for incidence contrasted with the mortality pattern, which increased progressively with age. The difference between the highest and lowest incidence rates across countries ranged from 89.6 per 100 000 men in 1985 to 385.8 per 100 000 men in 2007, while mortality rates across countries ranged from 23.7 per 100 000 men in 1983 to 35.6 per 100 000 men in 2006.
Conclusions: The epidemiological features of prostate cancer presented here are indicative of overdiagnosis varying over time and across populations. Although the results are ecological in nature and must be interpreted with caution, they do support previous recommendations that any future implementation of prostate cancer screening must be carefully designed with an emphasis on minimising the harms of overdiagnosis.
"
Risk of dementia after SGLT-2 inhibitors versus DPP-4 inhibitors in adults with type 2 diabetes,"Introduction
Dementia concerns damage to the brain parenchyma, resulting in a permanent degradation of higher cortical functions, mood, and even behaviour.1According to a World Health Organization (WHO) report in 2021, the number of people with dementia globally is expected to reach 78 million by 2030.2Despite the severe consequences of dementia, the success rate of the development for dementia drugs has been markedly low in the past two decades, leaving only extremely limited options for disease modifying treatment.3Evidence has, however, emerged to support the importance of modifiable risk factors for dementia, including diabetes.4According to a pooled analysis, type 2 diabetes is associated with a 60% greater risk of dementia,5predisposing such people to both Alzheimer’s disease and vascular dementia.6The mechanisms linking type 2 diabetes and dementia are multifactorial, involving insulin resistance, hypoglycaemic episodes, and vascular compromise.7In line with this, meta-analyses on observational studies have shown that certain antiglycaemic drugs may have neuroprotective effects in people with diabetes.8910
Sodium-glucose cotransporter-2 (SGLT-2) inhibitors are a newer class of antiglycaemic drugs that inhibit reabsorption of glucose in the proximal tubule. Key randomised controlled trials have shown significant cardiorenal protection from use of SGLT-2 inhibitors beyond glucose lowering effects.11SGLT-2 inhibitors are now considered one of the drug repurposing candidates for disease modifying treatment of dementia.12Recent evidence suggests neuroprotective effects of SGLT-2 inhibitors based on penetration of the drug through the blood-brain barrier, SGLT-2 expression in brain tissue, and direct inhibition of acetylcholinesterase, as well as indirect cardiometabolic benefits.13
Previous observational studies have suggested better preservation of cognitive function among people with type 2 diabetes treated with SGLT-2 inhibitors than other treatments, including dipeptidyl peptidase-4 (DPP-4) inhibitors,14151617another newer class of antiglycaemic drugs found to have no effect on cognitive performance in recent randomised controlled trials compared with sulfonylurea and placebo.1819The methodological approaches of these observational studies were often limited, however, and did not meet the active comparator new user design, leaving concerns about confounding or bias.1516A recent well designed study on residents in Ontario, Canada compared new users of SGLT-2 inhibitors with new users of DPP-4 inhibitors and found that the former were associated with a 20-34% reduced risk of dementia among people older than 66 years.14The effects on younger populations and specific types of dementia (eg, Alzheimer’s disease, vascular dementia) were not, however, examined. Moreover, it is unclear whether different patient characteristics such as concomitant treatment or comorbidity status would modify such drug effects. We therefore compared the risk of dementia among adults with diabetes younger than 70 years who initiated an SGLT-2 inhibitor or DPP-4 inhibitor using the nationally representative Korea National Health Insurance Service database.
Methods
Data source
We conducted a cohort study using data from the Korea National Health Insurance Service database during 2013-21. This database covers the entire population of Korea and provides longitudinal patient data, including personal characteristics, ICD-10 (international classification of diseases, 10th revision) diagnosis codes, procedures, prescription and dispensing records (drug names, prescription and dispensing dates, days’ supply, dose, and route of administration), and type of healthcare utilisation (outpatient, inpatient, or emergency department).20
Study design and population
We emulated a target trial for the outcomes of interest (see supplemental table S1 for the framework of the target trial emulation) using a propensity score matched active comparator new user cohort study design (see supplemental figure S1 for the detailed study design).
Adults aged 40-69 years with an ICD-10 code for type 2 diabetes who had initiated an SGLT-2 inhibitor or DPP-4 inhibitor were eligible for inclusion in the study (see supplemental figure S2 for the participant selection process and supplemental table S2 for ICD-10 codes used in this selection process). To implement a new user active comparator design, we only included initiators of the two competitive study drugs, an SGLT-2 inhibitor and a DPP-4 inhibitor, who had not been dispensed either drug for at least 365 days (the baseline period) before the first dispensing date of the study drug (the index date). To be included, individuals were required to be free of any dementia and related drugs ever before the index date. We also excluded those with ICD-10 diagnosis codes for type 1 diabetes mellitus, HIV, or end stage renal disease (or dialysis service) during the baseline period, and those who concomitantly used glucagon-like peptide-1 receptor agonists or thiazolidinedione on the index date.
Outcome measurement
Our primary outcome was incident dementia based on ICD-10 diagnosis codes in a primary position recorded on inpatient or outpatient claims (see supplemental table S3 for ICD-10 codes used to define outcomes).21To improve specificity of outcome ascertainment, we examined dementia defined by the diagnosis codes along with dispensing of dementia drugs (donepezil, rivastigmine, galantamine, or memantine) as a secondary outcome. In Korea, dementia drugs are reimbursed by the Rare and Intractable Diseases programme, where beneficiaries should qualify for a diagnosis certificate of dementia based on brain imaging and cognitive function testing. Other secondary outcomes were individual types of dementia (eg, Alzheimer’s disease, vascular dementia) in a primary position.
Control outcomes
To assess reproducibility of established relations and unmeasured systematic bias, we also compared the risk of positive and negative control outcomes between the two treatment groups (see supplemental table S3). Given the higher risk of genital infections associated with SGLT-2 inhibitors compared with DPP-4 inhibitors in randomised controlled trials, we examined genital infections as a positive control outcome.22We also examined osteoarthritis related encounters and cataract surgery as negative control outcomes. A null association with treatment is expected for appropriate negative control outcomes, which share unmeasured confounders with the outcome and are unaffected by treatment.23As with dementia, osteoarthritis and cataract are degenerative diseases of older people. Therefore, osteoarthritis related encounters and cataract surgery would share with dementia unmeasured confounders such as frailty, lifestyle, and healthcare system usage patterns associated with ageing, and cataract surgery would also share smoking and alcohol consumption.2425Osteoarthritis related encounters would be expected for symptomatic or advanced osteoarthritis. Thus we considered such encounters to be minimally affected by the study drugs despite mild weight reduction effect of SGLT-2 inhibitors.11Also, two meta-analyses reported a null association between the development of cataract and treatment with SGLT-2 inhibitors.2627Using a deviation from the null association between a negative control outcome and treatment, we estimated corrected hazard ratios and corresponding 95% confidence intervals (CIs) adjusting for residual confounding.2328
Covariates
We identified covariates related to diabetes severity and risk of dementia for the 365 day pre-index baseline period (see supplemental table S2 for ICD-10 codes used to ascertain covariates). The covariates included personal characteristics, sociodemographic factors, complications from diabetes (retinopathy, nephropathy, neuropathy, and diabetic foot), classes and number of antiglycaemic drugs, risk factors for dementia (ie, cardiometabolic risk factors, hearing loss, head trauma, fracture history, mood or mental disorders, and anticholinergic drugs), other comorbidities and related drugs, Charlson-Deyo comorbidity index,29and healthcare service use patterns such as hospital admissions, emergency department visits, and outpatient clinic visits.
Statistical analysis
We used propensity score matching to account for confounding. The propensity score was estimated for each comparison using a multivariable logistic regression model that included >110 baseline covariates (see supplemental table S4 for the full list). Nearest neighbour matching for SGLT-2 inhibitor versus DPP-4 inhibitor was done in a ratio of 1:1, with a caliper of 0.025 on the propensity score scale. Balance between covariates after propensity score matching was considered to have been achieved when the absolute standardised difference was <0.1 between the two treatment groups.30Propensity score matched incidence rates of primary and secondary outcomes were calculated per 100 person years.
We primarily used Cox proportional hazard models to estimate the hazard ratios and corresponding 95% CIs. Owing to the discrete difference in mortality between the two treatments,11we also presented hazard ratios (95% CIs) from Fine-Gray models, adjusting for competing risk of death.31The proportional hazard assumption was tested by adding the interaction term between treatment and follow-up time in the model. When the interaction was statistically significant, we performed a follow-up time stratified analysis to examine the time varying treatment effect. We sorted propensity score matched study participants into two groups according to their follow-up times (≤2 years or >2 years), then estimated a matched set stratified hazard ratio (95% CI) within the two groups.
In our primary as treated analysis, patients were followed from the day after the index date up to the first occurrence of the censoring events (outcome event, disenrollment, death, end of database (31 December 2021), or treatment change through discontinuation, switching, or adding). Drug discontinuation was defined as no dispensing within 90 days from the expected refill date. The expected refill date was calculated by adding days’ supply to the last dispensing date of the study drug. Participants who discontinued the study drug were followed up until the last expected refill date plus a 30 day grace period. Although switching between different SGLT-2 inhibitors or between different DPP-4 inhibitors was not a censoring event, adding or switching to other classes of antiglycaemic treatments resulted in immediate censoring. We performed an intention-to-treat analysis as our secondary analysis, where participants were followed up until censoring events except for treatment change to deal with concerns of informative censoring.
Sensitivity analyses—Firstly, to avoid reverse causation from delayed diagnosis of dementia, we started follow-up after 365 days from the index date in both as treated and intention-to-treat analyses (up to three years and the whole follow-up). Secondly, we applied a grace period of 180 or 365 days for the censoring by treatment change to capture delayed diagnoses made after the change of treatment. Thirdly, to eliminate the effect of hypoglycaemic episodes during treatment, analyses were done excluding those who concurrently used drugs with hypoglycaemia potential (insulin, sulfonylurea, or glinides) on the index date. Fourthly, we adjusted for the duration of diabetes mellitus for those who had an ascertainable type 2 diabetes diagnosis date, defined as the first date of an ICD-10 code for type 2 diabetes diagnosis in the primary position free of such codes for at least 365 days before the diagnosis date. Lastly, we utilised the entirety of new users of SGLT-2 inhibitors and DPP-4 inhibitors using propensity score based fine stratification and weighting to achieve greater generalisability.32
Subgroup analyses—Prespecified propensity score matched subgroup analyses were done based on participants’ age (≥60 years and <60 years), sex, concurrent metformin use, and baseline cardiovascular risk. The estimation of propensity score and matching were done separately for individual subgroups. The subgroup with high cardiovascular risk was defined as men aged ≥50 years and women aged ≥55 years who had at least one diagnosis of angina, myocardial infarction, stroke, or peripheral vascular disease during the one year pre-index period.20We tested interaction terms between the treatment and individual stratifying factors.
Patient and public involvement
This study analysed secondary data without patient involvement. Patients were not invited to be involved in the study design, development of outcomes, interpretation of the results, or drafting of the manuscript. The primary barrier against patient and public involvement was use of an administrative database, which requires a specific study design and pharmacoepidemiological method to ensure internal validity, leaving minimal potential for the patient and public to be engaged.
Results
Baseline patient characteristics
Supplemental figure S2 shows the selection process of the study cohort. We identified 112 663 new users of SGLT-2 inhibitors and 847 999 new users of DPP-4 inhibitors who were free of known dementia and did not use either of the study drugs at baseline. Before propensity score matching, most baseline covariates, including diabetes complications and number of antiglycaemic drugs, were overall relatively well balanced, reflecting the effectiveness of the active comparator new user design (table 1, also see supplemental table S4 for the distribution of the full list of covariates between the two groups). Some covariates showed imbalance, with standardised differences >0.1, particularly cardiovascular comorbidities, which were more prevalent among initiators of SGLT-2 inhibitors than among initiators of DPP-4 inhibitors (16.8%v10.6% for angina pectoris, 3.1%v1.6% for myocardial infarction, 7.8%v4.2% for heart failure, 66.6%v59.8% for hypertension, 78.8%v70.9% for hyperlipidaemia). After propensity score matching in a 1:1 ratio, 110 885 pairs of initiators of SGLT-2 inhibitors and DPP-4 inhibitors were included in the analysis (mean age 61.9 years, 55.7% men) (table 1, also see supplemental table S4). All propensity score matched baseline covariates, including psychiatric disorders, cardiovascular diseases, other comorbidities, use of drugs with anticholinergic activity, and use of other drugs, were well balanced (standardised differences <0.1). The study participants’ mean comorbidity score was 2.4 (standard deviation (SD) 1.8). Cardiometabolic factors were highly common, with 66.5% of participants having hypertension and 78.6% having hyperlipidaemia. Established cardiovascular diseases were observed in 16.7% of participants with angina, 6.4% with stroke, and 3.1% with myocardial infarction. The most common oral antiglycaemic agents used during the baseline period were biguanide (52.2%), followed by sulfonylurea (27.8%) and thiazolidinedione (8.2%). The most common index SGLT-2 inhibitor was dapagliflozin (58.6%), followed by empagliflozin (35.4%), and the most common index DPP-4 inhibitors were gemigliptin (22.7%), linagliptin (22.4%), and sitagliptin (20.4%) (see supplemental table S5).
Comparative risk of dementia between initiators of SGLT-2 inhibitors and DPP-4 inhibitors
The mean follow-up time of patients was 670 (SD 650) days, with 612 (SD 613) days for initiators of SGLT-2 inhibitors and 728 (SD 679) days for initiators of DPP-4 inhibitors (see supplemental table S6 for distribution of censoring events). A total of 1172 participants with newly diagnosed dementia were identified, with incidence rates per 100 person years of 0.22 for initiators of SGLT-2 inhibitors and 0.35 for initiators of DPP-4 inhibitors. The corresponding hazard ratio was 0.65 (95% CI 0.58 to 0.73;table 2). The lowered risk of dementia associated with use of SGLT-2 inhibitors compared with DPP-4 inhibitors was similarly observed for secondary outcomes: hazard ratio 0.54 (0.46 to 0.63) for dementia requiring drugs, 0.61 (0.53 to 0.69) for Alzheimer’s disease, and 0.48 (0.33 to 0.70) for vascular dementia. The results were consistent with those of intention-to-treat analyses: 0.65 (0.60 to 0.71) for dementia, 0.60 (0.54 to 0.67) for dementia requiring drugs, 0.63 (0.57 to 0.69) for Alzheimer’s disease, and 0.62 (0.49 to 0.79) for vascular dementia. Estimates for the Fine-Gray models were also similar. We found a 2.67-fold risk (95% CI 2.57-fold to 2.77-fold) of genital infections associated with SGLT-2 inhibitors versus DPP-4 inhibitors. The hazard ratios for association between treatment and negative control outcomes were 0.97 (95% CI 0.95 to 0.98) for osteoarthritis related encounters and 0.92 (0.89 to 0.96) for cataract surgery. When corrected using the association between treatment and cataract surgery, the hazard ratios for dementia increased by about 7.7% (see supplemental table S7), to 0.70 (0.62 to 0.80).
Follow-up time stratified analysis
A significant interaction (P<0.05) was observed between treatment and follow-up time for all outcomes except vascular dementia in the as treated analysis. The Kaplan-Meier curve diverged more in the later follow-up period for these outcomes (fig 1), indicating that the effect would be greater with longer treatment. According to the follow-up time stratified analyses (46 767 propensity score matched pairs treated for two or less years, 16 827 pairs treated for more than two years; see supplemental table S8 for the distribution of baseline covariates for individual stratified groups), the magnitude of association modestly increased with more than two years of treatment compared with two years or less for these outcomes (see supplemental table S9): hazard ratio for more than two years versus two years or less of treatment was 0.52 (95% CI 0.41 to 0.66)v0.57 (0.46 to 0.70) for dementia, 0.41 (0.29 to 0.57)v0.45 (0.33 to 0.61) for dementia requiring drugs, and 0.48 (0.37 to 0.63)v0.53 (0.41 to 0.68) for Alzheimer’s disease.
Sensitivity analyses
The results were highly consistent even after accounting for the 365 day lag time from the index date (table 3), with hazard ratios in as treated analyses of 0.57 (0.48 to 0.68) for dementia, 0.48 (0.38 to 0.61) for dementia requiring drugs, 0.55 (0.45 to 0.67) for Alzheimer’s disease, and 0.46 (0.26 to 0.80) for vascular dementia. In the intention-to-treat analyses with lag time applied, the hazard ratios were 0.80 (0.75 to 0.86) for dementia, 0.84 (0.77 to 0.91) for dementia requiring drugs, 0.80 (0.74 to 0.86) for Alzheimer’s disease, and 0.80 (0.66 to 0.98) for vascular dementia.
For as treated analyses with longer grace periods after treatment change, a slightly increased incidence rate of dementia was noted in both treatment groups but to a greater degree among initiators of SGLT-2 inhibitors, with a hazard ratio of 0.72 (0.65 to 0.80) for dementia for a grace period of 180 days and 0.76 (0.69 to 0.83) for a grace period of 365 days (see supplemental table S10). Decreased incidence rates of genital infections were also noted among initiators of SGLT-2 inhibitors.
The results were consistent regardless of concurrent use of a drug with hypoglycaemic potential (see supplemental tables S11 and S12), with a hazard ratio of 0.69 (0.60 to 0.80) for dementia. The duration of type 2 diabetes was identified for 45 088 propensity score matched pairs (1008v925 days for initiators of SGLT-2 inhibitors and DPP-4 inhibitors, respectively, with a standardised difference of 0.10). Consistent results were observed after adjusting for duration of type 2 diabetes (see supplemental tables S13 and S14), with a hazard ratio of 0.60 (0.50 to 0.72) for dementia. We also observed similar results in propensity score based fine stratification weighted analyses (see supplemental tables S15 and S16), with a hazard ratio of 0.68 (0.62 to 0.75) for dementia.
Subgroup analysis
Supplemental table S17 presents the baseline characteristics of the subgroups. The lower risk associated with SGLT-2 inhibitors was overall consistent across subgroups stratified by age, sex, concurrent metformin use, and baseline cardiovascular risk (fig 2, also see supplemental table S18). However, statistical significance was not achieved for the subgroups with relatively small outcome numbers (eg, those aged <60 years). We did not find any interaction between the treatment and individual stratifying factors.
Discussion
This large population based cohort study among adults aged 40-69 years with type 2 diabetes found a 35% reduced risk of dementia associated with use of SGLT-2 inhibitors compared with DPP-4 inhibitors. This finding persisted regardless of dementia type and across subgroups of populations with diverse characteristics. Highly consistent results over a range of secondary and sensitivity analyses supported the robustness of our study findings. Our findings also suggest that the treatment effect of SGLT-2 inhibitors escalated with time.
Relevance of study design to internal validity
An active comparator new user design is a powerful pharmacoepidemiological approach that effectively copes with both measured and unmeasured confounding in observational studies.33One of the key advantages of this approach would be that similar disease (type 2 diabetes in our example) severity and related comorbidity profile can be expected between the two treatment groups because the participants in both groups are at the beginning of a similar stage of a given treatment. International guidelines had equally recommended SGLT-2 inhibitors and DPP-4 inhibitors as second line treatment until December 201834when the revised guideline preferentially recommended use of SGLT-2 inhibitors in the presence of atherosclerotic cardiovascular disease, heart failure, or chronic kidney disease.35This approach also ensures that selection bias associated with depletion of susceptible people (to inefficacy or intolerance, or both) is avoided, allowing all individuals initiating the study drug to contribute to the follow-up from the start of the treatment. In this context, our study design offered greater internal validity than in previous studies.1516
Interpretation of results and comparison with other studies
We observed a known association between a positive control outcome and treatment.22The association for osteoarthritis related encounters was close to null (hazard ratio 0.97, 95% CI 0.95 to 0.98), which achieved statistical significance owing to excess power from a highly frequent outcome. A slight deviation (0.92, 0.89 to 0.96) from the null association was observed for cataract surgery. A bias measure (7.7% increased hazard ratio) based on this deviation indicated that the association between treatment and dementia was largely unexplained solely by residual confounding.
In preclinical studies, SGLT-2 inhibitors have shown direct neuroprotective effects through multiple pathways.13363738These drugs exhibited anticholinergic activity,13prevented ultrastructural changes of neurovascular units associated with cognitive decline in mice with diabetes,36and ameliorated amyloid β deposition and tau phosphorylation in the brain tissue of mice with Alzheimer’s disease and type 2 diabetes.37Diurnal catabolism induced by SGLT-2 inhibitors restored autophagy by downregulating the mTOR (mechanistic target of rapamycin) pathway, which is chronically activated in Alzheimer’s disease.38Based on these preclinical findings, SGLT-2 inhibitors may delay the progression of dementia in people with type 2 diabetes both for Alzheimer’s disease and for vascular dementia, independent of the cardiorenal benefits exerted by SGLT-2 inhibitors.
A considerable effect estimate found within a relatively short period (≤2 years) of follow-up needs attention. Dementia develops through a continuum of accumulated molecular and structural changes.7Heterogeneous states of disease progression yet to reach definitive dementia are likely to exist among people with type 2 diabetes at baseline or even after applying a one year lag time. This is likely true since mild cognitive impairment, a transitional state between normal ageing and dementia,7is prevalent among 12-18% and 23% of people aged ≥60 years in the US and Korea, respectively, with 10-15% of the annual conversion to dementia.3940Notably, mild cognitive impairment is 1.4~2.0 times more prevalent among people with type 2 diabetes with accelerated progression.41424344Because the time span between mild cognitive impairment and dementia has already been shortened, and progression is particularly rapid among people with type 2 diabetes, early risk reduction against dementia could be seen in the presence of effective treatment (see supplementary figure S3 for a schematic explanation). This scenario also complies with the finding that the cognitive benefits of SGLT-2 inhibitor use versus non-use were better noted for those with mild cognitive impairment than with normal cognitive function at baseline.17Moreover, the visible action of SGLT-2 inhibitors versus DPP-4 inhibitors was rapid, based on the time elapsed until the first statistically significant result as early as day 5 for the benefits on death and worsening heart failure.45
A recent prospective cohort study found that use of SGLT-2 inhibitors for more than three years improved cognitive function scores compared with non-use.46Although this finding suggests that longer treatment might generate more benefits, the study was subject to confounding by indication and immortal time bias owing to the comparison between users (eg, prevalent users) and non-users of SGLT-2 inhibitors.33Our study comparing new users of two competing drugs, SGLT-2 inhibitors and DPP-4 inhibitors, further supports favourable results for early initiation of the drug and prolonged treatment.
We observed attenuated results with lag time applied in intention-to-treat analyses and with longer grace periods. Since incidence rates of genital infections continually decreased among users of SGLT-2 inhibitors in these analyses, loss of treatment effect associated with misclassification of drug use played a role in driving the results towards null. Initiators of SGLT-2 inhibitors, however, were more frequently censored by treatment change than initiators of DPP-4 inhibitors. Because patients with risk factors for treatment change (non-adherence, inefficacy, or adverse events) can be more prone to develop dementia than patients without these risk factors, informative censoring may have overestimated the results in our as treated analysis. Nevertheless, the overall results between as treated and intention-to-treat analyses were similar (table 2), suggesting non-substantial informative censoring.
In subgroup analyses, we observed highly consistent results, but did not find an interaction between treatment and individual characteristics of the study population. Unlike the expectation that SGLT-2 inhibitors might be associated with greater benefits against the risk of vascular dementia than Alzheimer’s disease, the magnitude of association was accompanied by widely overlapping 95% CIs between the two types of dementia for all analyses. Thus, it is not surprising to observe no interaction between treatment and baseline cardiovascular risk. A recent meta-analysis also reported that the pooled beneficial association between dementia and use of SGLT-2 inhibitors versus other antiglycaemic treatments was not affected by cardiovascular diseases.10These findings suggest that the underlying mechanisms are not limited to cardiorenal pathways, possibly involving direct neuroprotective pathways observed in preclinical studies.13363738According to previous studies on metformin monotherapy versus no treatment, metformin was not associated with incident dementia.4748Based on these findings, concurrent use of metformin is unlikely to interact with SGLT-2 inhibitors in modifying the risk of dementia.
Strengths and limitations of this study
Several important strengths of this study deserve comment. Firstly, we used rigorous pharmacoepidemiological approaches, in particular we adopted an active comparator new user design and extensive propensity score matching.33The diagnosis codes in the primary position and applying disease specific drugs would increase the specificity of the outcome. The sensitivity analyses and control outcomes add relevant internal validity to this study. Secondly, compared with a previous study,14we included relatively younger people (aged 40-69 years) with type 2 diabetes, broadening the target population of benefits associated with use of SGLT-2 inhibitors. Thirdly, we used a nationally representative database, providing high generalisability. Fourthly, we performed comprehensive analyses for time varying comparisons of SGLT-2 inhibitors versus DPP-4 inhibitors, diverse subgroups, and individual types of dementia, presenting highly consistent results.
This study also has limitations. Firstly, owing to the observational nature of our study, it is inherently subject to residual or unmeasured confounding. Although we balanced many proxies of type 2 diabetes severity and comorbidities and used negative control outcomes, direct test results on serum glucose levels, renal function, severity of other comorbidities, health behaviours (eg, smoking and alcohol consumption), and duration of type 2 diabetes were not fully ascertainable from the claims data. Secondly, diagnoses of dementia are commonly delayed, rendering studies on dementia risk particularly susceptible to informative censoring, reverse causation, and outcome misclassification, which may have resulted in overestimation of our results. Thirdly, our study did not provide exact mechanisms of neuroprotection.
Conclusions
This large population based cohort study found that initiation of SGLT-2 inhibitors was associated with a 35% lower risk of dementia compared with initiation of DPP-4 inhibitors in people with type 2 diabetes aged 40-69 years. This association was similarly observed for Alzheimer’s disease and vascular dementia and was also consistent across subgroups. We observed a greater association with treatment duration longer than two years. These findings underscore the need for future randomised controlled trials.
Despite increasing numbers of people with dementia globally, current options for disease modifying treatments are limited
Type 2 diabetes substantially predisposes people to Alzheimer’s disease and vascular dementia through multiple pathways
A previous study suggested a decreased risk of dementia associated with sodium-glucose cotransporter-2 (SGLT-2) inhibitors versus dipeptidyl peptidase-4 (DPP-4) inhibitors among people with type 2 diabetes aged >66 years
This large population based cohort study among people with type 2 diabetes aged 40-69 years found a 35% lower risk of dementia associated with use of SGLT-2 inhibitors compared with DPP-4 inhibitors
This finding persisted regardless of dementia type and across subgroups of diverse population characteristics such as age, sex, concomitant use of metformin, and baseline cardiovascular risk
The treatment effect of SGLT-2 inhibitors compared with DPP-4 inhibitors increased with time
","Objective: To compare the risk of dementia associated with sodium-glucose cotransporter-2 (SGLT-2) inhibitors versus dipeptidyl peptidase-4 (DPP-4) inhibitors in adults aged 40-69 years with type 2 diabetes.
Design: Population based cohort study.
Setting: Korean National Health Insurance Service data, 2013-21.
Participants: 110 885 propensity score matched pairs of adults with type 2 diabetes aged 40-69 years who were initiators of either an SGLT-2 inhibitor or a DPP-4 inhibitor.
Main outcome measures: The primary outcome was new onset dementia. Secondary outcomes were dementia requiring drug treatment and individual types of dementia, including Alzheimer’s disease and vascular dementia. Control outcomes were genital infections (positive), and osteoarthritis related clinical encounters and cataract surgery (negative). Hazard ratios and 95% confidence intervals (CIs) were estimated using Cox models. Follow-up time stratified analyses (>2 years and ≤2 years) and subgroup analyses by age, sex, concomitant use of metformin, and baseline cardiovascular risk were performed.
Results: 110 885 propensity score matched pairs of initiators of an SGLT-2 inhibitor or a DPP-4 inhibitor were followed-up for a mean 670 (standard deviation 650) days, generating 1172 people with newly diagnosed dementia: incidence rate 0.22 per 100 person years in initiators of SGLT-2 inhibitors and 0.35 per 100 person years in initiators of DPP-4 inhibitors, with hazard ratios of 0.65 (95% CI 0.58 to 0.73) for dementia, 0.54 (0.46 to 0.63) for dementia requiring drugs, 0.61 (0.53 to 0.69) for Alzheimer’s disease, and 0.48 (0.33 to 0.70) for vascular dementia. The hazard ratios for the control outcomes were 2.67 (2.57 to 2.77) for genital infections, 0.97 (0.95 to 0.98) for osteoarthritis related encounters, and 0.92 (0.89 to 0.96) for cataract surgery. When calibrated for residual confounding measured by cataract surgery, the hazard ratio for dementia was 0.70 (0.62 to 0.80). The association was greater for more than two years of treatment (hazard ratio of dementia 0.57, 95% CI 0.46 to 0.70) than for two years or less (0.52, 0.41 to 0.66) and persisted across subgroups.
Conclusion: SGLT-2 inhibitors might prevent dementia, providing greater benefits with longer treatment. As this study was observational and therefore prone to residual confounding and informative censoring, the effect size could have been overestimated. Randomised controlled trials are needed to confirm these findings.
"
Comparative oral monotherapy of psychedelics and escitalopram for depressive symptoms,"Introduction
Common psychedelics belong to two classes: classic psychedelics, such as psilocybin, lysergic acid diethylamide (known as LSD), and ayahuasca; and entactogens, such as 3,4-methylenedioxymethamphetamine (MDMA).1Several randomised controlled trials have shown efficacy of psychedelics for people with clinical depression.23The proposed mechanism of its fast and persistent antidepressant effects is to promote structural and functional neuroplasticity through the activation of intracellular 5-HT2Areceptors in the cortical neurons.4Additionally, the increased neuroplasticity was associated with psychedelic’s high affinity directly binding to brain derived neurotrophic factor receptor TrkB, indicating a dissociation between the hallucinogenic and plasticity promoting effects of psychedelics.5A meta-analysis published in 2023 reported that the standardised mean difference of psychedelics for depression reduction ranged from 1.37 to 3.12,2which are considered large effect sizes.6Notably, the standardised mean difference of antidepressant trials is approximately 0.3 (a small effect size).78
Although modern randomised controlled trials involving psychedelics usually use a double blinded design, the subjective effects of these substances can compromise blinding.9Unsuccessful blinding may lead to differing placebo effects between the active and control groups, potentially introducing bias into the estimation of relative treatment effects.10Concerns have arisen regarding the overestimated effect sizes of psychedelics due to the issues of blinding and response expectancy.9Psychedelic treatment is usually administered with psychological support or psychotherapy, and thereby the isolated pharmacological effects of psychedelics remain to be determined.2Surprisingly, on 1 July 2023, Australia approved psilocybin for the treatment of depression11; the first country to classify psychedelics as a medicine at a national level.
To date, only one double blind, head-to-head randomised controlled trial has directly compared a psychedelic drug (psilocybin) with an antidepressant drug (escitalopram) for patients with major depressive disorder.12This randomised controlled trial reported that psilocybin showed a better efficacy than escitalopram on the 17 item Hamilton depression rating scale (HAMD-17).
We aimed to assess the comparative effectiveness and acceptability of oral monotherapy with psychedelics and escitalopram in patients experiencing depressive symptoms. Given that unsuccessful blinding can potentially lead to a reduced placebo response in psychedelic trials, we distinguished between the placebo responses in psychedelic and antidepressant trials. We also investigated the differences in patient responses between people who received extremely low dose psychedelics as a placebo and those who received a placebo in the form of a fake pill, such as niacin, in psilocybin trials.1314Our study allowed for a relative effect assessment of psychedelics compared with placebo responses observed in antidepressant trials.
Methods
The study protocol was registered with PROSPERO (CRD42023469014). We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) extension statement for reporting systematic reviews incorporating network meta-analysis (NMA) (appendix 1).15
Data sources and searches
A comprehensive search of the Medline, Cochrane Central Register of Controlled Trials (CENTRAL), Embase, PsycINFO, ClinicalTrial.gov, and World Health Organization’s International Clinical Trials Registry Platform databases were performed without language restrictions from database inception to 12 October 2023. We also searched the grey literature and reviewed reference lists of the included studies and related systematic reviews.23
Study selection
Eligible studies were randomised controlled trials with parallel group or crossover designs. We included: (i) adults (≥18 years) with clinically diagnosed depression (eg, major depressive disorder, bipolar disorder, or other psychiatric disorders with comorbid clinical depression) or life threatening diagnoses and terminal illness with depressive symptoms; and (ii) adults with assessment of treatment response (preapplication/postapplication) using standard, validated, and internationally recognised instruments, such as HAMD-17. The outcome of interest was the change in depressive symptoms at the end of treatment compared with the controls, and we only extracted data from the first phase of crossover randomised controlled trials to avoid carry-over effects. Eligible psychedelic randomised controlled trials (including psilocybin, lysergic acid diethylamide, MDMA, and ayahuasca without dosage limit) required oral monotherapy without the concomitant use of antidepressants. For escitalopram, we included only fixed dose randomised controlled trials that compared at least two arms with different doses of oral form escitalopram (maximum dose of 20 mg/day) with placebo because psychedelic therapies usually use a fixed dose study design. We also included randomised controlled trials that evaluated psychedelic monotherapy compared with escitalopram monotherapy. We excluded follow-up studies and studies with healthy volunteers. We also excluded conference abstracts, editorials, reviews, meta-analyses, case reports, and case series, as well as publications reporting duplicate data. We did not consider ketamine because this drug is usually administered parenterally and is not a classic psychedelic.16Screening and selection of the studies were performed independently by two authors. Discrepancies in study inclusion were resolved by deliberation among the reviewer pairs or with input from a third author. Appendix 2 shows the complete search strategies, and appendix 3 presents the reasons for exclusion.
Definition of outcomes, data extraction, and risk of bias assessment
The primary outcome was change in depressive symptoms from baseline (continuous outcome), as measured by a validated rating scale, such as HAMD-17. When multiple measurement tools were used, they were selected in the following order: the HAMD-17, Montgomery-Åsberg depression rating scale, and Beck depression inventory (second edition). To improve interpretability, all extracted depression scores were converted to corresponding HAMD-17 scores using a validated method.17We used a conservative correlation coefficient of 0.5 or other statistics (eg, t statistics) to calculate the standard deviation of change from baseline when unreported.18The secondary outcomes were all cause discontinuation and severe adverse events (categorical outcomes). Severe adverse events were classified as those resulting in any of a list of negative health outcomes including, death, admission to hospital, significant or persistent incapacity, congenital birth defect or abnormality, and suicide attempt. Outcome data were extracted from original intention-to-treat or last observation carrying forward analysis, as well as from estimates of mixed-effect models for repeated measures.
Two authors independently extracted and reviewed the data, each being reviewed by another author. WebPlot Digitizer (https://automeris.io/WebPlotDigitizer/) was used to extract numerical data from the figures. Two authors independently used the Cochrane randomised trial risk of bias tool (version 2.0) to assess the risk of bias in the included trials, and discrepancies were resolved by consensus.19
Data synthesis
To estimate the relative effect between two interventions, we computed mean difference on the basis of change values (with 95% credible interval) for continuous outcomes (change in depressive symptoms) and odds ratios for categorical outcomes (all cause discontinuation and severe adverse event). To assess the clinical significance of the relative effect, we evaluated whether the mean difference exceeded the minimal important difference, which is estimated to be 3 points for HAMD-17.20We defined high, low, and extremely low doses of the included psychedelics as follows: (i) psilocybin: high dose (≥20 mg), extremely low dose (1-3 mg), low dose (other range); and (ii) MDMA: high dose (≥100 mg), extremely low dose (≤40 mg), low dose (other range). Escitalopram was divided into escitalopram 10 mg and escitalopram ≥20 mg. In previous clinical trials, a dose of 1 mg of psilocybin or a dose range of 1-3 mg/70 kg were used as an active control because these doses were believed not to produce significant psychedelic effects.2122A dose of 5 mg/70 kg can produce noticeable psychedelic effects.22In many two arm psilocybin trials, the psilocybin dose in the active group typically falls within the range of 20-30 mg.12212324In a three arm trial, 25 mg was defined as high dose, and 10 mg was considered a moderate dose.21Another clinical trial also defined 0.215 mg/kg of psilocybin as a moderate dose for the active group.25Therefore, we used 20 mg and 3 mg as the boundaries for grouping psilocybin doses; when the dosage was calculated per kilogram in the study, we converted it to per 70 kg. For MDMA, in two trials with three arms, 125 mg was defined as high dose, and 30-40 mg was defined as active control.2627Thus, we used 100 mg and 40 mg as the boundaries for grouping MDMA doses.
We conducted random effects network meta-analysis and meta-analysis within a Bayesian framework.2829Previous meta-analyses considered all control groups as a common comparator; however, concerns have been raised regarding the overestimated effect sizes of psychedelics because of unsuccessful blinding and poor placebo response.9Therefore, we treated the three treatments as distinct interventions: the placebo response observed in psychedelic trials, the placebo response observed in antidepressant escitalopram trials, and extremely low dose psychedelics (ie, psilocybin and MDMA). We calculated the relative effects of all interventions compared with these three groups, indicating the following three conditions: (1) the treatment response of placebo response in the psychedelic trials is assumed to be lower than that of placebo response in antidepressant trials because of unsuccessful blinding.9As such, the relative effects compared with placebo response in the psychedelic trials represented potential overestimated effect sizes. (2) the placebo response in antidepressant trials is assumed to be the placebo response in antidepressant trials with adequate blinding, therefore, the relative effects compared with placebo response in antidepressant trials represents effect sizes in trials with adequate blinding. (3) Psychedelic drugs are usually administered with psychotherapy13or psychological support,14the relative effects of psychedelics compared with extremely low dose psychedelics might eliminate the concomitant effects from psychotherapeutic support, approximating so-called pure pharmacological effects.
In network meta-analysis, the validity of indirect comparison relies on transitivity assumption.30We assessed the transitivity assumption by comparing the distribution of potential effect modifies across treatment comparisons. In addition, we assessed whether the efficacy of escitalopram is similar in placebo controlled randomised controlled trials (escitalopramvplacebo response in antidepressant trials) and in the head-to-head randomised controlled trial (psilocybinvescitalopram) using network meta-analysis.12Furthermore, we assessed the efficacy of the different placebo responses (placebo response in the psychedelic trialsvplacebo response in antidepressant trials) as additional proof of transitivity. If the placebo response in antidepressant trials was better than that in the psychedelic trials, the transitivity assumption did not hold when grouping placebo response in antidepressant trials and placebo response in the psychedelic trials together. Finally, for the primary outcome (change in depressive symptoms), network meta-regression analyses were conducted to evaluate the impact of potential effect modifiers, including proportion of men and women in the study, mean age, baseline depression severity, disorder type, and follow-up assessment period. We assumed a common effect on all treatment comparisons for each of the effect modifiers. In other words, all interactions between the treatment comparisons and the effect modifier were constrained to be identical.
We also conducted the following sensitivity analyses: analysing studies of patients with major depressive disorder; excluding studies with a high risk of bias; adjusting for baseline depression severity; and using correlation coefficient of zero (most conservative) to calculate the standard deviation of change from baseline when unreported.
Publication bias was assessed by visual inspection of a comparison adjusted funnel plots. The first funnel plot used placebo response in the psychedelic trials as the comparator. The second funnel plot used placebo response in antidepressant trials as the comparator. The third funnel plot used both placebo response in the psychedelic trials and placebo response in antidepressant trials as comparators simultaneously. Additionally, we conducted the Egger test, Begg test, and Thompson test to examine the asymmetry of the third funnel plot. A previous meta-analysis reported that the standardised mean difference of psychedelics for depression reduction ranged from 1.37 to 3.12.2Therefore, we also transformed the effect size of mean difference to standardised mean difference (Hedges’ g) for the primary outcome. The global inconsistency of the network meta-analysis was examined by fitting an unrelated main effects model. Local inconsistency of the network meta-analysis was examined using node splitting methods.31Four Markov chains were implemented. 50 000 iterations occurred per chain and the first 20 000 iterations for each chain were discarded as a warm-up. Convergence was assessed by visual inspection of the trace plots of the key parameters for each analysis. The prior settings and convergence results are shown in appendix 4. All statistical analyses were done using R version 4.3.1. The network meta-analysis and pairwise meta-analysis within a Bayesian framework were fitted using the Bayesian statistical software called Stan within the R packages multinma28and brms,29respectively. The frequentist random effects network meta-analysis, funnel plots, and tests for funnel plot asymmetry were conducted using the R package netmeta. Reasons for protocol changes are in appendix 5.
Assessment certainty of evidence for the primary outcome
The certainty of evidence produced by the network meta-analysis was evaluated using GRADE (Grading of recommendations, assessment, development and evaluation).3233We used a minimally contextualised framework with the value of 3 (minimal important difference) as our decision threshold. The certainty of evidence refers to our certainty that the intervention had, relative to minimal intervention, any clinically minimal important difference. The optimal information size was calculated using a validated method.323334
Patient and public involvement
Both patients and the public are interested in research on novel depression treatments and their efficacy compared with existing antidepressants. However, due to a scarcity of available funding for recruitment and researcher training, patients and members of the public were not directly involved in the planning or writing of this manuscript. We did speak to patients about the study, and we asked a member of the public to read our manuscript after submission.
Results
Characteristics of included study
After searching the database and excluding duplicated records, we identified 3104 unique potential studies. We then screened the titles and abstracts of these studies for eligibility and excluded 3062 of them, in which 42 studies remained. Twenty six studies were excluded after an assessment of the full text for various reasons (appendix 3). We identified three additional studies through a manual search resulting in total 19 eligible studies (efigure 1). Details of the characteristics of the included studies are shown in etable 1. Protocols of psychological support or psychotherapy with psychedelic treatment are shown in etable 2. Overall, 811 people (mean age of 42.49 years, 54.2% (440/811) were women) were included in psychedelic trials (15 trials), and 1968 participants (mean age of 39.35 years, 62.5% (1230/1968) were women) were included in escitalopram trials (five trials).
Risk of bias of the included studies
No psychedelic study (0/15) had a high overall risk of bias (efigure 2A and efigure 3A). The percentages of studies with high, some concerns, or low risk of bias in the 15 psychedelic trials were as follows: 0% (k=15), 33% (k=5), and 67% (k=10) for randomisation; 0% (k=0), 33% (k=5), and 67% (k=10) for deviations from intended interventions; 0% (k=0), 13% (k=2), and 87% (k=13) for missing outcome data; 0% (k=0), 33% (k=5), and 67% (k=10) for measurements of outcomes; 0% (k=0), 67% (k=1), and 93% (k=14) for selection of reported results. No non-psychedelic studies (0/5) were rated as high risk of bias (efigure 2B and efigure 3B). The percentages of studies with high, some concerns, and low risk of bias in the five non-psychedelic trials were as follows: 0% (k=0), 80% (k=4), and 20% (k=1) for randomisation; 0% (k=0), 100% (k=5), and 0% (k=0) for deviations from intended interventions; 0% (k=0), 80% (k=4), and 20% (k=1) for missing outcome data; 0% (k=0), 80% (k=4), and 20% (k=1) for measurements of outcomes; 0% (k=0), 20% (k=1), and 80% (k=4) for selection of reported results.
Network meta-analysis
In the network structure, all interventions were connected, with two main structures (fig 1). All psychedelics were compared with placebo response in the psychedelic trials, and escitalopram was compared with placebo response in antidepressant trials. A head-to-head comparison of high dose psilocybin and 20 mg escitalopram connected the two main structures.12
In the main network meta-analysis, all interventions, except for extremely low dose and low dose MDMA, were associated with a larger mean difference exceeding the minimal important difference of 3 points on the HAMD-17 than with placebo response in the psychedelic trials (fig 2). Notably, placebo response in antidepressant trials (3.79 (95% credibile interval 0.77 to 6.80)) and extremely low dose psilocybin (3.96 (0.61 to 7.17)) were better than placebo response in the psychedelic trials, with mean differences exceeding 3 and 95% credibile intervals that did not cross zero. Additionally, in comparison with placebo response in antidepressant trials (fig 2), the relative effects of high dose psilocybin (6.52 (3.19 to 9.57)), escitalopram 10 mg (1.86 (0.21 to 3.50)), and escitalopram 20 mg (1.82 (0.16 to 3.43)) did not cross zero. Only high dose psilocybin resulted in a mean difference that was greater than 3. The standardised mean difference of high dose psilocybin decreased from large (0.88) to small (0.31) when the reference arm was changed from placebo response in the psychedelic trials to placebo response in antidepressant trials.
When compared with extremely low dose psilocybin (fig 2), only the relative effects of high dose psilocybin (6.35 (95% credibile interval 3.41 to 9.21)) and placebo response in the psychedelic trials (−3.96 (−7.17 to −0.61)) showed a larger mean difference exceeding 3, without crossing zero. All relative effects between interventions are showed in efigure 4. Importantly, the mean differences of high dose psilocybin compared with escitalopram 10 mg (4.66 (1.36 to 7.74); standardised mean difference 0.22), escitalopram 20 mg (4.69 (1.64 to 7.54); 0.24), high dose MDMA (4.98 (1.23 to 8.67); 0.32), and low dose psilocybin (4.36 (1.20 to 7.51); 0.32) all exceeded 3 and did not cross zero (efigure 4).
Transitivity assumption
The assessment of transitivity assumption is showed in efigure 5 and efigure 6. We compared the efficacy of escitalopram in the placebo controlled antidepressant trials8with that in the head-to-head trial (psilocybinvescitalopram)12using network meta-analysis and pairwise meta-analysis. The results of the network meta-analysis showed that the relative effects between these two study designs (0.64 (95% credibile interval −4.41 to 5.40), efigure 6A; 1.94 (−2.66 to 6.14), efigure 6B) included zero, and the mean differences did not exceed 3. Placebo response in antidepressant trials was better than placebo response in the psychedelic trials with a small effect size (3.79 (0.77 to 6.80), standardised mean difference 0.2), and the mean difference exceed 3 (fig 2).
Sensitivity analyses
When including only patients with major depressive disorder, the relative effects of escitalopram 20 mg, escitalopram 10 mg, ayahuasca, and high dose psilocybin were better than placebo response in antidepressant trials, while placebo response in the psychedelic trials was worse than placebo response in antidepressant trials (fig 3). However, only the mean differences for high dose psilocybin (6.82 (95% credibile interval 3.84 to 9.67)), ayahuasca (5.38 (0.02 to 10.61)), and placebo response in the psychedelic trials (−4.00 (−6.87 to −1.13)) exceeded 3. When compared with extremely low dose psilocybin (excluding the effects from concomitant psychotherapeutic support), only the 95% credibile intervals of the relative effects of high dose psilocybin (4.36 (0.54 to 8.27); standardised mean difference 0.30) and placebo response in the psychedelic trials (−6.46 (−10.41 to −2.32), standardised mean difference −0.46) exceeded 3 and did not cross zero (fig 3). All of the relative effects between interventions are showed in efigure 7. Notably, the relative effects of high dose psilocybin compared with escitalopram 10 mg (4.96 (1.97 to 7.82)), escitalopram 20 mg (4.97 (2.19 to 7.64)), and low dose psilocybin (3.82 (0.61 to 7.04)) all exceeded 3 and did not cross zero (efigure 7).
The other three sensitivity analyses showed similar findings with the main analyses: exclusion of studies with high risk of bias (efigure 8); adjustment of baseline depression severity (efigure 9); and use of most conservative correlation coefficient of zero (efigure 10).
All cause discontinuation and severe adverse event
When referencing placebo in psychedelic trials, no interventions were associated with higher risks of all cause discontinuation rate nor severe adverse event rate (efigure 11).
Network meta-regression and publication bias
In network meta-regression analyses, the 95% credibile intervals of the relative effects of the baseline depressive severity, mean age, and percentage of women, crossed zero (etable 3). The results of the statistical tests (Egger, Begg, and Thompson-Sharp tests) for funnel plot asymmetry and visual inspection of funnel plots did not show publication bias (efigure 12). The results of GRADE assessment are provided in the efigure 13. Most of the certainty of evidence for treatment comparisons was moderate or low.
Consistency assumptions
The back calculation methods for all the models (appendix 6) did not show any inconsistencies. The node splitting methods also did not show any inconsistencies (appendix 7).
Discussion
Principal findings
This network meta-analysis investigated the comparative effectiveness between psychedelics and escitalopram for depressive symptoms. Firstly, we found that the placebo response observed in antidepressant trials was associated with greater effectiveness than that observed in psychedelic trials. Secondly, when compared with placebo responses in antidepressant trials, only escitalopram and high dose psilocybin were associated with greater effectiveness, and only high dose psilocybin exceeded minimal important difference of 3. Notably, the effect size of high dose psilocybin decreased from large to small. Thirdly, among the included psychedelics, only high dose psilocybin was more likely to be better than escitalopram 10 mg or 20 mg, exceeding the minimally important difference of 3. Fourthly, in patients with major depressive disorder, escitalopram, ayahuasca, and high dose psilocybin were associated with greater effectiveness than placebo responses in antidepressant trials; however, only high dose psilocybin was better than extremely low dose psilocybin, exceeding minimal important difference of 3. Taken together, our study findings suggest that among psychedelic treatments, high dose psilocybin is more likely to reach the minimal important difference for depressive symptoms in studies with adequate blinding design, while the effect size of psilocybin was similar to that of current antidepressant drugs, showing a mean standardised mean difference of 0.3.7
Comparison with other studies
In a randomised controlled trial, treatment response was defined as the response observed in the active arm; placebo response was defined as the response observed in the control (placebo) arm.10Treatment response consists of non-specific effects, placebo effect, and true treatment effect; placebo response consisted of non-specific effects and placebo effect. Therefore, when the placebo effect is not the same for the active and control arms within an randomised controlled trial, the estimation of the true treatment effect is biased. For example, in a psychedelic trial, unsuccessful blinding may occur due to the profound subjective effects of psychedelics. This unblinding may lead to high placebo effect in the active arm and low placebo effect in the control arms, and the true treatment effect is overestimated.10Without addressing unequal placebo effects within studies, the estimation of meta-analysis and network meta-analysis are biased.10However, in most psychedelic trials, blinding was either reported as unsuccessful or not assessed at all. For example, two trials of lysergic acid diethylamide reported unsuccessful blinding,3536whereas the trial of ayahuasca only reported that five of 10 participants misclassified the placebo as ayahuasca.37In trials of MDMA, participants' accuracy in guessing which treatment arm they were in ranged from approximately 60-90%.2627383940In the case of most psilocybin trials, blinding was not assessed, with the exception of the study by Ross and colleagues in 2016.13In that study, participants were asked to guess whether the psilocybin or an active control was received, and the correct guessing rate was 97%. In our study, we established several network meta-analysis models addressing this issue, and we found that placebo response in the psychedelic trials was associated with less effectiveness than that in antidepressant trials. Therefore, the effect sizes of psychedelics compared with placebo response observed in psychedelic trials may be overestimated. All of the psychedelics’ 95% credibile intervals of the relative effects crossed zero when compared with the placebo response in antidepressant trials, except for high dose psilocybin.
The comparisons between psychedelics and escitalopram showed that high dose psilocybin was more likely to be better than escitalopram. Psilocybin was usually administered with psychotherapy or psychological support.1314Therefore, the greater effectiveness of psilocybin may be from not only pharmacological effects but also psychotherapeutic support. However, we also found that high doses of psilocybin was associated greater effectiveness than extremely low doses of psilocybin. This effect also indicates that the effectiveness of psilocybin cannot be attributed only to concomitant psychotherapy or psychological support.
In patients with major depressive disorder, ayahuasca, low dose psilocybin, high dose psilocybin, escitalopram 10 mg, and escitalopram 20 mg were associated with greater effectiveness than the placebo response in antidepressant trials . However, when compared with extremely low dose psilocybin, only high dose psilocybin was associated with better effectiveness; the standardised mean difference decreased from 0.38 (compared with placebo response in antidepressant trials) to 0.30 (compared with extremely low dose psilocybin). As such, the effectiveness of psilocybin should be considered with concomitant psychotherapeutic support in people with major depressive disorder. The effect size of high dose psilocybin was similar with antidepressant trials of patients with major depressive disorder showing a mean standardised mean difference of 0.3.78
Strengths and limitations of this study
This study has several strengths. We conducted separate analyses for placebo response in antidepressant trials, placebo response in psychedelic trials, and an extremely low active dose of psychedelics, thereby mitigating the effect of placebo response variations across different studies. This approach allowed us to assess the efficacy of psychedelics more impartially and make relatively unbiased comparisons than if these groups were not separated. This study supported the transitivity assumption of the efficacy of escitalopram in placebo controlled antidepressant trials with that in psilocybin versus escitalopram head-to-head trial, thereby bridging the escitalopram trials and psychedelic trials. We also performed various sensitivity analyses to ensure the validation of our statistical results.
Nevertheless, our study has several limitations. Firstly, we extracted only the acute effects of the interventions. A comparison of the long term effects of psychedelics and escitalopram remains unclear. Secondly, participants in the randomised controlled trials on MDMA were predominantly diagnosed with post-traumatic stress disorder, whereas participants in the randomised controlled trials on escitalopram were patients with major depressive disorder. However, depressive symptoms in post-traumatic stress disorder could be relatively treatment resistant, requiring high doses of psychotropic drugs.41Moreover, our study focused not only on major depressive disorder but also on the generalisability of psychedelic treatment for depressive symptoms. Thirdly, although all available studies were included, the sample size of the psychedelic randomised controlled trials was small (k=15). Fourthly, when using extremely low dose psychedelics as a reference group, the relative effect may also eliminate some pharmacological effects because our study found that extremely low dose psychedelics could not be considered a placebo. Fifthly, in network meta-analysis, direct evidence for one treatment comparison may serve as indirect evidence for other treatment comparisons,42and biases in the direct evidence might affect estimates of other treatment comparisons. Because the absolute effect of escitalopram in the head-to-head trial (high dose psilocybinvescitalopram 20 mg)12was lower than those of placebo controlled trials, the relative effects of high dose psilocybin might be slightly overestimated when compared with other treatments in the current study. We addressed this issue by use of a Bayesian network meta-analysis, distinguishing between placebo response in psychedelic trials and placebo response in antidepressant trials. Specifically, we only considered that the 95% credibile interval of the relative effect between two comparisons did not cross zero. Indeed, the relative effect of escitalopram 20 mg between these two study designs included zero. Finally, our network meta-analysis may not have sufficient statistical power to detect potential publication bias due to the scarcity of trials and participants.
Implications and conclusions
Serotonergic psychedelics, especially high dose psilocybin, appeared to have the potential to treat depressive symptoms. However, study designs may have overestimated the efficacy of psychedelics. Our analysis suggested that the standardised mean difference of high dose psilocybin was similar to that of current antidepressant drugs, showing a small effect size. Improved blinding methods and standardised psychotherapies can help researchers to better estimate the efficacy of psychedelics for depressive symptoms and other psychiatric conditions.
Psychedelic treatment resulted in significant efficacy in treating depressive symptoms and alleviating distress related to life threatening diagnoses and terminal illness
Meta-analyses have reported standardised mean difference of psychedelics for depression reduction ranging from 1.37 to 3.12, while antidepressant trials were approximately 0.3
No network meta-analysis has examined comparative efficacy between psychedelics and antidepressants for depressive symptoms, and effect sizes of psychedelics might be overestimated because of unsuccessful blinding and response expectancies
To avoid estimation bi","Objective: To evaluate the comparative effectiveness and acceptability of oral monotherapy using psychedelics and escitalopram in patients with depressive symptoms, considering the potential for overestimated effectiveness due to unsuccessful blinding.
Design: Systematic review and Bayesian network meta-analysis.
Data sources: Medline, Cochrane Central Register of Controlled Trials, Embase, PsycINFO, ClinicalTrial.gov, and World Health Organization’s International Clinical Trials Registry Platform from database inception to 12 October 2023.
Eligibility criteria for selecting studies: Randomised controlled trials on psychedelics or escitalopram in adults with depressive symptoms. Eligible randomised controlled trials of psychedelics (3,4-methylenedioxymethamphetamine (known as MDMA), lysergic acid diethylamide (known as LSD), psilocybin, or ayahuasca) required oral monotherapy with no concomitant use of antidepressants.
Data extraction and synthesis: The primary outcome was change in depression, measured by the 17-item Hamilton depression rating scale. The secondary outcomes were all cause discontinuation and severe adverse events. Severe adverse events were those resulting in any of a list of negative health outcomes including, death, admission to hospital, significant or persistent incapacity, congenital birth defect or abnormality, and suicide attempt. Data were pooled using a random effects model within a Bayesian framework. To avoid estimation bias, placebo responses were distinguished between psychedelic and antidepressant trials.
Results: Placebo response in psychedelic trials was lower than that in antidepression trials of escitalopram (mean difference −3.90 (95% credible interval −7.10 to −0.96)). Although most psychedelics were better than placebo in psychedelic trials, only high dose psilocybin was better than placebo in antidepression trials of escitalopram (mean difference 6.45 (3.19 to 9.41)). However, the effect size (standardised mean difference) of high dose psilocybin decreased from large (0.88) to small (0.31) when the reference arm changed from placebo response in the psychedelic trials to antidepressant trials. The relative effect of high dose psilocybin was larger than escitalopram at 10 mg (4.66 (95% credible interval 1.36 to 7.74)) and 20 mg (4.69 (1.64 to 7.54)). None of the interventions was associated with higher all cause discontinuation or severe adverse events than the placebo.
Conclusions: Of the available psychedelic treatments for depressive symptoms, patients treated with high dose psilocybin showed better responses than those treated with placebo in the antidepressant trials, but the effect size was small.
Systematic review registration: PROSPERO, CRD42023469014.
"
Suicide rates among physicians compared with the general population in studies from 20 countries,"Introduction
In 2019, suicide caused over 700 000 deaths globally, which was more than one in every 100 deaths that year (1.3%). While the worldwide age standardised suicide rate was estimated at 9.0 per 100 000 population, there was great variation between individual countries (from <2 to >80 suicide deaths per 100 000).1The overall global decline in suicide rates by 36% since 2000 is not a universal trend because some countries like the United States or Brazil saw an increase of roughly the same magnitude.12Among many other social and environmental factors, occupation has been shown to influence suicide risk beyond established risk factors such as low socioeconomic status or educational attainment.34567
Physicians are one of several occupational groups linked to a higher risk of death by suicide, and the medical community has a longstanding and often conflicted history in addressing this issue.8AJAMAeditorial from 1903 reviewed annual suicide numbers for US physicians and concluded that their suicide risk is higher compared with the general population.9A substantial amount of evidence has been accumulated globally in the 120 years since then, providing more insight on the topic and the challenges involved in its assessment. Most earlier research reported higher suicide rates for male and female physicians compared with the general population, and the mean effect estimates from the first meta-analysis in 2004 indicated a significantly increased standardised mortality ratio (SMR) of 1.41 for male physicians and 2.27 for female physicians.10This meta-analysis included 22 studies on suicide in physicians with observation periods between 1910 and 1998 and revealed some heterogeneity among study results, which was partly explained by the decline in risk over time. Similarly, another meta-analysis that included nine studies with observation periods between 1980 and 2015 reported a significantly decreased SMR of 0.68 for male physicians and a significantly increased SMR of 1.46 for female physicians.11
In addition to publication year, several other factors could potentially drive heterogeneity between the published studies. Methodological differences in study design, outcome measures, and level of age standardisation could explain heterogeneity between studies. Furthermore, individual countries and world regions have varying levels of stigma about suicide in general and among physicians in particular, associated with different risks of underreporting, access to support systems, and generally different training and working conditions.
In this study, we aimed to perform an appraisal of the currently available evidence on suicide deaths in male and female physicians compared with the general population. We also aimed to explore heterogeneity by considering a broader spectrum of potential covariates. We hypothesise that suicide rate ratios for male and female physicians have declined over time, but gender differences persist and suicide risk remains increased for female physicians.
Methods
Search strategy and study selection
This meta-analysis was conducted based on recommendations of the Cochrane Collaboration,12and is reported in accordance with the preferred reporting items for systematic review and meta-analyses (PRISMA) statement.13We searched for observational studies with data on suicide rates in physicians compared with the general population or similar using Medline, PsycINFO, and Embase. “Physician,” “mortality,” and “suicide” were entered as MeSH terms and text words and then connected through Boolean operators. The specific search strategy was developed and adapted for each database with the support of librarians from the Medical University of Vienna (supplement table S1). Following Schernhammer and Colditz,10we limited the search period to articles published after 1960 but updated it through to 31 March 2024. No constraints were placed on the language in which the reports were written, the region where study participants lived, or their age group. Articles published in languages other than English or German were screened with the help of the translation software DeepL14and colleagues fluent in these languages. Screening of the literature was done independently by two reviewers (CZ and SS). We also performed forward and backwards reference screening for the included articles and searched for unpublished data from sources and databases listed in included articles, such as the US National Institute for Occupational Safety and Health, the UK Office for National Statistics, Switzerland’s Federal Statistical Office, and Statistics Denmark.
We excluded studies that reported only on specific suicide methods in physicians, non-fatal suicidal behaviour or thoughts, mental health and burnout, and suicide prevention. We also excluded conference abstracts, editorials, case studies, and letters. Only reports with adequate data about physician deaths by suicide (not attempts) were eligible.
At the full text screening stage, we decided to only include rate based outcome measures that compare the suicide mortality in a physician population with the suicide mortality in a reference population. This includes the indirectly standardised mortality ratio (SMR), directly standardised rate ratio (SRR), and the comparative mortality figure. Even though their formulas and recommended uses differ and might yield slightly different results when calculated for the exact same population,15it can be argued that they are comparable estimates for the purpose of meta-analysing suicide deaths in physicians compared with a reference population. We also included rate ratios, even though their level of age standardisation is typically less detailed and only comprises one age group (with lower or upper age cutoff points). However, the proportionate mortality ratio expresses a different concept (the cause specific SMR divided by the all cause SMR, or the rate of suicides in all physician deaths divided by the rate of suicides in all population deaths). This outcome measure is not suitable for calculation of combined estimates with SMRs, especially in target populations with higher general life expectancy like physicians,16and was therefore not included. We also excluded studies that reported odds ratios and relative risk calculations because these are not based on rates.
We avoided overlapping time periods of the same geographical regions among included studies so that any physician death by suicide would only be counted once towards the pooled result. In case of overlaps, only one study was included, and the decision of which to include was based on three criteria in sequential order: sample size (higher number of observed suicides); risk of bias (lower risk of bias based on the Joanna Briggs Institute (JBI) checklist for prevalence studies); and recentness (more recent midpoint of observation period). We also excluded studies that only reported overall (and not gender stratified) suicide ratios, only covered physician subgroups (eg, medical specialties), or did not meet minimum requirements for sample size (ie, an expected number of one suicide). When necessary information for inclusion was missing from eligible studies or the source of data was unclear, we contacted the authors. We excluded studies if the necessary information could not be obtained. A detailed list of excluded references including reason for exclusion can be found in the supplement (table S2).
Data extraction and risk of bias
Data extraction was conducted by two reviewers (CZ and SS) using a standardised table in Microsoft Excel. If studies did not include an SMR, but reported the numbers of observed (O) and expected (E) suicides or the necessary information to calculate them, the SMR was calculated by the reviewers (SMR=O/E). If the studies did not include an SRR or rate ratio, but reported (age standardised) suicide rates per 100 000 person years for physicians (R1) and a suitable reference population (R2) for a similar time period, the SRR or rate ratio was calculated (SRR=R1/R2, rate ratio=R1/R2). For one study, R1 and R2 were estimated from graphs.17Because not all studies reported confidence limits and the ones that did used different methods, we calculated 95% confidence intervals (CIs) for all studies based on Fisher’s exact test using observed and expected suicide numbers. For SRRs or rate ratios, we calculated the expected suicides by treating the SRR as an SMR (E=O/SRR). Standard errors were derived from the calculated 95% CIs by using the formula recommended for ratios in the Cochrane handbook (standard error=(ln upper CI limit – ln lower CI limit)/3.92).12
In addition to variables relating to the main outcome, we extracted data on the following study characteristics to be used in sensitivity analyses: geographical location, observation period, age range, level of age standardisation, suicide classification, study design, and reference group. We used duplicate extraction and checked the final extraction table for errors to ensure accuracy.
Because there was no suitable validated scale to assess the quality of observational studies on mortality ratios, we used the JBI checklist for prevalence studies18as a critical appraisal tool for risk of bias assessment. Out of nine questions on this checklist, three were deemed not applicable owing to the investigation of mortality rather than morbidity (see supplement table S3a). Two reviewers (CZ and SS) independently evaluated a subsample of the included studies and the JBI checklist was subsequently further specified to achieve clear criteria for risk of bias assessment (see supplement table S3b). The same two reviewers then independently evaluated all studies (supplement table S4a and S4b). Consistency in rating was high, disagreements were resolved through discussion. If all applicable items of the JBI checklist were rated positive, a study was classified as having low risk of bias. If at least one item was rated negative or unclear, a study was classified as having moderate or high risk of bias.
Data analysis
We performed separate meta-analyses of suicide rate ratios for male and female physicians. Random effects models were chosen a priori owing to the assumption that the included studies represent a random sample of different yet comparable physician populations with some heterogeneity in effect size.19Random effects models were calculated based on the Hartung-Knapp method (also known as the Sidik-Jonkman method).20Cumulative meta-analyses were performed to examine changes in the overall mean effect estimate over time. Heterogeneity was assessed by Q tests, I2, T2, and prediction intervals.
Begg and Egger tests were conducted to evaluate the possibility of publication bias, which was also assessed by funnel plot and trim-and-fill analysis. We performed sensitivity analyses using meta-regression (for single covariates and adjusted for study observation period midpoint), including binary variables for several study characteristics (see supplement table S5a and S5b): risk of bias (low riskvmoderate or high risk studies), study design (registry based studiesvothers), outcome measures (SMRvothers), level of age standardisation (detailed with several age groups usedvothers), suicide classification (narrow international classification of diseases (ICD) definition without deaths of undetermined intentvothers), age range (studies with a cutoff point around retirement agevothers), and reference group (general populationvsimilar). We also performed meta-regressions for length of observation period and number of suicides. Subgroup analysis was performed to assess geographical differences in two categorisations: World Health Organization world regions (with studies from the Americas, European Region, and Western Pacific Region for male and female physicians, only one study from the African Region for male physicians, and no studies from the South East Asian and Eastern Mediterranean Region) and most common study origin regions, reflecting the accumulation of reports from certain parts of the world (US, UK, Scandinavia, other European countries, rest of the world). We also used subgroups to calculate mean effect estimates in older and more recent studies. Two groups were formed based on the midpoint of study observation period, with one subgroup consisting of the 10 most recent studies, and another subgroup with the remaining studies. To accommodate for multiple testing, we adapted the level of significance to P<0.01 for all sensitivity analyses.
We conducted a secondary meta-analysis on suicide rates in physicians compared with another reference group that was more similar than the general population in terms of socioeconomic status. Studies were included if they provided data on deaths by suicide in physicians as well as a group of other professions with similar socioeconomic status (all other eligibility criteria remained the same).
All analyses were performed with Stata (version 17). This study was registered at the International Prospective Register of Ongoing Systematic Reviews (PROSPERO) under CRD42019118956.
Patient and public involvement
Several authors of this paper have trained and worked as physicians, and lived through the loss of colleagues to suicide. Their firsthand experiences offered valuable insights similar to those typically provided by patients. Because of the highly methodical nature of a systematic review and meta-analysis, it was difficult to involve members of the public in most areas of the study design and execution. However, patient and public involvement representatives reviewed the manuscript after submission and offered suggestions on language, dissemination, and general improvements to increase its relevance to those affected by physician deaths by suicide.
Results
Included studies
The initial literature search yielded 23 458 studies. After removing duplicates and screening titles and abstracts, we were left with 786 articles. Application of the inclusion criteria resulted in 75 reports and we found a further 22 potentially eligible studies through reference list and registry based searches. Full text screening resulted in 38 studies for male physicians and 26 for female physicians that were eligible for analyses (fig 1). Because a few studies provided more than one effect estimate,2122a total of 42 datasets (male physicians) and 27 datasets (female physicians) were used for meta-analysis (table 1andtable 2).
Meta-analyses
The meta-analysis on suicide deaths in male physicians (fig 2) produced a mean effect estimate of 1.05 (95% CI 0.90 to 1.22). The Q test was highly significant (Q=460.2, df=41, P<0.001), and the I2of 94% indicated that a high proportion of variance in the observed effects was caused by heterogeneity in true effects compared with sampling error. The variance of true effect size estimated with T2was 0.216, the standard deviation T was 0.465. The resulting prediction interval ranged from 0.41 to 2.72, which indicates that in 95% of all comparable future studies in male physician populations, the true effect size will fall in this interval. This finding reflects a high level of dispersion, suggesting that the suicide rates are decreased in some male physician populations but increased in others compared with the general population. Meta-regression confirmed calendar time (measured by midpoint of study observation period) as a highly significant covariate (β=−0.015, P<0.001), with an adjusted R2indicating an explained proportion of 52% of between-study variance.
The mean effect estimate for suicide deaths in female physicians (fig 3) was 1.76 (95% CI 1.40 to 2.21). The Q test for heterogeneity was highly significant (Q=143.2, df=26, P<0.001), and the I2of 84% indicated a high proportion of variance caused by heterogeneity in true effects, with T2estimated at 0.278 and T at 0.523. The prediction interval ranged from 0.58 to 5.35, so the dispersion of the true effect size across studies on female physicians was also substantial, ranging from decreased suicide rates in some female physician populations to considerably increased rates in others. The midpoint of study observation period also showed a highly significant association with the pooled estimate in a meta-regression (β=−0.024, P<0.001), explaining 87% of between-study variance.
A decrease in suicide rate ratios over time is shown by cumulative meta-analyses (supplement figure S1a and S1b). A decline in pooled estimates is observed for female physicians throughout all studies, and a decline for studies with midpoints of observation period after 1985 can be seen for male physicians.
Further analyses
We performed sensitivity analyses across all studies using meta-regression. We did not observe any significant (P<0.01) results for male or female physicians, for study design, outcome measures, level of age standardisation, suicide classification, age range, reference group, length of observation period, and number of suicides. We found a significant association between risk of bias and effect size for male (β=−0.475, P=0.001) and female (β=−0.601, P=0.003) physicians, but when adjusting for midpoint of observation period, this association was no longer significant.
Egger test and Begg test gave no evidence of publication bias for studies on male or female physicians. The funnel plots showed no asymmetry, although they did reflect the high heterogeneity between studies (figure S2a and S2b). The non-parametric trim-and-fill analyses imputed no studies for male or female physicians, therefore no difference in effect size was found for observed versus observed plus imputed studies.
We also performed subgroup analyses based on geographical study location in two different categorisations: WHO world regions and most common study origin regions. With both analyses, the decrease in effect sizes over time was visible in most subgroups, and lower effect sizes were observed especially in studies from Asian countries (supplement figures S3a, S3b, S4a, and S4b). This finding translates to lower overall suicide rates for male physicians in the Western Pacific Region of 0.61 (95% CI 0.35 to 1.04), or similarly, for studies outside of Europe and the US with 0.69 (0.45 to 1.06). This pattern was not observed for female physicians, although the suicide rate ratio for the Western Pacific Region (1.06, 0.34 to 3.32) was also the lowest compared with all other subgroups.
Given that calendar time has been shown to have a strong association with effect size, we also performed a subgroup analysis of the 10 most recent studies versus all older studies. For male physicians (supplement figure S5a), the mean effect estimate in the subgroup of 32 older datasets was increased at 1.17 (0.96 to 1.41), whereas in the subgroup of the 10 most recent studies it was significantly decreased at 0.78 (0.70 to 0.88). For female physicians (supplement figure S5b), the mean suicide rate ratio in the subgroup of 17 older studies was significantly increased at 2.21 (1.63 to 3.01). In the subgroup of the 10 most recent studies, the mean effect was still significantly increased at a lower level of 1.24 (1.00 to 1.55).
Secondary meta-analysis
We conducted another meta-analysis on suicide rates in physicians compared with other professions of similar socioeconomic status and identified eight studies that compared male physicians with a reference group of other academics, other professionals, other health professionals, or members of social class I (supplement figure S6 and table S6). The pooled effect estimate was significantly increased at 1.81 (95% CI 1.55 to 2.12). The Q test (Q=17.6, df=7, P=0.01) was significant, but the I2of 58% and the prediction interval of 1.15 to 2.87 indicated a lower level of heterogeneity compared with the main analysis, and a more similar effect size across studies. We found five studies on female physicians (supplement table S6). The results of these studies appeared similar to those for male physicians, but we deemed the number of eligible studies too low for a random effects meta-analysis.62
Discussion
In this meta-analysis summarising the available evidence on physician deaths by suicide, we found the rate ratio for female physicians to be significantly raised, but not for male physicians. This result confirmed our hypothesis that mean effect estimates would be lower than in a previous meta-analysis on the subject published in 2004.10Calendar time was identified as a significant covariate in both analyses, indicating decreasing suicide rate ratios for physicians over time. The high level of heterogeneity in results from different studies suggests that suicide risk for male and female physicians is not consistent across various physician populations. Therefore, the pooled effect estimate is only of limited use in describing the overall suicide risk for physicians compared with the general population. In a secondary meta-analysis, the suicide rate ratio of male physicians was shown to be significantly raised when other professional groups with similar socioeconomic status were used as a reference group, with less heterogeneity across study results.
Strengths and limitations of this study
We did not impose any language restrictions on our search strategy so that relevant studies from different geographical regions were found. Consequently, we were able to include a large number of studies from 20 countries providing overall and recent summary estimates based on a complete assessment of the available evidence. This study also explored a range of covariates as potential causes for heterogeneity.
Several weaknesses should also be mentioned. Underreporting of suicide deaths might be more common for physicians compared with the general population,8influencing ratios between those two populations in the original studies. Despite the large number of included reports, several geographical regions are still underrepresented in the available evidence, which limits the generalisability of findings.
Comparison with other studies
A systematic review on physician deaths by suicide included a meta-analysis of studies with observation periods between 1980 and 2015,11but found only nine eligible studies (a third of which were already included in the first meta-analysis by Schernhammer and Colditz10). This analysis was also subject to some methodological limitations, such as using a potentially arbitrary starting point for study observation periods and not accounting for overlap between included studies (therefore counting some physician deaths by suicide twice). Another systematic review and meta-analysis on physician and healthcare worker deaths by suicide included only one new study compared with Schernhammer and Colditz10and so did not provide an updated estimate.63Additionally, this analysis included a large US study that reported increased proportionate mortality ratios, impacting the pooled estimate for male physicians towards showing an effect.
Meaning of the study
The results of this study suggest that across different physician populations, the suicide risk is decreasing compared with the general population, although it remains raised for female physicians. The causes of this decline are unknown, but several factors might play a part. The critical appraisal of the included studies indicated better study quality among more recent studies, which might have contributed to the decrease in effect sizes over time. Meta-regression results by Duarte and colleagues suggested that the decrease in suicide risk in male physicians was driven by a reduction in the rate of suicide deaths in physicians rather than an increase in suicide deaths in the population.11This finding could mean that physicians have benefitted more from general or targeted suicide prevention efforts compared with the general population, which is testament to the repeated calls for more awareness and interventions to support the mental health of physicians.6465Furthermore, the proportion of female physicians has increased over recent decades, and the average proportion of female physicians across all OECD (Organisation for Economic Co-operation and Development) countries reached 50% in 2021.66This change is likely to affect working conditions in a historically male dominated field that could be relevant to the mental health of workers. Some evidence exists that occupational gender composition affects the availability of workplace support and affective wellbeing, with higher support levels in mixed rather than male dominated occupations.6768
It is important to note, however, that considerable heterogeneity exists in the suicide risk of different physician populations that is still partly unexplained. Working as a physician is probably associated with different risk and protective factors across diverse healthcare systems, as well as training and work environments. Additionally, prevailing attitudes and stigma about mental health and suicide could vary. Societal influences on suicide rates over time might affect physicians differently compared with the general population (eg, mental health stigma might differ for physicians compared with the general population, and change at a different rate). Therefore, it seems plausible that the relation between suicide deaths in physicians compared with the general population differs between regions and countries.
Policy implications
Overall, this study highlights the ongoing need for suicide prevention measures among physicians. We found evidence for increased suicide rates in female physicians compared with the general population, and for male physicians compared with other professionals. Additionally, the decreasing trend in suicide risk in physicians is not a universal phenomenon. An Australian study found a substantial increase in suicide risk for female physicians, which doubled between 2001 and 2017.58The recent covid-19 pandemic has put additional strain on the mental health of physicians, potentially exacerbating risk factors for suicide such as depression and substance use.6970Other important risk factors include suicidal ideation and attempted suicide, and their prevalence among physicians was estimated by a recent meta-analysis. The results suggest higher levels of suicidal ideation among physicians compared with the general population, whereas the prevalence of suicide attempts appeared to be lower.71This finding could indicate that suicidal intent in physicians is more likely to result in fatal rather than non-fatal suicidal behaviours.72A systematic review on mental illness in physicians concluded that a coordinated range of mental health initiatives needs to be implemented at the individual and organisational level to create workplaces that support their mental health.73Evidence exists for effective physician directed interventions, but hardly any research on organisational measures to address suicide risk in physicians.74Continued advances in organisational strategies for the mental wellbeing of physicians are essential to support individual medical institutions in their efforts to foster supportive environments, combat gender discrimination, and integrate mental health awareness into medical education and training.
Recommendations for future research
In addition to more primary studies from world regions other than Europe, the US, and Australia, future research also needs to systematically look into other factors beyond study characteristics that might explain the heterogeneity in suicide risk in physicians. Such research would help in identifying physicians who are at risk, with targeted prevention measures and ways to adapt them to different clinical and cultural contexts. Because geographical or national differences appear to be important factors, future studies on suicide risk in physicians should bear in mind that the specific settings of any physician population might influence their risk and resilience factors to a much higher degree than previously assumed. Other major events that affect healthcare, such as the covid-19 pandemic, could also have a large impact. Future research is needed to assess any covid-19 related effects on suicide rates in physicians around the world.
Many studies reported increased suicide rates for physicians, and a 2004 meta-analysis found significantly increased suicide rates for male and female physicians compared with the general population
Evidence on increased suicide rates for physicians is inconsistent across countries
Suicide rate ratios for physicians appear to have decreased over time, but are still increased for female physicians
A high level of heterogeneity exists across studies, suggesting that suicide risk varies among different physician populations
Further research is needed to identify physician populations and subgroups at higher risk of suicide
","Objectives: To estimate age standardised suicide rate ratios in male and female physicians compared with the general population, and to examine heterogeneity across study results.
Design: Systematic review and meta-analysis.
Data sources: Studies published between 1960 and 31 March 2024 were retrieved from Embase, Medline, and PsycINFO. There were no language restrictions. Forward and backwards reference screening was performed for selected studies using Google Scholar.
Eligibility criteria for selecting studies: Observational studies with directly or indirectly age standardised mortality ratios for physician deaths by suicide, or suicide rates per 100 000 person years of physicians and a reference group similar to the general population, or extractable data on physician deaths by suicide suitable for the calculation of ratios. Two independent reviewers extracted data and assessed the risk of bias using an adapted version of the Joanna Briggs Institute checklist for prevalence studies. Mean effect estimates for male and female physicians were calculated based on random effects models, with subgroup analyses for geographical region and a secondary analysis of deaths by suicide in physicians compared with other professions.
Results: Among 39 included studies, 38 studies for male physicians and 26 for female physicians were eligible for analyses, with a total of 3303 suicides in male physicians and 587 in female physicians (observation periods 1935-2020 and 1960-2020, respectively). Across all studies, the suicide rate ratio for male physicians was 1.05 (95% confidence interval 0.90 to 1.22). For female physicians, the rate ratio was significantly higher at 1.76 (1.40 to 2.21). Heterogeneity was high for both analyses. Meta-regression revealed a significant effect of the midpoint of study observation period, indicating decreasing effect sizes over time. The suicide rate ratio for male physicians compared with other professions was 1.81 (1.55 to 2.12).
Conclusion: Standardised suicide rate ratios for male and female physicians decreased over time. However, the rates remained increased for female physicians. The findings of this meta-analysis are limited by a scarcity of studies from regions outside of Europe, the United States, and Australasia. These results call for continued efforts in research and prevention of physician deaths by suicide, particularly among female physicians and at risk subgroups.
Systematic review registration: PROSPERO CRD42019118956.
"
Clinical value of guideline recommended molecular targets and genome targeted cancer therapies: cross sectional study,"Introduction
Advances in molecular characterization of tumors and the development of targeted treatments hold promise for improving patients’ survival and quality of life. Groundbreaking examples include trastuzumab-pertuzumab for HER2 expressing breast cancer and osimertinib for EGFR mutated non-small cell lung cancer.12Although these therapies are now considered standards in cancer care, genome targeted treatments offer limited benefit for most patients with metastatic cancer owing to the small subset of potential responders and challenges of drug resistance and cancer progression.34
The increasing importance of genomic testing in cancer care is accompanied by advances in next generation sequencing. With a growing number of genome targeted cancer drugs being approved or under investigation, the opportunities for precision cancer medicine have broadened, particularly for patients facing advanced treatment resistant disease. In May 2024 the website MyCancerGenome.org provided a catalog of 18 271 genetic biomarkers and 5519 genome targeted cancer therapies available.5However, despite the widespread implementation of this technology, the clinical relevance of many genomic alterations is unknown, risking overestimation of the benefits of tailored therapies.67
In response to these challenges, the European Society for Medical Oncology (ESMO) developed a grading system for molecular targets—the ESMO Scale for Clinical Actionability of Molecular Targets (ESCAT).8ESCAT aims to prioritize actionable genomic alterations as markers for selecting patients for targeted therapies. By categorizing genomic alterations into 10 tiers of decreasing evidence for benefit (tiers I to X), this framework prioritizes targets on the basis of levels of evidence, considering clinical trial design as well as specific treatment contexts and indications (supplementary table A). For example, germline BRCA1/BRCA2 mutations as targets for rucaparib are classified as tier I-A in ovarian cancer,9tier I-B in prostate cancer,10tier II-B in pancreatic cancer,11and tier III-A in lung cancer.12
Concern is growing about the increasing use of surrogate measures in pivotal trials of cancer drugs leading to regulatory approval, without evidence for overall survival or quality of life benefits.1314This concern is intensified by the high costs of these new treatments. To assist physicians, patients, and regulators in choosing therapies, ESMO created the Magnitude of Clinical Benefit Scale (ESMO-MCBS15) to evaluate the clinical benefit associated with new drug therapies for cancer on the basis of treatment effectiveness, adverse events, and quality of life (supplementary table B).
Limited data are available to guide application of precision oncology in clinical decisions about cancer treatment.161718In the case of genome targeted cancer therapies approved by the US Food and Drug Administration (FDA) from 2015 to 2022, fewer than a third showed meaningful added benefits for patients at the time of approval, as indicated by the ESMO-MCBS and ESCAT frameworks.19The National Comprehensive Cancer Network (NCCN) guidelines are cancer specific recommendations extensively used in clinical practice worldwide.20NCCN guidelines cover more than 97% of US patients with cancer. Additionally, 47% of registered users accessing these guidelines are from outside the US, and downloads of guidelines span more than 180 countries.21US public and private insurers rely on the NCCN for decisions on coverage.222324We therefore evaluated the validity of the targets and the value of the outcomes used in the clinical trials supporting genome targeted cancer drugs recommended in advanced cancer by the NCCN guidelines.
Methods
Search strategy
We obtained the most recent versions of the NCCN guidelines for solid cancers on 1 May 2023.20Data were extracted between May 2023 and August 2023. We first extracted genome targeted oncology therapies and their genomic alteration targets for advanced cancer recommendations. We defined genome targeted drugs as those approved on the basis of a genomic test whereby the drug was designed to target a specific genomic alteration. We excluded drugs related to cytotoxic chemotherapy or hormonal therapy and non-genome targeted drugs. We then extracted the references to the publications supporting those drug recommendations. We classified the evidence supporting NCCN recommendations as no evidence provided, case report or series, observational study, review article, phase I trial, phase I-II trial, phase II trial, or phase III trial. Where the NCCN did not provide a specific reference, we did searches on PubMed and Google Scholar to collect supporting evidence. Our search terms included the name of the genome targeted therapy, the specific genomic alteration targeted, the type of cancer, and the setting of the recommendation. We then extracted the characteristics and outcomes of the trials from the publications, including sample size, trial design (randomized versus single arm), blinding (blinded versus open label), phase of clinical trial, primary efficacy endpoints supporting the approval (overall survival versus an intermediate measure (for example, progression-free survival, overall response rate, or duration of response)), and information about quality of life and toxicity when available. If multiple or overlapping studies covered the same recommendation, we prioritized the most robust, preferring phase III trials to phase II/I trials and preferring endpoints such as overall survival to intermediate endpoints. If multiple references from the same study supported the same recommendation, we gave preference to those with the most up-to-date data. For studies that did not provide information on quality of life, we searched the following criteria in PubMed: quality of life, patient reported outcomes, the name of the drug, the name of the trial, and the ClinicalTrials.gov identification number. We also collected data for the treatment setting (first line or subsequent line(s)), NCCN levels of evidence (1, 2A, or other), and NCCN preference categories (“preferred regimen,” “other recommended regimens,” or “useful in certain circumstances”) (supplementary table C).
Finally, we searched Drugs@FDA25to identify all new and supplemental indications for genome targeted drugs approved for the treatment of solid tumors up to August 2023, following the methods previously reported.26We then categorized recommendations into FDA approved and non-approved (off label) indications.
Data synthesis and scoring
To assess the clinical evidence level supporting specific genomic alterations as targets for the drugs in our study, we used the ESCAT framework.8ESCAT tier I genomic alterations are clinically relevant on the basis of a clinical trial showing improved outcomes for the genomic alteration-therapy combination. The distinction between tier I-A, I-B, and I-C is based on the level of evidence, determined by the design of the clinical trial analyzing the biomarker (I-A: prospective, randomized; I-B: prospective, single arm; I-C: basket trial). For tissue agnostic drug approvals, defined as drugs approved on the basis of genome targets irrespective of organ site or histology, we maintained the tier assignment as tissue agnostic drug group, even if data on objective response rate were unavailable or no responses were observed for specific subtypes. ESCAT tier II alterations are those that hold potential clinical relevance, on the basis of evidence from retrospective clinical trials (II-A) or prospective trials that do not show survival benefits (II-B). Tier III targets are predicted targets linked to an expected improvement in outcome on the basis of clinical trial data from other tumor types. Tier IV targets are those that show potential in pre-clinical studies but lack more rigorous evidence of efficacy in a given cancer. Tier V targets are genomic alterations linked to responses to a targeted drug match that might serve as suitable targets for combination therapy strategies. Tier X alterations lack evidence as therapeutic targets and should not be considered in clinical decision making.8See supplementary table A for more detail on ESCAT tiers and examples.
To assess clinical benefit, we applied the ESMO-MCBS framework to each study. We used publicly available forms,27and for studies supporting FDA approvals we cross checked the results with the ESMO website.28We defined substantial clinical benefit as grade 4 or 5 for studies of non-curative intent.15Supplementary table B shows the different available forms, levels of evidence, and examples.
Initially, the ESCAT framework was designed as a clinical benefit centered system, ranking genomic alterations deemed ready for routine use (ESCAT evidence class I-A and I-B) as targets for precision medicine in cancer with substantial improvements as defined by the ESMO-MCBS v1.1 (grade 4 or 5 for non-curative intent).8However, in our view, this definition has two major limitations. Firstly, previous studies have indicated that ESMO-MCBS scores of ≥3 correlate with positive health technology assessments, suggesting that new therapies with these scores deliver meaningful benefits that justify reimbursement.29Secondly, more cancer drugs have recently received regulatory approval on the basis of high levels of early efficacy in smaller biomarker defined populations, including agnostic drug approvals, through the increased use of overall response rate evaluated in single arm trials.30However, these trials must show not only substantial efficacy but also improvements in quality of life or be supported by data from confirmatory post-marketing studies to meet the high clinical benefit thresholds with the ESMO-MCBS framework. These criteria are often difficult to meet at the time of approval.26
To overcome these limitations, we expanded our study to include a broader definition of high benefit genome based cancer treatments and added a second group for drugs indicating moderate benefit. We rated molecular targets associated with substantial clinical benefit (ESMO-MCBS grade 4 or 5) and qualifying for ESCAT category levels I-A, I-B, and I-C as high benefit genome based cancer treatments. We classified molecular targets with a grade 3 on ESMO-MCBS and ESCAT tiers I-A, I-B, and I-C as having promising but unproven benefits.
Statistical analysis
We used Fisher’s exact test to compare genome based cancer therapies with high benefit and promising but unproven benefit versus those with low benefit. We explored associations between recommendations approved with and without FDA approval and the characteristics of recommendations by using the Fisher’s exact test for categorical data and the Mann-Whitney U test for non-parametric data. All analyses were univariate, as multivariable analysis was not feasible owing to the limited number of off label recommendations. All P values resulted from two sided tests, and significance was determined at P<0.05 (BM SPSS, version 26.0 for Windows). We applied no corrections for multiple significance testing. This research adhered to the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) reporting guidelines for cross sectional studies.
Patient and public involvement
Patients were not involved in the formulation of the research question or the establishment of outcome measures or in developing plans for the design or implementation of the study. No patients were consulted for insights on interpreting or writing up results. The study was initiated before patient and public involvement was common. However, results will be disseminated via conference presentations, publications in lay media, and interactive online tools.
Results
Of a total of 515 NCCN recommendations, 411 met our inclusion criteria (fig 1) and covered 74 genome targeted drugs targeting 50 driver alterations. Of these, 346 (84%) recommendations were associated with evidence of varying quality from clinical trials or studies and 65 (16%) relied on pre-clinical studies and/or case reports (16; 4%), extrapolation from other tumor types (42; 10%), or evidence that could not be identified (7; 2%) (table 1). Of the 411 recommendations, 49 (12%) lacked citations. We identified supporting literature for 42 of these, leaving seven (2%) cases in which the underlying evidence could not be identified.
Characteristics of NCCN recommendations and supporting trials
Of 411 NCCN recommendations, 246 (60%) were for FDA approved indications (table 1). Most NCCN recommended treatment options were judged by the NCCN as having category 2A evidence (322; 78%) and 33 (8%) were assigned category 1. Of the 395 recommendations with NCCN preference categories endorsed by NCCN authors (16 recommendations lacked NCCN author endorsements for any preference category), 198 (50%) were rated as “useful in certain circumstances” and 113 (29%) were classified as “preferred.”
We included 346 studies (table 2). The median sample size was 26 (range 0-991) patients. Among 104 tissue agnostic drug trials, 32% (n=33) did not include patients of that specific subtype and 73% (n=76) had fewer than 10 patients within a subtype. Seventy eight per cent (271/346) of NCCN recommendations were supported by only phase I or phase II trials, 76% (262/346) were supported by single arm studies, and 93% (323/346) were supported by open label studies. The most common primary endpoint in 271 (78%) studies was “objective response rate.” Within this subgroup, the median objective response rate was 46% and the median duration of response was 9.4 months. For 57 (21%) of these studies, the median duration of response was not reached. However, 77 (28%) studies did not report the objective response rate for the specific tumor type analyzed, and 129 (48%) lacked available data on duration of response.
Ratings for molecular targets and clinical benefit
Among the 411 NCCN target recommendations eligible for scoring with ESCAT, 60% (246/411) were categorized as clinically significant (tier I: I-A 18% (72/411); I-B 17% (70/411); I-C 25% (104/411)), whereas one third (24%; 100/411) were categorized as potentially relevant (tier II (II-A 6% (23/411); II-B 19% (77/411)) or tier III (10%; 42/411)). Fewer than a 10th had a relevance yet to be determined (tiers IV to X). These included 2% (10/411) supported by pre-clinical studies (tier IV), 1% (6/411) using a co-targeting strategy (tier V), and 2% (7/411) that did not have any supporting evidence (tier X) (fig 2).
When we applied the ESMO-MCBS evaluation framework to the 267 trials supporting genome targeted recommendations that were scorable, only 32 (12%) were grades 4 or 5, representing a finding of substantial clinical benefit. Among the 235 that did not meet this threshold, 121 (51%) were grade 3, 73 (31%) were grade 2, 29 were grade 1 (12%), and 12 (5%) were grade 0. ESMO-MCBS could not be applied to the remaining 144 (35%) of 411 recommendations. In 65 instances, the recommendations were derived from case reports, pre-clinical studies, or extrapolation from other tumor types or lacked identifiable references. In 77 cases, the primary endpoints of single arm studies were unsuitable for assessment owing to unreported objective response rate. The remaining two trials could not be scored using the ESMO-MCBS, because the outcomes did not achieve statistical significance.
Molecular targets from ESCAT category level I that were associated with substantial clinical benefit according to ESMO-MCBS accounted for 12% (32/267) of the trials and were designated as high benefit genome based cancer treatments. Molecular targets within ESCAT category level I linked to a grade 3 in ESMO-MCBS constituted 33% (88/267) of the trials and were categorized as treatments with promising but unproven benefit (fig 3). The NCCN guidelines were more likely to assign genome targeted drugs with high and promising but unproven clinical benefits into category 1 (23% v 3%; P<0.001) or the preferred category (53% v 25%; P<0.001), in contrast to less beneficial therapy options (table 3).
Characteristics of NCCN recommendations for genome targeted treatments with and without FDA approval
As shown intable 4, of the 118 preferred interventions endorsed by NCCN guidelines, 62 (53%) were categorized as “recommendations with high or promising but unproven benefit.” In addition, genome based cancer treatments with high and promising but unproven benefit were more likely to be approved by the FDA (61%v16%; P<0.001). Recommendations for off label uses were less frequently based on randomized trials (18%v26%; P=0.03), phase III trials (11%v25%; P=0.009), or studies with blinding (0%v9%, P=0.001).
Discussion
Among more than 400 NCCN recommendations extracted from 22 guidelines, only about an eighth of genome based treatments for solid cancer were rated as highly likely to provide clinical benefit, whereas around a third were identified as having a promising but unproven substantial benefit, according to the ESCAT and ESMO-MCBS frameworks. Given the pivotal role of the NCCN in guiding global oncology practice and determining insurance coverage of cancer drugs in the US, these findings have broad implications for clinical decision making and reimbursement policies.
Challenges and implications of genome targeted therapies in oncology practice
Although personalized therapy may yield survival or quality of life benefits for some patients, certain targets lack proven efficacy and some might even be suboptimal choices. Among targets in the lower ESCAT tiers III to X (representing a third of our cohort), offering a patient enrolment in a clinical trial could be a similarly valid alternative approach, if one is available.
Roughly 10% of NCCN recommendations were classified by ESCAT as tier III-A on the basis of clinical benefit in other tumor entities, including ALK inhibitors in biliary tract cancers, uterine sarcoma, thyroid carcinoma, or melanoma. Although a rationale supports targeted therapy such as ALK inhibitors in certain cancers, clinical evidence for their efficacy outside of ALK rearranged non-small cell lung cancer and inflammatory myofibroblastic tumor is limited or absent.3132Genetic alterations in cancer are commonly distributed across diverse disease types, but the predictive accuracy of these alterations varies depending on the specific cancer type. For example, in the case of HER2 amplification, the effectiveness of treatments can vary substantially among different cancer types. Use of pertuzumab (with trastuzumab and chemotherapy) has shown an overall survival advantage of more than 15 months for patients with HER2 positive metastatic breast cancer compared with trastuzumab and chemotherapy alone.1Conversely, in HER2 positive metastatic biliary tract cancer and salivary gland tumors—for which trastuzumab and pertuzumab do not hold FDA approval but were included in NCCN guidelines—the observed “objective response rate” ranged from 23% to 60% without survival endpoint data.3334
Decisions regarding treatment based on comprehensive genome profiling in routine oncology practice remain challenging. Practice guidelines play a crucial role in standardizing cancer care and promoting equal access to adequate treatments, as well as guiding decisions about resource allocation. In recent years, clinical guidelines such as those of the NCCN have increasingly endorsed genomic testing in cancer care. In 2017 NCCN guidelines urged profiling for non-small cell lung cancer targeting ALK, ROS1, and EGFR alterations.35In our 2023 review, endorsements spanned BRCA1/2, BRAF, FGFR, homologous recombination deficient, KIT, KRAS, MSI-H, dMMR, MET, NTRK, PALB2, PIK3CA, RET, and TMB and encompassed around 50 specific genomic alterations. However, prospective evidence to support non-selective molecularly guided treatment decisions is limited. Multiple single arm studies have shown objective response rates ranging from 1% to 30% in patients with tumors containing actionable alterations.1736373839An initial randomized study evaluating personalized medicine for cancer showed no meaningful difference in outcomes with the use of multigene sequencing for patients with metastatic cancers refractory to standard of care compared with unmatched therapies.16In this context, the ESCAT framework can be useful for patients and prescribers to identify actionable genomic alterations in patients with cancer and assist clinicians in prioritizing the use of genome targeted therapies.
Extensive research and validation have confirmed the applicability of ESMO-MCBS and ESCAT frameworks.404142434445For example, pooled data from the SAFIR02-BREAST and SAFIR-PI3K trials showed progression-free survival benefit in patients with HER2 non-overexpressing breast cancer receiving matched treatments classified as level I/II according to ESCAT (9.1 months versus 2.8 months with chemotherapy; hazard ratio 0.41, 95% confidence interval (CI) 0.27 to 0.61; P<0.001) compared with chemotherapy.43By contrast, no benefit was shown in the intention-to-treat population (5.5 months versus 2.9 months; hazard ratio 0.77, 95% CI 0.56 to 1.06; P=0.11) or the ESCAT beyond level II subgroup (2.8 months versus 3.1 months; 1.15, 0.76 to 1.75; P=0.49). Our study and these findings reinforce the importance of molecularly guided treatment decisions supported by evidence. Further research and dissemination of decisions aids such as the ESCAT and ESMO-MCBS frameworks can assist in clinical decision making around these treatments, which are also invariably expensive.
Contemporary oncologic care frequently incorporates the use of drugs off label, with approximately 30% of prescriptions falling into this category.2346Off label prescriptions represent a substantial part of annual healthcare costs in the US, potentially accounting for half of all oncologic drug use.47A study examining 10 commonly prescribed cancer drugs found that about 30% of their use was off label, leading to an annual cost of $4.5 bn (£3.5 bn; €4.2 bn).46Such use may be evidence based even in the absence of a formal FDA indication, but this is often not the case. Off label use has not diminished for genome targeted drugs; enhanced access to comprehensive genomic profiling facilitates the detection of targetable tumor alterations, even for drugs that lack a formal indication. In our study, 40% of the recommendations for genome targeted therapies were off label. Compared with FDA approved indications, NCCN off label recommendations were less likely to be supported by phase III, blinded, randomized trials or to have overall survival as the primary endpoint.
Policy implications of findings
Our study identifies several areas of potential improvement in guideline driven cancer care. Firstly, we found that only 25% of NCCN recommendations were supported by evidence from randomized controlled trials. Among the 84 indications supporting cancer drugs approved by the FDA from 2015 to 2022, fewer than half (46%) were backed by evidence from randomized trials.48When evaluating the clinical evidence supporting NCCN recommendations, most trials had one or more of the following characteristics: single arm studies; reliance on “objective response rate” as the primary endpoint; based on subgroup analyses; and recommendations based on limited sample sizes, uncertainty about the benefits of those drugs,14and potentially greater risk of post-marketing safety related problems.4950When recommendations were based on objective response rate results, data were missing for one third of therapies and half lacked duration of response data, primarily owing to a limited number of patients with a specific target. These examples highlight the importance of ongoing data collection, ideally integrated into clinical studies and registries, to comprehensively evaluate the risks and benefits of molecular targeted therapies in a continuous manner.51Examples of this model include DRUP (NCT02925234), TAPUR (NCT02693535), and MoST (ACTRN12616000908437).
Our findings also emphasize the need to refine the NCCN’s recommendation algorithm to better consider the levels of evidence for predictive biomarkers in specific cancers. The approval of new treatments for specific and narrow patient populations defined by genomic markers presents a challenge in implementing precision medicine in clinical practice. The rarity of many therapeutic targets means that clinical practice guidelines play a pivotal role, particularly in treatment stratification for genome targeted drug recommendations. Several approaches could enhance the clarity of the NCCN guidelines. Firstly, adherence to the RIGHT (Reporting Items for Practice Guidelines in Healthcare) statement and AGREE-II (Appraisal of Guidelines for Research and Evaluation II) instrument, which could enhance the transparency and methodological quality of guidelines.52Secondly, highlighting the hierarchy among different treatment options, incorporating the highest level of scientific evidence supporting each recommendation and the most current updates available. Finally, integrating the ESMO-MCBS scale and the ESCAT scale into the guidelines, similarly to their use in the ESMO guidelines; this would enable physicians to interpret genomic data more effectively and empower patients to participate more fully in shared decision making.
Strengths and limitations of study
We did an extensive review of 22 solid cancer guidelines, covering more than 400 NCCN recommendations. Additionally, in cases in which references were not provided or lacked information about targetability, we did independent research to find supporting evidence for recommendations beyond the references provided by the NCCN.
This study has limitations. Firstly, data on drugs can evolve over time after their market release and may have changed since our analysis. Secondly, some NCCN recommended uses may lack FDA approval because pharmaceutical companies chose not to register the new indication—for example, in the case of a less profitable drug. Thirdly, we did not apply corrections for multiple significance testing, which could affect the interpretation of borderline P values as hypothesis generating. Finally, 76% of trials in our cohort had a single arm design; as a result, only a minority of these single arm studies met the criteria for substantial clinical benefit as defined in ESMO-MCBS version 1.1. To overcome this limitation, we broadened our study to include an additional cohort comprising drugs that achieved an ESMO-MCBS grade of 3 and ESCAT tier I, indicating moderate benefit.
Conclusions
Decisions about treatment based on comprehensive genome profiling in oncology practice remain challenging. Patients who have not responded to standard therapies have limited treatment options, which adds to the complexity of selecting the most appropriate therapy. Genetic profiling offers the potential to identify additional treatment possibilities in such cases, but the abundance of data requires a systematic approach to navigate the results. Benefit frameworks such as ESMO-MCBS and ESCAT can assist patients, prescribers, and payers in discerning which genome targeted therapies are supported by the highest quality evidence. The NCCN guidelines play a crucial role in real world clinical practice. Efforts should be focused on improving NCCN recommendations to enable evidence based treatment selection based on next generation sequencing results. Such measures could assist patients, prescribers, and payers in discerning which uses of drugs are supported by the highest quality evidence.
","Objective: To assess the clinical benefit and actionability of molecular targets for genome targeted cancer drugs recommended for clinical practice by the National Comprehensive Cancer Network (NCCN).
Design: Cross sectional study.
Participants/setting: Genome targeted cancer drugs recommended by NCCN guidelines in the advanced setting.
Main outcome measures: Molecular target actionability was assessed using the European Society for Medical Oncology (ESMO) Scale for Clinical Actionability of Molecular Targets (ESCAT). Clinical benefit of genome targeted oncology therapies was evaluated using the ESMO-Magnitude of Clinical Benefit Scale (ESMO-MCBS). Molecular targets at ESCAT category level I associated with studies showing substantial clinical benefit by ESMO-MCBS (grades 4-5) were designated as high benefit, and those linked to studies achieving an ESMO-MCBS grade of 3 were categorized as being of promising but unproven benefit.
Results: 411 recommendations related to 74 genome targeted drugs targeting 50 driver alterations were examined. Most recommendations (346/411; 84%) were associated with clinical trials of various phases, but 16% (65/411) relied on only case reports or pre-clinical studies. However, clinical trials mostly comprised phase I or phase II (271/346; 78%), single arm (262/346; 76%) studies. The primary endpoint assessed in most trials was overall response rate (271/346; 78%) rather than survival. ESCAT tier I targetability encompassed 60% (246/411) of target recommendations, 35% (142/411) were classified as tier II or III, and 6% (23/411) had their relevance yet to be determined (tiers IV to X). When ESMO-MCBS was applied to 267 scorable trials, only 12% (32/267) showed substantial clinical benefit (grades 4-5) and 45% (121/267) were grade 3. When both frameworks were combined, 12% (32/267) of trials supported a determination of high benefit and 33% (88/267) indicated promising but unproven benefit. Of the 118 interventions endorsed by NCCN authors as preferred, 62 (53%) applied to treatments with high or promising but unproven benefit.
Conclusion: According to the ESCAT and ESMO-MCBS frameworks, about one eighth of genome based treatments for solid cancer were rated as likely to offer a high benefit to patients, whereas around a third were identified as offering a promising but unproven substantial benefit. Ensuring that NCCN recommendations are aligned with expected clinical benefits is crucial for promoting informed, evidence based, genomic guided treatment decisions.
"
Decompression alone or with fusion for degenerative lumbar spondylolisthesis,"Introduction
Degenerative lumbar spondylolisthesis is a forward slippage of one vertebra relative to the next vertebra below, caused by degeneration of facet joints and discs, and vertical shear forces between the vertebrae.1Degenerative lumbar spondylolisthesis commonly occurs in the population aged 50 years and older, and is more frequent in women.2People with a narrowing of the spinal canal at the same lumbar level (spinal stenosis) often have leg and back pain, neurogenic claudication, and impaired physical function. In clinical practice, patients with degenerative spondylolisthesis have symptomatic spinal stenosis and a concomitant spondylolisthesis.34
Surgery is recommended in selected patients who have had no improvement after non-surgical care.3Decompression of the narrowed spinal canal has traditionally been the main objective of operative treatment.4Following suggestions from studies from the early 1990s,567adding instrumented fusion (the use of bone grafts, screws, rods, and other devices to fuse the slipped vertebrae) became the preferred surgical method.89More recently, evidence from randomised controlled trials and meta-analyses have indicated that decompression alone is sufficient for up to two years of follow-up.1011121314
By contrast, one randomised controlled trial found that additional instrumented fusion gave superior results to decompression alone.15
Results from randomised controlled trials that include outcomes from more than two years after surgery are sparse and contradictory.111516In this study, we present the five year results of the Norwegian degenerative spondylolisthesis and spinal stenosis (Nordsten-DS) trial to assess whether decompression alone is non-inferior to decompression with instrumented fusion.
Methods
Trial oversight
Nordsten-DS is an investigator initiated, multicentre, randomised, open label trial designed to evaluate the non-inferiority of decompression alone compared with decompression with instrumented fusion at two, five, and 10 years after the initial surgery.17The Regional Committee for Medical and Health Research Ethics of Central Norway (project identifier 2013/366) approved the trial. The trial reporting follows the consolidated standards of reporting trials (CONSORT) guidelines.18We previously published the trial protocol and the statistical analysis plan.1719Information regarding patient involvement is provided in section 2 of appendix.
An interim analysis at two years was conducted during patient recruitment when 150 included participants had completed the one-year follow-up to ensure trial safety and efficacy, following the protocol.1017
Enrolment and randomisation
The complete inclusion and exclusion criteria have been reported previously,1017and are provided in table S2 in appendix. In brief, eligible patients were 18-80 years of age, with clinical symptoms of lumbar spinal stenosis (neurogenic claudication or radiating leg pain) verified by magnetic resonance imaging, and an at least 3 mm spondylolisthesis solely at the stenotic level on standing radiographs. We included patients regardless of whether they presented with signs of instability, such as predominant back pain, higher grade of spondylolisthesis, slippage or angulation of vertebral bodies on flexion-extension radiographs, and facet joints with increased fluid or high sagittal angle.152021Patients were excluded if they had a thoracolumbar scoliosis of more than 20 degrees, excessive foraminal stenosis (ie, a deformed nerve root in the intervertebral foramen), were previously operated at the level of spondylolisthesis, or had a former fracture or fusion surgery in the thoracolumbar region. We included patients referred to public orthopaedic and neurosurgical departments by the primary care givers for surgical evaluation. The surgeons who conducted the trial surgeries were involved in screening for patient eligibility. The decision to undergo surgery or further non-surgical care was based on shared decision making. The shared decision making process was not explicitly outlined in the study protocol but is well anchored in the Norwegian clinics’ best practices and patient rights laws.22Participating surgeons were well versed in balancing patient's expectation and potential gain from surgery with the risks of complications or an undesirable outcome. Patients who opted for surgery after shared decision making were invited to participate in the trial. They received the best available information for and against fusion surgery and on the scarcity of evidence for one treatment being superior, both in oral and written form. All patients who accepted trial participation gave written consent before randomisation. Section 2 in the appendix provides information about the surgical departments’ contribution to the enrolment of participants.
The Medinsight database hosted by the clinical trial unit at Oslo University Hospital allowed for the computer generated random assignment of the eligible participants in a 1:1 ratio to undergo either decompression alone or decompression with instrumented fusion. The sequence was concealed from the investigators and stratified according to site using random block sizes of four and six participants. The trial coordinating centre at the research and communication unit for musculoskeletal health at Oslo University Hospital forwarded the treatment assignments by email to local trial coordinators who documented this information in patients’ records and informed the surgeons. Individual participants and their surgeons were not masked to the treatment assignment.
The routines for the collection and storage of data have been previously described.10All data, stored at the clinical trial unit at Oslo University Hospital, were inaccessible to the research group until 23 March 2023; confirmation is provided in section 4.1 in appendix.
Interventions
The participants assigned to decompression alone were operated with a decompression preserving the posterior midline (without removal of the spinous process or the supraspinous-interspinous ligament complex). The approach could be bilateral, ipsilateral, or ipsilateral with a crossover to the contralateral side. For the participants assigned to decompression with instrumented fusion, a posterior decompression (with or without preserving midline structures, at the surgeon’s discretion) was followed by implantation of pedicle screws with rods and bone grafting across the level of spondylolisthesis, and optional use of an intervertebral fusion device. Implants were selected according to established practices at the trial centres. All participating surgeons routinely performed the procedures used in the trial. A microscope or magnifying glass was recommended for the decompression procedure in both treatment groups.
Outcome measures
The primary outcome was a reduction in the Oswestry disability index (version 2.0) of 30% or more from baseline to five year follow-up,23defined as a clinically important outcome.24The disability index comprises 10 items that assess functional impairment with a total score from 0 to 100, with higher scores indicating more disability.
Secondary outcomes were the mean score changes in the Oswestry disability index, the Zurich claudication questionnaire,25which assesses symptom severity (range 1-5, higher scores indicating more severity), functional impairment (1-4, higher scores indicating more impairment), and satisfaction with treatment (1-4, higher scores indicating lower satisfaction); the numeric rating scale26for leg pain and for back pain, which assesses pain experienced during the past week (range 0-10, with higher scores indicating more pain), and the score on the three level version of the EuroQol Group 5-Dimension (EQ-5D-3L) questionnaire (ranging from −0.59 to 1.0, with higher scores indicating better health related quality of life).27A seven point global perceived effect scale measuring the self-perceived benefit of the surgery was used, and participants’ responses of “much worse” or “worse than ever” were also used to assess adverse outcomes. All questionnaires were translated into Norwegian and validated for psychometric properties (section 4.2.1 in appendix). To evaluate adverse events and treatment during follow-up, we assessed the frequency of complications, patient reported neurological symptoms (sensory, motor, or both) in the lower limbs, subsequent surgeries on the index level or adjacent lumbar levels, use of pain medication, and use of other health services related to the participants’ spine health (ie, physiotherapy chiropractor, acupuncture, and visits to hospitals and general practitioners).
Statistical analysis
All primary and secondary outcomes were analysed in a full analysis set, that is, the modified intention-to-treat set consisting of all the participants who received the trial treatment assigned at randomisation and had available data at one or more time point after randomisation.28The null hypothesis (H0) was that the proportion of participants who met the primary outcome (a reduction of 30% or more in the Oswestry disability index) should be 15 percentage points lower in the decompression group than in the fusion group. The predefined non-inferiority margin was based on established knowledge that decompression alone is less extensive, less invasive, cheaper, and possibly safer,2930which would justify an acceptable loss of effectiveness. A difference of 15 percentage points corresponds to a number needed to benefit from additional fusion of seven (number needed to treat was 100/15=6.67).31This means that at least seven patients need instrumented fusion in addition to decompression to meet one additional patient with a successful outcome. To reject H0, 116 participants were required in each group to be 80% certain (power) that the lower limit of a 95% confidence interval (CI) of the difference (decompression alone minus decompression with instrumented fusion) in the percentage of participants with a successful outcome on Oswestry disability index was above −15 percentage points.32Considering a possible dropout of 10%, 128 participants were required in each group.
To declare non-inferiority for decompression alone, the null hypothesis had to be rejected in the analyses of both the modified intention-to-treat set with multiple imputation of missing data (information provided in section 4.3.1 in appendix) and in a per protocol set. The per protocol set consisted of all the participants in the modified intention-to-treat set who did not undergo a subsequent surgery at the index level or an adjacent lumbar level during the follow-up period and had available data for the primary outcome. Two sensitivity analyses were performed: one in the modified intention-to-treat set with complete cases (without imputation for missing data) and one in which missing values at five years were replaced by values recorded at two years, when available.
The primary outcome and all categorical secondary outcomes were analysed with Newcombe hybrid score confidence intervals.33This included the proportion of participants with a clinically meaningful improvement as assessed by the Zurich claudication questionnaire and numeric rating scale for leg and back pain. All repeated continuous outcomes (scores on the Oswestry disability index, Zurich claudication questionnaire, numeric rating scale for leg pain, numeric rating scale for back pain, and EQ-5D-3L) were analysed with linear mixed models. The linear mixed models contained fixed effects for treatment, time, the interaction between treatment and time, the trial centre, and a random intercept at the patient level. Time was modelled as piecewise linear with knots at three months and two years. Based on the fitted models, mean values were estimated with 95% CIs at baseline (inclusion), three months, one year, two years, and five years after surgery, the change from baseline to five years within each treatment group, and the between group difference (with 95% CIs) in change from baseline to five years.
The assumption of normally distributed data was assessed with visual inspection of histograms and descriptive statistics, and no major deviations were observed. We did not predefine any method for adjustment of confidence intervals for multiple comparisons of secondary outcomes. These results are presented as point estimates with unadjusted confidence intervals from which no definite conclusions can be made. The analyses were done using Stata/SE software, version 17.0.
Patient and public involvement
Patient involvement is an important factor in the Nordsten trials. This paper's patient representative and co-author (IL) is a member of the Nordsten scientific board and working group. She regularly participates in discussions to ensure that the patients' perspectives and involvement are adequately integrated into the research process. She bridges the gap between researchers and the Norwegian Back and Spine Patients Association, facilitating communication and collaboration. In furtherance of this, she has created the first draft of a popular science piece covering the present five year results, which will be distributed by letter to the study participants and the funders.
Results
From 12 February 2014 to 18 December 2017, we screened 738 patients who were referred to 16 Norwegian public orthopaedic and neurosurgical clinics for degenerative spondylolisthesis, of whom 267 were enrolled in the Nordsten-DS trial (fig 1). The randomisation assigned 134 participants to the decompression group and 133 to the fusion group. At five year follow-up, seven participants (3%; three from the decompression group and four from the fusion group) had died and 25 (10%; nine to the decompression group and 16 to the fusion group) were lost to follow-up, resulting in available data for patient reported outcome measurements from 121 participants in the decompression group and 109 in the fusion group. The modified intention-to-treat set consisted of 133 participants assigned to the decompression group and 129 participants in the fusion group (one patient withdrew consent before surgery, and four did not receive the assigned treatment). The per protocol set consisted of 189 participants: 100 in the decompression group and 89 in the fusion group (44 were reoperated), of which seven had missing data for primary outcome (two at baseline and five at five years). Primary outcomes were missing in 29 people (three at baseline and 26 at five years) (fig 1).
Table 1shows that the treatment groups had similar patient characteristics, outcome measurements, and radiological parameters at baseline.
Primary outcome
In the analysis of participants in the modified intention-to-treat set with multiple imputation, 84/133 (63%) in the decompression group and 81/129 (63%) in the fusion group met the primary outcome (an Oswestry disability index reduction of at least a 30% from baseline to five year follow-up). The difference between the groups was 0.4 percentage points (95% CI −11.2 to 11.9). In the per protocol set, the results were 65/100 (65%) in the decompression group and 59/89 (66%) in the fusion group, a difference of −1.3 percentage points (−14.5 to 12.2). The 95% CIs were within the predefined non-inferiority margin of −15 percentage points in both analysis sets. The lower bounds of the 95% CIs corresponded to numbers needed to treat of 8.9 (100/11.2) in the in the modified intention-to-treat set with multiple imputation and 6.9 (100/14.5) in the per protocol set, which means that at least seven to nine patients needed to be fused to have one additional patient meet at least a 30% improvement in functional status. The results of the sensitivity analyses were in the same direction as the primary analysis and did not cross the non-inferiority margin (fig 2).
Secondary outcomes
The mean change in Oswestry disability index from baseline to five years was −17.8 in the decompression group and −17.8 in the fusion group (mean difference −0.02 (95% CI −3.9 to 3.8)). The mean change in leg pain measured by the numeric rating scale showed values of −3.5 in the decompression group and −2.9 in the fusion group (mean difference −0.59 (95% CI −1.36 to 0.18)). For back pain the results were −2.8 and −2.6 (−0.22 (−0.95 to 0.52)), respectively. Mean change in Zurich claudication questionnaire and the EQ-5D also had similar small differences (table 2). Figures in the appendix show the results of complete cases analyses of patient reported continuous outcomes from baseline to five year follow-up. The between-group differences in percentages of participants meeting a clinically meaningful improvement according to the Zurich claudication questionnaire and numeric rating scale pain scales from baseline to five years after surgery were in line with the results of the primary outcome (table 2).
Table 3shows the recorded adverse events from two to five year follow-up, 16 (13%) of 119 in the decompression group and 21 (19%) of 109 in the fusion group reported new neurological sensory and/or motor symptoms of the lower limbs. About 5% in each group perceived themselves to be substantially deteriorated (“much worse” or “worse than ever”) according to the GPE score (table S4 in appendix).
Except for higher blood loss during surgery and incidence of dural tears in the fusion group, no significant between group differences were observed regarding adverse events. Neither the consumption of pain medication nor the use of health services was different during follow-up (table 3and tables S4 and S6 in appendix). A subsequent lumbar surgery was done in 21 (16%) of 129 participants in the decompression group and in 23 (18%) of 125 participants in the fusion group. Of these, comparing people in the decompression group and fusion group, 11 and 15 had subsequent surgery from index surgery to two year follow-up, and 6 and 11 had surgery from two to five years. Some participants had more than one reoperation, giving a total number of 28 subsequent surgeries in each group.
Discussion
The Nordsten-DS randomised controlled trial involving 267 participants with lumbar spinal stenosis and degenerative spondylolisthesis showed that surgery with decompression alone was non-inferior to surgery with decompression and instrumented fusion at five year follow-up. Results of secondary outcomes concerning pain, symptom severity, functional status, and reoperation rates were in accordance with the primary outcome.
Strengths and limitations
Some major strengths of this trial were its large sample size, the external data monitoring, the use of validated outcomes, the high follow-up rate, and the strong involvement of a patient representative (IL).40The pragmatic design, wherein patients were recruited from 16 public institutions and surgery was performed by both orthopaedic and neurosurgical spine surgeons, improved the generalisability of the results.
Due to the eligibility criteria, one cannot generalise the trial results to patients with degenerative scoliosis, severe foraminal stenosis, and previous surgery at the index level or with spondylolisthesis at multiple levels. Another limitation was the absence of double blinding; only the data analyst was masked to treatment assignments.
An evidence based margin of non-inferiority for this research question does not exist, which is why the predefined −15 percentage points limit was chosen empirically. The 95% CI of the between group difference in percentages reaching the primary outcome did not cross the non-inferiority margin in this analysis. However, the sample size for the per protocol analysis was below the a priori required sample size.
Comparison with other studies
Two randomised controlled trials from 2016 of patients with degenerative spondylolisthesis included longer term follow-ups, similar to the current trial.1115One included participants from seven Swedish hospitals.11The trial had a superiority design, did not present information about radiological instability, and had available data for 80 (59%) of 135 participants at five year follow-up. The Swedish trial found no between group differences in Oswestry disability index and pain scores, corresponding to our findings. The reoperation rate over the course of five years from index surgery was 22% in the decompression group and 21% in the fusion group, quite similar to our findings.
The other trial recruited 66 patients from five US spine centres, 51 of whom were from one site.15Each centre had one surgeon who performed all the surgeries in the trial. At four year follow-up, 45 participants (68%) had data for analysis. Compared with our trial, the US trial had a less pragmatic design. They only included grade 1 lumbar spondylolisthesis (slip of 3-14 mm). Furthermore, they did not include patients who had a dynamically unstable condition, defined as motion of the spondylolisthesis of more than 3 mm measured on dynamic radiographs, or those with mechanical low back pain in the upright posture. Their results at a four year follow-up favoured fusion, as assessed by the generic physical component summary score of the 36-item short-form health survey. The reoperation rate of that study at four years was 34% in the decompression alone group, and all patients were deemed to have clinical instability. In the fusion group, 14% had a subsequent surgery, and all had adjacent level degeneration. In our trial, there were slightly more reoperations in the decompression group during the first two years, mainly operated with a subsequent fusion at the index level, while more participants in the fusion group had a subsequent operation between two and five year follow-up, mainly at a new lumbar level.
The reasons for the noticeable difference in reoperation rates between the US trial compared with the present trial and the Swedish trial are unknown. Diverging reoperation rates could partially be explained by differences in treatment traditions and radiological assessment. If a patient operated on without additional fusion complains of persisting back pain, the threshold for offering reoperation with fusion could be low. A surgical alternative is less apparent when the patient is primarily operated with fusion. The rationale might be that the back pain is caused by spondylolisthesis so-called instability.152021
In this trial, non-inferiority for decompression alone was maintained over five years and the reduction in back pain was similar between the groups, even though a high number of participants had radiological and clinical signs of instability. A secondary exploratory Nordsten-DS study on treatment effect modifiers did not find that participants with more typical preoperative signs of instability and back pain benefited from an additional fusion.41The high prevalence of non-specific low back pain in the general population and the scarcity of evidence for a causal relationship between back pain and degenerative spondylolisthesis are valid arguments for not routinely offering subsequent fusion surgery for persistent back pain.4243
Decompression without fusion is a faster,101115less invasive,44safer,13and more cost-effective treatment for patients with degenerative spondylolisthesis.44Despite the two year results from randomised controlled trials and meta-analyses recommending decompression alone as primary treatment for these patients,1011121314153445only a few countries have reported a change in surgical practice.4647In the US, the fusion rate for degenerative spondylolisthesis continued to increase from 67% in 2016 to 90.4% in 2019.48To ensure implementation of evidence from follow-ups longer than two years, the results from the present trial need to be acknowledged by patients and healthcare providers as well as by decision and policy makers. Selective use of information supporting fusion surgery will lead to patients receiving more extensive and risky surgery than is necessary.
For successful shared decision making, clinicians should thoroughly inform patients about the pros and cons of alternative surgical and non-surgical treatments and communicate corresponding realistic prognoses for reaching pain and functional goals. Unfortunately, very little is known about patients’ goals regarding spine surgery. A recent study49showed that patients’ preoperative expectations may be higher than the commonly reported outcomes of spinal surgery.10111550Future investigations should assess patients’ functional and pain goals before surgery relative to their perceived benefits after surgery.
The results of this five year analysis cannot exclude the possibility that subgroups of patients may benefit from an additional fusion (eg, age, gender, socioeconomic status, and different radiological variables). At the two year follow-up, we did not identify any subgroups that would favour one of the two treatments.41Following the Nordsten-DS trial protocol, we will also investigate potential treatment effect modifiers in a separate study related to the five year follow-up, as well as alongside the 10 year follow-up.17
Conclusion
In this multicentre, randomised trial of patients with degenerative lumbar spondylolisthesis, the five year results of decompression alone were non-inferior to those of decompression with instrumented fusion. A subsequent reoperation occurred in about one in five participants in both groups. The results expand on the current evidence that, for most of these patients, fusion surgery is superfluous.
Two year follow-up data indicate that additional instrumented fusion is a superfluous adjunct to decompression surgery for patients operated on for lumbar stenosis and degenerative spondylolisthesis
Still, in most countries, additional fusion surgery prevails as the first treatment option despite disadvantages such as increased risk and costs
The little of change in surgical practice may be due to concerns about inferior outcomes and higher reoperation rates for people operated with decompression only
Nordsten-DS is the first trial with a sufficient sample size and follow-up rate to investigate outcomes in the longer term
At five year follow-up, surgery with decompression alone gave non-inferior clinical results and similar reoperation rates compared with additional fusion surgery
This new evidence supports surgeons, patients, and administrators to choose the simpler, cheaper, and safer type of surgery
","Objective: To assess whether decompression alone is non-inferior to decompression with instrumented fusion five years after primary surgery in patients with degenerative lumbar spondylolisthesis.
Design: Five year follow-up of a randomised, multicentre, non-inferiority trial (Nordsten-DS).
Setting: 16 public orthopaedic and neurosurgical clinics in Norway.
Participants: Patients aged 18-80 years with symptomatic lumbar spinal stenosis and a spondylolisthesis of 3 mm or more at the stenotic level.
Interventions: Decompression surgery alone and decompression with additional instrumented fusion (1:1).
Main outcome measures: The primary outcome was a 30% or more reduction in Oswestry disability index from baseline to five year follow-up. The predefined non-inferiority margin was a −15 percentage point difference in the proportion of patients who met the primary outcome. Secondary outcomes included the mean change in Oswestry disability index, Zurich claudication questionnaire, numeric rating scale for leg and back pain, and EuroQol Group 5-Dimension (EQ-5D-3L) questionnaire.
Results: From 12 February 2014 to 18 December 2017, 267 participants were randomly assigned to decompression alone (n=134) and decompression with instrumented fusion (n=133). Of these, 230 (88%) responded to the five year questionnaire: 121 in the decompression group and 109 in the fusion group. Mean age at baseline was 66.2 years (SD 7.6), and 69% were women. In the modified intention-to-treat analysis with multiple imputation of missing data, 84 (63%) of 133 people in the decompression alone group and 81 (63%) of 129 people in the fusion group had a at least a 30% reduction in Oswestry disability index, a difference of 0.4 percentage points. (95% confidence interval (CI) −11.2 to 11.9). The respective results of the per protocol analysis were 65 (65%) of 100 in the decompression alone group and 59 (66%) of 89 in the fusion group, a difference of −1.3 percentage points (95% CI −14.5 to 12.2). Both 95% CIs were higher than the predefined non-inferiority margin of −15%. The mean change in Oswestry disability index from baseline to five years was −17.8 in both groups (mean difference 0.02 (95% CI −3.8 to 3.9)). Results:  of the other secondary outcomes were in the same direction as the primary outcome. From two to five year follow-up, a new lumbar operation occurred in six (5%) of 123 people in the decompression group and 11 (10%) of 113 people in the fusion group, with a total from baseline to five years of 21 (16%) of 129 people and 23 (18%) of 125, respectively.
Conclusions: In participants with degenerative spondylolisthesis, decompression alone was non-inferior to decompression with instrumented fusion five years after primary surgery. Proportions of subsequent surgeries at the index level or an adjacent lumbar level were no different between the groups.
Trial registration: ClinicalTrials.govNCT02051374
"
Intake of sugar sweetened beverages among children and adolescents,"Introduction
In 2015, obesity was estimated to affect more than 100 million children and adolescents, in line with observed increases in body mass index among this population from 1975 to 2016 in most world regions.143Among the main risk factors for obesity, unhealthy diets play a crucial role.2In particular, intake of sugar sweetened beverages (SSBs) has been consistently reported to increase the risk of obesity among children and adolescents.23This is especially concerning because obesity in childhood tends to persist into adulthood, increasing the risk of type 2 diabetes, cardiovascular disease, and premature mortality.4Explanations for the increase in intake of SSBs include globalization of markets, transformation of food systems, aggressive marketing strategies directed at children and adolescents, and lack of (or poor) regulatory measures to limit intake.56In studies at national and subnational level, policies and strategies such as taxation on sugar sweetened drinks, restrictions on food marketing, regulations for front-of-package labeling, and restrictions at school level have proven to curb the intake of SSBs among children and adolescents.678
Although quantifying the intake of SSBs among children and adolescents is critical to further evaluate the impact of these beverages on disease and the effectiveness of policies to control intake, recent national estimates among young people are unavailable for most countries.6The lack of such data prevents an analysis of the trends in SSB intake over time, as well as the role of key sociodemographic factors such as age, sex, education, and urbanicity to more accurately inform current and future policies. In this study we present SSB intakes among children and adolescents aged 3-19 years at global, regional, and national level and trends over time from 1990 to 2018, jointly stratified at subnational level by age, sex, parental level of education, and area of residence.
Methods
Study design
This investigation is based on a serial cross sectional analysis of SSB intakes from the Global Dietary Database 2018 for 185 countries. Details on the methods and standardized data collection protocol are described in detail elsewhere.910111213Compared with the Global Dietary Database 2010, innovations include major expansion of individual level dietary surveys and global coverage up to 2018; inclusion of new data jointly stratified at subnational level by age, sex, education level, and urban or rural residence; and updated modeling methods, covariates, and validation to improve prediction of stratum specific mean intakes and uncertainty. This present analysis focused on children and adolescents aged 3-19 years.
Data sources
The approach and results of our survey search strategy by dietary factor, time, and region are reported in detail elsewhere.11We performed systematic online searches for individual level dietary surveys in global and regional databases: PubMed, Embase, Web of science, LILACS, African Index Medicus, and the South-east Asia Index Medicus, using search terms “nutrition”or“diet”or“food habits”or“nutrition surveys”or“diet surveys”or“food habits”[mesh]or“diet”[mesh]or“nutrition surveys”[mesh]or“diet surveys”[mesh]and(“country of interest”). Additionally, we identified surveys through extensive personal communications with researchers and government authorities throughout the world, inviting them to be corresponding members of the Global Dietary Database. The search included surveys that collected data on at least one of 54 foods, beverages, nutrients, or dietary indices, including SSBs. A single reviewer screened identified studies by title and abstract, a random subset of articles was screened by a second reviewer to ensure consistency and accuracy, and a third reviewer screened studies to ensure that survey inclusion criteria were met. Surveys were prioritized if they were performed at national or subnational level and used individual level dietary assessments with standardized 24 hour recalls, food frequency questionnaires, or short standardized questionnaires (eg, Demographic Health Survey questionnaires). When national or subnational surveys at individual level were not identified for a country, we searched for individual level surveys from large cohorts, the World Health Organization (WHO) Global Infobase, and the WHO Stepwise Approach to Surveillance database. When individual level dietary surveys were not identified for a particular country, we considered household budget surveys. We excluded surveys focused on special populations (eg, exclusively pregnant or nursing mothers, individuals with a specific disease) or cohorts (eg, specific occupations or dietary patterns). Supplementary methods 1-3, supplementary tables 1-2, and supplementary figure 1 provide additional details on the methods. The final Global Dietary Database model incorporated 1224 dietary surveys from 185 countries, with 89% representative at national or subnational level, thus covering about 99.0% of the global population in 2018. Among these, 450 surveys reported data on SSBs, 85% of which provided individual level data. These 450 originated from 118 countries and surveyed a total of 2.9 million individuals, with 94% being representative at national or subnational level (see supplementary tables 4 and 5). Supplementary data 1 provides details on the characteristics of the survey.
Data extraction
For each survey, we used standardized methods to extract data on survey characteristics and dietary metrics, units, and mean and standard deviation of intake by age, sex, education level, and urban or rural residence (see supplementary methods 1).12All intakes are reported adjusted to 5439 kilojoules (kJ) daily (1300 kilocalories (kcal) daily) for ages 3-5 years, 7113 kJ/day (1700 kcal/day) for ages 6-10 years, and 8368 kJ/day (2000 kcal/day) for ages 11-19 years. SSBs were defined as any beverages with added sugars and ≥209 kJ (50 kcal) for each 237 g serving, including commercial or homemade beverages, soft drinks, energy drinks, fruit drinks, punch, lemonade, and aguas frescas. This definition excluded 100% fruit and vegetable juices, non-caloric artificially sweetened drinks, and sweetened milk. All included surveys used this definition.
Data modeling
Our model estimates intakes of SSBs for years for which we have survey data available. To incorporate and deal with differences in data comparability and sampling uncertainty, we used a bayesian model with a nested hierarchical structure (with random effects by country and region) to estimate the mean consumption of SSBs and its statistical uncertainty for each of 264 population strata across 185 countries for 1990, 1995, 2000, 2005, 2010, 2015, and 2018. Our model incorporated seven world regions: central and eastern Europe and central Asia, high income countries, Latin America and the Caribbean, the Middle East and north Africa, south Asia, southeast and east Asia, and sub-Saharan Africa. Our team and others (eg, the Global Burden of Disease study) have previously used this (or similar) classification for world regions, which aims to group nations by general similarities in risk profiles and disease outcomes. Although the current analysis only focuses on children and adolescents aged 3-19 years, the model used all age data to generate the strata predictions. Modeling all age groups jointly allows the use of the full set of available data and covariates to inform estimates, including age patterns, relationships between predictors and SSB intakes, and influence of covariates (eg, dietary assessment methods).
Primary inputs were the survey level quantitative data on SSB intakes (by country, time, age, sex, education level, and urban or rural residence), survey characteristics (dietary assessment method, type of dietary metric), and country-year specific covariates (see supplementary methods 2). The model included overdispersion of survey level variance for surveys that were not nationally representative or not stratified by smaller age groups (≤10 years), sex, education level, or urbanicity. Survey level covariates addressed potential survey bias, and the overdispersion parameter non-sampling variation due to survey level error (from imperfect study design and quality). The model then estimated intakes jointly stratified by age (<1, 1-2, 3-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-79, 80-84, 85-89, 90-94, ≥95 years), sex, education (≤6 years, >6-12 years, >12 years), and urbanicity (urban, rural). For children and adolescents (age <20 years) the stratification by education refers to parental education.
The uncertainty of each stratum specific estimate was quantified using 4000 Monte Carlo iterations to determine posterior predictive distributions of mean intake jointly by country, year, and sociodemographic subgroup. We computed the median intake and the 95% uncertainty interval (UI) for each stratum as the 50th, 2.5th, and 97.5th percentiles of the 4000 draws, respectively. For model selection and validation, we compared results from fivefold cross validation (randomly omitting 20% of the survey data at the stratum level and using that to evaluate predictive ability, run five times), compared predicted country intakes with survey observed intakes, assessed implausible estimates (see supplementary table 2), and visually assessed global and national mean intakes using heat maps.
A second bayesian model was used to strengthen time trend estimates for dietary factors (including SSBs) with corresponding available date on food or nutrients from the Food and Agriculture Organization’s food balance sheets14or the Global Expanded Nutrient Supply dataset.15No time component was formally included in the model; rather, time was captured by the underlying time variation in the model covariates. This second model incorporated country level intercepts and slopes, along with their correlation estimated across countries. The model is commonly referred to as a varying slopes model structure, and it leverages two dimensional partial pooling between intercepts and slopes to regularize all parameters and minimize the risk of overfitting.1617The final presented results are a combination of these two bayesian models, as detailed in supplementary methods 3.
Statistical analysis
Global, regional, national, and within country population subgroup intakes of SSBs and their uncertainty were calculated as population weighted averages using all 4000 posterior predictions for each of the 264 demographic strata in each country-year. Population weights for each year were derived from the United Nations (UN) Population Division,18supplemented with data for education and urban or rural status from Barro and Lee19and the UN.20
Intakes were calculated as 248 g (8 oz) servings weekly, or two thirds of a common 355 mL (12 oz) can of a sugar sweetened drink weekly. Absolute changes and percentage changes in consumption between 1990 and 2005, 2005 and 2018, and 1990 and 2018 were calculated at the stratum specific prediction level to account for the full spectrum of uncertainty and standardized to the proportion of individuals within each stratum in 2018 to account for changes in population characteristics over time. Stratum specific predictions were summed to calculate the differences in intake between all children and adolescents aged 3-19 years, high and low parental education (>12 years and ≤6 years, respectively), and urban and rural residence, further stratified by sex, age, parental education, and area of residence, as appropriate.
National intakes of SSBs and trends were assessed by sociodemographic development index, including trends over time between 1990 and 2005, 2005 and 2018, and 1990 and 2018. The sociodemographic development index is a measure of the development of a country or region, ranging from 0 to 1, with 0 representing the minimum level and 1 the maximum level of development of a given nation, and it is based on income per capita, average educational attainment, and fertility rates.21Our UIs are derived from a bayesian model and can be interpreted as at least 95% probability that the true mean is contained within the interval. For comparisons between groups (or over time), if the 95% UI of the difference (or change over time) does not include zero, this can be interpreted as at least 95% probability of a true difference. No hypothesis testing was conducted, as estimation with uncertainty has been recognized as a more informative approach.22
Patient and public involvement
No patients or members of the public were involved in the study as we did not collect data directly from individuals, the funding source did not provide support for direct patient and public involvement, and the study was initiated before patient and public involvement was common. The present analysis used modeled data derived from dietary data that had been previously collected, and we engaged with a diverse set of 320 corresponding members in nations around the world.
Results
Global, regional, and national SSB intakes in 2018
In 2018, the mean global intake of SSBs among children and adolescents was 3.6 (standardized serving=248 g (8 oz)) servings/week (95% UI 3.3 to 4.0), with wide (sevenfold) variation across world regions, from 1.3 servings/week (1.0 to 1.9) in south Asia to 9.1 (8.3 to 10.1) in Latin America and the Caribbean (table 1). Among the 25 countries with the largest population of children and adolescents worldwide, mean highest intakes were in Mexico (10.1 (9.1 to 11.3)), followed by Uganda (6.9 (4.5 to 10.6)), Pakistan (6.4 (4.3 to 9.7)), South Africa (6.2 (4.7 to 8.1)), and the US (6.2 (5.9 to 6.6)); while the lowest intakes were in India and Bangladesh (0.3 servings/week each) (fig 1, also see supplementary figure 9). Of the 185 countries included in the analysis, 56 (30.3%) had mean SSB intakes of ≥7 servings/week, representing 238 million young people aged 3-19 years, or 10.4% of the global population for this age group.
SSB intake by sex and age in 2018
Globally, regionally, and nationally, SSB intakes between male and female children and adolescents aged 3-19 years did not differ noticeably, as observed by the 95% UI of the differences including zero (table 1, also see supplementary tables 7 and 8). Intake of SSBs in young people was greater with increasing age globally and regionally, although with varying magnitude of these differences by region (table 1andfig 2). For instance, intakes of SSBs exceeded 9 servings/week among children aged ≥10 years in Latin America and the Caribbean and in the Middle East and north Africa but were just over 1 serving/week among young people of the same age in south Asia. Regionally, patterns of intake by age were similar between young people (see supplementary figure 2). Considering both age and region, the highest weekly intakes of SSBs were in Latin America and the Caribbean in 15-19 year olds (11.5 servings/week) and lowest in southeast and east Asia in 3-4 year olds (0.9 servings/week) (table 1). Among the 25 most populous countries, the highest intakes of SSBs were in Mexico among 10-14 year olds (11.9 servings/week) and 15-19 year olds (12.8 servings/week) and lowest in Kenya and China among 3-4 year olds (0.2 servings/week each) (supplementary table 6).
SSB intake by parental education and urbanicity in 2018
Intakes of SSBs were greater in children and adolescents from urban areas than those from rural areas (4.6 servings/week (4.2 to 5.0)v2.7 servings/week (2.4 to 3.1);table 1). When parental education and area of residence was assessed jointly, globally the highest intakes of SSBs were among children and adolescents of parents with high education in urban areas (5.15 servings/week (4.76 to 5.64)), representing 11.2% of the global population of children and adolescents (fig 3). Regionally, a similar pattern was observed in Latin America and the Caribbean, south Asia, and sub-Saharan Africa, with the largest intakes of SSBs in children and adolescents of parents with high and medium education in urban and rural areas in Latin America and the Caribbean (≥9 servings/week each), representing 56% of the population of children and adolescents in that region. Intakes of SSBs by area of residence and education were inverted in the Middle East and north Africa, with larger intakes among children and adolescents from rural areas and of parents with lower education, and little variation was observed in other world regions. See supplementary tables 7, 9, and 10, supplementary figures 3 and 4, and supplementary results for further details on SSB intakes by parental education and area of residence.
Trends in SSB intake during 1990-2005, 2005-18, and 1990-2018
Supplementary tables 11-14 and supplementary figures 5-8 show absolute global, regional, and national intakes of SSBs for 1990 and 2005. Globally, from 1990 to 2018, intakes among children and adolescents increased by 0.68 servings/week (95% UI 0.54 to 0.85; 22.9%) (fig 4, also see supplementary data 2). The magnitude of global increase was similar from 1990 to 2005 (0.33 (0.25 to 0.43); 11.0%) and from 2005 to 2018 (0.35 (0.26 to 0.47); 10.7%). However, regionally, changes did not follow the same global pattern. Between 1990 and 2005, increases in intakes of SSBs were observed in most regions, with the largest increase in high income countries (1.48 (1.37 to 1.60); 29.1%), little change in central and eastern Europe and central Asia and in south Asia, and a decrease in Latin America and the Caribbean (−1.20 (−1.54 to −0.88); −12.7%). More recently, from 2005 to 2018, increases continued in most regions, with the largest in sub-Saharan Africa (1.38 (1.01 to 1.85); 49.2%), except for south Asia where little change was evident and high income countries where intakes decreased (−1.59 (−1.71 to −1.47); −24.1%). In the overall period from 1990 to 2018, the largest regional increase was in sub-Saharan Africa (2.17 (1.60 to 2.88); 106%), with other world regions showing steady, more modest increases over time. Exceptions were high income countries and Latin America and the Caribbean, where intakes increased after 1990 and then decreased close to 1990 levels by 2018. The supplementary results and supplementary table 15 describe regional trends over time by age, sex, parental education, and urbanicity.
Among the 25 most populous countries, the largest increase from 1990 to 2005 was in the US (2.95 (2.73 to 3.17); 43.2%) and the largest decrease was in Brazil (−3.42 (−3.95 to −2.97); −40.6%) (see supplementary data 2 and supplementary figure 9). From 2005 to 2018, the largest increase was in Uganda (4.30 (2.31 to 7.39); 173%), and the largest decrease was in the US (−3.55 (−3.81 to −3.30); −36.4%). Overall, between 1990 and 2018, the largest increased was in Uganda (6.73 (4.38 to 10.39); 5573%) and the largest decrease was in Brazil (−3.29 (−3.79 to −2.86); −39.0%) (see supplementary data 2 and supplementary figure 10). The supplementary results and supplementary tables 16-19 show trends over time within the 25 most populous countries by age, sex, parental education, and urbanicity.
SSB intakes and trends by sociodemographic development index and obesity
In 1990 and 2005 a positive correlation was evident between national intakes of SSBs and sociodemographic development index, with greater intakes observed in countries with a higher sociodemographic development index (see supplementary figures 11 and 12). However, this correlation was no longer present in 2018 (r=−0.001, P=0.99). Intakes of SSBs and prevalence of obesity were positively correlated in both 1990 (r=0.28, P<0.001) and 2018 (r=0.23, P<0.001) (see supplementary figure 13).
Discussion
Intakes of SSBs among children and adolescents aged 3-19 years in 185 countries increased by 23% (0.68 servings/week (0.54 to 0.85)) from 1990 to 2018, parallel to the rise in prevalence of obesity among this population globally.23We found a positive correlation between intake of SSBs and prevalence of obesity among children and adolescents in all years. This finding needs particular attention given the incremental economic costs associated with overweight and obesity globally, which are projected to increase from about $2.0tn (£1.6tn; €1.9tn) in 2020 to $18tn by 2060, exceeding 3% of the world’s gross domestic product.24Chronic diet related conditions such as obesity have been recognized as part of a global syndemic along with undernutrition given their interaction and shared underlying societal drivers.25Tackling drivers of obesity and other diet related diseases among children and adolescents is also critical to be better equipped for potential future pandemics, as cardiometabolic conditions such as obesity, diabetes, and hypertension were top drivers of increased risk of hospital admission and death with covid-19.26The increase in intakes of SSBs among children and adolescents corresponded to nearly twice the absolute increase in intake observed among the adult population from 1990 to 2018, for which policies targeting specifically children and adolescents are critical.13Young people are particularly appealing to the food industry as they are easily influenced by food marketing, having an effect on not only their current intakes but also their preferences as they develop into adulthood.27Their susceptibility to marketing, rising trends in obesity, and accelerated increases in intakes of SSBs underline the necessity for interventions such as taxes, regulations on front-of-package labeling, and regulations in the school environment to curb intakes of SSBs.682728
Changes in intakes of SSBs in children and adolescents from 1990 to 2018 varied substantially by world region. As with the adult population, the largest increase from 1990 to 2018 was in sub-Saharan Africa, emphasizing the need for prompt interventions in this region. Young people in the Middle East and north Africa and in southeast and east Asia showed a more accelerated increase in SSB consumption compared with adults, underlining the importance of policies targeting young people in these regions. The Middle East and north Africa had the second highest intakes of SSBs among children and adolescents in 2018, which differed from our findings among adults, where the Middle East and north Africa occupied third place after sub-Saharan Africa.
Latin America and the Caribbean experienced an overall decrease in intakes of SSBs from 1990 to 2005, which could be attributed to the economic crisis experienced among most of the major economies in the region during this period,29in addition to potential greater health awareness as a result of healthy eating campaigns in several countries in the region.30In contrast, the increases in intakes in this region from 2005 to 2018 may relate to economic recovery, increased marketing campaigns, and industry opposition to public policies to reduce the intake of SSBs.31These findings align with findings in the adult population of this region.13Over the past 30 years, Latin America and the Caribbean has undergone an accelerated transformation in the food systems, resulting in wider availability of unhealthy foods, including SSBs, that could explain the large intakes in this region.7Moreover, the influence of multinational corporations responsible for ultra-processed foods, marketing strategies targeted at young people, lack of (or poor) regulatory measures to limit the intake of SSBs have also been consistently observed in Latin America and other regions with improving economies.167The use of social media and TV to target advertising at young people has been identified as being especially high in Latin America as well as in the Middle East.627
High income countries experienced an overall decrease in intakes of SSBs from 2005 to 2018. This might be explained by the increasing scientific and public health attention on the harms of SSBs as well as obesity in these nations during this period, which may have led to increased media and public awareness about the harms to health associated with SSBs, wider formulation, promotion, and availability of non-caloric sweetened beverage substitutes, and, more recently, taxation on SSBs in several of these nations.32
The potential role of sociodemographic factors on intakes of SSBs was evidenced by the large variations in intake by parental education and urbanicity, particularly in south Asia and sub-Saharan Africa, evidencing the need to account for these factors in the design of policies and interventions. At national level, the correlation between intake of SSBs and sociodemographic development index changed from positive in 1990 to null in 2018 (see supplementary figure 11), suggesting that the association between the two might be reversing. This is similar to what was observed in adults, where the association between intake of SSBs and sociodemographic index changed from null to negative from 1990 to 2018.13Our new findings show similar directional trends in national and subnational intakes of SSBs among young people compared with adults,13although with generally higher absolute intakes among young people, suggesting nation specific influences on SSB intakes are at least partly shared across the lifespan. Further efforts are needed to incorporate data on other social determinants of health, such as income, access to water, access to healthcare, and race/ethnicity to elucidate additional potential heterogeneities.
Strengths and limitations of this study
Our study has several strengths. We assessed and reported global, regional, and national estimates of SSB intakes jointly stratified by age, sex, parental education, and urbanicity among children and adolescents. Compared with previous estimates, our current model included a larger number of dietary surveys, additional demographic subgroups, and years of assessment. Our updated bayesian hierarchical model better incorporated survey and country level covariates—and addressed heterogeneity and uncertainty about sampling and modeling.1333Intakes were estimated from 450 surveys—mostly representative at national and subnational levels and collected at individual level—and represented 87.1% of the world’s population. Other recent estimates for global intakes of SSBs relied mostly on national per capita estimates of food availability (eg, Food and Agriculture Organization food balance sheets) or sales data.34Such estimates can substantially overestimate and underestimate intake compared with individual level data35and are less robust for characterizing differences across population subgroups. Our estimates are informed by dietary data at individual level collected from both 24 hour recalls (24% of surveys), considered the ideal method for assessing nutritional intakes of populations), and food frequency questionnaires (61% of surveys), a validated approach for measuring intakes of SSBs36(see supplementary table 4).
Overall, our findings should be taken as the best currently available, but nonetheless imperfect, estimates of SSB intakes worldwide. Even with systematic searches for all relevant surveys, we identified limited availability of data for several countries (particularly lower income nations) and time periods.11Thus, estimated findings in countries with no primary individual level surveys have higher corresponding uncertainty, informing surveillance needs to assess SSBs nationally and in populations at subnational level. Particularly, we identified limited surveys for south Asia (n=9) and sub-Saharan Africa (n=22), which might have affected the accuracy of our estimates in those regions (see supplementary table 4). This finding emphasizes the critical need for further efforts in data collection and surveillance, particularly in these regions. Categorization by age, parental education, and urbanicity were in groups rather than in more nuanced classifications, balancing the interest in subgroup detail versus the realities required from a global demographic effort of de novo harmonized analyses of individual level dietary data from hundreds of different dietary surveys and corresponding members globally. All types of dietary assessments include some errors, whether from individual level surveys, national food availability estimates, or other sources. Our model’s incorporation of multiple types and sources of dietary assessments provided the best available estimates of global diets, as well as the uncertainty of these estimates. For instance, self-reported data rely on the memory and personal biases of the respondents, thus introducing potential bias from underreporting or overreporting of actual intakes. Furthermore, assumptions relating to standardization of serving sizes, SSB definitions, energy adjustment, and disaggregation at household level, as well as of no interaction between sociodemographic variables in our model, could have impacted our estimates. To minimize these limitations, we used standardized approaches and carefully documented each survey’s methods and standardization processes to maximize transparency.
Our definition and data collection on SSBs excluded 100% fruit juice, sugar sweetened milk, tea, and coffee, given that evidence for health effects of these beverages is inconsistent and does not achieve at least probable evidence for causal harms.3738These differences may relate to additional nutrients, such as calcium, vitamin D, fats, and protein in milk, caffeine and polyphenols in coffee and tea, and fiber and vitamins in 100% juice; or to differences in rapidity of consumption and drinking patterns. Each of these beverages is generally also excluded in policy and surveillance efforts around SSBs. A recent meta-analysis suggested a modest positive association between 100% fruit juices and body mass index in children (0.03 units higher for each daily serving),39highlighting the need for more research on the health impacts of these and other beverages in children. Sweetened milks are mostly targeted at children and adolescents, and in some regions are mostly consumed by the youngest children.40Given that our SSBs definition excluded sweetened milk, this could partially explain the low intakes observed in our study among the youngest age categories. Future studies should also look into characterizing intakes of sweetened milks, especially in countries such as the US, Australia, Pakistan, and Chile where high intakes among children and adolescents have been reported.4041Home sweetened teas and coffees were not explicitly excluded from the definition of SSBs at the time of data collection, but tea and coffee were collected as separate variables and thus most likely excluded by data owners from the SSBs category. SSBs were defined as beverages with added sugars and ≥209 kJ (50 kcal) per 237g serving, capturing most of the SSBs during the time period of our investigation that typically contained about 418 kJ (100 kcal) per serving. More recently, some SSBs with slightly less than 10 g of added sugar have entered the market. As these are a relatively recent addition, their exclusion is unlikely to meaningfully alter our findings, but future research should focus on more refined surveillance of SSBs to allow flexibility in beverage group definitions—for example, similar to the data harmonized in our collaboration with the FAO/WHO GIFT food consumption data tool.42Our current definition leveraging product name and caloric content to identify beverages with added sugar across the world ensures consistency in reporting.
Conclusion
Intakes of SSBs among children and adolescents aged 3-19 years in 185 countries increased by almost a quarter from 1990 to 2018, parallel to the rise in prevalence of obesity among this population globally. Policies and approaches at both a national level and a more targeted level are needed to reduce intakes of SSBs among young people worldwide, highlighting the larger intakes across all education levels in urban and rural areas in Latin America and the Caribbean, and the growing problem of SSBs for public health in sub-Saharan Africa. Our findings are intended to inform current and future policies to curb SSB intakes, adding to the UN’s 2030 Agenda for Sustainable Development for improving health and wellbeing, reducing inequities, responsible consumption, poverty, and access to clean water.
The intake of sugar sweetened beverages (SSBs) has been consistently reported to increase the risk of obesity among children and adolescents
This is especially concerning given that obesity in childhood tends to persist into adulthood, increasing the risk of type 2 diabetes, cardiovascular disease, and premature mortali","Objective: To quantify global intakes of sugar sweetened beverages (SSBs) and trends over time among children and adolescents.
Design: Population based study.
Setting: Global Dietary Database.
Population: Children and adolescents aged 3-19 years in 185 countries between 1990 and 2018, jointly stratified at subnational level by age, sex, parental education, and rural or urban residence.
Results: In 2018, mean global SSB intake was 3.6 (standardized serving=248 g (8 oz)) servings/week (1.3 (95% uncertainly interval 1.0 to 1.9) in south Asia to 9.1 (8.3 to 10.1) in Latin America and the Caribbean). SSB intakes were higher in older versus younger children and adolescents, those resident in urban versus rural areas, and those of parents with higher versus lower education. Between 1990 and 2018, mean global SSB intakes increased by 0.68 servings/week (22.9%), with the largest increases in sub-Saharan Africa (2.17 servings/week; 106%). Of 185 countries included in the analysis, 56 (30.3%) had a mean SSB intake of ≥7 servings/week, representing 238 million children and adolescents, or 10.4% of the global population of young people.
Conclusion: This study found that intakes of SSBs among children and adolescents aged 3-19 years in 185 countries increased by 23% from 1990 to 2018, parallel to the rise in prevalence of obesity among this population globally. SSB intakes showed large heterogeneity among children and adolescents worldwide and by age, parental level of education, and urbanicity. This research should help to inform policies to reduce SSB intake among young people, particularly those with larger intakes across all education levels in urban and rural areas in Latin America and the Caribbean, and the growing problem of SSBs for public health in sub-Saharan Africa.
"
Estimating the economic effect of harm associated with high risk prescribing of oral non-steroidal anti-inflammatory drugs,"Introduction
Non-steroidal anti-inflammatory drugs (NSAIDs) alleviate pain and inflammation by inhibiting isoenzymes of cyclooxygenase (known as COX-1 and COX-2). However, many adverse events are known to occur in people who use NSAIDs, particularly in relation to gastrointestinal bleeding, renal dysfunction, and cardiovascular function.123NSAIDs, including aspirin, are responsible for 30% of hospital admissions related to an adverse drug event, mainly due to gastrointestinal bleeding, myocardial infarction, stroke, and renal injury.45
Due to safety concerns for primary care, prescribing of oral NSAIDs in many countries has been reducing for the past 25 years. Total numbers of prescriptions have decreased and selection of potentially safer NSAIDs and prescribing of gastroprotective drugs have increased. Along with improved management ofHelicobacter pyloriinfection, these changes have contributed to a 52% reduction in peptic ulcer incidence between 1997 and 2005,6and reductions in NSAID prescribing in people with cardiovascular disease after regulatory warnings in 2004.789In England, primary care prescribing of NSAIDs fell by about 12% between 2017 and 2022.10Overall trends showed that, in 2022, naproxen accounted for 69% of prescriptions, prescribing was reduced for NSAIDs associated with higher cardiovascular risk, COX-2 inhibitor prescribing increased, and topical NSAID prescribing was sustained.10
NSAIDs have been the target of many prescribing safety initiatives including: increased emphasis on safer prescribing in general practitioner (GP) training and in GP targeted educational packages; supporting practices to identify patients with high risk prescribing; improvements to GP clinical record prescribing safety tools111213; and community pharmacist prescribing quality schemes.14However, NSAID prescribing is still common in people at high risk of adverse events due to older age, previous peptic ulcer, heart failure, chronic kidney disease, or prescription of other medications that can increase bleeding risk.151617These priority areas are being targeted by IT based, medical safety interventions, led by pharmacists, originally rolled out across the East Midlands region,17followed by national roll-out in England.18However, evaluation of these initiatives has largely been restricted to examining effect on prescribing rather than impact on population level outcomes. Identifying the most harmful and costly types of hazardous prescribing is likely to be important for policy makers, clinicians, and patients. For example, the main purpose of the Medicines Safety Improvement Programme in England is to address the most important causes of severe harm associated with medicines.19
The aims of this study were to quantify the prevalence in England of five types of high risk prescribing of oral NSAIDs, and to estimate the harm and NHS costs incurred by this prescribing to inform the targeting and evaluation of policy interventions.
Methods
Previous work has identified five prescribing safety indicators as the most likely to be associated with avoidable harm from NSAIDs.2021The five high risk areas are (1) prescription of an oral NSAID, without co-prescription of gastroprotection, to a patient aged 65 years or older; (2) prescription of an oral NSAID, without co-prescription of gastroprotection, to a patient with a history of peptic ulceration; (3) prescription of warfarin or directly-acting oral anticoagulant in combination with an oral NSAID; (4) prescription of an oral NSAID to a patient with heart failure; and (5) prescription of an oral NSAID to a patient with chronic kidney disease (estimated glomerular filtration rate <45 mL/min).
If a patient in one of these high risk areas was affected by potentially hazardous NSAID related prescribing, we have referred to this occurrence as a hazardous prescribing event (HPE). In this study, use of low dose aspirin was not included in the analysis.
We first obtained the prevalence of each HPE in the general population of England. We then estimated the harm associated with each of the five NSAID specific HPEs at the patient level, expressed as quality adjusted life years (QALYs) lost and the cost to the NHS in England of managing that harm. The combination of these two sets of data allowed the estimation of annual national levels of patient harm and NHS costs linked to NSAID prescribing in the five defined high risk groups.
Prevalence of each HPE in the general population of England
The prevalence of each HPE in the general population of England was estimated from data reported from a nationwide study of prescribing safety in England carried out by this team.18This study determined the number of patients affected by each of the five prescribing safety indicators (ie, numerator) and the number of patients in each of the five groups classified as being at risk of being affected (ie, denominator). The report for this study provides a baseline number of people who were affected by each HPE cross sectionally in April 2020, based on data from the national roll out of PINCER.18The study included 2430 of 7131 general practices in England), from 130 clinical commissioning groups, and searches of over 23m patient records. The prevalence data came from the 1060 general practices (total patient population of 10 906 453) that provided data at two or more time points. We expect the sample to be representative of GP practices in England due to geographical spread of practices (across England) and size of the sample. The prevalence of each HPE was calculated by dividing the number of people with the HPE by the total patient population. The supplementary file (appendix 1) provides details of the practices that provided data for these prevalence estimates.
Patient level harm and NHS cost of hazardous prescribing of NSAIDs
To estimate the patient level harm and costs associated with each HPE, a treatment pathway, or model, need to be designed to reflect the likely events that occur when people experience a HPE related to an NSAID. This model included what acute events happened, how frequently, and how serious was it for the patient. Additionally, details were needed as to how was short and long term quality of life, mortality risk, and health care resource consumption likely to be affected by this acute event. These models (also called cohort level state transition (Markov) models) were developed to generate estimates of patient outcomes (measured as QALYs) and cost to the NHS in England associated with each of the five high risk areas described above. QALYs are a measure of health status that combines quality of life and quantity of life into a single numerical value. Quality of life is measured as health utility on a 0-to-1 scale where 1 indicates perfect health and 0 indicates death. One QALY is equivalent to one year lived in perfect health, or two years lived in middling health (ie, a health utility of 0.5 over two years). In our model, the QALYs included reflect the negative health effects of adverse health events and do not include health benefits of taking NSAIDs. The supplementary file provides details of the overall methodological approach (appendix 2, section 1) and methods used to develop each of the models (appendix 2, sections 2 to 5).
The models were developed and reported according to standard validation and reporting criteria.2223A health economic analysis plan was developed and is available from the authors. Replicable literature reviews were used to identify model inputs. Face validity was ascertained through continuous feedback from clinical and patient experts during the model build. Where possible, we used and adapted existing published models to optimise design.
Reflecting the prescribing indicator descriptions, we did not include topical NSAIDs because this method of application is considered relatively safe,24or other parenterally administered NSAIDs because of the very low levels of prescribing.25We did not treat COX-2 inhibitors separately from other NSAIDs because all NSAIDs can cause serious adverse effects, albeit with some variation in specific risks of gastro-intestinal and cardiac events, for example.26
This study estimates the harm associated with long term prescribing of oral NSAIDs in high risk groups by estimating the risk of the outcomes in the presence and absence of the HPE. In the HPE cohort, the probabilities of an adverse event are increased by the presence of the NSAID (in heart failure, co-prescription of oral anticoagulant, and chronic kidney disease), or by the use of the NSAID in the absence of gastroprotection (in older people or those with previous peptic ulcer). The cohort in the non-HPE group was assumed to have had the following treatments: older people, or those with previous peptic ulcer were assumed to have had NSAIDs plus gastroprotection; people taking oral anticoagulant, or those with heart failure or chronic kidney disease were assumed to have taken paracetamol. These assumptions were validated with our patient and clinical experts.
Data were required for each of the five economic models. We collected cohort baseline characteristics. All data categories reflected real-life patient cohort characteristics (age, sex, and relevant diagnosis) as closely as possible, by selecting data sources that either were from relevant UK cohorts, or from cohorts that closely resembled UK real life patients. We collected data for health states. Each model included health states for key adverse events associated with the HPE and death. Relevant adverse events were identified from published literature and verified through discussion with experts. Data for probabilities of harm associated with HPEs and subsequent events after harm has occurred were collected. The absolute risk of harm (adverse events), associated with HPEs was derived from large population based observational studies, wherever possible. Health status was collected and expressed as utility, associated with a particular health state, where 1.0 is equivalent to perfect health and 0 is equivalent to being dead. We used published estimates of health status to attach a utility value to each health state in the Markov models. Resources consumed in a particular health state were noted. Where available, data from large population based observational datasets were used to estimate resource use associated with harm. If these were not available, we used expert opinion to estimate resource use. We attached UK reference unit costs to resource use to construct a total cost for each health state (eg, NHS costs post-gastric bleed).Table 1summarises the health states included in each of the models. The probability, utility, and cost parameters, and their sources, are summarised for each model in tables 6.1 and 6.2 in the appendix.
We determined the impact by estimating the effect of HPE on a patient via the impact on QALYs and costs. The follow-up period was chosen to be sufficient to capture relevant costs and effects without exacerbating uncertainty owing to excessive extrapolation. We set the period at 10 years in the primary (base case) analysis, after consultation with our clinical experts who suggested that this time was sufficient to encompass events of plausible interest. As such, our base case models estimate QALYs and costs over a 10 year period following the onset of the HPE. We tested this decision by varying the time horizon between five and 20 years in sensitivity analysis. We took the perspective of the NHS: we only included the costs incurred by the NHS of managing the consequences of the HPE. People value present costs and benefits more than future costs and benefits, so studies with a time horizon over one year needed to discount future costs and benefits. The QALYs and costs were discounted at the recommended rate of 3.5% per annum, and the resulting effect was examined in a sensitivity analysis.27The cost year used was 2020-21 (currency: UK £). Models were built in Microsoft Excel. Each model was populated with probability, cost and health status data, to allow for the generation of the point estimates and uncertainty in distributions of discounted outcomes (QALYs) and NHS costs in a cohort affected by a specific type of HPE, and a cohort not affected. We assumed that the association between exposure time and risk of adverse event was roughly linear, supported to an extent by the literature for gastrointestinal, renal, and cardiovascular outcomes.2829For the probabilistic analysis, distributions appropriate for input parameters were chosen.30If no measure of uncertainty was available for the β or gamma distribution, we assumed that the standard deviation defining the distribution was 20% of the mean.30The probabilistic analysis was based on 10 000 samples. We made an assumption in the absence of empirical evidence around usual length of exposure to the HPE without an event precipitating review. Length of exposure to the HPE was assumed to be the lifetime of the model (10 years), unless the patient had an adverse event when it was assumed that the hazardous prescribing was stopped.
Sensitivity analysis was used to examine the effects of varying the following parameters: time horizon of five years and 20 years; no discount rate; duration of HPE exposure varied from 0.25 to 10 years. Our primary analysis assumes effects are additive (ie, people who have had multiple HPEs may experience multiple harms). Sensitivity analysis was carried out to minimise the overlap by deducting the number of people at risk for anticoagulants, heart failure, and chronic kidney disease from the group of people older than 65 who are at gastrointestinal risk so that people aged over 65 will only be at risk of gastrointestinal harm if they are not part of the oral anticoagulant, heart failure, or kidney disease cohorts. Further sensitivity analysis examined the effect of assuming NSAIDs in use are a weighted average of celecoxib, diclofenac sodium, ibuprofen, and naproxen (compared with the base case assumption of 100% naproxen).
Scaling up prevalence, harm, and cost
To estimate the number of people with each HPE across England, the prevalence was multiplied by the number of people registered with a GP practice in England on 1 December 2023 (n=63 049 603).31Finally, the number of people estimated to be affected by each HPE was multiplied by the respective per person QALY loss and cost to estimate the overall burden of each HPE in England. The QALY losses and costs were added together across all five HPEs to estimate an approximate combined burden.
Patient and public involvement and engagement
Patients and clinicians were involved in all stages of this work including AC, who is a co-author. Extensive consultation was undertaken during the development of the economic models, with patients, GPs, and pharmacists consulted at multiple design stages. Their input influenced model structure and treatment pathways, as well as how to report findings.
Results
The cost and harm associated with each HPE is summarised intable 2. A net QALY loss was observed for each HPE when comparing the mean QALYs generated per person in the presence and absence of the HPE. The QALY losses per person ranged from 0.01 (95% credibility intervals (CI) 0.01 to 0.02) in those with a previous peptic ulcer, to 0.11 (0.04 to 0.19) in people with chronic kidney disease. The difference in the mean costs to the NHS in the presence and absence of the HPE ranged from non-statistically significant increases (£14 (€17; $18) (−£71 to £98) per person with heart failure) to statistically significant increases (£1097 (£236 to £2542) per person concurrently taking oral anticoagulants.
HPE prevalence is summarised intable 3. HPE prevalence per 1000 patients ranged from 0.11 in people with a previous peptic ulcer who were given NSAIDs (and no gastroprotective drugs) to 1.70 in older adults given NSAIDs (and no gastroprotective drugs).
Scaling up the HPE prevalence from the sample of 10 906 453 to the English population registered with a GP (n=63 049 603) produced an estimate of 162 219 HPEs associated with NSAIDs in one year in England. Of these events, 66.3% were prescribing NSAIDs to older adults with no gastroprotective drug (table 3). We estimated that these HPEs will be associated with 778 excess deaths in England in the subsequent 10 years, as well as substantial gastrointestinal, neurological, cardiac, and renal morbidity (fig 1). In particular, the analyses highlighted NSAIDs’ propensity to induce an acute event in people with a chronic condition (>6700 heart failure exacerbations and >3200 acute kidney injuries over 10 years).
The estimated total cost and QALY loss over 10 years following the onset of each of these HPE related to NSAIDs is estimated for the population of England (table 3). The most common HPE (NSAIDs in older adults with no gastroprotective drug) resulted in 1929 (95% CI 1416 to 2425) QALYs lost, costing £2.46m (95% CI £0.65m to £4.68m). The HPE associated with greatest harm and cost nationally was in people taking oral anticoagulants, with 2143 (894 to 4073) QALYs lost, costing £25.41m (£5.25m to £60.01m). The estimated total QALY loss across all the HPEs was 6335 (4471 to 8658) and an NHS cost of £31.43m (£9.28m to £67.11m).
Figure 2shows the degree of uncertainty around this estimate. Across 10 000 simulations drawing from the full distribution of uncertain parameters, 99.94% suggested that hazardous prescribing related to NSAIDs is associated with additional costs to the NHS and all showed that these prescribing events had a negative impact on patient health.
When we varied each model parameter over the range of its plausible values, none was influential enough in isolation to overturn the cost and QALY impacts associated with NSAID related HPEs (figure S2, appendix 3). The inputs with the biggest effect on costs were those relating to stroke, an outcome that only featured in our analysis of NSAIDs in people taking oral anticoagulants. The parameters that affected QALYs most related to stroke and acute kidney injury. However, the parameter that made the biggest difference in both dimensions was the length of time for which we assumed people remained affected by the HPE. Further exploration of this parameter showed that NSAIDs were associated with harm and cost even if exposure was brief, and that at least half of the harm was likely to be experienced within the first one and a half years (fig 3).
When we shortened the models’ time horizons, the effect of QALY losses and cost associated with NSAID related hazardous prescribing reduced. The opposite was mostly the case when we lengthen the time horizon or reduce the discount rate. Counterintuitively, however, expected excess costs were slightly lower with a 20 year time horizon than in the base case. This effect was due to the kidney disease model, where people affected by an HPE have a higher chance of premature death whereas, in the absence of the HPE, a greater proportion of people live long enough to consume expensive resources as their disease progresses. In a final sensitivity analysis, we explored the potential for double counting in our analysis by deducting 100% of people in the oral anticoagulant, heart failure, and kidney disease cohorts from the group of people aged over 65 who are at risk of gastrointestinal harm. The data showed that the results were not meaningfully affected by our base case assumption of additive effects, with cost and QALY reducing by 5-10%.
Discussion
Principal findings
The most commonly occurring HPE is of NSAIDs in older people without gastroprotective drugs (prevalence 1.70/1000 patients), which was associated with an estimated total loss of 1929 QALYs and a cost of £2.46 m. However, although NSAID with oral anticoagulant was much less common (0.37/1000 patients), it was associated with larger QALY loss (2143) and much larger costs (£25.41m). The five NSAID related HPEs caused a loss of 6335 QALYs at a cost to the NHS of £31.43m in England over 10 years. Reducing the length of time the patient was assumed to be affected by the HPE substantially reduced estimates of harm and cost, but at least half the estimated harm in the full model accrued within the first one and a half years of treatment.
Comparisons with other literature
Most studies worldwide have estimated the harm and costs associated with NSAIDs and focused on the short term costs of admission to hospital for gastrointestinal bleeds related to NSAIDs, sometimes including dyspepsia costs.32333435363738Our modelling approach incorporated a more complete estimate of gastrointestinal effects over a longer time period, including that associated with rebleeding, and includes more comprehensive primary care costs, which increased the estimates of harm and cost.39The effect of NSAIDs on outcomes beyond gastrointestinal outcomes, such as renal and cardiovascular outcomes, has been less widely investigated. A review reported effects on these three types of outcomes but did not estimate costs.29Two studies incorporated NSAID related cardiovascular risk into their model when estimating the harm and costs associated with NSAIDs.4041While both studies used approaches similar to ours, they did not explicitly report the harms and costs associated with cardiovascular risk related to NSAIDs. Interest in the burden associated with renal harms of NSAIDs has been increasing in recent years: a 2021 Japanese study estimated the economic burden of renal events associated with NSAID use in patients with chronic kidney disease.32This study only calculated one year costs and did not assess the effect of acute kidney injury on risk of chronic kidney disease progression. As such, their results are difficult to compare directly with our 10 year results; however, even in one year, they suggest that costs in Japan can reach $1779 for hospital admissions and $33 018 for dialysis per person.
Strengths and limitations
Key strengths of the study are the estimation of the harm and NHS costs incurred by NSAID prescribing across range of common gastrointestinal, cardiovascular, and renal harms possible in key groups at high risk for adverse events. These factors included longer term harms and costs, such as those associated with gastrointestinal rebleeding, and acute kidney injury associated increases in the risk of chronic kidney disease progression. Combining these model estimates with real-world data on HPE rates provides a more complete picture of the scale of harm associated with NSAIDs than previous analyses.
The study has several limitations, around assumed dose, length of exposure, accounting for all harms, and assumptions around independence of multiple HPEs, although we explicitly explored many of these limitations in sensitivity analysis. Firstly, we did not have access to data about the length of time people are affected by specific HPEs. Recent work in the UK suggests that more than 60% of patients in England who have an NSAID prescribed for regular use have it prescribed for more than two months.42Based on advice from clinicians, our models assumed exposure for the length of the model (10 years), unless an adverse event alerts the prescriber to take corrective action. A sensitivity analysis for shorter durations of exposure found (as expected) that shorter durations of exposure were associated with lower harms, but that at least half of observed harm accrued in the first one and a half years. In addition, NSAID exposure often varies as underlying pain increases or decreases, but the model assumes constant dosage. However, the source data for risk of adverse events comes from routine data where exposure will have similar variation, so we do not expect this to cause bias. The model is also constrained to a 10 year time horizon, which might underestimate costs and harms from HPEs, although sensitivity analysis using a 20 year time horizon does not alter the core conclusions. As naproxen was the most prescribed NSAID in England, the models assume the NSAID is standard dose naproxen. Therefore. not accounting for variation in drug or dosage is a limitation. However, data for the effect of drug or dose on all the adverse events under investigation were sparse.
Secondly, the models assume that harms are additive but independent, but a sensitivity analysis testing this assumption found only small reductions in estimated cost and QALY impacts. Our previous work on the incidence of these HPEs suggested that very few patients had more than one type of HPE.17
Thirdly, not all harms are accounted for which underestimates harm and cost. We have focused on selected high risk groups prioritised for the PINCER roll-out but not all NSAID harms were captured. For example, we did not include the harm of acute kidney injury that might occur in people with normal renal function43or cardiac outcomes in people with ischaemic heart disease taking NSAIDs.44The prescribing data used in our study did not capture over-the-counter use of NSAIDs, although over-the-counter doses tended to be lower than prescribed doses, with a lower risk of adverse drug events.4546
The use of UK prescribing data, patient management pathways, and unit costs means that providers and policy makers from other settings will need to interpret our findings with caution before extrapolating to their settings. However, the types, severity, and probability of harm related to NSAIDs are likely to be transferable, and can be combined with local prescribing data, patient management pathways, and unit costs.
What this work adds, and implications for prescribers and policy makers
NSAIDs continue to be an important source of avoidable harm and healthcare costs, despite a range of regulatory and prescribing quality initiatives to reduce their use in high risk populations.479111741The key implication for policy and practice is that despite quite large improvements in high risk prescribing of NSAIDs in the past 10-15 years, more work needs to be done. In practice, reducing NSAID prescribing on an individual level by prescribers is challenging because it depends on how much pain a patient is in, how well they respond to NSAIDs, and how well they respond to other analgesics or interventions. We have focused on use of NSAIDs in high risk groups, rather than all NSAID prescribing because reducing NSAID prescribing in low risk groups should not be a policy priority. Our findings are designed to be relevant to current policy initiatives in England, focusing on problematic polypharmacy (ie, taking multiple drugs concurrently) as one of the main consequences of overprescribing.48This study provides a baseline estimate of harm and costs, and the models provide a framework to support robust evaluation of interventions to reduce high risk prescribing of NSAIDs.
Further research should extend this work: to other kinds of NSAID risk; to better understand exposure over time and account for it; and to optimise effectiveness of NSAID alternatives perhaps, which might be pharmacological or non-pharmacological. We also suggest that increasing patient awareness of the risks of NSAIDs needs to be considered as part of interventions to reduce high risk prescribing of NSAIDs. Other areas targeted by national policy might also benefit from the same explicit approach to inform choice of focus.
Conclusions
NSAIDs continue to be a source of avoidable harm and healthcare costs, despite a range of initiatives to reduce their use, especially in populations at high risk. The risk of harm and associated costs appear to outweigh their benefit in these populations; therefore, a concerted effort should be made to continue to include NSAIDs in patient safety and deprescribing initiatives.
Non-steroidal anti-inflammatory drugs (NSAIDs) are one of the most widely prescribed groups of medicines worldwide, although prescribing rates are reducing
NSAIDs can cause patient harm, such as gastrointestinal bleeding, myocardial infarction, stroke, and renal damage
Problematic NSAID prescribing in high risk groups is prevalent, with 107 000 people older than 65 years being prescribed NSAIDs without gastroprotection, annually
NSAIDS can cause most harm when prescribed in people who are also taking oral anticoagulants
Problematic NSAID prescribing costs NHS England an estimated £31m (€37m; $39m) over 10 years, with a loss of about 6300 quality adjusted life years
","Objectives: To quantify prevalence, harms, and NHS costs in England of problematic oral non-steroidal anti-inflammatory drug (NSAID) prescribing in high risk groups.
Design: Population based cohort and economic modelling study.
Setting: Economic models estimating patient harm associated with NSAID specific hazardous prescribing events, and cost to the English NHS, over a 10 year period, were combined with trends of hazardous prescribing event to estimate national levels of patient harm and NHS costs.
Participants: Eligible participants were prescribed oral NSAIDs and were in five high risk groups: older adults (≥65 years) with no gastroprotection; people who concurrently took oral anticoagulants; or those with heart failure, chronic kidney disease, or a history of peptic ulcer.
Main outcome measures: Prevalence of hazardous prescribing events, by each event and overall, discounted quality adjusted life years (QALYs) lost, and cost to the NHS in England of managing harm.
Results: QALY losses and cost increases were observed for each hazardous prescribing event (vno hazardous prescribing event). Mean QALYs per person were between 0.01 (95% credibility interval (CI) 0.01 to 0.02) lower with history of peptic ulcer, to 0.11 (0.04 to 0.19) lower with chronic kidney disease. Mean cost increases ranged from a non-statistically significant £14 (€17; $18) (95% CI −£71 to £98) in heart failure, to a statistically significant £1097 (£236 to £2542) in people concurrently taking anticoagulants. Prevalence of hazardous prescribing events per 1000 patients ranged from 0.11 in people who have had a peptic ulcer to 1.70 in older adults. Nationally, the most common hazardous prescribing event (older adults with no gastroprotection) resulted in 1929 (1416 to 2452) QALYs lost, costing £2.46m (£0.65m to £4.68m). The greatest impact was in people concurrently taking oral anticoagulants: 2143 (894 to 4073) QALYs lost, costing £25.41m (£5.25m to £60.01m). Over 10 years, total QALYs lost were estimated to be 6335 (4471 to 8658) and an NHS cost for England of £31.43m (£9.28m to £67.11m).
Conclusions: NSAIDs continue to be a source of avoidable harm and healthcare cost in these five high risk populations, especially in inducing an acute event in people with chronic condition and people taking oral anticoagulants.
"
Personal protective effect of wearing surgical face masks in public spaces,"Introduction
As of 3 November 2023, more than 76.9 million confirmed SARS-CoV-2 infections and more than 6.9 million deaths with covid-19 have been recorded worldwide.1Although public health and social measures, such as wearing face masks and school closures, were widely implemented to limit the spread of the virus,2evidence on the effectiveness and unintended consequences of these measures is limited.34
Systematic reviews of observational studies have reported an association between wearing face masks and lower risk of respiratory infections.56On the basis of findings from 10 randomised trials, however, the authors of a recent Cochrane review concluded that use of a face mask in the community had little or no effect on risk of developing a respiratory viral infection.7They also noted that adverse effects were rarely measured and poorly reported.7Several factors could explain the seemingly discrepant findings from observational studies and randomised trials, including the higher risk of bias inherent to observational studies, insufficient power of the randomised controlled trials, or low adherence to the intervention.7
We carried out a pragmatic randomised trial to evaluate the personal protective effect of wearing surgical face masks in public spaces over 14 days on self-reported symptoms consistent with respiratory infection, compared with not wearing face masks.
Methods
Study design and participants
We conducted a pragmatic parallel two arm individually 1:1 randomised superiority trial. Details on the rationale, design, and statistical analysis plan can be found elsewhere.89The trial was performed according to a published protocol, with exceptions (see Protocol Amendments section), and the principles outlined in the Declaration of Helsinki. We followed the Consolidated Standards of Reporting Trials (CONSORT) guidelines (see supplementary material, table 1).
The trial took place in Norway between 10 February 2023 and 27 April 2023. This was after the most acute phase of the covid-19 pandemic, but during the normal influenza season in the Nordic countries.10No public health or social measures were enforced by Norwegian authorities on the general population during the trial.
To be eligible for inclusion, individuals had to be aged at least 18 years, be willing to be randomly assigned to either wear face masks (intervention) or not wear face masks (control) in public spaces when near to other people for a period of 14 days, and provide written informed consent (online consent form). No exclusion criteria were applied.
Participants were recruited from multiple locations across Norway, using a diverse range of methods, with participant entry occurring predominantly in three phases. The initial phase was triggered by publicity through Norwegian national TV, radio, and various media channels, including paid print advertisements. The two following phases commenced after engaging two data collection firms that invited members of their survey panels to take part in our study.
Protocol amendments
One amendment was made to the protocol before the start of the trial. In the original protocol, we proposed evaluating the effectiveness of wearing either FFP3 respirators or surgical face masks, or not wearing a mask. We revised this design in a protocol update published on 6 December 2022, narrowing the scope of the study to investigate the effectiveness of surgical face masks only. This modification was done based on sample size considerations and to simplify the procedure for trial participants and pharmacy staff. After the 14 day trial period, but before unblinding, we introduced four modifications: two sensitivity analyses were added to deal with missing data; no exploratory analysis of immune status and covid-19 was to be carried out owing to low numbers of reported covid-19 infections; the main analysis was unadjusted rather than adjusted; and we decided to primarily report the main results as odds ratios rather than risks ratios. The supplementary material provides details of the adjusted analyses and risk ratios.
Intervention
Participants in the intervention arm were assigned to wear a surgical face mask when close to people in public spaces (eg, shopping centres, streets, and public transport) over a 14 day period (mask wearing at home or work was not mentioned). These participants collected a pack of 50 three ply, disposable, surgical face masks (type II/IIR, compliant with the EN 14683 standard) from their nearest pharmacy, provided at no cost using a unique verification email. The email also contained instructions on the proper use of face masks in line with Word Health Organization (WHO) recommendations.11Participants assigned to the control arm were to remain mask-free when close to people in public spaces.
Randomisation and blinding
Eligible participants were randomised 1:1 to the intervention or control arm. We used Nettskjema, an independent web based survey tool, to randomise participants using a computer generated pseudorandom sequence over which we had no influence.12Randomisation occurred after consent had been obtained and the baseline survey completed.
It was not possible to blind participants owing to the nature of the intervention. The researchers and study statistician were blinded to intervention allocation throughout the trial, and all main analyses were performed blinded. After analysis but before unblinding, we agreed how we would interpret the results, including possible explanations for the degree and direction of imbalance of missing outcome data.13
Procedures
After consent had been obtained, trial participants immediately completed an online questionnaire about sociodemographic and lifestyle factors, beliefs about face masks and risk of infection, and face mask use in the two weeks before the study period. On completion of the questionnaire, the participants were randomised and notified of the arm to which they had been assigned both in Nettskjema and by email (see supplementary material, table 2). The email encouraged participants to see their doctor for a covid-19 polymerase chain reaction (PCR) test if they experienced symptoms of respiratory symptoms or covid-19. In Norway, covid-19 PCR tests were analysed by laboratories that directly notified a national registry, the Norwegian Surveillance System for Communicable Diseases. We sent a follow-up questionnaire (see supplementary material, table 3) on day 17, three days after the 14 day intervention, asking participants about outcomes, use of public transport, testing behaviours, adherence to the face mask intervention, and any adverse events. Participants identified themselves using their national identification number and therefore could be linked to the Norwegian Surveillance System for Communicable Diseases, the National Population Register, and the Norwegian Immunisation Registry.
Outcomes
Nettskjema was used to record baseline and outcome data. The primary outcome was self-reported respiratory symptoms consistent with a respiratory infection. This outcome is a composite that required participants to give a positive response to having experienced symptoms of a cold or covid-19, and having experienced fever and one respiratory symptom (stuffy or runny nose, sore throat, coughing, sneezing, heavy breathing); or one respiratory symptom and at least two other symptoms (body ache, muscular pain, fatigue, reduced appetite, stomach pain, headache, loss of smell).
Secondary outcomes were self-reported positive covid-19 test results (confirmed by either PCR or rapid antigen self-test), positive covid-19 test result registered with the Norwegian Surveillance System for Communicable Diseases, self-reported sick leave (participants who answered “I don’t know” were coded as missing data in the complete case analysis), self-reported healthcare use for respiratory symptoms (eg, visit to the family doctor), and self-reported healthcare use for any injury. All outcomes from the follow-up questionnaire, Norwegian Surveillance System for Communicable Diseases, and Norwegian Immunisation Registry were binary and assessed over a 17 day period after randomisation (days 1-17 of the trial).
Statistical analysis
We calculated that a minimum of 2692 participants (1346 in each arm) would be required to detect a risk reduction of 30% from an assumed 10% risk of infection in the control arm to a 7% risk in the intervention arm, with a two sided α of 0.05 (significance criterion) and 80% power. The power calculation is described in detail elsewhere.8
We estimated marginal odds ratios14using unadjusted logistic regression for all outcomes following the intention-to-treat principle. The intention-to-treat analysis included all eligible participants who had provided consent. We report point estimates with two sided 95% confidence intervals and P values.
As prespecified, missing primary outcome data was accounted for using multiple imputation via chained equations.15We imputed and analysed 50 completed datasets and combined the estimates using Rubin’s rules.16The prespecified imputation model included intervention and all auxiliary variables collected in the baseline form.
In a non-prespecified sensitivity analysis, we estimated Manski-type bounds: best case and worst case bounds on intervention effect calculated by assuming that missing outcome data either maximally favoured wearing a surgical face mask or maximally favoured no face mask.17In another non-prespecified analysis, we also considered three less extreme scenarios using a method similar to the mean score method suggested by White et al (table 1).18
Scenario 1 assumes that the incidence of infection in the intervention arm is the same between those who did and did not drop out, but 50% lower for those who dropped out versus did not drop out in the control arm. Scenario 2 assumes that the incidence of infection in the control arm is the same between those who did and did not drop out, but 50% higher for those who dropped out versus did not drop out in the intervention arm. Scenario 3 assumes that the incidence of infection in the control arm is 50% lower for those who dropped out versus did not drop out, but 50% higher for those who dropped out versus did not drop out in the intervention arm.
The non-prespecified sensitivity analyses, including choice of scenarios, were decided on during the blinded assessment of the trial results.13We also performed complete case analyses, including participants with complete data at baseline and follow-up. Prespecified subgroup analyses were also performed to explore potential effect modification owing to sex (male or female), age (<30 years, 31-59 years, or ≥60 years), household includes children (yes or no), regular use of face masks (<50% of the time or ≥50% of the time), and beliefs about wearing face masks and risk of infection (reduce risk, no effect, or increase risk). We conducted all analyses using R version 4.2.2. No data monitoring committee was involved in the trial.
Patient and public involvement
Neither patients nor public representatives were involved in the design, implementation, or analysis of this study, largely due to time constraints as the trial was conducted in a window of opportunity during the influenza season. Participation in the trial was open to all adults (≥18 years) residing in Norway.
Results
Between 10 February 2023 and 27 April 2023, 5086 individuals read the consent form. Of these, 4647 (91%) provided consent, completed the baseline form, and were randomised (2371 to the intervention arm and 2276 to the control arm;fig 1). Of those randomised, we excluded 72 from the intention-to-treat analysis owing to ineligibility (<18 years; n=20) or withdrawal of consent (n=52). This left 4575 participants for inclusion in the intention-to-treat analysis (fig 1). At follow-up, 479 (20.7%) participants in the intervention arm and 295 (13.1%) in the control arm did not respond to the questionnaire (fig 1).Table 2shows the baseline characteristics of the participants.
Primary outcome
Overall, 163 (8.9%) participants in the intervention arm and 239 (12.2%) in the control arm self-reported respiratory symptoms. In the intention-to-treat analysis, which used the prespecified multiple imputation via chained equations analysis and included data from 4575 participants, the estimated effect on the primary outcome of self-reported respiratory symptoms was in favour of the face mask intervention (odds ratio 0.71, 95% CI 0.58 to 0.87; P=0.001; absolute risk difference −3.2%, 95% CI −5.2% to −1.3%; P<0.001) (table 3). The complete case analysis (n=3801) supported the findings of the main analysis (odds ratio 0.71, 95% CI 0.57 to 0.87; P<0.001; absolute risk difference −3.3, 95% CI −5.2 to −1.3; P<0.001). Supplementary material, tables 5 and 6 provide details of the adjusted analysis and relative risks. The results from the adjusted analysis were essentially identical to those of the main unadjusted analysis. The non-prespecified sensitivity analyses comparing different scenarios of the missing outcome data (table 3) suggest that the intervention was effective in scenarios 1 and 2 (odds ratio 0.76, 95% CI 0.62 to 0.92; P=0.006 and 0.79, 0.65 to 0.95; P=0.01, respectively) but not in scenario 3 (0.85, 0.70 to 1.03; P=0.08). The Manski-type bounds (table 3) covered all possible missingness mechanisms and therefore included large beneficial and detrimental effects.

Secondary outcomes
Overall, 42 participants, equally distributed between the two arms, self-reported covid-19 either by PCR or antigen test (odds ratio 1.07, 95% CI 0.58 to 1.98; P=0.82; absolute risk difference 0.1%, 95% CI −6.0% to 8.0%; P=0.82) (table 3).
The Norwegian Surveillance System for Communicable Diseases registered covid-19 test results for 37 participants in the control arm and 32 in the intervention arm (n=69 (1.5%); P=0.06 for difference in proportions). Two tests were positive in the control arm and none in the intervention arm (effect estimate and 95% CI not estimable owing to lack of events in the intervention arm;table 3).
In total, 144 (6.3%) participants in the control arm and 102 (4.5%) in the intervention arm reported needing healthcare during the trial. Of these participants, 29 (20%) in the control and 23 (23%) in the intervention arm reported that this was due to respiratory symptoms, whereas 40 (28%) participants in the control arm and 27 (26%) in the intervention arm reported other reasons.
Self-reported sick leave was equally distributed between the intervention and control groups (odds ratio 1.00, 95% CI 0.81 to 1.22; P=0.97). In total 29 participants answered, “I don’t know” and were coded as missing data in this analysis.
Adherence
Among participants in the intervention arm, 450 (25%) reported always wearing a face mask, 753 (41%) wearing face masks more than 75% of the time, 265 (14%) wearing face masks 75-50% of the time, and 357 (19%) wearing face masks less than 50% of the time. Among participants in the control arm, 1865 (95%) reported not wearing face masks.
At follow-up, the percentage of participants who reported commuting to work by public transport was comparable between the control and intervention arms (60% and 58%, respectively; P=0.32). Attendance at cultural events was, however, more frequent among participants in the control arm compared with those in the intervention arm (39% and 32%, respectively; P<0.001). Similarly, a larger percentage of participants in the control arm (62%) visited restaurants compared with those in the intervention arm (53%; P<0.001).
Adverse effects
In total, 155 participants (3.4%; 143 in the intervention arm) reported adverse effects, with 128 participants describing these experiences using the free text field in the questionnaire. The most reported adverse event (80 participants) was unpleasant comments from other people when wearing a face mask in public spaces and feeling “silly” being the only one wearing a face mask in public. Some participants (n=40) reported that wearing face masks was uncomfortable or tiring owing to difficulty breathing, fogging of glasses, and poor fit.
Subgroup analyses
Figure 2shows results for the prespecified subgroup analyses. The only analysis for which significant effect modification was estimated was for participants’ beliefs about wearing face masks and risk of infection (P=0.04 for interaction). A beneficial effect was estimated for participants who reported that they believed face masks reduced the risk of infection. Estimates for participants who reported that they believed face masks had no effect or increased risk were consistent with benefit, no effect, and harm. However, owing to the small number of events in these subgroups, the confidence intervals were wide and therefore these estimates lack precision.
Discussion
We found that wearing surgical face masks in public spaces reduced the risk of self-reported respiratory symptoms among Norwegian adults. The results support the claim that face masks may be an effective measure to reduce the incidence of self-reported respiratory symptoms consistent with respiratory tract infections, but the effect size was moderate. With a 12.2% risk of being infected and an absolute risk reduction of −3.2% (95% CI −5.2% to −1.3%), wearing a face mask reduced the risk to 8.9%, equivalent to around 3300 fewer infections per 100 000 people. Wearing face masks in public spaces was safe and generally well tolerated. The most reported adverse effects were unpleasant comments from other people.
Comparison with other studies
Our findings are consistent with the two randomised trials of face masks conducted during the covid-19 pandemic. Although the Danish face mask trial from 2020 was similar in many respects to ours, a major difference was it used a positive covid-19 result based on rapid antigen testing as the main outcome, whereas we relied on self-reporting of respiratory symptoms.19The Danish trial reported a point estimate similar to ours, but its findings were uncertain study owing to low statistical power (odds ratio 0.82, 95% CI 0.54 to 1.23). In a trial in Bangladesh, 600 rural villages were randomised to face mask promotion strategies or no intervention at community level.20The trial reported a smaller, but statistically significant reduction in symptomatic seroprevalence at nine weeks (adjusted prevalence ratio 0.91, 95% CI 0.82 to 1.00) in favour of face mask promotion.20Compared with the earlier face mask trials, our findings provide a more precise estimate of effect. When our study is compared with not only the earlier face masks studies to prevent covid-19, but also the face mask studies for influenza prevention, our findings indicate a somewhat larger effect.37
The proposed mechanisms of action for face masks include limiting droplet and aerosol transmission.21We decided to evaluate the effect of surgical face masks that were recommended by WHO during the covid-19 pandemic.22It has been presumed that FFP2 masks (N95 by American standards) protect people better than surgical masks because of their higher filtering rate, but randomised trials23and meta-analyses2425suggest that surgical masks offer similar protection to FFP2/N95 masks in healthcare settings. We did not, however, study mechanisms of action or effectiveness in healthcare settings.
Limitations of this study
Our trial has several limitations. Firstly, outcome data were missing for 13.7% and 20.7% of the participants in the control arm and intervention arm, respectively. This is broadly similar to the 19% missing outcome data in the Danish face mask trial.19We mitigated the impact of missing outcome data by prespecifying and using multiple imputation. Since it is plausible that the imbalance in missing data was due to outcome (ie, outcome data may be missing not at random), we performed non-prespecified sensitivity analyses to explore whether our main finding was robust to more extreme assumptions about missingness. Only under extreme and arguably implausible assumptions (eg, scenario 3 and the upper end of the Manski-type bounds) do the non-prespecified sensitivity analyses suggest that the intervention is not beneficial.13Moreover, the estimate from multiple imputation is similar to the complete case estimate.
Secondly, our primary outcome was self-reported rather than an objective outcome or based on immunological biomarkers. Although an outcome based on a PCR test result would have provided more specific information about infections, our primary outcome assessed symptoms that are important to both individuals and the public in a real world setting. For example, self-reported respiratory symptoms had important consequences during the covid-19 pandemic, such as denial of air travel. We encouraged our participants to take a covid-19 test when they felt unwell, but testing was neither mandatory nor recommended at the time of our trial, and the number of participants taking a test was low.
Thirdly, blinding of participants was not possible owing to the nature of the intervention and it is not intended in a pragmatic trial aiming to provide evidence in a real world setting. However, we cannot deny that awareness of intervention allocation may have introduced bias in reporting of symptoms either way. The group allocation might also have led to additional effects on participants’ behaviour. For instance, a higher proportion of participants in the control arm reported attending cultural events and restaurants during the trial period. Furthermore, some participants in the intervention arm reported feeling awkward wearing face masks when there was no official requirement to do so. This finding may imply that non-participants tended to keep a larger social distance from participants in the intervention arm, which could be seen as an inherent effect of wearing face masks.
Fourthly, our results apply under trial conditions and therefore caution is needed when generalising the findings to other settings. The duration of our trial period was only 14 days, so our findings might not apply to situations where face masks are used for longer periods. The most acute phase of the covid-19 pandemic was over when we conducted our study, but the trial period occurred during a normal influenza season in the Nordic countries.10
Fifthly, although we acknowledge environmental concerns associated with face mask usage (eg, manufacture and transport emissions, littering, landfill), these were not measured in our study.
Finally, we focused on evaluating the personal protective effects of wearing surgical face masks, specifically examining how effectively they safeguard wearers against infection. We did not study source control (preventing the spread of infection from the mask wearer to other people). The total effect of wearing face masks, including both personal protection and source control, therefore could be higher than our findings suggest.
Strengths of this study
Study strengths include the pragmatic randomised design, prespecified analyses, and transparent treatment of missing outcome data, which included non-prespecified sensitivity analyses.1718Trial staff were blinded. The outcomes we studied are likely important to individuals as well as from a public health perspective. People could participate regardless of where they lived in Norway, an approach that allowed broad recruitment, and probably increased the wider applicability of our findings. The results from our trial represent real world evidence on the effect of wearing surgical face masks. Similar trials should consider investigating other public health and social measures.
Future research should study face masks for source control. Research should also concentrate on the protective effectiveness of face masks for vulnerable populations, such as elderly people and individuals with pre-existing health conditions. For example, people with lung disease are typically at higher risk of respiratory tract infections, so the benefits of wearing face masks for personal protection need to be weighed against potential adverse effects, such as discomfort and breathing difficulties. It is vital to explore alternatives to single use masks that are sufficiently effective but minimise environmental harm, tackling the ecological problems linked to extensive and long term face mask use, as occurred during the covid-19 pandemic. Finally, future studies should consider including cost-benefit analyses.
Conclusion
Wearing surgical face masks is superior to not wearing surgical face masks in reducing the risk of respiratory symptoms over 14 days. The effect size was moderate, but wearing a face mask is a simple intervention with low burden and of relatively low cost and is one of several public health and social measures that may be worth considering for reducing the spread of respiratory infections.
The effectiveness of face masks as a protective measure against infection is uncertain
Observational studies suggest that face masks reduce the risk of respiratory tract infections
Findings from randomised trials are, however, highly uncertain owing to methodological limitations such as insufficient statistical power
Our pragmatic trial provides evidence that wearing surgical face masks in public spaces reduces the incidence of self-reported respiratory symptoms consistent with respiratory infections in real world settings
Unlike most earlier trials of face mask, our study was sufficiently powered
Similar trials can and should be conducted for other public health and social measures
","Objective: To evaluate the personal protective effects of wearing versus not wearing surgical face masks in public spaces on self-reported respiratory symptoms over a 14 day period.
Design: Pragmatic randomised superiority trial.
Setting: Norway.
Participants: 4647 adults aged ≥18 years: 2371 were assigned to the intervention arm and 2276 to the control arm.
Interventions: Participants in the intervention arm were assigned to wear a surgical face mask in public spaces (eg, shopping centres, streets, public transport) over a 14 day period (mask wearing at home or work was not mentioned). Participants in the control arm were assigned to not wear a surgical face mask in public places.
Main outcome measures: The primary outcome was self-reported respiratory symptoms consistent with a respiratory infection. Secondary outcomes included self-reported and registered covid-19 infection and self-reported sick leave.
Results: Between 10 February 2023 and 27 April 2023, 4647 participants were randomised of whom 4575 (2788 women (60.9%); mean age 51.0 (standard deviation 15.0) years) were included in the intention-to-treat analysis: 2313 (50.6%) in the intervention arm and 2262 (49.4%) in the control arm. 163 events (8.9%) of self-reported symptoms consistent with respiratory infection were reported in the intervention arm and 239 (12.2%) in the control arm. The marginal odds ratio was 0.71 (95% confidence interval (CI) 0.58 to 0.87; P=0.001) favouring the face mask intervention. The absolute risk difference was −3.2% (95% CI −5.2% to −1.3%; P<0.001). No statistically significant effect was found on self- reported (marginal odds ratio 1.07, 95% CI 0.58 to 1.98; P=0.82) or registered covid-19 infection (effect estimate and 95% CI not estimable owing to lack of events in the intervention arm). Self-reported sick leave was equally distributed between the intervention and control groups (marginal odds ratio 1.00, 0.81 to 1.22; P=0.97).
Conclusion: Wearing a surgical face mask in public spaces over 14 days reduces the risk of self-reported symptoms consistent with a respiratory infection, compared with not wearing a surgical face mask.
Trial registration: ClinicalTrials.govNCT05690516.
"
Learning implementation of a guideline based decision support system to improve hypertension treatment in primary care in China,"Introduction
Hypertension is the leading modifiable risk factor for cardiovascular disease and death, affecting an estimated 1.3 billion people worldwide in 2019.123Based on the data from a nationwide survey in 2012-15, around 245 million Chinese adults had hypertension, and the treatment and control rates were low at 41% and 15%, respectively.4The use of evidence based antihypertensive treatments, a key strategy for lowering blood pressure and reducing cardiovascular risk,5678is suboptimal in China. Inappropriate prescribing behaviour and clinical inertia are common.91011Among patients treated for hypertension, 68% were taking only one drug,4and about 8% of drugs used were not recommended by guidelines.11
Improving the performance of primary care doctors—the mainstay of the hypertension care workforce in China—is a key first step towards improving blood pressure control and patient outcomes.9However, China is facing multiple challenges to achieve this goal, given the constrained and unevenly distributed resources.91012In 2021, primary care doctors provided half of the outpatient care (4.3 billion visits) in China.13Despite such heavy workloads, they often have inadequate training. In 2021, 56% of doctors in community health centres had an education level below medical college.14Continuing education and financial incentives for these doctors are also insufficient.10An affordable and scalable strategy is needed to improve blood pressure management.
A clinical decision support system (CDSS), characterised by integrating patient data and guideline recommendations at the point of care, has the potential to improve the performance of primary care doctors.915Despite its promise of delivering scalable and sustainable care, the CDSS has been rarely adopted in China.10Previous studies in resource constrained countries such as India have provided evidence of the effectiveness of CDSS for the management of hypertension in primary care settings.1617These studies tested CDSS’s effect on blood pressure change and showed inconsistent results.1617Few studies have been primarily designed to systematically and quantitatively assess the effect of CDSS on guideline accordant treatment prescribing.18
We developed a CDSS that could generate a tailored antihypertensive regimen and tested whether the use of a CDSS could improve the provision of guideline accordant antihypertensive treatment compared with usual care in Chinese primary care settings. We also assessed the impact of the CDSS on blood pressure change and blood pressure control.
Methods
Trial design
The Learning Implementation of Guideline-based decision support system for Hypertension Treatment (LIGHT) trial was a pragmatic, cluster randomised trial conducted in 94 primary care practices in four urban regions of China: Luoyang (central China), Jining (east China), and Shenzhen (south China, including two regions). To ensure the wide feasibility of using electronic health record data and the CDSS, the trial was designed to recruit primary care practices in urban regions. Details of the trial rationale, design, and methods have been described previously.19In each region, after screening during a three month baseline period, eligible primary care practices were randomised to receive either CDSS guided treatment or usual care. Eligible patients were enrolled during the first three months after site randomisation. They were asked to attend the clinic in the primary care practice at least every three months, as recommended by the National Basic Public Health Services Programme (see supplementary appendix section S2)20and Chinese Guidelines for Hypertension Prevention and Management in Primary Care.21At each visit, a 1-3 month supply of antihypertensive drugs could be dispensed.22The duration of follow-up was nine months. Owing to the covid-19 pandemic in 2020, we extended the scheduled follow-up period in each region for about one month to four months according to the policies of local Centers for Disease Control and Prevention.
Recruitment and participants
Primary care practices were eligible for the trial if they stored at least one agent from each of four classes of antihypertensive drugs for hypertension treatment (angiotensin converting enzyme inhibitors or angiotensin receptor blockers, beta blockers, calcium channel blockers, diuretics); had computers and internet access to use the bespoke electronic health record for this trial; and had at least 100 local patients registered for hypertension management in the National Basic Public Health Services Programme.20
Patients were eligible for the trial if they had an established diagnosis of hypertension and were registered for hypertension management in the National Basic Public Health Services Programme20; had a systolic blood pressure <180 mm Hg and diastolic blood pressure <110 mm Hg; and were taking 0-2 classes of antihypertensive drugs. The diagnosis of hypertension was based on a systolic blood pressure ≥140 mm Hg or diastolic blood pressure ≥90 mm Hg, or both, from three measurements on different days, or the patients were taking antihypertensive drugs according to the Chinese Guidelines for Hypertension Prevention and Management in Primary Care.21Patients were excluded if they had a self-reported history of coronary artery disease, chronic kidney disease, or heart failure, or were intolerant to two or more classes of antihypertensive drugs (see supplementary appendix section S3).
Interventions
The CDSS was developed using the hypertension treatment guideline for primary care in China,21which is generally consistent with international guidelines.2324Based on the measured blood pressure, current use of antihypertensive drugs, specific clinical indications, contraindications, and patient reported drug adverse effects or intolerance, the CDSS would generate recommendations to escalate treatment until patients achieved blood pressure control; recommend guideline accordant regimen, including the class and dose of antihypertensive drugs; and optimise treatment for patients with specific indications (eg, diabetes), intolerance, or contraindication. To ensure feasibility and patient safety, the CDSS would not recommend a regimen for patients with complications such as coronary artery disease, chronic kidney disease, or heart failure.
Supplementary appendix section S4 depicts use of the CDSS. Before prescribing took place during each visit, the doctors had to click an icon to obtain CDSS recommendations before receiving a regimen. We encouraged doctors to discuss the CDSS recommended regimen with patients. The final prescription was up to the shared decision between doctors and patients. If doctors did not follow the recommendations, a pop-up alert reminded them to adjust the prescription. If doctors refused to follow the alert, the relevant reasons were collected. Additional alerts were triggered if prescriptions involved contraindicated drugs, under-dosage, or over-dosage. Doctors in the control group used the same electronic health record to collect data but did not receive recommendations or alerts. Prescribing decisions were based on their knowledge and experience.
Randomisation and blinding
Primary care practices were randomised to intervention group or control group (1:1 ratio) using a central computerised randomisation program. Randomisation was performed in four regions sequentially (see supplementary appendix section S5) and stratified by the proportion of hypertension related visits with guideline accordant treatment during the baseline period, and the characteristics of the primary care practice (including the hospital to which the practice was affiliated, the type of primary care practice, or district; see supplementary appendix section S6 for details of the stratifying factors for each region). Given the nature of the intervention, it was not possible to blind the practice allocation to the doctors. Participants were not informed of their allocation, as consent for the study was waived. To minimise the bias for outcome analyses, the independent statistician was fully blinded to practice allocation.
Effectiveness outcomes
The primary outcome was the proportion of hypertension related visits at which appropriate treatment was provided. Appropriate treatment was defined as a prescription in line with prespecified specifications (see supplementary appendix section S7). This specification was based on the current countrywide guideline21and adapted to the treatment scenarios for this trial. Briefly, these included initiating and titrating antihypertensive treatments for patients with inadequate blood pressure control, switching to guideline accordant antihypertensive drugs, and refraining from prescribing contraindicated drugs. The appropriateness of the prescription was automatically assessed using a computerised algorithm. Secondary outcomes included the proportion of hypertension related visits with acceptable treatment, defined as either appropriate or non-appropriate but with acceptable reasons for failing to titrate antihypertensive treatment (acceptable reasons were self-reported and self-measured home blood pressure within the acceptable range, or possible antihypertensive treatment related events (syncope, injurious fall, hypotension, or bradycardia)); outcome measures with average change in systolic blood pressure from baseline to the last scheduled follow-up; and the proportion of participants with controlled blood pressure (<140/90 mm Hg) at the last scheduled follow-up. An exploratory outcome was the proportion of participants with vascular events (a composite of cardiac death, non-fatal stroke, and non-fatal myocardial infarction).
Safety outcome
The safety outcome was patient reported antihypertensive treatment related events of syncope, injurious fall, symptomatic hypotension or systolic blood pressure <90 mm Hg, or bradycardia (see supplementary appendix section S8).
Data collection
We provided a bespoke electronic health record for each primary care practice. Baseline characteristics of practices and participants were collected through questionnaires in this electronic health record. For sites not equipped to use electronic health records, we installed the bespoke electronic health record in their computers. For sites equipped to use local electronic health records, we embedded our electronic health record into the existing one as a module to improve workflow. At each visit, blood pressure was measured twice with an automated sphygmomanometer (Omron HBP-1300) after five minutes of rest with an interval of 1-2 minutes, and the mean of two measurements was recorded. In the electronic health record, doctors recorded the measured blood pressure, medical history, antihypertensive drug use, self-measured home blood pressure (if provided), patient reported treatment related adverse events (see supplementary appendix section S8), drug intolerance, drug adherence (see supplementary appendix section S9), and vascular events.
Sample size
We conducted a pragmatic randomised trial to assess the effectiveness of a CDSS in different settings. Owing to the lack of similar previous studies for reference, we estimated statistical power based on the number of potentially eligible sites in the Luoyang region initially. We assumed that at least 10 sites would be needed in the intervention group and 10 in the control group and that the baseline appropriate rate of treatment would be 55%. With a moderate intra-site correlation of 0.05 and a within patient correlation of 0.1, under the maximum type I error of two sided α=0.05 and statistical power of 90%, we determined that three hypertension related visits per patient would be needed for 50 patients at each site to detect an 18% absolute difference in the proportion of appropriate treatments (ie, 55% appropriate treatments in the control group, 73% in the intervention group). Subsequent enrolment and randomisation were to be carried out in four regions involving 94 participating sites. Under the same assumptions as above and maximum 25% loss to follow-up of patient at each visit, we determined that about 12 000 patients would provide at least 90% power to detect a difference of 4% in the appropriate treatment rate—an average effect of CDSS use reported in a previous study.25
Statistical analysis
All analyses were based on the intention-to-treat principle. After cluster randomisation of primary care practices, all the data of enrolled patients from each visit during the study period were included for analyses, regardless of whether the primary care practice or patient completed the study. We summarised the characteristics of the practices, doctors, and participants by study group. Standardised differences between the two groups were calculated by generalised mixed effects regression models, with sites as the random effect. A standardised difference >0.25 indicated imbalance.2627The analysis unit for the appropriate or acceptable treatment rates was visit, and for the other outcomes was participant. We used generalised linear mixed effect regression models with a logit and identity link function for the binary and continuous response variables, respectively. In these models, we included both practice and participant as random intercepts for visit level analysis and included practice as random intercepts for participant level analysis. The estimates of the intervention effect were obtained by averaging over the random effects. The region, baseline rate of appropriate antihypertensive treatment (median or higher or less than median), and calendar time were included as fixed effects. No data were missing for covariates. The analysis of the primary outcome and secondary outcome of acceptable appropriate treatment rate was based on available data of visits. The analyses of average change in systolic blood pressure and the proportion of participants with controlled blood pressure were based on the last scheduled visit, and without imputation. As a sensitivity analysis, we conducted multiple imputation for the missing blood pressure data of the last scheduled follow-up. We also conducted an analysis of blood pressure control defined as a systolic blood pressure <130 mm Hg and a diastolic blood pressure <80 mm Hg. Subgroup analyses were performed by implementation region; baseline tertiles of site appropriate treatment rates; education level of doctors; and age, sex, education level, use of antihypertensive drugs, and baseline systolic blood pressure of participants. In addition, we conducted additional analyses for outcomes among the patients with baseline blood pressure ≥140/90 mm Hg.
We considered a P value <0.05 (two sided test) to be statistically significant for the primary outcome. We also used a significance level of 0.05 for other outcomes, but these findings should be interpreted with caution as the analyses were not statistically powered. Additional details on statistical analyses are provided in the statistical analysis plan and in supplementary appendix sections S10 and S11 and tables S2 and S3. SAS version 9.4 (SAS Institute) was used for all statistical analyses.
Patient and public involvement
No patients were involved in the design of the study or review of our manuscript. We were unable to involve patients and members of the public in this study owing to lack of funding and expertise in conducting patient and public involvement focus groups. Although no patients or members of the public were directly involved in this study, the clinical investigators’ clinical practice with patients informed the design and rationale of this study.
Results
Characteristics of practices and participants
A total of 94 primary care practices were randomised (46 to CDSS and 48 to usual care), of which two in the CDSS group withdrew after enrolment. During the baseline period, median appropriate treatment rates of primary care practices in the CDSS group and usual care group were 63.0% and 60.0%, respectively. Between August 2019 and 2021, 12 137 participants were enrolled in the trial: 5755 in the CDSS group (median 123 participants per cluster) and 6382 participants in the usual care group (median 135 participants per cluster). The median duration of follow-up was 11.6 months, during which a total of 23 113 visits (median 4.0 visits per participant) in the CDSS group and 27 868 visits (median 4.0 visits per participant) in the usual care group were included. Overall, 86.0% (4952/5755) of participants in the CDSS group and 91.6% (5845/6382) in the usual care group completed follow-up (fig 1, also see supplementary appendix table S4).
Baseline characteristics of primary care practices, doctors, and participants were well balanced between the two groups (table 1). The mean age of the doctors was 46 (standard deviation (SD) 12) years, 59.1% were women, 91.0% had an educational attainment of medical college level or higher, and all were licensed. The mean age of the participants was 61 (SD 13) years and 42.5% were women. The mean systolic blood pressure of the participants was 134.1 (SD 14.8) mm Hg, and 92.3% were using at least one class of antihypertensive drug.
Primary outcome
The proportion of hypertension related visits with appropriate treatment was significantly higher in the CDSS group (77.8%v62.2%; absolute difference 15.2 percentage points (95% confidence interval (CI) 10.7 to 19.8); P<0.001; odds ratio 2.17 (95% CI 1.75 to 2.69); P<0.001) compared with the usual care group (table 2,fig 2, also see supplementary appendix section S11). Subgroup analyses showed that the CDSS improved appropriate treatment across regions and subgroups (fig 3).
Secondary and exploratory outcomes
The proportion of hypertension related visits with acceptable treatment was higher in the CDSS group (84.7%v70.4%, absolute difference 12.6 percentage points (95% CI 8.1 to 17.2); P<0.001) than in the usual care group. Participants in the CDSS group had a 1.6 mm Hg (95% CI −2.7 to −0.5) greater reduction in systolic blood pressure (−1.5 (SD 16.2) mm Hgv0.3 (SD 16.1) mm Hg; P=0.006) and a non-significant improvement in blood pressure control of 4.4 percentage points (95% CI −0.7 to 9.5) (69.0%v64.6%; P=0.07) compared with those in the usual care group. When analysis was limited to those with baseline blood pressure ≥140/90 mm Hg, participants in the CDSS group had a 1.9 mm Hg (95% CI −3.6 to −0.3; P=0.02) greater reduction in systolic blood pressure and an improvement in blood pressure control of 5.0 percentage points (95% CI −1.0 to 11.1; P=0.08) compared with those in the usual care group (see supplementary appendix table S5). After multiple imputation for missing blood pressure data from the last scheduled follow-up, the use of CDSS significantly improved the rate of blood pressure control of 4.1 percentage points (95% CI 3.5 to 4.8; P<0.001) (see supplementary appendix table S6). In addition, when using a definition for blood pressure control as systolic blood pressure <130 mm Hg and diastolic blood pressure <80 mm Hg, the effectiveness of the CDSS on blood pressure control was consistent with the main analysis (see supplementary appendix table S7). The proportion of patients with good adherence to prescribed drugs in the CDSS group was lower than in the usual care group (3343/5706 (58.6%)v4642/6291 (73.8%), standardised difference 0.33). CDSS use resulted in better blood pressure control among patients with good drug adherence, but not among those with poor drug adherence (see supplementary appendix table S8). The proportion of participants reporting vascular events between the two groups was similar (1.0%v0.7%; absolute difference 0.1 percentage points (95% CI −0.3 to 0.5; P=0.43) (table 2).
Safety outcomes
The rates of patient reported antihypertensive treatment related adverse effects were low and similar between the two groups (see supplementary appendix table S9).
Discussion
In this large, cluster randomised trial conducted in 94 primary care practices in China, the use of a CDSS led to a 15 percentage point absolute increase in the proportion of hypertension related visits with guideline accordant prescribed treatment, and a modest reduction in systolic blood pressure of 1.6 mm Hg. Patient reported treatment related adverse events were rare. The CDSS appeared effective across regions and sites with varying baseline guideline accordant treatment rates, and in settings with different educational attainment of the primary care doctors.
Comparison with other studies
In this trial we quantified the effectiveness of CDSS for enhancing primary hypertension care using comprehensive guideline based metrics. The observed effect of the CDSS corresponded to an estimated 26% relative increase28in guideline accordant treatment. Previous studies have assessed the effectiveness of CDSSs on a range of process outcomes for hypertension care, including drug prescribing, clinical tests completed, and number of clinic visits, and reported absolute 2 to 20 percentage point improvements in the corresponding process outcome,29303132but few provided evidence on the extent of guideline based prescribing. A trial conducted in 14 US hospitals, assessed a CDSS designed to improve appropriate drug prescribing.18The CDSS could recommend a pharmacologically appropriate drug class based on several patient characteristics, including age, race, and disease (ie, diabetes, coronary artery disease, and heart failure) but did not provide dosage to intensity treatment. The CDSS in the current study was not designed to guide treatment for coronary artery disease or heart failure for two reasons—firstly, because patients with these conditions are more likely to follow regimens provided by doctors from hospitals rather than primary care practices, and, secondly, because of concern about the complexity of an individualised and specific algorithm and patient safety.
This trial provided insights into the effectiveness of CDSS as a single factor intervention for improving the control of hypertension in primary care. Several previous CDSS studies focused on change in blood pressure, and the results were mixed,161732partly because blood pressure could be affected by multiple factors. In the current study, the marginal effect of using a CDSS on blood pressure control should be interpreted in light of several points. Unlike other studies that enrolled patients with uncontrolled blood pressure,1617this trial included two thirds of patients with blood pressure under control at baseline. Therefore, the overall effect of a CDSS on blood pressure could have been diluted. This point can be supported by the finding of the subgroup analysis, which showed a larger CDSS related reduction in systolic blood pressure among patients with uncontrolled blood pressure than those with controlled blood pressure at baseline. Moreover, the potential benefit of the CDSS on blood pressure control in this study may be offset by poor adherence of patients to prescribed drugs, as we found that CDSS use led to significant blood pressure control among patients with good adherence, but not in the overall population. As this trial was primarily aimed at enhancing doctors’ prescribing behaviour through CDSS use, it did not involve multicomponent interventions. In future research, integrating a patient engagement approach with CDSS use for enhancing shared decision making might improve patients’ adherence to treatment and their long term outcomes.3334Furthermore, the benefit of some appropriate treatments, such as switching to angiotensin converting enzyme inhibitors or angiotensin receptor blockers for patients with diabetes and blood pressure under control, was more reflected by the improvement of long term outcomes rather than reduction in blood pressure. Notably, a 4.4 percentage point improvement in blood pressure control rate, as observed in this study, could translate into a considerable reduction of cardiovascular mortality and morbidity in larger populations.35
Strengths of this study
The overall design of this study was distinguished by several unique features, which were considered to improve implementation, acceptance, and generalisability of CDSS use in primary care settings. Firstly, the CDSS was integrated into the clinical workflow by being embedded into the electronic health record that collected the relevant information for usual hypertension care. Without interruption to routine clinical care, simple data collection minimised the burden of CDSS use. As primary care doctors need to click an icon only once, the CDSS could provide immediate, specific, and tailored recommendations at the point of care without unnecessary alerts. These features rendered the CDSS more acceptable to primary care doctors,15even if they had little experience of using a CDSS.
Secondly, we included primary care practices, involving a large sample of hypertensive patients, across three urban cities with a broad range of baseline appropriate treatment rates and doctors with different levels of education. The large sample size enabled us to assess the effectiveness of CDSS across regions and various subgroups. We observed the effects of CDSS were different across regions, partly owing to the regional variations in appropriate treatment and blood pressure control rates at baseline. Nevertheless, substantial improvement in appropriate treatment was found in the overall population after accounting for heterogeneity in different sites from four regions, suggesting that CDSS had been well implemented and accepted in diverse clinical settings. Subgroup analyses indicated that CDSS could be generalised to other similar primary care settings in the future, particularly for underserved regions with poorer blood pressure control.
Thirdly, the CDSS was designed to serve as a tool to provide a decision aid for improving the process of shared decision making for hypertension care. The doctors were not mandated to follow recommendations generated from CDSS—the final prescription was based on a shared decision between doctors and patients. The reasons for CDSS recommendations not being followed were collected. The findings may provide valuable information for improving adherence to CDSS recommendations in the future.
Limitations of this study
Some limitations need to be considered. Firstly, we used a cluster randomised design to avoid contamination across groups. Doctors in the control group were, however, aware of the purpose of the trial and thus this might have had a positive impact on provision of usual care. If that was the case, the effect of CDSS might be underestimated. Secondly, this trial was conducted in primary care practices in urban China where most doctors have attained an education level of medical college or higher; the effectiveness of the CDSS in rural areas remains to be confirmed. It may be speculated that the CDSS would be equally or more effective in rural areas, since we observed similar improvements in practices with lower appropriate treatment rates at baseline or among doctors with lower education levels. Thirdly, the use of the CDSS was restricted to participants without coronary artery disease, chronic kidney disease, or heart failure, which limited the generalisability of the results. Fourthly, the trial was unable to evaluate the sustainability of the CDSS intervention and was underpowered to assess its effectiveness on cardiovascular events. Fifthly, we did not assess safety outcomes such as electrolyte disorders or renal dysfunction. Finally, we did not collect doctors’ satisfaction with the intervention, which could help its implementation outside of the trial. This will be investigated in future studies.
Policy implications
Our study has implications for strengthening the primary care systems in China and other resource restricted regions or countries with heavy burdens of cardiovascular disease.363738Unlike other multicomponent intervention strategies394041that inherently require more resources to implement and are more complex to scale-up, CDSS could potentially serve as a low cost, efficient, scalable, and sustainable means to improve access and equity to high quality care of hypertension. Primary care practices included in this study might differ in some ways from those in other regions of China and other resource restricted regions or countries, such as the availability of health information systems, antihypertensive drugs, and qualification of primary care providers. Despite these limitations, in rural areas where computers are not equipped to use a CDSS, the CDSS could support decision making through mobile phones or tablets, where highly compatible electronic health records for hypertension management can be installed. Another difference is that in this study the antihypertensive drugs were dispensed every 1-3 months, but in many primary care settings from other resource restricted regions or countries, a shorter prescription period may be common since a reliable supply chain of antihypertensive drugs may be difficult owing to problems with storage capacity.42CDSS could be more useful when more frequent visits and prescriptions are needed. Of note, CDSS in this study was used by primary care doctors with higher education levels. As non-doctor health workers play an important role in task sharing or shifting strategies to mitigate the under-qualification and deficiency problems of primary care doctors,4143future investigations could assess the effectiveness of CDSS among these staff with appropriate training and supervision.
Conclusions
The use of CDSS for hypertension management statistically significantly improved guideline accordant primary care for antihypertensive treatment and led to a modest reduction in blood pressure. This strategy offers a promising approach to delivering high quality care for hypertension efficiently and safely, particularly for resource constrained regions with a heavy burden of cardiovascular diseases like China.
Hypertension care in China is suboptimal, with a large geographical variation
Improving the performance of primary care doctors by ensuring their prescribing behaviour follows current guidelines is a key step towards improving blood pressure control and patient outcomes
Although clinical decision support systems have the potential to improve hypertension care in a low cost and efficient way, evidence for their effectiveness in improving the use of guideline accordant antihypertensive treatment in primary care is limited
The use of a clinical decision support system in primary care in China statistically significantly improved guideline accordant antihypertensive treatment and led to a modest reduction in blood pressure
Web extraExtra material supplied by authors
","Objective: To evaluate the effectiveness of a clinical decision support system (CDSS) in improving the use of guideline accordant antihypertensive treatment in primary care settings in China.
Design: Pragmatic, open label, cluster randomised trial.
Setting: 94 primary care practices in four urban regions of China between August 2019 and July 2022: Luoyang (central China), Jining (east China), and Shenzhen (south China, including two regions).
Participants: 94 practices were randomised (46 to CDSS, 48 to usual care). 12 137 participants with hypertension who used up to two classes of antihypertensives and had a systolic blood pressure <180 mm Hg and diastolic blood pressure <110 mm Hg were included.
Interventions: Primary care practices were randomised to use an electronic health record based CDSS, which recommended a specific guideline accordant regimen for initiation, titration, or switching of antihypertensive (the intervention), or to use the same electronic health record without CDSS and provide treatment as usual (control).
Main outcome measures: The primary outcome was the proportion of hypertension related visits during which an appropriate (guideline accordant) treatment was provided. Secondary outcomes were the average reduction in systolic blood pressure and proportion of participants with controlled blood pressure (<140/90 mm Hg) at the last scheduled follow-up. Safety outcomes were patient reported antihypertensive treatment related events, including syncope, injurious fall, symptomatic hypotension or systolic blood pressure <90 mm Hg, and bradycardia.
Results: 5755 participants with 23 113 visits in the intervention group and 6382 participants with 27 868 visits in the control group were included. Mean age was 61 (standard deviation 13) years and 42.5% were women. During a median 11.6 months of follow-up, the proportion of visits at which appropriate treatment was given was higher in the intervention group than in the control group (77.8% (17 975/23 113)v62.2% (17 328/27 868); absolute difference 15.2 percentage points (95% confidence interval (CI) 10.7 to 19.8); P<0.001; odds ratio 2.17 (95% CI 1.75 to 2.69); P<0.001). Compared with participants in the control group, those in the intervention group had a 1.6 mm Hg (95% CI −2.7 to −0.5) greater reduction in systolic blood pressure (−1.5 mm Hgv0.3 mm Hg; P=0.006) and a 4.4 percentage point (95% CI −0.7 to 9.5) improvement in blood pressure control rate (69.0% (3415/4952)v64.6% (3778/5845); P=0.07). Patient reported antihypertensive treatment related adverse effects were rare in both groups.
Conclusions: Use of a CDSS in primary care in China improved the provision of guideline accordant antihypertensive treatment and led to a modest reduction in blood pressure. The CDSS offers a promising approach to delivering better care for hypertension, both safely and efficiently.
Trial registration: ClinicalTrials.govNCT03636334.
"
Intraosseous versus intravenous vascular access in upper extremity among adults with out-of-hospital cardiac arrest,"Introduction
Out-of-hospital cardiac arrest affects millions of people worldwide annually.1Bystander cardiopulmonary resuscitation and early defibrillation have been shown to improve outcomes, especially for patients with a shockable rhythm.23However, a larger proportion of patients worldwide have non-shockable rhythms. Despite the expected lower impact, timely vascular access is also vital to facilitate the prompt administration of drugs, fluid resuscitation, and other interventions, all of which play a crucial role in resuscitation. Peripheral intravenous and intraosseous access are two of the most common types of vascular access in pre-hospital settings, and both have been an integral part of advanced life support.45Guidelines for resuscitation suggest prioritising the intravenous route for administering drugs during out-of-hospital cardiac arrest and using the intraosseous route as a back-up when intravenous access is not possible.45
Previous retrospective studies have attempted to compare outcomes between intraosseous and peripheral intravenous vascular access in adult patients with out-of-hospital cardiac arrest.6789101112131415161718None of these studies has reported a statistically significant association between intraosseous access and improved patient outcomes after resuscitation, and some studies have further indicated that intravenous access leads to better survival outcomes.7111214161718However, all these studies have common limitations; hence, objective interpretation of the results and drawing definitive conclusions are challenging. In some studies, the choice of vascular access was at the discretion of the healthcare providers,789whereas intraosseous access was applied only when the intravenous route was unsuccessful in other studies.1017Under these circumstances, the patients who received intraosseous access possibly had worse conditions and inherited resuscitation time bias.19Furthermore, the anatomical sites for intraosseous access placement varied. Although the humeral intraosseous route provides more rapid drug delivery to the right ventricle,2021its use may be less prevalent in some health systems.922A recent meta-analysis of nine observational studies found no significant association between intraosseous versus intravenous access and clinical outcomes.23Moreover, reviews have highlighted the heterogeneity across the observational studies and emphasised significant concerns about selection bias and confounding.2324
Compared with the intravenous route, the intraosseous route offers quicker and more reliable access in cardiac arrest situations with collapsed peripheral veins,25resulting in a higher first attempt success rate and significantly shorter time to medication than conventional intravenous access.2627However, the clinical significance of variation in vascular access remains uncertain, underscoring the need for further high quality prospective studies. In this study, we hypothesised that adult patients with out-of-hospital cardiac arrest who underwent intraosseous vascular access attempts would have higher rates of survival at hospital discharge than those who underwent intravenous access.
Methods
Trial design
The multicentre, clustered, pragmatic, randomised controlled VICTOR (Venous Injection Compared To intraOsseous injection during Resuscitation of patients with out-of-hospital cardiac arrest) trial was conducted in four advanced life support ambulance service teams in Taipei City from 6 July 2020 to 30 June 2023. Owing to the covid-19 pandemic, recruitment was temporarily suspended between 20 May 2021 and 31 July 2021. We recruited all emergency medical technicians-paramedics in the four participating advanced life support ambulance service teams belonging to Taipei City Fire Department, and all emergency responsibility hospitals in Taipei City (supplementary appendix 1). Each paramedic had completed 1280 hours of training that was regulated by the Taiwan Ministry of Health and Welfare.28They were trained and authorised to perform both intravenous and intraosseous procedures in pre-hospital settings. All paramedics in the trial took at least a four hour course comprising one hour of lectures and three hours of hands-on practice sessions on intraosseous insertion. The emergency medical service configuration in Taiwan and Taipei city was briefly described in supplementary appendix 2. Eligible patients were enrolled automatically under the waiver of informed consent at the time of the study. Informed consent was obtained from the survivors or their legal representatives after enrolment.
Patient population
Adult (age ≥20 years) patients with out-of-hospital cardiac arrest treated by participating emergency medical service agencies were eligible for inclusion. The exclusion criteria were signs of obvious death (presence of rigor mortis or livor mortis); family’s do-not-resuscitate order at the scene; contraindications for intravenous access (presence of local infection, burns, compromised skin, or arteriovenous fistula formation at the intended entry site) or intraosseous access (signs of infection at the intended entry site, possible fracture of the extremity, or prosthesis or orthopaedic procedure near the insertion site); return of spontaneous circulation achieved before the intervention; cardiac arrest during transportation to the hospital; vascular access established before the arrival of the trial trained paramedic; and other reasons for exclusion: traumatic out-of-hospital cardiac arrest, known or suspected pregnancy, known or suspected age <20 or >80 years, cancelled ambulance call, and patient transported to the hospital before the arrival of emergency medical technicians-paramedics.
Randomisation and intervention
The trial used cluster randomisation based on biweekly periods from 6 July 2020 to 30 June 2023. Four participating advanced life support ambulance service teams were assigned to either intravenous or intraosseous interventions biweekly, as illustrated in supplementary figure A. The allocation sequence was generated by a computer using code created by a research statistician, and the study centre instructed the clusters to change to either the intraosseous or intravenous intervention according to the sequence of random allocation. To balance the cases that received successful vascular access, the clusters were randomised in a 1:2 allocation ratio to intervention (intraosseous) versus control (intravenous), as intraosseous attempts were considered twice as likely to be successful in a previous study.29
Patients in the intervention group received a mechanical intraosseous puncture (EZ-IO, Teleflex), and the control group received an intravenous puncture. The protocol in the intraosseous group was limited to an attempt at the humeral bone; patients in the control group received intravenous puncture in the upper extremities. Trained paramedics were limited to a maximum of one attempt at intraosseous insertion or two attempts at intravenous procedures. Before the trial, emergency medical technicians in Taiwan were not authorised to perform intraosseous insertion. Given that intraosseous insertion is in its pilot phase, and scenes of out-of-hospital cardiac arrest and hospitals in Taiwan are not far apart, the protocol required that if the initial access attempt failed, the patient would be promptly transported to the hospital without attempting alternative access methods, mirroring the standard practice before the trial.
After vascular access was established, 1 mg of adrenaline (epinephrine), followed by 10 mL of normal saline, was rapidly pushed through the access. Except for vascular access, all resuscitations performed at the scene, including delivery of shocks and administration of anti-arrhythmic drugs, followed the local standard protocol (supplementary appendix 3).
Clinical outcomes
The primary outcome was survival to hospital discharge. The secondary outcomes were return of spontaneous circulation; survival to hospital admission, surrogated by sustained return of spontaneous circulation in some overcrowded hospitals where admission might be delayed303132; and favourable neurological outcomes at hospital discharge. We defined sustained return of spontaneous circulation as return of spontaneous circulation for at least two hours and survival with a favourable neurological outcome as a Cerebral Performance Category score ≤2.33The initial care providers, including paramedics and emergency physicians, could not be blinded to the study intervention because the interventions were clearly visible, whereas the substantial in-hospital caregivers were unaware of the pre-hospital interventions after the removal of the initial intraosseous access.
Statistical analysis
We estimated the required study sample size on the basis of an assumed differences of 5% of the primary outcome in the two arms, with a power of 80% at a 5% significance level on a two sided test to detect the difference from 10% survival to 15% (expected survival).3435With a 1:2 randomisation ratio, we needed 1506 patients (502 in the intraosseous group and 1004 in the intravenous group). Assuming that 5% of the patients would have incomplete data or missing outcomes, we estimated that we needed a final sample size of 1581 patients. Taking a conservative estimation of the intracluster correlation coefficient of 0.03, the required total sample size was 1680 patients. We used the Pocock boundary for determining whether to prematurely stop the clinical trial, and two interim analyses were planned.36
The primary and secondary outcomes were analysed and reported on an intention-to-treat basis, including all patients who underwent assigned randomisation, and cases with missing outcomes were excluded from the analysis. We also present the results of the per protocol analysis, which included patients who strictly adhered to the study intervention according to their allocation (successful intravenous or intraosseous access). We summarised the trial data by using different statistical measures based on variable distributions. We calculated means and standard deviations for normally distributed variables, and we used medians and interquartile ranges for non-normally distributed variables. We summarised categorical variables by using sample size and percentage. We analysed continuous variables by using Student’s t test or a Wilcoxon rank sum test; we used a χ2or Fisher’s exact test to analyse categorical variables. We analysed primary and secondary outcomes by using regression models with and without adjustment for covariates, including age, sex, arrest characteristics, location, initial rhythm, and time intervals between key events and initial response.
We used fixed effects logistic regression models to obtain unadjusted and adjusted odds ratios, and we calculated 95% confidence intervals. We also did pre-specified subgroup analyses according to age, initial presenting rhythm (shockable versus non-shockable), witnessed versus not witnessed arrest, bystander cardiopulmonary resuscitation versus non-bystander cardiopulmonary resuscitation, response intervals, region, time to vascular access, time to administration of first dose drug, and total pre-hospital drug dose. We used post hoc generalised estimating equations in the intention-to-treat and per protocol populations to assess within cluster correlation. We did supplementary analyses to evaluate the effectiveness of pre-hospital medications. Statistical significance was set at a P value of <0.05. We used SAS software, version 9.2, for statistical analyses.
Patient and public involvement
Several conferences involving the research team, Taipei City Fire Department, Taipei City Government's Department of Health, and relevant stakeholders were held to establish a consensus on trial management, before the study was implemented. Our research encountered a distinctive barrier in involving patients experiencing out-of-hospital cardiac arrest owing to the immediate and critical nature of their condition. The unpredictable and abrupt onset of out-of-hospital cardiac arrest made participation of or feedback from patients before the start of the trial practically impossible. The involvement of patients in the early phase of the trial was hindered by the outbreak of the covid-19 pandemic.
Results
Patient characteristics and study intervention
From 6 July 2020 to 30 June 2023, a total of 276 clusters with 7780 patients were assessed for eligibility and were randomised to either the intraosseous or intravenous group. Owing to a lower than expected enrolment rate and the additional challenges posed by the covid-19 pandemic, the trial was extended to a third year. Two interim analyses were conducted and the trial continued, as neither result reached the stopping threshold. After the exclusion of 6009 patients who met the pre-defined exclusion criteria, 1771 patients with out-of-hospital cardiac arrest were enrolled, and outcomes were available for 1732 (97.8%) patients: 741 in the intraosseous group and 991 in the intravenous group.Figure 1shows the reasons for exclusion, which were similar in the two groups.Table 1shows the baseline characteristics of the patients. Twelve cases of protocol violation occurred: 10 cases in which intravenous access was inserted after failed intraosseous attempts and two cases in which intraosseous access was inserted after failed intravenous attempts.
Primary outcome and secondary outcomes
Primary outcome data were available for 741 (97.6%) patients in the intraosseous group and 991 (97.9%) patients in the intravenous group. We did not apply multiple imputations owing to the low number of missing cases (2.2%; 39 of 1771 enrolled patients). In the intraosseous group, 79 (10.7%) patients were discharged alive, compared with 102 (10.3%) patients in the intravenous group (odds ratio for survival to hospital discharge 1.04, 95% confidence interval 0.76 to 1.42; P=0.81). For the secondary outcomes, the proportion of patients achieving pre-hospital return of spontaneous circulation (odds ratio 1.23, 0.89 to 1.69; P=0.21), sustained return of spontaneous circulation (0.92, 0.75 to 1.13; P=0.44), and favourable neurological outcome (Cerebral Performance Category score 1 or 2) (1.17, 0.82 to 1.66; P=0.39) was not significantly different between the two groups (table 2). The results of the per protocol analyses, presented in supplementary tables A and B, showed no difference between the two groups after adjustment. The intracluster coefficient correlation calculated by using generalised estimating equations was 0.01, and the post hoc generalised estimating equations analysis to evaluate the effect of clusters showed no significant differences (supplementary table C)
Subgroup analysis
We did several pre-specified subgroup analyses based on the intention-to-treat principle and found that the absence of significant differences in the proportion of patients surviving to hospital discharge between the two study groups was consistent across all subgroups (fig 2). Subgroup analyses of secondary outcomes are shown in supplementary figures B-D.
Discussion
In this pragmatic trial of 1771 adult patients with out-of-hospital cardiac arrest, the proportion of patients surviving to hospital discharge did not differ significantly between the groups with initial intraosseous and intravenous vascular access. In addition, we observed no statistically significant associations with the proportion of patients with pre-hospital return of spontaneous circulation, sustained return of spontaneous circulation, and favourable neurological outcomes. Although not achieving statistical significance, the results of our study showed that for every 100 patients assigned to intraosseous rather than intravenous access, approximately two extra returns of spontaneous circulation and one extra neurologically favourable survival (Cerebral Performance Category score ≤2) occurred, without extra patients having severe disability (fig 3). The current resuscitation guidelines suggest deferring the use of intraosseous access in resuscitation after out-of-hospital cardiac arrest, but our study offers a different insight that could inform pre-hospital vascular practices and serve as a basis for future research.
Possible explanations for findings
Although we observed a trend towards a higher proportion of patients with pre-hospital return of spontaneous circulation after intraosseous access, these differences were not statistically significant. Two potential reasons for this finding exist. Firstly, although we observed higher success rates in the intraosseous group, the intraosseous intervention was not as fast as expected in real scenarios. Notably, establishing humeral intraosseous access is time consuming, including removing clothing to expose the humeral head, using specialised equipment, and securing access placement without impeding cardiopulmonary resuscitation. In a randomised trial comparing tibial intraosseous, humeral intraosseous, and peripheral intravenous access, the time to achieve initial success in humeral intraosseous placement was the longest.37However, the paramedics were more familiar with tibial intraosseous access, and the patients in the humeral intraosseous group had a higher average weight. Despite practising with intraosseous needles in the training arm, time was needed for paramedics to become comfortable with the procedure and shorten the insertion time in real clinical situations. Secondly, we observed an extended overall pre-hospital stay; in our study, the overall pre-hospital time increased by about 4 min on average compared with the previous study in the same region two years earlier.30The most likely contributing factor would be the influence of covid-19, which was also seen in another area in Taiwan.38During the pandemic, adjustments were made to the resuscitation protocol. For every case of out-of-hospital cardiac arrest, emergency medical technicians-paramedics had to be equipped with N95 masks, fluid resistant gowns, hair caps, goggles, full face shields, and gloves, potentially leading to clumsiness during performance. Despite the temporary suspension of the trial during the recruitment period due to the covid-19 outbreak, the ongoing impact persisted.
In the per protocol analysis, patients who received successful intraosseous access showed a notably lower rate of sustained return of spontaneous circulation and a trend towards poorer outcomes before adjustment, compared with intravenous access (supplementary tables A and B). However, the outcomes reversed after adjustment. Comparing the two groups, patients who successfully received intravenous access had a higher proportion of witnessed arrests and presence of shockable rhythms, which are indicators of better outcomes. This result can be explained by the assumption that the success rate of intraosseous access is less affected by the patient’s condition, whereas attempting an intravenous route is a selective process. The possible mechanism behind the equal effect observed between the two interventions in our trial suggests that patients with successfully established intravenous access have the best condition, followed by those with successful intraosseous access, with the poorest condition being in those with failed intravenous access (supplementary table D). The findings highlight the importance of our prospective randomised controlled trial, whereas previous retrospective studies were largely affected by inherent biases in the selection of studied patients.
Comparison with other studies
To clarify the effect of pre-hospital medication, we compared patients receiving adrenaline as per the study protocol and those with no adrenaline administration owing to failure to establish a route (supplementary tables E and F). The results showed that patients who received adrenaline had a higher proportion of short term outcomes such as pre-hospital return of spontaneous circulation, and the effect was more prominent in patients with non-shockable rhythms. However, the use of adrenaline was not associated with survival to hospital discharge. These findings are comparable to those of a recent network meta-analysis.39Furthermore, for patients with shockable rhythm who received amiodarone, we did not find a significant difference in survival between intravenous and intraosseous routes (supplementary table G). In a post hoc analysis of the ALPS randomised controlled trial, comparison of two routes of administration for ventricular tachycardia/ventricular fibrillation found that intravenous administration of amiodarone resulted in better survival than did intraosseous administration.9This difference in outcomes may be attributed to the drug’s lipophilicity and its local interaction with bone marrow.40However, in our study, we included all non-traumatic patients with out-of-hospital cardiac arrest, both non-shockable and shockable. Given that only a small portion of patients in our trial had a refractory shockable rhythm, the result was inconclusive. Future studies to answer these complex clinical questions are warranted.
Recent resuscitation guidelines recommend that healthcare providers initially establishing intravenous access for drug administration during cardiac arrest is reasonable and that intraosseous access is appropriate if the intravenous route fails or is not feasible.45However, all previous studies supporting the practice were retrospective in design, lacked a meticulously defined intervention protocol, and could potentially introduce significant selection bias into the results.24Moreover, site specificity would also be a problem that might affect outcomes with intraosseous administration.4In a study involving 10 cases of out-of-hospital cardiac arrest, the mean time from humeral intraosseous access to the right ventricle was 5.6 s, and it was theoretically quicker than tibial access.20Therefore, our study followed a strictly defined protocol to control the comparability of insertion sites in which intraosseous access was limited to the humeral head, whereas intravenous access was directed to the upper extremities to reduce the concerns about different distances from the insertion site to the heart. Also, a recent retrospective study found that upper extremity intraosseous access was associated with slightly better outcomes than lower extremity intraosseous access.41
Implications of findings
To the best of our knowledge, this is the first randomised clinical study completed to compare two different methods of vascular access. The pragmatic trial design of the study mirrors real world scenarios, thereby offering invaluable insights into how interventions are performed under everyday conditions and increasing the practical relevance of its findings. When applying resuscitation guidelines worldwide, the global evidence should be modified for local solutions. In Taipei, as the average transport time from the scene to the destination hospital is less than six minutes, making additional attempts may only increase the overall pre-hospital time without providing actual benefits to patients. Hence, the trial emergency medical technicians-paramedics made only limited attempts at establishment of vascular access and patients were promptly transferred to hospital after initial failure to minimise unnecessary delays.
On the basis of the results of our study, indicating that intraosseous access did not lead to worse outcomes, the intraosseous route may be regarded as a potential first line option for vascular access rather than being seen as secondary to the intravenous route. However, the costs related to intraosseous access were higher than those for intravenous access, and the level of specific intravenous training among pre-hospital personnel varied across different emergency medical service systems. Therefore, the decision between intravenous and intraosseous access should be tailored to the specific characteristics and needs of each local emergency medical service system.
Limitations of study
This study has some limitations. Firstly, we did not include patients aged >80 years, who account for approximately 40% of patients with out-of-hospital cardiac arrest according to previous experience, and the effects of the two different interventions are not known in this population. As this represents the initiation of intraosseous use in Taiwan, our primary concern was to minimise complications, considering the increased susceptibility of the older population to osteoporosis. Secondly, the study is at risk of being underpowered owing to an overoptimistic expectation of the difference in survival between the two groups, and it did not account for the outcomes of patients who did not receive medication owing to failed access. The assumption was based on the correlation between delayed drug administration and unfavourable outcomes.35Furthermore, the time required for intraosseous insertion exceeded expectations and could potentially offset the overall benefits in patients receiving intraosseous access. However, if we re-estimated the sample size from the true effect obtained by the study result, more than 184 000 patients would be needed to detect the difference. Enrolling such a large sample size in a pre-hospital clinical trial on patients with out-of-hospital cardiac arrest would be impracticable. Thirdly, despite the study being planned with a 2:1 randomisation ratio, the number of participants evaluated included in each group unexpectedly ended up being similar. After reviewing details with on-site emergency medical technicians-paramedics, we considered that two possible factors may have contributed to the uneven distribution. The trial was greatly affected by the covid-19 pandemic and was temporarily suspended. Uncontrollable modifications included single tier dispatch and advanced life support teams specialising in transfers of patients with covid-19. In addition, the intravenous group experienced a higher proportion of prolonged pre-hospital time, potentially leading to lower turnover and fewer overall emergency medical service dispatches. Fourthly, in-hospital management was unavailable in our trial, which may have influenced the results. Actual overcrowding in healthcare facilities during or after the pandemic and its effects on patient outcomes could not be clearly quantified. However, the level of hospital transfer was similar between the two groups. Therefore, we believe that in-hospital management would not be directionally biased by this factor. Finally, the patient and public involvement in our study was not fully aligned with contemporary expectations, representing a limitation in the research design that may affect the applicability of the findings.
Conclusions
We found that initial establishment of vascular access through the intraosseous route in patients with out-of-hospital cardiac arrest did not yield a different outcome in terms of survival to hospital discharge, pre-hospital return of spontaneous circulation, sustained return of spontaneous circulation, and favourable neurological outcomes, compared with intravenous access. Intraosseous vascular access may not be a deferred choice for resuscitation in patients with out-of-hospital cardiac arrest. The optimal decision making process for vascular access based on various characteristics of patients and emergency medical service systems should be explored.
","Objective: To compare the effectiveness of intraosseous versus intravenous vascular access in the treatment of adult patients with out-of-hospital cardiac arrest.
Design: Cluster randomised controlled trial.
Setting: The VICTOR (Venous Injection Compared To intraOsseous injection during resuscitation of patients with out-of-hospital cardiac arrest) trial involved emergency medical service agencies with all four advanced life support ambulance teams in Taipei City, Taiwan. The enrolment period spanned 6 July 2020 to 30 June 2023 and was temporarily suspended between 20 May 2021 and 31 July 2021 owing to the covid-19 pandemic.
Participants: Adult (age 20-80 years) patients with non-traumatic out-of-hospital cardiac arrest.
Interventions: Biweekly randomised clusters of four participating advanced life support ambulance teams were assigned to insert either intravenous or intraosseous access.
Main outcome measures: The primary outcome was survival to hospital discharge. Secondary outcomes included return of spontaneous circulation, sustained return of spontaneous circulation (≥2 hours), and survival with favourable neurological outcomes (cerebral performance category score ≤2) at hospital discharge.
Results: Among 1771 enrolled patients, 1732 (741 in the intraosseous group and 991 in the intravenous group) were included in the primary analysis (median age 65.0 years; 1234 (71.2%) men). In the intraosseous group, 79 (10.7%) patients were discharged alive, compared with 102 (10.3%) patients in the intravenous group (odds ratio 1.04, 95% confidence interval 0.76 to 1.42; P=0.81). The odds ratio of intraosseous versus intravenous access was 1.23 (0.89 to 1.69; P=0.21) for pre-hospital return of spontaneous circulation, 0.92 (0.75 to 1.13; P=0.44) for sustained return of spontaneous circulation, and 1.17 (0.82 to 1.66; P=0.39) for survival with favourable neurological outcomes.
Conclusions: Among adults with non-traumatic out-of-hospital cardiac arrest, initial attempts to establish vascular access through the intraosseous route did not result in different outcomes compared with intravenous access in terms of the proportion of patients surviving to hospital discharge, pre-hospital return of spontaneous circulation, sustained return of spontaneous circulation, and favourable neurological outcomes.
Trial registration: NCT04135547ClinicalTrials.govNCT04135547.
"
"Trends in long term vaping among adults in England, 2013-23","Introduction
Electronic cigarettes are an effective tool for smoking cessation12and are likely much less harmful than conventional cigarettes.3They are not, however, risk-free, particularly for people who have never smoked.3As such, a challenge many countries currently face is how to regulate e-cigarettes so that they are available and appealing to smokers as a means to quit smoking while minimising uptake among those who would not have otherwise smoked. Until recently, England appeared to have struck the right balance, with e-cigarettes widely used by smokers in quit attempts4and rarely used by young people and never smokers.56However, the prevalence of vaping has risen in recent years, particularly among adolescents (11-17 years)5and young adults (18-24 years).46This increase in prevalence has largely been attributed to the introduction of new disposable devices.78It is not clear how far this rise reflects an increase in short term, experimental use versus sustained, regular use. In addition, little is known about how the types of products used by long term vapers—and how often they use them—is changing over time. Understanding this is important for gauging the likely public health and environmental effects of this rise in vaping uptake and for informing policy decisions.
The use of e-cigarettes increased rapidly among adolescents and young adults in the US between 2015 and 2019, resulting in widespread concern about public health.910Further analyses of the data showed that much of the use was experimental,11and evidence of an overall increase in the population burden of nicotine dependence was limited.12Consistent with this pattern of use, and following new regulations restricting the availability of e-cigarettes, the prevalence of use in the same population subsequently declined.13Recently in the UK, vaping has increased noticeably among adolescents and young adults.57It is important to understand the extent to which this represents experimental or dependent use, and how this differs across subgroups of the population (eg, by smoking status, age, gender, and socioeconomic position). One indicator of more dependent use is the prevalence of long term vaping.
Long term vaping could be harm reducing or harmful to public health, depending on who is using the products and what they would otherwise be doing. Transitioning to long term vaping from cigarette smoking—the most harmful form of nicotine use—would reduce harm for people who are unable or unwilling to stop all nicotine use.3Conversely, long term vaping could lead to harm among those who would have never started smoking cigarettes.3Long term vaping with disposable e-cigarettes specifically may also have a substantial environmental impact14: these products are designed for single use, so they generate more waste than rechargeable vaping products (ie, refillable and pod devices).1516
The Smoking Toolkit Study (a nationally representative, cross sectional survey) collects detailed data on vaping among adults in England each month. In the current study we used data collected from 2013 to 2023 to examine how the prevalence of long term (>6 months) vaping changed overall and by vaping frequency (daily or non-daily) in adults during this period, and the main type of device used (disposable, refillable, or pod). We also investigated whether changes in any long term vaping and in long term vaping using a disposable device differed by smoking status, age, gender, and occupational social grade.
Methods
Pre-registration
The study protocol and analysis plan were pre-registered on Open Science Framework (https://osf.io/n2785/). We also carried out additional unplanned analyses before and after peer review (see statistical analysis section).
Design
Data were drawn from the ongoing Smoking Toolkit Study, a monthly cross sectional survey of a representative sample of adults in England.17The study uses a hybrid of random probability and simple quota sampling to select a new sample of about 1700 teenagers and adults (≥16 years) each month. Comparisons with sales data and other national surveys indicate that key variables, including sociodemographic characteristics, smoking prevalence, and cigarette consumption, are nationally representative.1718
The methods have been described in detail elsewhere.1719Briefly, England is split into 165 665 output areas, each comprising about 300 households. These areas are then stratified according to established geodemographic characteristics and geographical region, then randomly selected into an interviewer’s list.
Before the covid-19 pandemic these areas were randomly allocated to interviewers who travelled to their selected areas to engage with one household member aged ≥16 years. Face-to-face computer assisted interviews were conducted until quotas based on factors influencing the probability of being at home (ie, working status, age, and gender) were fulfilled. Morning interviews were avoided to maximise the availability of participants.
In-person interviews were halted in March 2020 owing to covid-19 social distancing restrictions and replaced by telephone interviews from April 2020 onwards with adults aged ≥18 years. The lower age limit returned to 16 years from January 2022. Telephone interviews are now conducted by landline or mobile using random digit dialling or by targeted mobile. For the sample processed, each eligible landline telephone number across England has a random probability of selection proportionate to population distribution (ie, stratification of the landline telephone database by and within Government Office Region; see19for further details). To maximise response rates, sampling takes place mostly by landline earlier in the day and mostly by mobile phone later in the day.
The two methods of data collection show good comparability: when social distancing restrictions were lifted, we ran a parallel telephone and face-to-face survey wave and obtained similar estimates for key sociodemographic, smoking, and nicotine product use measures.20
For this study, we used data collected from participants surveyed between October 2013 (the first wave to assess vaping among all adults) and October 2023 (the most recent data at the time of analysis). Since April 2022, vaping duration and device characteristics have only been assessed quarterly as a means of reducing the cost of data collection for the Smoking Toolkit Study owing to limited funding. We excluded waves in which vaping duration were not assessed (May, June, August, September, November, and December 2022, and February, March, May, August, and September 2023); trends were modelled based on data collected in all other waves. Data on the main device type used have only been collected since July 2016, so trends in this outcome were limited to the period from July 2016 to October 2023. Because data were not collected from teenagers aged 16 and 17 years between April 2020 and December 2021, we restricted our sample to those aged ≥18 years for consistency across the time series.
Measures
Long term vaping was defined as current vaping for a period of more than six months. Current vaping was assessed within several questions asking about use of a range of nicotine products, depending on the participant’s smoking status. Current smokers were asked: “Which, if any, of the following are you currently using to help you cut down the amount you smoke?” and “Do you regularly use any of the following in situations when you are not allowed to smoke?”; current smokers and those who had quit in the past year were asked: “Can I check, are you using any of the following either to help you stop smoking, to help you cut down or for any other reason at all?”; and non-smokers were asked: “Can I check, are you using any of the following?” We considered those who reported using an e-cigarette (ie, selected the response option “electronic cigarette” or “Juul”) in response to any of these questions to be current vapers. Other response options included different types of nicotine replacement therapy (eg, nicotine gum, nicotine patch), heated tobacco products (heat-not-burn cigarette (eg, iQOS, heatsticks)), and nicotine pouches (tobacco-free nicotine pouch or pod or “white pouches” placed on the gum). The supplementary file provides details of these items (including an additional question on nicotine vaping assessed in a subset of waves). These items capture all vaping rather than just nicotine vaping, although most vapers (87%) sampled in waves that assessed nicotine content said that their usual device contained nicotine.
Current vapers were asked: “How long have you been using this nicotine replacement product or these products for?” Response options were <1 week, 1-6 weeks, >6-12 weeks, >12-26 weeks, >26-52 weeks, and >52 weeks.
Participants who reported vaping for more than six months were considered long term vapers.21This definition was used to indicate possible dependent use. Because this measure for duration of use was not specific to vaping but applied to all non-combustible nicotine products the participant reported using, we conducted a sensitivity analysis in which we restricted the definition of long term vaping to those reporting no current use of nicotine replacement therapy, heated tobacco products, or nicotine pouches.
Vaping frequency was assessed by asking vapers: “How many times per day on average do you use your nicotine replacement product or products?” Response options were 1, 2, 3-4, 5-7, 8-11, ≥12, not every day but at least once a week, not every day and less than once a week, and don’t know.
We considered those who reported use at least once a day to be vaping daily and those who reported use less than once a day to be vaping non-daily. Because this measure was not specific to vaping, we conducted a sensitivity analysis restricting the definition of daily and non-daily vaping to those reporting no current use of nicotine replacement therapy, heated tobacco products, or nicotine pouches.
We assessed the main type of device used from July 2016 onwards by asking vapers: “Which of the following do you mainly use . . .?” Response options were disposable (“A disposable e-cigarette or vaping device (non-rechargeable)”), refillable (“An e-cigarette or vaping device with a tank that you refill with liquids (rechargeable)” or “A modular system that you refill with liquids (you use your own combination of separate devices: batteries, atomizers, etc.)”), and pod (“An e-cigarette or vaping device that uses replaceable pre-filled cartridges (rechargeable).”)
Smoking status was assessed by asking participants which of the following best applied to them: I smoke cigarettes (including hand-rolled) every day; I smoke cigarettes (including hand-rolled), but not every day; I do not smoke cigarettes at all, but I do smoke tobacco of some kind (eg, pipe, cigar, or shisha); I have stopped smoking completely in the last year; I stopped smoking completely more than a year ago; and I have never been a smoker (ie, smoked for a year or more).
Those who responded to the first three of these options were considered current smokers; those who responded they had stopped smoking in the past year were considered recent former smokers; and those who responded they had stopped smoking more than a year ago were considered long term former smokers. Those who responded they had never been a smoker were considered never-smokers, and given the wording of the response options, this group would capture some people who have smoked at all in the past. However, our question does ask people to choose which of the response options best applies to them; the “smoked for a year or more” is an elaboration rather than a definition. As such, it is likely that people who have smoked cigarettes with any kind of recent frequency (regardless of whether it is more than a year) are more likely to respond to having smoked cigarettes (including hand-rolled) every day or not every day (if currently doing so) or to have stopped smoking completely in the past year (if they have recently stopped). Other national surveys use alternative definitions, but the surveys have produced consistently similar estimates of prevalence, and the Smoking Toolkit Study and sales data are also closely aligned.22
We modelled age as a continuous variable using restricted cubic splines. Descriptive data were also provided by age group (18-24, 25-34, 35-44, 45-54, 55-64, and ≥65 years).
Gender was self-reported as man or woman. In more recent waves, participants have also had the option to describe their gender in another way; those who identified in another way were excluded from analyses by gender owing to low numbers.
Occupational social grade was categorised as ABC1 (includes managerial, professional, and upper supervisory occupations) and C2DE (includes manual routine, semi-routine, lower supervisory, state pension, and long term unemployed). This occupational measure of social grade is a valid index of socioeconomic position that is widely used in research in UK populations. It has been identified as particularly relevant in the context of tobacco use.23
Statistical analysis
Data were analysed in R version 4.2.1. The Smoking Toolkit Study uses raking to weight the sample to match the population in England. This profile is determined each month by combining data from the UK Census, the Office for National Statistics mid-year estimates, and the National Readership Survey.17The following analyses used weighted data. Missing cases were excluded on a per analysis basis.
Among all adults, we reported the prevalence and corresponding 95% confidence interval (CI) of six long term vaping categories by survey year: vaping (>6 months), daily vaping (>6 months, and currently vaping daily), non-daily vaping (>6 months, and currently vaping non-daily), vaping using a disposable device (>6 months, and currently mainly or exclusively using disposable e-cigarettes), vaping using a refillable device (>6 months, and currently mainly or exclusively using refillable e-cigarettes), and vaping using a pod device (>6 months, and currently mainly or exclusively using pod e-cigarettes).
We used logistic regression to analyse trends in these long term vaping outcomes over the study period, with time (survey month) modelled using restricted cubic splines with five knots (decided a priori and pre-registered, on the basis that this number would be sufficient to accurately model trends across years without overfitting; in cubic spline models, the results are robust to the specific position and number of knots (at least when choosing between four or five knots)24). This approach allowed for flexible and non-linear changes over time, while avoiding categorisation.
To explore moderation of trends in any long term vaping and in long term vaping using a disposable device by smoking status, age, gender, and occupational social grade, we repeated these models including the interaction between the moderator of interest and time—thus allowing for time trends to differ across subgroups. Each of the interactions was tested in a separate model. Age was modelled using restricted cubic splines with three knots (placed at the 5%, 50%, and 95% quantiles), to allow for a non-linear association between age and long term vaping.
We used predicted estimates from these models to plot the prevalence of long term vaping over the study period among all adults and within each subgroup of interest. As age was modelled continuously using splines, we displayed estimates for six specific years of age (those aged exactly 18, 25, 35, 45, 55, and 65 years) to illustrate how trends differed across the age spectrum. The model used to derive these estimates included data from participants of all ages, not only those with the previous exact specified ages.
Before peer review, we modelled age specific trends in long term vaping (ie, repeated the model testing the interaction between age and time) among participants who had never regularly smoked—those who responded that they had never been a smoker (ie, smoked for a year or more). This allowed us to examine the extent to which the increase in long term vaping we observed among never smokers differed by age.
After peer review, we added five further unplanned analyses. The first was a segmented regression analysis that tested the association of the rise in popularity of vaping using disposable devices among young adults in England with changes in the trend in long term vaping. Previous data from the Smoking Toolkit Study indicated that disposable e-cigarette use in Great Britain was relatively rare (<1%) up to May 2021, but rose gradually to 2% (11% among 18 year olds) by April 2022.7We therefore split the time series into two periods: October 2013 to May 2021 (pre-disposables period) and June 2021 to October 2023 (disposables period). We used logistic regression to model the trend in long term vaping before the interruption (underlying secular trend; coded 1 . . .n, wherenwas the total number of waves) and the change in the trend (slope) post-disposables relative to pre-disposables (coded 0 before the rise in disposable e-cigarette use, and 1 . . .mfrom June 2021 onwards, wheremwas the number of waves after June 2021). We assumed linear trends pre-interruption and post-interruption. The model was adjusted for seasonality, which was modelled using a smoothing term with cyclic cubic splines specified. We also adjusted for the onset of the covid-19 pandemic (coded 0 to February 2020 and 1 from March 2020), to account for any influence of the pandemic on vaping patterns. For ease of interpretation, we multiplied coefficients by 12 to convert results from monthly to annual trends.
The second unplanned analysis added after the peer review repeated our (spline and segmented regression) trend analyses restricting the definition of long term vaping to those who did not also use other non-combustible nicotine products, to explore the possible influence of the measures assessing duration and frequency of use not referring exclusively to vaping. The third repeated our primary (spline) models for long term daily vaping and long term non-daily vaping among participants who had never regularly smoked, to explore whether the growth in vaping among this subgroup was predominantly daily or non-daily. The fourth repeated our primary models for long term vaping, among all adults and by age, with adjustment for psychological distress, to explore the extent to which the rise in long term vaping we observed may have been driven by rising levels of distress in the population over this period, particularly among young adults.25Because data on psychological distress were only collected between April 2020 and June 2023, these analyses were restricted to participants surveyed during this period, and we reduced the number of knots used to model time from five to three to avoid overfitting the data (given the much shorter time period). The fifth provided detailed descriptive data on vaping frequency among long term vapers (ie, the proportion selecting each response option for the item assessing vaping frequency), to provide more information beyond daily versus non-daily use.
Patient and public involvement
The wider Smoking Toolkit Study is discussed several times a year with a diverse patient and public involvement group, and the authors regularly attend and present at meetings at which patients and members of the public are included. Interaction and discussion at these events help shape the broad research priorities and questions. A mechanism also exists for generalised input from the wider public: each month, interviewers seek feedback on the questions from all respondents, who are representative of the English population. This feedback is limited and usually relates to understanding of questions and item options. No patients or members of the public were involved in setting the research questions or the outcome measures, nor were they involved in the design and implementation of this specific study.
Results
A total of 197 266 (unweighted) adults aged ≥18 years in England were surveyed between October 2013 and October 2023. We excluded 17 541 surveyed in months in which vaping duration was not assessed, resulting in a final analytical sample of 179 725 participants. Of these, 125 751 were surveyed between July 2016 and October 2023 and provided data for analyses by the main type of vaping device used. Supplementary table S1 shows the characteristics of the whole analysed sample and the subsample analysed by device type.
Trends in long term vaping among adults
Our primary model indicated that across the study period, the proportion of adults reporting long term vaping increased from 1.3% to 10.0% (table 1). The increase over time was non-linear: prevalence increased from 1.3% to 3.3% between October 2013 and July 2017, was stable at 3.3% between July 2017 and August 2019, then increased again—with a particularly sharp rise from late 2021—reaching 10.0% by October 2023 (fig 1).
The unplanned segmented regression analysis was consistent with this rise being associated with an increase in popularity of disposable e-cigarettes (fig 1). Before June 2021, the prevalence of long term vaping increased by 11.3% per year (relative risk for trend 1.113 (95% CI 1.092 to 1.133)). However, this trend increased sharply from when disposable e-cigarettes became popular (relative risk for trend change 1.245 (1.181 to 1.312)), with prevalence rising by 38.6% per year since (relative risk for trend×relative risk for trend change 1.113×1.245=1.386). These percentages represent the yearly relative rather than absolute percentage point increase.
A greater increase occurred in long term daily vaping than long term non-daily vaping over time (table 1and supplementary table S2). In October 2013, equal proportions of long term vapers reported vaping daily and non-daily (0.6% and 0.6% of adults, respectively;table 1; estimates do not sum to the total prevalence of long term vaping owing to some missing data on vaping frequency). The trend in long term daily vaping mirrored the trend in any long term daily vaping, increasing to 6.8% of adults by October 2023 (fig 2). Meanwhile, the prevalence of long term non-daily vaping remained relatively stable (between 0.6% and 0.7%) up to May 2021, then increased to 1.6% by October 2023 (fig 2). The difference between the prevalence of long term daily vaping compared with long term non-daily vaping was less pronounced among never smokers (1.5% (1.1% to 2.1%)v0.6% (0.3% to 0.9%) in October 2023; supplementary figure S1). When we investigated changes in vaping frequency among long term vapers in detail (using unmodelled data aggregated by survey year), a notable decrease was observed from 2013/14 to 2022/23 in the proportions who said they were vaping less than weekly (from 18.3% to 5.7%) or at least weekly but less than daily (from 20.6% to 8.9%), and increases in the proportions who said they were vaping ≥12 times a day (from 10.6% to 25.8%) or who did not know (from 6.9% to 18.9%; supplementary table S3).
Trends also differed according to the main type of device used. In July 2016, when device type was first assessed, most long term vapers mainly or exclusively used refillable devices (2.5% of adults) and few mainly or exclusively used disposable devices (0.1% of adults,table 1; estimates by main device type used do not sum to the total prevalence of long term vaping because each device type was modelled separately). By October 2023, similar proportions of vapers mainly or exclusively used refillable or disposable devices (4.6% and 4.9% of adults, respectively;table 1). The prevalence of long term vaping using a disposable device was low (0.1%) between October 2013 and March 2021, then increased rapidly to 4.9% by October 2023 (fig 2). The prevalence of long term vaping using a refillable device increased from 2.5% to 2.7% between October 2013 and September 2017, was stable at 2.7% to September 2019, then increased to 4.6% by October 2023 (fig 2). The prevalence of long term vaping using a pod device was roughly stable (between 0.3% and 0.5%) from October 2013 to March 2021, then increased steadily to 1.0% by October 2023 (fig 2).
Across the study period, 577/6173 long term vapers (9.3%) also used other non-combustible nicotine products (of whom 516 used nicotine replacement therapy, 48 heated tobacco products, and 36 nicotine pouches). Results of sensitivity analyses in which these participants were not counted as long term vapers showed a similar pattern (supplementary table S4 and figures S2 and S3), although absolute estimates of prevalence were slightly lower—for example, the overall estimate for prevalence of long term vaping in October 2023 was 9.1% (95% CI 8.3% to 9.9%) compared with 10.0% (9.2% to 10.9%) in the primary analysis.
Differences by smoking status, age, gender, and occupational social grade
The increase in long term vaping occurred predominantly among current and former smokers, but a rise also occurred among never smokers in more recent years (from <0.5% to March 2021 to 3.0% by October 2023;table 2,fig 3).
Changes in long term vaping were similar across ages up to 2019, but prevalence then increased more rapidly among young adults than older adults (fig 3), resulting in a strong inverse age gradient in long term vaping (eg, reaching 22.7% among 18 year oldsv4.3% among 65 year olds by October 2023;table 2). This pattern of results persisted after adjustment for psychological distress (supplementary figure S4). A similar inverse age gradient was also observed among never smokers (eg, reaching 16.1% among 18 year oldsv0.3% among 65 year olds; supplementary table S5 and figure S5).
The prevalence of long term vaping initially rose slightly more quickly among men, and as a result was significantly higher among men than women between June 2015 and December 2022 (fig 3). However, prevalence then increased more quickly among women than men from late 2021, closing this gap (fig 3). As of October 2023, no significant difference was observed in the prevalence of long term vaping between men and women (10.1% and 9.9%, respectively;table 2).
The prevalence of long term vaping was consistently higher among those from less advantaged social grades compared with more advantaged social grades, but time trends were similar (fig 3).
Recent changes in long term vaping using disposable devices by smoking status, age, gender, and occupational social grade followed similar patterns to those observed for changes in any long term vaping (table 2,fig 4).
Discussion
In England, the prevalence of long term (>6 months) vaping increased substantially from 2013 to 2023. In October 2013, when e-cigarettes were still relatively new and less effective in delivering nicotine than current devices, around one in 80 adults was a long term vaper. This number increased to one in 30 adults by mid-2017 and was stable for several years. Then a rapid rise started in 2021, and by October 2023 one in 10 adults in England reported having been vaping for more than six months. The rise in long term vaping was largely driven by an increase in long term daily vaping. The absolute increases in long term vaping were most pronounced among people with a history of regular smoking, but an increase also occurred among people who had never regularly smoked. Growth was also most pronounced in young adults, including among those who had never regularly smoked. This pattern persisted after adjustment for rises in psychological distress over this period.25Between October 2013 and March 2021, most long term vapers mainly or exclusively used refillable e-cigarettes, and few used disposable devices. However, the prevalence of long term vaping using disposable devices subsequently rose rapidly, and by October 2023 similar proportions of adults mainly or exclusively used disposable and refillable devices. The prevalence of long term vaping using a pod device increased between March 2021 and October 2023 but remained relatively rare.
Comparison with other studies
The timing of the sharp rise in long term vaping coincided with the rising popularity of new disposable e-cigarettes from Spring 2021.78This suggests that the recent increase in vaping among adults—particularly young adults—in England does not just reflect an increase in experimental use, but rather that a substantial number of those taking up vaping are going on to vape long term. Our data also suggest that most of those vaping long term are vaping daily, including more than two thirds of those who have never regularly smoked (ie, it is not just infrequent use over a long period). In addition, although in recent years small increases in long term vaping using refillable and pod devices have occurred, the increase in long term vaping using disposable devices was substantially larger. This indicates that many vapers are now continuing to use disposable devices over the long term, rather than transitioning to rechargeable devices.
We observed notable differences in trends in long term vaping by age and smoking status. The recent and rapid rise in long term vaping followed a clear inverse age gradient that mirrored the pattern we have seen for the prevalence of current vaping using disposable devices over this period.78The rise occurred predominantly among current and former smokers, but an increase in long term vaping also occurred among those who had never regularly smoked. This is not necessarily a cause for concern if vaping is diverting people who would have otherwise smoked towards a less harmful nicotine product. Simulation modelling of trends in smoking and vaping among adolescents in the US suggests a substantial diversion effect is likely.26However, if these people would not have otherwise taken up smoking, then taking up vaping as a regular habit will expose them to greater harm than if they had neither vaped nor smoked.3With a growing proportion of people using e-cigarettes, it becomes increasingly unlikely that all would have used cigarettes instead, given the declining trends in prevalence of cigarette smoking.27Across our study period, national estimates of smoking prevalence among adults in England fell from 18.8% in 2013 to 12.9% in 2022.27Recent data indicate a rise in the proportion of young adults using inhaled nicotine (ie, vaping or smoking) since disposable e-cigarettes became popular in the absence of an acceleration in the decline in smoking, although declines in smoking appear to have been most pronounced in age groups with the largest increase in vaping.2728Our current study shows that the proportion of young adults vaping long term is now higher than smoking rates have been in this age group since 2017.27A similar pattern has been documented among adolescents, with rates of current vaping among 11-17 year olds in Great Britain in 2023 higher than smoking rates in this age group have been for more than a decade.5
Our definition of long term vaping was use of e-cigarettes for a period of more than six months and was an operational definition to indicate possible dependent use. It is not necessarily indicative of a length of time at which we would expect substantial harms from vaping to have already accumulated. In discussions about the risks associated with e-cigarettes, a key concern is that the harms of long term use are not yet fully understood—in this context, long term use refers to decades rather than the much shorter duration of use focused on in our analysis. Even most of the well established harmful effects of tobacco smoking on health outcomes only become apparent after years and decades, rather than months, of use. Existing evidence suggests the long term health risks of vaping are likely to be substantially lower than the risks of smoking.3
Policy implications
Our findings raise two key issues with implications for policy. Firstly, disposable e-cigarettes appear to be attracting young adults to establish long term e-cigarette use. Long term vaping among young adults has increased exceptionally since 2021, and as of October 2023 it did not yet show signs of stopping. This adds weight to calls for tighter regulation of vaping products to reduce their appeal to young people29and highlights the urgency of this action. Disposable e-cigarettes might appeal to young people for a variety of reasons, including affordability, sleek design and branding, attractive in-store displays, and ease of access.293031Any policies intended to reduce young people’s interest in vaping, however, must be carefully considered in case they discourage people from using e-cigarettes as","Objective: To examine trends in long term (>6 months) vaping among adults in England.
Design: Population based study.
Setting: England.
Participants: 179 725 adults (≥18 years) surveyed between October 2013 and October 2023.
Main outcome measures: Time trends in prevalence of long term vaping using logistic regression, overall and by vaping frequency (daily or non-daily), and main type of device used (disposable, refillable, or pod).
Results: The proportion of adults reporting long term vaping increased non-linearly, from 1.3% (95% confidence interval 1.1% to 1.5%) in October 2013 to 10.0% (9.2% to 10.9%) in October 2023, with a particularly pronounced rise from 2021. This rise included an increase in long term daily vaping, from 0.6% (0.5% to 0.8%) to 6.7% (6.0% to 7.4%). Absolute increases in long term vaping were larger among people with a history of regular smoking (current smokers: 4.8% (4.0% to 5.8%) to 23.1% (20.4% to 25.9%); recent former smokers: 5.7% (3.4% to 9.2%) to 36.1% (27.6% to 45.4%); long term former smokers: 1.4% (1.0% to 1.9%) to 16.2% (14.2% to 18.4%)), but an increase also occurred among people who had never regularly smoked (0.1% (0.0% to 0.2%) to 3.0% (2.3% to 3.8%)). Growth was also more pronounced in young adults (eg, reaching 22.7% (19.2% to 26.5%) of 18 year oldsv4.3% (3.6% to 5.2%) of 65 year olds), including among those who had never regularly smoked (reaching 16.1% (11.1% to 22.7%) of 18 year oldsv0.3% (0.1% to 0.6%) of 65 year olds). Between October 2013 and March 2021, most long term vapers mainly or exclusively used refillable electronic cigarettes (2.5% to 3.3% of adults) and few (0.1% of adults) used disposable devices. However, prevalence of long term vaping using disposable devices subsequently rose rapidly, and by October 2023 similar proportions of adults mainly or exclusively used disposable and refillable devices (4.9% (4.2% to 5.7%) and 4.6% (4.0% to 5.3%), respectively).
Conclusions: The prevalence of long term vaping increased substantially among adults in England during 2013-23. Much of this increase occurred from 2021, coinciding with the rise in popularity of disposable e-cigarettes. Half of long term vapers now mainly or exclusively use disposable devices. The growth was concentrated among people with a history of regular smoking, but an increase also occurred among people who never regularly smoked, especially young adults.
"
Covid-19 infection and vaccination during first trimester and risk of congenital anomalies,"Introduction
Women infected with covid-19 during pregnancy have a higher risk of pregnancy complications.123Based on this evidence, and studies showing that pregnant women are at increased risk of severe disease from covid-19,34the authorities in most countries recommend that pregnant women get vaccinated against covid-19.5678Because pregnant women are not often included in randomised controlled trials of vaccines before marketing, evidence relating to the safety of vaccines during pregnancy relies on observational data after the introduction of vaccines. Vaccination of pregnant women against covid-19 was therefore recommended before conclusive safety data were available. Studies evaluating the safety of covid-19 vaccines among pregnant women after marketing have been reassuring, providing no evidence of an increased risk of pregnancy complications.9101112
Limited evidence is available about the risk of major congenital anomalies after infection with1314or vaccination against1516171819covid-19. A study of 92 pregnancies in women infected with covid-19 during the first trimester and 292 without covid-19 infection (data from the International Registry of Coronavirus Exposure in Pregnancy) indicated no increased risk of any major congenital anomalies (relative risk 1.2, 95% confidence interval (CI) 0.3 to 4.2).13A registry based study from Scotland reported similar findings of no increased risk of congenital anomalies after infection with covid-19 (1574 with infection and 4722 without infection; adjusted odds ratio 0.94; 95% CI 0.57 to 1.54), in addition to no increased risk with covid-19 vaccination (6731 with vaccination and 20 193 without vaccination; adjusted odds ratio 1.00, 95% CI 0.81 to 1.22).18An Israeli study of 24 288 pregnancies (2134 in women vaccinated against covid-19 during the first trimester) also reported no increased risk of congenital anomalies after vaccination (relative risk 0.69, 95% CI 0.44 to 1.04).16A smaller study in the United States found that 27 of 534 infants whose mothers were unvaccinated against covid-19 and 109 of 2622 infants whose mothers were vaccinated any time during pregnancy were diagnosed with congenital anomalies (P value 0.35).17Finally, a study of 1450 pregnancies in the COVI-PREG registry from Switzerland and France did not find evidence of an increased risk of congenital anomalies after vaccination against covid-19 during the first trimester (adjusted relative risk 0.89, 95% CI 0.12 to 6.80).19
Most of these studies had an inadequate sample size to robustly examine these rare outcomes,13151719were not able to study first trimester exposure,1518or did not investigate subgroups of congenital anomalies.15161719For congenital anomalies, first trimester exposure is of particular interest.20Because not all major congenital anomalies are detected at birth, follow-up information from the first year of life is important to reduce misclassification.21As a consequence, it has only recently become possible to study these outcomes after covid-19 infection and vaccination in early pregnancy. The objective of this study was to study the risk of major congenital anomalies according to infection with or vaccination against covid-19 during the first trimester.
Methods
Study population
We studied liveborn singleton infants in Sweden, Denmark, and Norway with estimated start of pregnancy between 1 March 2020 and 14 February 2022. Births were identified through the Swedish Pregnancy Register,22the Danish National Patient Register (registrations of international classification of disease, 10th revision (ICD-10) codes Z38, O80-84, and P95),23and the Medical Birth Registry of Norway.24The Danish and Norwegian data included all births nationally, while the Swedish data included 94% of all births (in 18 of 21 Swedish regions). We required a minimum of nine months (275 days) of postnatal follow-up (end of follow-up in the three different national registry linkages was 31 March 2023 for Sweden, 31 December 2022 for Denmark, and 15 September 2023 for Norway). To avoid oversampling of preterm pregnancies, we excluded births that were not able to reach 42 completed gestational weeks and have nine months of postnatal follow-up by the end of follow-up in the national linkages. We obtained information on maternal socioeconomic measures, infections with covid-19, and vaccination against covid-19 from national databases (see supplementary appendix for details).
Covid-19 infection and vaccination
The exposures of interest were infection with or vaccination against covid-19 during the first trimester (13 weeks plus six days). The two exposures were evaluated separately. We did not evaluate the role of a combined exposure, or exclude those exposed to the other exposure of interest from the reference group. The start of pregnancy was estimated from the date of birth minus the gestational age in days (the gestational age of the pregnancy was estimated by ultrasound for more than 90% of births, or by day of last menstrual period). Information on laboratory confirmed polymerase chain reaction (PCR) positive tests for covid-19 was obtained from mandatory reports to SmiNet at the Public Health Agency for Sweden,25from the Norwegian Surveillance System for Communicable Diseases for Norway,26and information on PCR and antigen positive tests was obtained from the Microbiology Database at the State Serum Institute for Denmark.2728In Denmark, positive antigen tests that were followed by a negative PCR test within four days were excluded, and 15% of the included positive tests were only based on antigen tests. Until around March to April 2022, pregnant women who tested positive on a self-administered antigen test were advised to obtain a confirmatory PCR test.
Information on vaccination was obtained from mandatory national vaccination registries. We restricted the analysis to the two mRNA vaccines from Pfizer-BioNTech (BNT162b2) and Moderna (mRNA-1273), and excluded women who had received other covid-19 vaccines. At the beginning of the covid-19 pandemic, vaccination during the first trimester was not recommended for the general population of pregnant women, but could be considered for those at high risk. Pregnant women were advised to get vaccinated from the second trimester onwards, starting from May 2021 in Sweden, July 2021 in Denmark, and August 2021 in Norway. Women vaccinated during the first trimester at the beginning of the pandemic include those who were vaccinated before realising they were pregnant, and those who had a particularly high risk of infection or severe disease from covid-19 owing to underlying chronic conditions or their job (ie, healthcare workers). General recommendations for pregnant women to get vaccinated during the first trimester started in January 2022 in Norway, but these recommendations were not issued in Sweden and Denmark. eTable 1 gives a brief overview of major changes to recommendations.
Congenital anomalies
We defined major congenital anomalies identified during the first nine months of life according to the EUROCAT (European Surveillance of Congenital Anomalies) classification, guide 1.5.29Information on congenital anomalies was based on data from the National Birth Registry (Norway), the Pregnancy Register (Sweden), and national patient registries (all countries). The national patient registries include all inpatient and outpatient contact with specialist healthcare services based on mandatory reporting. Four digit ICD-10 codes (QXX.XX) were not available in Norway and Sweden, and so three digit codes were used (QXX.X). Anomalies were categorised as any major congenital anomaly, congenital heart defects, nervous system anomalies, eye anomalies, ear, face and neck anomalies, respiratory anomalies, oro-facial clefts, gastrointestinal anomalies, abdominal wall defects, congenital anomalies of kidney and urinary tract, genital anomalies, and limb anomalies. eTable 2 presents the ICD-10 codes used to define the subgroups of anomalies. We do not show results when less than five infants with maternal exposure were reported across the three countries.
Covariates
We obtained information on maternal age (<25, 25-29, 30-34, 35-39, and ≥40 years), parity (0, 1, ≥2), maternal educational level (≤9 years, 10-12 years, >12 years), household income based on the national distributions (in thirds), maternal region of birth (Scandinavia, other European countries, Middle East or Africa, other or unknown), estimated date of start of pregnancy (estimated as date of birth minus gestational age in days; continuous), smoking in pregnancy (yes or no), body mass index before pregnancy or early in pregnancy (World Health Organization categories: underweight, normal weight, overweight, obese, unknown), and pre-existing chronic condition before pregnancy (yes or no; included hypertension, chronic kidney disease, asthma, cardiovascular disease, thrombosis, diabetes, and epilepsy). eTable 3 presents information on the ICD-10 codes used to define these chronic conditions. All of these covariates were considered potential confounders for infection with and vaccination against covid-19.
Statistical analysis
All analyses were conducted separately for each country, and subsequently combined using a random effects meta-analysis. We developed a detailed analysis plan harmonising the definition of all variables before starting the analysis, which was followed by the analysts in all three countries. Common analysis scripts were developed for Denmark and Norway using Stata, while separate scripts were developed for Sweden using SAS. Heterogeneity between the countries was examined using the I2statistic. We do not show country specific estimates for legal reasons relating to small numbers of certain outcomes. We first examined the risk of congenital anomalies after infection with covid-19 during the first trimester using logistic regression. The multivariable model adjusted for maternal age at the start of pregnancy, parity, educational level, household income level, maternal region of birth, estimated date of start of pregnancy, smoking during pregnancy, body mass index before or early in pregnancy, any history of chronic conditions, and vaccination against covid-19 during the first trimester. We accounted for the dependency between multiple pregnancies to the same woman by using cluster variance estimation. We also explored differences in the risk of congenital anomalies according to the covid-19 viral variants using the same calendar time cut-off points for major circulating variants, as previously published.30
We also evaluated the risk of congenital anomalies according to vaccination against covid-19 during the first trimester. This analysis was restricted to pregnancies starting after 1 January 2021 when vaccines became available. The reference group comprised women not vaccinated in the first trimester (including women vaccinated before pregnancy and after the first trimester). The multivariable model was adjusted for the same covariates as in analyses of covid-19 infection, in addition to infection with covid-19 during the first trimester. We also evaluated differences according to the two different mRNA vaccines (BNT162b2 and mRNA-1273), and conducted a sensitivity analysis that excluded infants of mothers who remained unvaccinated at the end of follow-up (21%).
For both exposures, we conducted two sensitivity analyses. In the first sensitivity analysis, we required at least 12 months of follow-up after birth.21In the second sensitivity analysis, we excluded infants with congenital anomalies that are known to have a main genetic cause (defined by the following codes: Q619, Q751, Q754, Q771, Q772, Q780, Q796, Q821, Q85, Q87, Q90-99).31The analyses were conducted using Stata version 17 (Statacorp, TX, USA) and SAS version 9.4 (SAS Institute, NC, USA).
Patient and public involvement
Because this study was based on deidentified data from national health registries, it was not permitted or possible to contact any registered individuals directly. The advisory group responsible for national guidelines for vaccination of pregnant women against covid-19 at the Norwegian Institute of Public Health provided important feedback and was informed of preliminary findings throughout the project. The researchers did not have the necessary infrastructure or funding available to further pursue additional patient or public partnership for this specific project.
Results
A total of 343 066 singleton infants were included in the analysis of covid-19 infection (fig 1). The distribution of background characteristics was very similar across the three countries (table 1). There was a slightly higher proportion of infants born to women from the Middle East or Africa in Sweden, while the proportion of mothers with an underlying chronic disease appeared to be lower in Denmark than in Sweden and Norway. A total of 17 704 infants were diagnosed with a major congenital anomaly within a nine month follow-up, corresponding to 516 per 10 000 live births. Only 737 out of 17 704 (4.2%) had anomalies in two or more major congenital anomaly subgroups.Table 2shows the rates of major congenital anomalies.Figure 2shows the gestational age distributions for date of covid-19 infection (date of registered positive test) and vaccination against covid-19 during the first trimester, while eFigure 1 shows the distribution of calendar time for the start of pregnancy, infection with and vaccination against covid-19 during the first trimester.
Risk of congenital anomalies according to infection with covid-19
A total of 10 229 infants (3%) had mothers with covid-19 infection during the first trimester. These mothers had higher parity, lower educational level, lower household income level, and were more likely to be born in the Middle East or Africa (eTable 4). We did not find an increased risk of any major congenital anomalies after infection with covid-19 during the first trimester, with an adjusted odds ratio of 0.96 (95% confidence interval 0.87 to 1.05;table 3). Likewise, we did not find an increased risk of specific subgroups of congenital anomalies after maternal infection, with adjusted odds ratio ranging from 0.84 (0.51 to 1.40) for eye anomalies to 1.12 (0.68 to 1.84) for oro-facial clefts (table 3). There was some heterogeneity in the risk estimates for different congenital anomalies according to exposure to infection during the first trimester (I2statistic: 11% for kidney and urinary anomalies, 29% for gastrointestinal anomalies, 50% for limb anomalies, and 0% for the other outcomes evaluated). Exposure to the index variant of covid-19 during the first trimester occurred in 3124 pregnancies, with 2065 to alpha and 5040 to delta variants. We observed no notable differences in the risk according to these three viral variants, although the results were associated with a high degree of uncertainty (eTable 5). Analyses among infants with at least 12 months of follow-up (compared with at least nine months in the main analysis), or excluding infants with presumed genetically related anomalies showed similar results (eTables 6 and 7).
Risk of congenital anomalies according to vaccination against covid-19
We included 152 261 infants in the analysis of congenital anomalies according to vaccination (fig 1). Among these, 29 135 (19%) had maternal exposure to covid-19 vaccination during the first trimester: 22 322 (77%) mothers received the BNT162b2 vaccine and 6813 (23%) had the mRNA-1273 vaccine. Mothers vaccinated against covid-19 during the first trimester had higher education and household income, were more likely to have an underlying chronic disease, and were more likely to be overweight or obese (eTable 8).
We did not find an increased risk of any major congenital anomaly among infants whose mothers were vaccinated against covid-19 during the first trimester, with an adjusted odds ratio of 1.03 (95% CI 0.97 to 1.09;table 4). When we examined subgroups of anomalies, adjusted estimates ranged from 0.84 (0.31 to 2.31) for nervous system anomalies to 1.69 (0.76 to 3.78) for abdominal wall defects (table 4). The estimates for 10 of 11 of the subgroups of anomalies were less than 1.04, indicating no notable increased risk. There was some heterogeneity in the estimates for the risk of the different congenital anomalies according to exposure to vaccination during the first trimester (I2statistic: 32% for congenital heart defects, 40% for gastrointestinal anomalies, 44% for kidney and urinary anomalies, 76% for nervous system anomalies, and 0% for the other outcomes evaluated).
Restricting our analysis to infants with at least 12 months of follow-up, excluding infants with genetic disorders, or excluding infants of mothers who remained unvaccinated at the end of follow-up yielded similar results (eTables 9-11). We found no clear evidence of a difference in the risk of congenital anomalies according to exposure to the two different mRNA vaccines (eTable 12). Only 386 infants were exposed to infection and vaccination during the first trimester, therefore we did not evaluate the role of a combined exposure, or exclude those exposed to the other exposure of interest from the reference group.
Including fetal deaths and late induced abortions
In Norway, we had information on all fetal deaths and induced abortions after 12 completed gestational weeks in the birth registry. These data included 1227 pregnancies during the follow-up period in addition to the live births being studied. Of these 1227 pregnancies ending in a fetal death or induced abortion, 64 (rate 522 per 10 000 pregnancies) had a major congenital anomaly registered. When we examined the risk of any major congenital anomaly according to infection including these pregnancies, the results were similar to the main results (adjusted odds ratio 0.98, 95% CI 0.86 to 1.11 including these additional pregnanciesv0.97, 0.85 to 1.11 in the main analysis) and with vaccination against covid-19 (1.07, 0.97 to 1.17v1.06, 0.97 to 1.17). No information on terminations with data on anomalies were available for Denmark and Sweden.
Discussion
Principal findings
Our Nordic registry based study did not find an increased risk of any major congenital anomalies among infants whose mothers had covid-19 infection or covid-19 vaccination during the first trimester. No notable heterogeneity in the risk was apparent according to viral variants, although larger studies are needed to provide more robust evidence.
Strengths and limitations
Strengths of the current study include the large population based sample, the inclusion of data from several countries, and the evaluation of subgroups of anomalies. The rate of major congenital anomalies in our study is higher than that reported in the EUROCAT statistics for the included countries. This difference is probably explained by the lack of follow-up after birth for most regions in the EUROCAT statistics, changes in the definitions according to the latest updated version of the EUROCAT guidelines, and misclassification resulting in false positive registrations32—for example, when a child is under evaluation or being diagnosed. We do not expect any of these points to differ according to the exposures of interest, and any such non-differential misclassification could therefore have resulted in an underestimation of the associations of interest. As an example, we examined the limb anomaly subgroup further because the Danish rate for limb anomalies was especially high—primarily because of inclusion of hip anomalies recorded between birth and six weeks of age, which have been shown to have a high false positive rate.33When excluding these diagnoses, the rate more than halved, but the overall estimates for the risk of limb anomalies according to covid-19 infection or vaccination were not appreciably affected.
The study also has limitations. Our analysis is restricted to live births. We chose not to include stillbirths in the main analysis because the presence of congenital anomalies is poorly recorded for this group, and we did not have information on fetal deaths or induced abortions for all countries. It is unlikely that our analyses of vaccination against covid-19 and risk of congenital anomalies is biased owing to exclusion of fetal deaths because no increased risk of miscarriage or stillbirth according to vaccination was observed.1134However, the results for infection with covid-19 might be underestimated because an increased risk of stillbirth with covid-19 infection has been reported previously in our study population, and we do not know if congenital anomalies were contributing factors in these stillbirths.30Differences in the testing strategy could have influenced our ability to identify women infected with covid-19.
We also acknowledge that we did not capture self-administered antigen tests for covid-19. However, for most of the study period, women who tested positive using an antigen test were advised to get a confirmatory PCR test. In Denmark, from March 2021 until March 2022, 80-90% of positive antigen tests were followed up by a confirmatory PCR test.35Similar estimates are unfortunately not available for Sweden and Norway. It is possible that women with a higher risk of having a baby with a congenital anomaly had a greater likelihood of getting tested, for example older women and those with various underlying chronic conditions and using drugs, or women with a previous child with anomalies. It is also possible that pregnancies followed for congenital anomalies in specialised antenatal care or fetal medicine units were more likely to be tested for covid-19. We only had three digit ICD-10 codes available in Norway and Sweden, and we chose to be conservative in the exclusion of minor anomalies, which might have resulted in an underestimation of some of the associations of interest.
There might also be unmeasured or residual confounding influencing our results, for example through unmeasured factors such as differences in underlying genetic risk, use of drugs, or from measurement error or categorisation of included confounders. However, because none of the included factors are very strong risk factors for congenital anomalies, we do not expect the categorisation to be a large problem. The adjustment for underlying chronic conditions should account partially for use of drugs for these conditions. Moreover, we believe it is the underlying conditions, and not the drugs themselves, which might affect the likelihood of covid-19 infection and vaccination. To explore the scope of drug use, we checked how many pregnant women in Sweden and Norway were taking teratogens from a predefined list,36and found that only 104 pregnancies across the two countries had maternal exposure during the first trimester. Because the use of these drugs was so rare, it is unlikely that it influenced our results. The role of underlying genetic risk might be further evaluated in larger datasets using a sibling comparison. Unmeasured confounding could have led us to overestimate the associations of interest. However, because our findings are largely null, if associations were even weaker then our conclusion of no indication of adverse effects of vaccinations still holds.
Comparison with other studies
Our findings are in line with previous studies indicating no strong evidence for an increased risk of any major congenital anomalies after infection with1314or vaccination against1516171819covid-19. Most of these existing studies had a modest sample size and inadequate power,13151719and were therefore not able to examine subgroups of congenital anomalies.15161719The studies that examined subgroups of anomalies indicated no notable increased risk after infection or vaccination.1418These studies also had limited postnatal follow-up, and are likely to have underestimated the number of anomalies. Therefore, we add to the current evidence with our results showing that there appears to be no robust evidence of an increased risk of any of the subgroups of congenital anomalies as defined by the EUROCAT classification. Additionally, we provide some exploratory evidence that there do not appear to be any major differences in the risk according to covid-19 viral variants, although these results should be further explored in future studies.
Policy implications
Evidence supports an increased risk of certain pregnancy complications, including preterm birth and stillbirth, among women with covid-19 infection during pregnancy.330We did not find any evidence of an increased risk of congenital anomalies after covid-19 infection, but newer variants were not included. However, current knowledge is in line with the new viral variants becoming less harmful.37Vaccination of pregnant women protects the women and the infants from adverse outcomes. Furthermore, we did not find any indication that vaccination against covid-19 during the first trimester increased the risk of anomalies, providing additional evidence about the safety of vaccination in pregnant women. Overall, our findings support the current recommendations to vaccinate pregnant women against covid-19.
Conclusions
Covid-19 infection and vaccination during the first trimester of pregnancy were not associated with risk of congenital anomalies.
Existing studies on the risk of major congenital anomalies after infection with or vaccination against covid-19 are limited
Because the first trimester is the most important time for covid-19 exposure, and postnatal follow-up is necessary to identify anomalies not observed at birth, studies of any increased risk of major congenital anomalies have only recently become possible
Covid-19 infection or vaccination during the first trimester was not associated with congenital anomalies
Any differences in the risk of congenital anomalies according to viral variants of covid-19 should be further examined in future studies
","Objectives: To evaluate the risk of major congenital anomalies according to infection with or vaccination against covid-19 during the first trimester of pregnancy.
Design: Prospective Nordic registry based study.
Setting: Sweden, Denmark, and Norway.
Participants: 343 066 liveborn singleton infants in Sweden, Denmark, and Norway, with an estimated start of pregnancy between 1 March 2020 and 14 February 2022, identified using national health registries.
Main outcome measure: Major congenital anomalies were categorised using EUROCAT (European Surveillance of Congenital Anomalies) definitions. The risk after covid-19 infection or vaccination during the first trimester was assessed by logistic regression, adjusting for maternal age, parity, education, income, country of origin, smoking, body mass index, chronic conditions, and estimated date of start of pregnancy.
Results: 17 704 (5.2%) infants had a major congenital anomaly. When evaluating risk associated with covid-19 infection during the first trimester, the adjusted odds ratio ranged from 0.84 (95% confidence interval 0.51 to 1.40) for eye anomalies to 1.12 (0.68 to 1.84) for oro-facial clefts. Similarly, the risk associated with covid-19 vaccination during the first trimester ranged from 0.84 (0.31 to 2.31) for nervous system anomalies to 1.69 (0.76 to 3.78) for abdominal wall defects. Estimates for 10 of 11 subgroups of anomalies were less than 1.04, indicating no notable increased risk.
Conclusions: Covid-19 infection and vaccination during the first trimester of pregnancy were not associated with risk of congenital anomalies.
"
Lee Silverman voice treatment versus NHS speech and language therapy versus control for dysarthria in people with Parkinson’s disease,"Introduction
Parkinson’s disease is a progressive, neurodegenerative disorder leading to declining motor function and non-motor conditions such as dementia, depression, and anxiety. A common motor feature is dysarthria (often referred to as hypokinetic dysarthria), which may lead to reduced speech volume, word stress patterns, and fluency; speech that is monotone in pitch with imprecise articulation; changed voice quality and breath support; and an irregular speech rhythm.1Dysarthria related to Parkinson’s disease negatively affects communication, social activities, and participation, potentially leading to stigmatisation, social isolation, and reduced quality of life.2345Dysarthric symptoms vary in their response to increased dopaminergic medication and can become worse with subthalamic stimulation surgery.67
Speech and language therapy (SLT) for people with dysarthria related to Parkinson’s disease aims to maximise communication. Therapy is through exercise interventions targeting motor skills, approaches to support communication between the person with Parkinson’s disease and their family, and the use of alternative or augmentative aids to facilitate communication. Several SLT approaches are available to people with Parkinson’s disease throughout the UK National Health Service (NHS), although variations in methods and dosage are evident.8Lee Silverman Voice Training (LSVT LOUD), for example, is an approach that was developed in the USA, and is partially available in the UK91011; it is an intensive intervention that targets increased vocal loudness through vocal exercises and functional speech tasks. This treatment is unusual in that the method is highly protocolised.
Before conducting this trial, a Cochrane systematic review that included data from two randomised controlled trials (n=41) showed that participants randomly assigned to SLT had increased vocal loudness with two speech samples (5.4 dB and 11.0 dB) compared with people who had no SLT.12The small number of trials, limited sample sizes, and high risk of bias due to inadequate or poorly reported randomisation and allocation concealment, meant that evidence was insufficient to determine the effectiveness of SLT for Parkinson’s related dysarthria compared with no treatment. Another review,13which compared different SLT approaches, did not have sufficient evidence to recommend one SLT approach over another. Overall, 25, mostly small, randomised controlled trials of SLT interventions were published. These trials showed some improvement in outcome measures of vocal loudness when speaking and reading. However, few trials measured communication participation, and only two small randomised controlled trials reported outcomes at 12 months and one at 24 months (appendix 1 supplementary background information).
Following our PD COMM pilot trial of SLT in Parkinson's disease,14we developed the UK-wide trial to assess the effectiveness of two current SLT approaches in a pragmatic context in response to a National Institute for Health and Care Research-Health Technology Assessment commissioned funding call. Pragmatic trials are designed to reflect the realities of clinical practice.15We aimed to assess the clinical effectiveness of two SLT approaches versus no SLT for dysarthria in a pragmatic randomised controlled trial with a large number of people with Parkinson’s disease, using patient-reported outcome measures to reflect the impact of dysarthria on participants’ lives. The three options of LSVT, NHS SLT, or no SLT reflects a common treatment scenario within the NHS. We used the PRECIS-2 tool to assess the affect that trial design decisions would have on applicability.16The trial was registered in the ISRCTN registry, numberISRCTN12421382.
Methods
Design
We conducted a multicentre, three arm, parallel group, unblinded, randomised controlled trial with concurrent process and economic evaluations conducted in the UK. The process and economic evaluations will be reported in detail elsewhere. Participants were recruited consecutively, with no selection, and randomised at the level of the individual in a 1:1:1 ratio to LSVT LOUD, NHS SLT, or no SLT (control). Participants who were randomly assigned to no SLT could be referred for SLT at the end of trial or during the trial, if deemed medically necessary. If SLT was required for any participant in the no SLT group, then the type and dosage was determined by the therapist responsible for their care. We did not withdraw participants if they did not adhere with their randomly allocated treatment. Participants were followed-up at three, six, and 12 months after randomisation because this was reflective of the assessment time periods used by the NHS staff after treatment. The trial sites and their staff were NHS locations in England, Scotland, and Wales, which were already providing an SLT service. We made changes to the protocol, detailed in appendix 1, table A.
People were eligible to be included in the trial if they had a diagnosis of idiopathic Parkinson’s disease as defined by the 1988 UK Parkinson’s disease Brain Bank Criteria17and if they (or their carer) reported problems with their speech or voice.
We excluded people with Parkinson’s disease who: had dementia, as clinically defined by their specialist clinician; had a history of vocal strain or previous laryngeal surgery or evidence of laryngeal pathology including vocal nodules9; or received SLT for Parkinson’s related dysarthria in the previous two years.
These criteria reflect the population who would be provided with SLT due to voice or speech problems on the NHS, except for previous SLT in the past two years, which would normally not exclude a patient from receiving SLT and also might not have excluded people with dementia assessed as able to comply with treatment. The additional exclusions were to ensure that previous SLT did not bias this study, based on previous work by Ramig and colleauges,18additionally, out of concern that patients who had dementia would not be able to comply with the intervention and complete assessments, following feedback from the PD COMM pilot trial.
Randomisation and masking
A central web based randomisation system was developed and held at the Birmingham clinical trials unit. Randomisation used a minimisation process with age (≤59, 60-70, >70 years), disease severity using the Hoehn and Yahr staging19(1.0-2.5, 3.0-5.0), and severity of speech using the voice handicap index20total score (minimal ≤33, mild 34-44, moderate 45-61, severe >61) as the minimisation variables. A random factor was included within the minimisation algorithm to avoid the treatment allocation becoming predictable.
After providing written informed consent, and completion and collection of all baseline data, the person with Parkinson’s disease could be randomised into the trial. To ensure concealment of the next treatment allocation, the local collaborator accessed the secure, central, web-based randomisation system hosted at the Birmingham clinical trials unit to obtain the intervention group that the participant was randomised to. To avoid overloading local services or delays between randomisation and a participant starting treatment, local availability of SLT was confirmed before randomisation. Due to the nature of the interventions, the trial was not blinded.
Procedures
Key members of the site research team were required to attend either a meeting or a teleconference covering aspects of the trial design, protocol procedures, adverse event reporting, collection, and reporting of data, and record keeping. Therapists in the trial were registered with UK regulatory body the Health and Care Professions Council, which sets standards for education, training, and practice.
SLT departments of community based outpatient secondary care provided the interventions and collected trial data. Where specific needs were required, or where the SLT service routinely offered it, care was provided at home.
LSVT LOUD
Delivered over four weeks, LSVT LOUD consisted of four, face-to-face or remote, 50 min sessions each week. Sessions consisted of repetitions of maximum sustained “ah” phonation for as long as possible and then using high and low pitch glides held for 5 s, each in a good quality, loud voice, followed by 10 self-generated functional sentence repetitions.11Functional movement exercises using a speech production hierarchy that progressed from reading single words to phrases, sentences, paragraphs, and conversations, were tailored to individual participant’s goals. A fundamental part of LSVT LOUD is retraining of auditory sensory feedback.
Participants were set home based practice activities for up to 5-10 mins daily on treatment days and 15 mins twice daily on non-treatment days.11Twenty one centres had access to the LSVT companion software, which provided an option of remote delivery.21Only speech and language therapists or therapist assistants trained in LSVT LOUD could deliver the intervention.22
NHS SLT
Generic NHS SLT is poorly defined within the published literature with no widely accepted standards for content and dosage of intervention. Therefore, local practices for NHS SLT were accepted, except for those within the LSVT LOUD protocol. Some isolated techniques, such as vocal loudness exercises, can be common to both SLT approaches but the distinction between trial interventions could be preserved with the individualised treatment approach, the broader range of NHS SLT strategies and techniques, the intensity of delivery regimen, and the overall dose. NHS SLT dosage was determined by the local therapist in response to individual participants’ needs. Prior research suggested that NHS SLT participants would receive an average of one session per week over six to eight weeks.8
Outcomes
The primary outcome was the voice handicap index total score at three months post randomisation.20Both vocal assessments and participant reported outcomes were trialled in the PD COMM pilot trial.14The voice handicap index score that assessed participants was chosen because of the prohibitive additional time involved in vocal assessments, the potential for vocal assessments to skew the results in favour of LSVT LOUD due to the focus on vocal loudness, and the trial’s focus on participants’ self-perception of functional communication using voice or speech. This assessment is also commonly used in clinical practice with people with Parkinson’s disease.
The voice handicap index is a patient reported measure of the impact of communication difficulties and has a score between 0 and 120 (a low score being positive).20Secondary outcomes included the voice handicap index subscales; Parkinson’s disease questionnaire-3923; questionnaire on acquired speech disorders (also known as living with dysarthria)24; EuroQol5D25(five level version); icepop capabilities measure for older adults26; resource use; adverse events); Hoehn and Yahr stage19, and carer quality of life (Parkinson’s disease questionnaire–carers).27
Adverse events
Adverse events in people with Parkinson’s disease are well known, therefore, we reported only adverse events specific to SLT or serious adverse events related to vocal strain or injury. Vocal strain could be identified by patients reporting symptoms and therapists noticing clinical signs such as hoarseness. Deaths, if not deemed a serious adverse event according to the trial definition, were reported to the sponsors (Birmingham clinical trials unit) to ensure further trial data collection forms were not sent out. Data for adverse events were sought for all three trial arms.28
Statistical analysis
The primary comparisons in PD COMM were LSVT LOUD versus no SLT and NHS SLT versus no SLT. We also compared LSVT LOUD with NHS SLT. We used the intention-to-treat principle for all primary analyses for both primary and secondary outcomes. All estimates of differences between groups are presented with two sided, 99% confidence intervals, which was a deviation from the protocol in which 95% confidence intervals were stated, to allow for adjustment for multiple comparisons. Statistical analysis was undertaken using the statistical software packages: SAS software, version 9.4, and Stata version 17.
To estimate differences in the voice handicap index total score at three months between the two groups of interest, a linear regression model was used with the voice handicap index baseline score and the minimisation variables: age and severity of Parkinson’s disease (Hoehn and Yahr) included in the model as covariates. Various supporting (eg, per protocol) and sensitivity analyses (eg, to assess impact of missing data) were undertaken for the primary outcome. Subgroup analyses were also performed for the primary outcome to assess whether the treatment effect differed according to age, baseline voice severity, and Parkinson’s disease severity.
Continuous secondary outcome measures (eg, Parkinson’s disease questionnaire-39) were analysed using linear regression models adjusting for relevant baseline score and the minimisation variables (baseline voice handicap index, age and severity of Parkinson’s disease). The primary analysis of the secondary outcomes was at three months as per the primary outcome. Secondary analyses assessed the outcomes at both six and 12 months using linear regression analysis as per the primary analysis, and also using repeated measures models that included all data across the three, six, and 12 month assessment points. Adverse events and Hoehn and Yahr stage at 12 months were summarised descriptively. Medication doses were recorded, and we calculated levodopa dose equivalents for all medication using the accepted formula.29Where the participant had a non-professional carer, the carer was also invited to join the trial and complete the Parkinson’s disease questionnaire-Carer questionnaire at three, six, and 12 months.
Sample size
As the minimal clinically important change score for the voice handicap index, our primary outcome, has not yet been established, a 10 point difference in voice handicap index between both types of SLT and no SLT (control) as observed in the Parkinson’s disease COMM pilot trial was used to inform the sample size calculations.30Using a two sided t-test and the upper standard deviation of 26.27 obtained from the pilot trial (effect size 0.38) with 80% power and α=0.01; 163 participants per group were required. A sample size of 546 participants in total (182 participants per arm) was planned, anticipating 10% attrition.
Process evaluation
For the intervention process evaluation, individual participant data were extracted from treatment record forms and therapy notes for a subset of trial participants. A piloted data extraction form, designed with reference to TIDieR and dysarthria management guidelines descriptions,31supported the categorisation of therapy descriptions across both SLT interventions. One researcher completed the data extraction forms and a second independently checked a sample. Interviews with patients were also completed to explore their experiences of the implementation of their intervention.
Trial oversight
Independent trial steering and data monitoring committees provided oversight and included members with Parkinson’s disease. Interim data analyses of the primary outcome and adverse events were supplied in confidence to the data monitoring and ethics committee. This committee could recommend discontinuation of the trial to the trial steering committee if the recruitment rate or data quality were unacceptable, or if any issues were identified that may compromise participant safety.
Patient and public involvement
This project was originally designed in 2010 in response to a commissioned call by the National Institute for Health and Care Research for this specific trial design. The National Institute for Health Research, Health Technology Assessment commissioning stream works with stakeholders including patients and the public to prioritise questions to commission, as such, patients and the public were involved in this trial design. The project was not funded at the time of commissioning, so we developed a standalone pilot trial to test for feasibility and acceptability of the proposed trial. During the pilot trial, we surveyed patients with Parkinson’s disease from our patient and public involvement group. We asked these patients what was more important to them: vocal loudness or ability to communicate, which helped to determine the primary outcome measure for the substantive PD COMM trial. From the results of the pilot trial, we refined our design, based on the acceptability of the outcomes to the participants, and clinicians and selected our primary outcome measure based on the views of the patient and public involvement group and the co-applicants. The commissioned call was then funded by the National Institute for Health and Care Research.
The Birmingham Clinical Trials Unit hosts a patient and carer group with experience of neurological disease. The PD COMM group also worked with the local Parkinson’s UK branch and several individuals who contributed to the development, design, interpretation, oversight, reporting, and dissemination of the study.
We fully accept that the patient and public involvement was significantly less than we would do today, however, this was not considered unusual at the time particularly given the nature of the commissioned call and the substantial input of the participants and patient and public involvement group in the pilot trial.
Results
Over the 42 month recruitment period from 26 September 2016 to 16 March 2020, 388 people with dysarthria related to Parkinson’s disease were randomly assigned from 41 of 42 recruitment centres: 130 participants to LSVT LOUD, 129 participants to NHS SLT, and 129 participants to no SLT (fig 1). The covid-19 pandemic impacted on the provision of SLT services for people with Parkinson’s disease and, following discussions with the trial steering committee and the funder, the trial was closed to recruitment in 30 November 2020 after a period of recruitment suspension (16 March until 30 November) and before achieving the recruitment target (388 of 546 recruited; 71% of target). In total, 109 (84%) of 130 participants started LSVT LOUD, 119 (92%) of 129 participants started NHS SLT, and 120 (93%) of 129 participants did not receive SLT for the no SLT group. Reasons for withdrawal from the trial varied and included: SLT was too burdensome, Parkinson’s disease deteriorated, other commitments, and participant wanted SLT.
Participants were predominantly male (286/388; 74%), about half were 70 years or older and just under two thirds had mild (Hoehn and Yahr stage ≤2.0) Parkinson’s disease (table 1). Data collection form return rates were high throughout the trial; for the primary outcome, 99% of baseline forms were returned and 86% or more were returned at each time point. The total time recorded spending on the interventions was three times greater for the LSVT LOUD group and delivered over more sessions (mean 1216 mins (standard deviation 454); median 16 sessions) than in the NHS SLT group (404 mins (234); five sessions) over a shorter period (LSVT LOUD seven weeks (7); NHS SLT 11 weeks (11)) (appendix 1, table B). The companion software was used for seven LSVT LOUD participants from five sites with a range of three to eight sessions per participant. Some of the activities related to the therapy were similar in time allocation across the interventions (ie, goal setting, information provision and advice, and liaison/ onward referral). By contrast, active therapy time per participant differed with a mean of 752 mins (standard deviation 287) for LSVT LOUD plus 15 min (45) for other therapy given to the LSVT LOUD group compared with 149 mins (113) of therapy in the NHS SLT group, reflecting LSVT LOUD’s greater therapy intensity (hours per week) and frequency (days per week) (appendix 1, table B).
From evaluation into a subset of records of participants who completed therapy, most SLT interventions were delivered by qualified speech and language therapists on a one-to-one basis in outpatient settings. Some participants received therapy in a group (NHS SLT) or remotely via computer software (LSVT LOUD) and some received therapy from an assistant (across SLT interventions) (appendix 1, table B). As expected, LSVT LOUD activity was only reported in the therapy records of LSVT participants, including the use of LSVT worksheets.32NHS SLT mainly described impairment based and compensatory therapy, but also application of augmentative and alternative communication strategies, functional therapy, and generalisation. Both interventions used sheets, lists, pictures, and reading passages and magazines to practise speech production techniques learned in therapy. The treatment content reports showed variability and the likely tailoring of interventions to individual participants’ needs. Many LSVT LOUD records reported tailoring by level of difficulty and functional relevance, but such tailoring was less frequently reported in NHS SLT (appendix 1, table B).
Participants were considered adherent if they attended at least 14 of 16 LSVT LOUD sessions within 3 months of randomisation, if they completed their NHS SLT sessions within three months of randomisation, or if they received no therapy in the no SLT group. Adherence to LSVT LOUD was similar (59%; 77/130) to NHS SLT (54%; 70/129), although not as high as for the no SLT group (93%; 120/129). Participants in the no SLT (control) group were considered not adherent if they reported receiving SLT over the course of the 12 month follow-up, with an exception for SLT only for dysphagia. Patient interview data showed considerable determination to engage with the trial interventions successfully; although, some patients indicated that they found the intensity of LSVT LOUD to be challenging. The support of family members and adjustment of personal and family routines were key to facilitating participation.
For the voice handicap index total score at three months (primary outcome), LSVT LOUD was 8 points lower (ie, better) than for no SLT (−8.0 points (99% confidence interval (CI) −13.3 to 2.6), P<0.001). No evidence suggested a difference between the NHS SLT and no SLT groups (1.7 points (−3.8 to 7.1), P=0.43). The total score for voice handicap index in the LSVT LOUD group was nearly 10 points lower than that in the NHS SLT group (−9.6 points (−14.9 to −4.4), P<0.001) (table 2). Preplanned supporting and sensitivity analyses of the primary outcome were conducted and aligned the results of the primary outcome analysis (appendix 1, table C). The secondary analyses of the primary outcome, voice handicap index total score at six and 12 months and over the whole 12 months using a repeated measures analysis, gave similar results to those observed in the primary analysis at three months (table 3).
The subgroup analyses of the primary outcome with an exploratory hypothesis reported evidence of an interaction between the severity of the impact of voice problems (voice handicap index) and treatment (test for interaction P=0.007), but not for Parkinson’s disease severity (P=0.7) or age (P=0.7). Generally, the intervention effect increased as the baseline voice handicap index score increased; for example, for LSVT LOUD, greater benefits were observed among those reporting more severe voice handicap index scores at baseline (appendix 1, table D).
For all subscales (emotional, functional, and physical) of the voice handicap index (secondary outcomes), the scores were lower (ie, better) for LSVT LOUD compared with no SLT and NHS SLT at three months and for the overall trial period, with significant benefits observed for the emotional and functional subscales. No evidence suggested a difference between NHS SLT and no SLT at any time point across all three voice handicap index subscales (table 2andtable 3).
At three months, QASD scores (secondary outcome) were lower (ie, better) with LSVT LOUD compared with no SLT and NHS SLT. No evidence suggested a difference between the NHS SLT and no SLT groups (table 2). Similar results were seen at six and 12 months, and over the whole trial follow-up period (table 3).
The Parkinson’s disease questionnaire-39 (secondary outcome) assesses eight domains (mobility, activities of daily living, emotional wellbeing, stigma, social support, cognition, communication, and bodily discomfort) and overall quality of life. At three months, the largest differences were observed in the communication domain for LSVT LOUD versus no SLT: −6.2 points (99% CI −11.9 to −0.6, P=0.004), which exceeded the minimum clinically important difference for this domain (table 2andtable 3, and appendix, table E). For the icepop capabilities measure for older adults and EuroQol5D utility and visual analogue scores (secondary outcomes), no evidence of a difference was found for any of the comparisons at any time point (table 2andtable 3).
The carer quality of life summary index score (secondary outcome) was lower (ie, better) for both LSVT LOUD and no SLT groups when compared with NHS SLT at three months (table 2). Differences in favour of LSVT LOUD and no SLT when compared with NHS SLT were also observed in the anxiety and depression subscale for the carers at three months (appendix 1, table F).
At 12 months, the median Hoehn and Yahr stages were similar to baseline, and the amount of treatment (reported using levodopa equivalency) had increased since baseline (appendix 1, table G). No serious adverse events were reported in this trial. Adverse events were reported in 36/130 (28%; 93 adverse events) participants in the LSVT LOUD group, 16/129 (12%; 46 adverse events) participants in the NHS SLT group, and none in the no SLT group. Most adverse events reported were vocal strain, with a higher number in the LSVT LOUD group (80 events) compared with the NHS SLT group (45 events). Two participants from the LSVT LOUD group crossed over to the NHS SLT group following a vocal strain adverse event. One participant who experienced a dry aching throat following LSVT LOUD completed only nine sessions.
Discussion
Statement of principal findings
LSVT LOUD was more effective at reducing the participant reported impact of voice problems for people with dysarthria related to Parkinson’s disease than was NHS SLT and no SLT after three months. These results remain robust when the potential effects of non-adherence to treatment and the impact of missing data were investigated. The continued benefit of LSVT LOUD on dysarthria over the 12 month trial period compared with NHS SLT and no SLT is encouraging, but re-intervention might still be required should the treatment effect wear off or as their Parkinson’s disease progresses and their dysarthria deteriorates. A benefit was also observed in quality of life related to communication (using the Parkinson’s disease questionnaire-39) for patients randomly assigned to receive LSVT LOUD, which exceeded the minimal clinically important change score of 4.2.33The higher rate of vocal strain with LSVT LOUD treatment was mostly a minor, transient issue at an acceptable rate in relation to the level of benefit; although, the occurrence of this adverse effect reinforces the need for management by suitably skilled therapists. We consider the higher costs of delivering the intensive LSVT LOUD face to face by a qualified speech and language therapist and alternative methods of adapting delivery that could support a more sustainable service delivery in the economic analysis that will be published separately. However, given the relative benefits, the PD COMM trial results support the adoption of LSVT LOUD as an effective SLT intervention option for dysarthria related to Parkinson's disease.
NHS SLT reflected mixed theoretical therapeutic intervention tailored to the individual by the therapist. In PD COMM, no clear evidence suggested a benefit for NHS SLT compared with no SLT or LSVT LOUD after three months. The confidence intervals are, however, moderately wide, which may reflect variability in the intervention offered. NHS therapy was delivered at a much lower intensity and did not show benefit over control. Therefore, these results should not be interpreted as evidence of no beneficial effect for all NHS SLT theoretical approaches, across all dosages. Further research is required to understand the effectiveness of specific aspects of the intervention, including dosage.
Strengths and weaknesses of the study
In terms of trial limitations, most participants were in the early stages of Parkinson’s disease with mild speech impairment, which may not reflect the whole population of people with Parkinson’s disease who need SLT.34We did not collect sufficient screening data to assess this aspect. Differences in access to therapy and intervention format could not be concealed from participants, making trial blinding unfeasible. Trial outcomes were questionnaires completed by participants or carers. Thus, participants’ and carers’ knowledge and expectations about their treatment allocations, particularly to no treatment, may have contributed to an increased risk of performance bias.
The disadvantage of many previous trials of dysarthria related to Parkinson’s disease has been the use of sound pressure level (ie, speech volume) without including participant reported outcome measures. The use of a participant reported outcome measure is an advantage because the voice handicap index measures how the participant perceives the impact that their voice problems on their daily activities and their quality of life, which is a more meaningful measure of communication for them.1213Future trials of new interventions may benefit from developments in the field of clinically relevant participant reported outcome measures,35which explore participation rather than impairment outcomes.
Variation in experience levels of speech and language therapists, particularly with respect to being newly trained in LSVT LOUD specifically for the trial, presents a risk that this trial may not have captured the full potential of the SLT approaches. While the duration of treatment between the two active treatment populations differs, the treatment could have been stopped at any point within the three month window from randomisation. As a result, both interventions could have finished near to the primary outcome data collection point and both interventions could have happened in a short period. Finally, due to the covid-19 pandemic, the trial closed early (suspended March 2020, closed November 2020) to recruitment, and thus did not recruit to the planned sample size. Additionally, some follow-up data were lost because the data could not be accessed within the time frame. However, we do not believe that these amendments would have changed the trial’s overall conclusions and clinical implications. Our meta-analysis of the PD COMM pilot and full trial data supports this assumption (appendix 1, figures 2, 3, and 4).
Possible use for clinicians and policy makers
This large, pragmatic trial compares two commonly used SLT approaches against each other and against no therapy. A robust signal shows that, after three months, LSVT LOUD is effective compared with no SLT for the reduction of dysarthria related to Parkinson’s disease, which persists throughout the 12 months from starting treatment. This effect, combined with the lack of evidence of effectiveness of NHS SLT shown in this trial, means that optimal use of speech and language therapy resources for people with Parkinson’s disease need to be discussed.
Unanswered questions and future research
Evidence from language rehabilitation in relation to people who had strokes suggests that effective therapeutic interventions were associated with dosage (total hours), frequency (number of days per week), and intensity (hours per week) regimens beyond a specific threshold. Thus, SLT may have a dose effect, and treatment threshold might be relevant and interact with participant characteristics, such as severity.36The PD COMM trial was not designed to provide evidence about the relative benefits of NHS SLT versus LSVT LOUD at equal doses or different dose combinations.
Attention should, however, be given to factors beyond the treatment content when determining the make-up of future SLT services: the availability of speech and language therapists, access to outpatients, home and remote visits, software, costs, and frequency of treatment required. This trial also encourages a closer look at the effect of SLT provision on carers, and further research involving outcomes for ca","Objectives: To assess the clinical effectiveness of two speech and language therapy approaches versus no speech and language therapy for dysarthria in people with Parkinson’s disease.
Design: Pragmatic, UK based, multicentre, three arm, parallel group, unblinded, randomised controlled trial.
Setting: The speech and language therapy interventions were delivered in outpatient or home settings between 26 September 2016 and 16 March 2020.
Participants: 388 people with Parkinson’s disease and dysarthria.
Interventions: Participants were randomly assigned to one of three groups (1:1:1): 130 to Lee Silverman voice treatment (LSVT LOUD), 129 to NHS speech and language therapy, and 129 to no speech and language therapy. LSVT LOUD consisted of four, face-to-face or remote, 50 min sessions each week delivered over four weeks. Home based practice activities were set for up to 5-10 mins daily on treatment days and 15 mins twice daily on non-treatment days. Dosage for the NHS speech and language therapy was determined by the local therapist in response to the participants’ needs (estimated from prior research that NHS speech and language therapy participants would receive an average of one session per week over six to eight weeks). Local practices for NHS speech and language therapy were accepted, except for those within the LSVT LOUD protocol. Analyses were based on the intention to treat principle.
Main outcome measures: The primary outcome was total score at three months of self-reported voice handicap index.
Results: People who received LSVT LOUD reported lower voice handicap index scores at three months after randomisation than those who did not receive speech and language therapy (−8.0 points (99% confidence interval −13.3 to −2.6); P<0.001). No evidence suggests a difference in voice handicap index scores between NHS speech and language therapy and no speech and language therapy (1.7 points (−3.8 to 7.1); P=0.43). Patients in the LSVT LOUD group also reported lower voice handicap index scores than did those randomised to NHS speech and language therapy (−9.6 points (−14.9 to −4.4); P<0.001). 93 adverse events (predominately vocal strain) were reported in the LSVT LOUD group, 46 in the NHS speech and language therapy group, and none in the no speech and language therapy group. No serious adverse events were recorded.
Conclusions: LSVT LOUD was more effective at reducing the participant reported impact of voice problems than was no speech and language therapy and NHS speech and language therapy. NHS speech and language therapy showed no evidence of benefit compared with no speech and language therapy.
Trial registration: ISRCTN registryISRCTN12421382.
"
Effectiveness of behavioural interventions with motivational interviewing on physical activity outcomes,"Introduction
Physical inactivity, or the failure to meet physical activity recommendations, is one of the leading risk factors for non-communicable diseases,1and it is responsible for an estimated 9% of premature deaths worldwide.2The benefits on health of being physically active are dose dependent, so most people—including those who currently achieve physical activity recommendations—are likely to benefit from being more physically active.3
Guidelines from the World Health Organization recommend that adults (aged 18-64 years) should engage in a minimum of 150-300 minutes of moderate intensity, or 75 minutes of vigorous intensity, physical activity each week, combined with strength training activities to develop or maintain strength in major muscle groups, as well as a reduction in sedentary time.4Despite longstanding policy initiatives, however, one in three women and one in four men do not meet the levels of physical activity set out in the guidelines.56A systematic review found that individual level interventions to promote physical activity that provided professional advice and guidance with continued support can encourage people to be more physically active in the short to medium term.7More research is, however, needed to establish which behaviour change techniques are most effective in the long term.7
Motivational interviewing is a communication technique commonly used in multicomponent, complex interventions to elicit behavioural change.8It is a patient centred counselling style that helps patients change their problematic behaviours by exploring and resolving their ambivalence towards behavioural change in a non-confrontational style.9Motivational interviewing empowers patients to increase their autonomous motivation, such that change arises from within the individual rather than being imposed by others,1011and it has been used successfully in people who smoke, have addiction problems, or have an eating disorder, and in diabetes management.121314Therefore motivational interviewing may be a useful technique to help people achieve physical activity guidelines, since interventions that include motivational interviewing could feasibly be delivered at scale by healthcare professionals who have regular contact with people.
Previous systematic reviews and meta-analyses have reported that interventions with motivational interviewing led to a small but significant increase in physical activity in the short term in patients with specific health conditions.151617However, less consideration has been given to longer term effects and the effects in general population samples. Motivational interviewing requires the people who deliver the interventions to undergo specialist training and continued professional development to learn and develop the skills required to enhance motivation towards behaviour change. Interventions that include motivational interviewing therefore require extended intervention contact time and sessions, resulting in additional time and financial resources for delivery. As such it is important to determine the effectiveness of interventions with motivational interviewing and to examine the durability of the effect beyond the active intervention period.
We systematically reviewed the evidence from randomised controlled trials for behavioural interventions that included motivational interviewing for the promotion of physical activity in adults. Additionally, we examined the effect of treatment duration, durability of any effect, and the effectiveness in groups selected on the basis of pre-existing disease or health conditions, or in the general population who were not specifically selected because of their health condition or disease status.
Methods
This systematic review and meta-analysis was performed in accordance with the PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines.18A protocol was developed and prospectively registered with PROSPERO and is available athttps://www.crd.york.ac.uk/prospero/display_record.php?RecordID=219881.
Eligibility criteria
Eligible criteria were randomised controlled trials, including cluster randomised trials, in adults (≥18 years) that compared interventions comprising motivational interviewing to support or promote physical activity as the primary or secondary treatment goal versus interventions without a motivational interviewing component. The interventions with motivational interviewing had to specify that a component of the intervention included the core principles of motivational interviewing as outlined by Miller and Rollick.11These principles include having a clear focus on the behaviour change (in this case, physical activity), empathetic listening to establish a relationship, and evoking patients’ own motivation for change.9We determined that the study was eligible in terms of intervention content if authors stated motivational interviewing or motivational interviewing techniques were applied. Using a checklist, reviewers allocated eligible comparator interventions to one of three groups: no intervention, minimal control (including usual care) intervention, or active control intervention, all of which used alternative approaches or interventions to promote physical activity that did not include motivational interviewing techniques. To be eligible for inclusion the study had to include a quantitative physical activity outcome at baseline and follow-up.
Outcomes
Eligible studies needed to report at least one of the physical activity outcomes of interest—total physical activity, moderate to vigorous physical activity (MVPA), or sedentary time, or a combination of these—using a quantitative unit (eg, steps/day, min/day, min/week, energy expenditure, metabolic equivalents (METs)). When a study did not report all the outcomes of interest, we included the study results in the analysis for only the outcomes reported. If studies used more than one method to assess total physical activity and sedentary time outcomes, we prioritised device-measured outcomes over self-reported outcome measures. If more than one device-measured method was reported, we prioritised measures reporting time spent on physical activity, followed by step counts, distance walked, and energy expenditure or metabolic equivalent of task. If no result for a device-measured outcome was available, we extracted self-reported measures such as questionnaires and diaries.
To determine the effect of interventions with motivational interviewing on physical activity over time, we examined effectiveness at 0-3 months, 4-6 months, 7-12 months, and >1 year from baseline.
Search methods for identification of studies
We systematically searched seven electronic databases (CINAHL, Embase, AMED, Medline, PsychINFO, SPORTDiscus, and Cochrane Central Register of Controlled Trials) for articles, including theses, published from inception until 1 March 2023. Searches were restricted to studies in English language. To locate further relevant publications we performed forward and backwards citation searches of previous systematic reviews. To identify ongoing clinical trials, we searched ClinicalTrials.gov and contacted the authors of published study protocols if there was uncertainty about a trial’s status (ie, if the anticipated completion date was overdue but we could not identify the published study). Supplementary table 1 presents a sample of our search strategy.
Study selection, data extraction, and risk of bias
After duplicates had been removed, a combination of two reviewers (DS, MM, AH, SZ, PD, GW, MK, NMA) independently screened the titles and abstracts of identified studies using the Cochrane systematic review software Covidence (www.covidence.org).19Two reviewers then independently assessed the full text of articles against the defined eligibility criteria, with discrepancies resolved by discussion or by consultation with a third reviewer. A prespecified and piloted data extraction form was used to obtain key information from included studies on study setting, population characteristics, intervention characteristics (according to the Template for Intervention Description and Replication20), and outcome data. Data were extracted by one reviewer and verified by a second reviewer. Disagreements were resolved by consultation with a third reviewer.
Risk of bias assessment
Two reviewers independently assessed risk of bias of included studies using version 2 of the Cochrane risk of bias tool for randomised trials.21Studies were judged to be at low or high risk of bias or to have some concerns in several domains: randomisation process, deviation from the intended intervention, missing outcome data, measurement of the outcome, and selection of reported results. Overall ratings were taken from the most biased rating across all domains (ie, if one domain was judged to be high then the overall rating was high). Disagreements between reviewers were discussed until consensus was reached.
Data synthesis and analysis
We extracted the mean and standard deviation (SD) for outcome measures. If these were not reported or unavailable, they were estimated using reported data or graphical figures, or if only medians were available we used these as a direct replacement for mean values, as recommended by the Cochrane Handbook.22We contacted authors for missing data and clarification when necessary. To overcome variability in the way physical activity outcomes were measured in different studies, we calculated the difference in the change in physical activity from baseline (pre-intervention) to follow-up between intervention and comparator groups using standardised mean difference (SMD) with 95% confidence interval (CI). Because studies dealt with missing follow-up data in different ways, to reduce spurious heterogeneity we extracted the complete case data and then used the baseline observation carried forward for missing data to recalculate the change in physical activity.23For studies that were eligible for inclusion but did not provide enough data for meta-analysis, we synthesised the study results narratively.
Pooled data were summarised using Hartung-Knapp-Sidik-Jonkman random effects meta-analysis.24Based on feedback from our patient and public involvement group, to make findings more meaningful for the main findings we transformed the SMDs from the pooled analyses on total physical activity, MVPA, and sedentary time into equivalent weighted mean differences in daily steps (for total physical activity), weekly minutes (for MVPA), and daily minutes (for sedentary time).22For this conversion, we used median SDs of 2940 steps/day for total physical activity, 211 min/week for MVPA, and 87 min/day for sedentary time.25If a study contributed more than one intervention arm to a meta‐analysis, we divided the control group equally between interventions to avoid double counting in the pooled result.
Two types of meta-analyses were performed. For the meta-analyses on overall main effects, we included the longest follow-up measure of physical activity from each study. For meta-analyses split by follow-up assessment time, each study was eligible for inclusion once in each follow-up group (0-3 months, 4-6 months, 7-12 months, and >1 year). If a study reported outcomes at several follow-up time points within each follow-up group (eg, four weeks and 12 weeks), we used the longest follow-up in the analysis (ie, the study was only included once in each follow-up group analysis).
We used the Cochrane Q test to identify heterogeneity, and quantified it using the I2statistic and the between study variance τ2. We followed Cochrane Handbook recommendations to interpret I2values (<0.4 representing a small effect, 0.4-0.7 a moderate effect, >0.7 a large effect).22Heterogeneity was explored by determining the effect of several variables on the outcomes: comparator type, intervention durations, outcome assessment method, and participant disease or health condition status.
Funnel plots were generated and Egger’s test was performed to detect small study and publication bias. For all statistical analyses, we considered an α of <0.05 to be statistically significant. STATA SE 17.0 was used for all analyses. The statistical code used in the analysis is available athttps://github.com/nerysastbury/MI_SR.git.
To determine the effect of excluding studies at high risk of bias on overall outcomes, we performed a sensitivity analysis. We had planned a sensitivity analysis excluding studies that reported poor intervention fidelity—however, although studies did report assessing fidelity of the intervention, the outcomes of the assessments were poorly reported, resulting in the inability to create discrete groups based on fidelity outcomes, which is required to undertake a sensitivity analysis.
Two reviewers (NMA and SZ) independently rated the certainty of evidence for each outcome using GRADE (Grading of Recommendations Assessment, Development and Evaluation). The certainty of the evidence was assessed for the domains of risk of bias, inconsistency, indirectness, imprecision, and publication bias.26
Patient and public involvement
We convened a focus group of five individuals who self-identified as not being physically active but wanted to increase daily physical activity or had been advised to do so by a healthcare professional. The purpose of the review was described in detail to them, and the findings were explained. Our patient and public involvement panel agreed the review was useful, and it provided feedback on interpretation of the findings and suggested we describe the results in a more meaningful way. This led to our decision to convert SMD into more meaningful outcomes to be more easily interpretable by members of the public, and we present these conversions alongside the main results.
Results
A literature search on 1 March 2023 identified a total of 7323 unique records, of which 359 full texts were assessed for eligibility. The main reasons for exclusion at the full text stage were that studies did not specify use of motivational interviewing in their intervention or they did not report a quantitative physical activity outcome measure (fig 1). In total, 97 unique randomised controlled trials comparing the effect of 105 interventions comprising motivational interviewing in 27 811 participants were included.27282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123
Characteristics of included studies
Supplementary table 2 provides the characteristics of the included studies. Study sample sizes varied from 23 to 4283 participants, with six studies comprising >1000 participants.5257627382118The median age of participants was 55.5 (interquartile range (IQR) 45.7-64.2) years and median baseline body mass index (BMI) was 28.9 (27.2-30.8). The median proportion of female participants was 66% (34%). Most of the included studies were conducted in high income countries in North America (n=44, 45%), Europe (n=35, 36%), Asia (n=2, 2%), Middle East (n=1, 1%), and Australasia (n=12, 12%), with only three studies (2%) conducted in low and middle income countries (Iran, North Korea, Turkey). Around one quarter of the studies (n=25, 28%) were conducted in generally healthy participants, with the remainder (n=72, 74%) in patients with a health condition or pre-existing disease. The most common health condition or disease of interest was cardiovascular disease (including risk reduction and secondary prevention; n=19, 20%), with fewer studies conducted specifically in people with overweight or obesity (n=10, 10%) and those with musculoskeletal conditions (including osteoarthritis and rheumatoid arthritis; n=9, 9%). Thirty three studies assessed outcomes using device-measured methods such as pedometers or accelerometers, and 47 studies used self-reported methods, including questionnaires, physical activity logs, and diaries.
Motivational interviewing was delivered in a variety of ways between the studies (see supplementary table 3). The number of motivational interviewing sessions offered to participants assigned to receive interventions with motivational interviewing ranged from one to 70 over an intervention period of one to 24 months, with a median duration of 32.9 (23.1-60.0) minutes each. In half of the motivational interviewing interventions (n=53, 50%), treatment duration lasted up to three months, with the remaining interventions reporting longer durations: 4-6 months (n=25, 24%), 7-12 months (n=24, 23%), and >1 year, to a maximum of 24 months (n=3, studies, 3%). Most studies (n=89, 92%) reported who delivered the intervention with motivational interviewing, and 74 studies (76%) reported the training and qualifications of the interventionists, which ranged from undergraduate students to experienced psychologists, with considerable variation in amount of training and experience of delivering motivational interviews.
Most interventions with motivational interviewing (n=96, 91%) were delivered in a one-to-one format. Five interventions (5%) were delivered in a combination of individual and group motivational interviewing sessions, and four interventions (4%) offered group sessions only. Motivational interviewing was delivered using a range of modalities, with 33 interventions (31%) delivered face-to-face, 31 (30%) by telephone, and six (6%) through the internet or a mobile application, and 35 studies (n=33) used a combination of in-person and remote delivery methods.
Most of the interventions with motivational interviewing were compared with no intervention or minimal control interventions (n=74, 70%) and 31 (30%) had an active comparator that was either an alternative behavioural intervention to promote physical activity that did not include motivational interviewing of similar (n=11, 10%) or less intensity (n=20, 19%) to the intervention with motivational interviewing being delivered.
Risk of bias and quality assessment
In studies reporting total physical activity, most (n=41, 52%) were judged to be at overall high risk of bias, four (5%) were judged to be at overall low risk of bias, and 34 (43%) had some concerns (fig 2).
The main reason for a high overall risk of bias judgement was primarily from bias due to the missing outcome data domain (eg, emphasis on per protocol analysis, incorrect procedures to account for missing data, loss to follow-up concerns). Supplementary figure 1 presents details of the risk of bias assessments for individual studies.
Funnel plot asymmetry was found among studies included in the meta-analysis of motivational interviewing on total physical activity, MVPA, and sedentary time (Egger’s g=2.51 (95% CI 1.34 to 3.66), 2.34 (0.71 to 3.97), and −2.62 (−4.49 to −0.74), respectively) suggesting that studies could be missing in the literature that reported null or negative findings of motivational interviewing or small study effects (supplementary figure 2).124
Overall effect of motivational interviewing on physical activity
Interventions including motivational interviewing were superior to comparators for total physical activity (n=76, 19 732 participants, 86 comparisons; SMD 0.45, 95% CI 0.33 to 0.65; I2=90.8%), equivalent to 1323 extra steps/day (95% CI 970 to 1911) (fig 3, supplementary figure 3). Three studies could not be included in this meta-analysis because they failed to report SDs, or information was insufficient that would permit calculation of SDs. Of these studies excluded from the meta-analysis, none reported a difference in physical activity between motivational interviewing and control groups.414890
Interventions including motivational interviewing were also superior to comparators for MVPA (n=42, 10 683 participants, 44 comparisons; SMD 0.45 (95% CI 0.19 to 0.71); I2=91.3%), equivalent to 95 extra min/week (95% CI 40 to 150) (fig 3, supplementary figure 4). Three studies reporting MVPA outcomes could not be included in the meta-analysis because they failed to report SDs or information to permit calculation of SDs was insufficient. However, these studies reported minimal or no effect of interventions with motivational interviewing on MVPA outcomes.414952
Interventions including motivational interviewing were superior to comparators in reducing sedentary time (n=23, 2673 participants, 24 comparisons; SMD −0.582, 95% CI 1.03 to 0.14; I2=88.3%) (fig 3, supplementary figure 5) equating to 51 fewer minutes (95% CI −90 to −12) of sedentary time each day.
Effect of motivational interviewing on physical activity and sedentary time
As heterogeneity in the outcome measures was considerable, we analysed whether the type of comparator groups had an influence on the overall findings.
No or minimal comparator intervention—Interventions including motivational interviewing were superior to no or minimal comparator interventions (eg, usual care) for total physical activity (n=55, 16 079 participants, 60 comparisons; SMD 0.56 (95% CI 0.37 to 0.76); I2=90.4%), MVPA (n=28, 8318 participants, 30 comparisons; 0.45 (0.24 to 0.65); I2=90%), and sedentary time (n=13, 1734 participants, 15 comparisons; −0.59 (1.18 to −0.01); I2=85.5%) (supplementary figures 6-8).
Less intensive comparator interventions—Some studies compared interventions including motivational interviewing with other active interventions of lower intensity (eg, educational website on how to increase physical activity, single lecture on self-management) that did not include motivational interviewing. Evidence was lacking for a difference between groups for total physical activity (n=18, 2312 participants, 18 comparisons; 0.23 (−0.09 to 0.56); I2=92.9%), MVPA (n=12, 1997 participants, 12 comparisons; 0.61 (−0.436 to 1.65); I2=94.3%), or sedentary time (n=7, 598 participants, 7 comparisons; −0.65 (−1.80 to 0.51); I2=93.7%) (supplementary figure 9).
Similar intensity comparator interventions—To isolate the effect of motivational interviewing on physical activity in complex interventions, we compared interventions including motivational interviewing with comparator interventions of similar intensity that did not include motivational interviewing. Evidence was lacking for a difference between groups for total physical activity (n=7, 1340 participants, eight comparisons; 0.43 (−0.08 to 0.927); I2=83.3%), MVPA n=2, 368 participants, two comparisons; 0.02 (−0.55 to 0.59); I2=0%), or sedentary time (n=1, 302 participants, one comparison; −0.08 (−0.31 to 0.152) (supplementary figure 10).
We compared effects in studies with device-measured outcomes with studies using self-reported outcome assessment methods. No significant between group heterogeneity was found for device-measured and self-reported measured groups for total physical activity (P=0.33), MVPA (P=0.43), or sedentary time (P=0.07), and heterogeneity remained substantial within these subgroups (supplementary figures 11-13).
Device-measured outcome assessment methods—In the studies using device-measured outcome assessment methods, evidence suggested that interventions with motivational interviewing were superior to comparators for total physical activity (n=25, 19 732 participants, 27 comparisons; 0.37 (0.10 to 0.64); I2=91.3%), MVPA (n=42, 10 683 participants, 44 comparisons; 0.61 (0.19 to 0.71); I2=90.7%), and sedentary time (n=11, 1123 participants, 13 comparisons; −0.26 (−0.89 to 0.38); I2=86.8%).
Self-reported outcome assessment methods—For the studies that used self-reported outcome assessment methods, interventions with motivational interviewing were superior to comparators for total physical activity (n=54, 15 152 participants, 59 comparisons; 0.53 (0.34 to 0.73); I2=90.6%), MVPA (n=23, 8243 participants, 24 comparisons; 0.37 (0.11 to 0.62); I2=91.7%), and sedentary time (n=9, 1550 participants, 11 comparisons; −1.01 (−1.65 to −0.37); I2=90.1%).
Durability of effectiveness of motivational interviewing over time
We examined the effectiveness of interventions with motivational interviewing at 0-3 months, 4-6 months, 7-12 months, and >1 year from baseline. For all follow-up times up to 12 months, interventions with motivational interviewing were superior to comparators for total physical activity, MVPA, and sedentary time, with a trend for declining effect size with increasing duration of follow-up (fig 4). For studies with follow-up beyond one year, evidence that interventions with motivational interviewing were any different from comparators for any of the three outcomes was lacking (fig 4, supplementary figures 14-25). Heterogeneity between studies in each outcome at each time point was substantial, apart for sedentary time at >1 year follow-up, where only two eligible studies were included in the meta-analysis (I2=0%).
Effect of treatment duration
For interventions of up to three months’ duration, those with motivational interviewing were superior to comparators for total physical activity at the end of the intervention period (n=34, 2182 participants, 32 comparisons; 0.70 (0.46 to 0.92); I2=79.2%) (supplementary figure 26), but evidence was lacking for a statistically significant difference between groups after the intervention ended (supplementary figures 27 and 28). Heterogeneity was substantial at all time points.
For interventions of 4-6 months’ duration, those with motivational interviewing were superior to comparators for total physical activity at the end of the intervention period (n=19 studies, 3218 participants, 20 comparisons; 0.99 (0.49 to 1.49); I2=94.1%) (supplementary figure 29), and these effects were sustained at 7-12 months follow-up (n=4, 1221 participants, 2 comparisons; 0.23 (0.10 to 0.37); I2=0%) (supplementary figure 30), but not for follow-up beyond one year (n=2, 648 participants, 2 comparisons; 0.004 (−0.18 to 0.19); I2=0%) (supplementary figure 31). Heterogeneity was substantial at the end of intervention follow-up, but the results of longer term follow-up showed no heterogeneity.
For interventions of 7-12 months’ duration, those with motivational interviewing were superior to comparators for total physical activity at the end of the intervention period (n=17, 11 262 participants, 21 comparisons; 0.26 (0.06 to 0.47); I2=89.7%) (supplementary figure 32), but evidence of a difference between groups in studies reporting follow-up beyond one year was lacking (n=5 studies, 8358 participants, 7 comparisons; 0.13 (−0.11 to 0.37); I2=78.6%) (supplementary figure 33).
For interventions of 12 months’ duration or longer, no statistically significant difference was found between interventions with motivational interviewing and comparators for physical activity at the end of the intervention period (n=3 studies, 1103 participants, 73 comparisons; −0.20 (–2.24 to 1.85); I2=98.2%) (supplementary figure 34).
For MVPA outcomes, interventions with motivational interviewing of up to 12 months’ duration were superior to comparators at 0-3 months follow-up (supplementary figure 35), but no significant differences were evident at longer follow-up times (supplementary figures 36-38). Interventions with motivational interviewing of longer than one year’s duration were not statistically significantly different from comparators at any follow-up time (supplementary figures 35-38).
For sedentary time outcomes, evidence for interventions with motivational interviewing being superior to comparators was lacking, regardless of intervention duration at any follow-up time (supplementary figures 39-42).
Impact of participant health status
In studies of people with a pre-existing health condition or disease, evidence suggested that interventions with motivational interviewing led to greater increases in total physical activity (n=60, 10 525 participants, 66 comparisons; 0.55 (0.35 to 0.76); I2=91.2%) and MVPA (n=34, 7094 participants, 36 comparisons; 0.46 (0.26 to 0.67); I2=90.3%), and reductions in sedentary time (n=18, 2171 participants, 20 comparisons; −0.72 (−1.2 to −0.22); I2=87.8%) (supplementary figures 43-45).
In studies of people not specifically selected on the basis of a pre-existing health condition or disease, interventions with motivational interviewing were superior to comparators for total physical activity (n=19 studies, 9207 participants, 20 comparisons; 0.35 (0.13 to 0.57); I2=89.6%). Evidence that interventions with motivational interviewing were superior to comparators for MVPA (n=8, 3589 participants; 0.43 (−0.99 to 1.84); I2=94.6%) or sedentary time (n=4, 502 participants; −0.02 (−1.27 to 1.23); I2=90.5%) (supplementary figures 46-48) was lacking.
Sensitivity analysis
We conducted sensitivity analyses on the principal outcomes, excluding those studies judged at overall high risk of bias.
When studies at high risk of bias were excluded from analysis, interventions with motivational interviewing remained superior to comparators for total physical activity (n=38, 8467 participants, 40 comparisons; 0.54 (0.31 to 0.78); I2=89.2%) (supplementary figure 49) but showed no difference for MVPA (n=21, 4935 participants, 22 comparisons; 0.41 (−0.07 to 0.90); I2=92.1%) (supplementary figure 50) or sedentary time (n=13, 1476 participants, 3 comparisons; −0.22 (−0.70 to 0.26); I2=82.7%) (supplementary figure 51).
Certainty of evidence
Certainty in the effect estimates for interventions with motivational interviewing on total physical activity was rated as low.
Sensitivity analysis removing studies judged at overall high risk of bias did not affect the outcome on total physical activity. Heterogeneity was, however, substantial (I2=90.8%), which could not be fully explained by comparator type, follow-up duration, intervention duration, disease status of participants, device outcome assessment method, or risk of bias. Furthermore, funnel plot asymmetry was evident, suggesting potentially over-optimistic estimates of the effect of interventions with motivational interviewing on physical activity, possibly due to publication and small study bias. Certainty in the effect estimate was therefore downgraded one level owing to unexplained inconsistency, and one level owing to publication bias.
Certainty in the effect estimates for interventions with motivational interviewing on MVPA was rated as very low.
Heterogeneity was substantial (I2=91.3%), which could not be fully explained by comparator type, follow-up duration, intervention duration, disease status of participants, device outcome assessment method, or risk of bias. Funnel plot asymmetry was evident, suggesting publication or small study bias, and sensitivity analysis excluding studies judged to be at high risk of bias resulted in no difference between interventions with motivational interviewing and comparators. Certainty of evidence was therefore downgraded two levels owing to very serious unexplained inconsistency, and one level owing to publication bias.
Certainty of the effect estimates for sedentary time was very low.
Heterogeneity was substantial (I2=88.3%), which could not be fully explained. Funnel plot asymmetry was significant, and the sensitivity analysis removing studies judged at overall high risk of bias resulted in no difference between interventions with motivational interviewing and comparators, suggesting risk of bias was high. Certainty of evidence for sedentary time was therefore downgraded two levels owing to very serious unexplained inconsistency, and one level owing to publication bias.
Discussion
Overall, 97 randomised controlled trials examining the effectiveness of 105 interventions comprising motivational interviewing to increase physical activity were included in this review. The totality of the evidence showed that interventions with motivational interviewing led to a greater increase in total physical activity (an extra 1300 steps/day) and MVPA (an extra 95 min/day) and reductions in sedentary time (50 fewer min/day) compared with comparator interventions. Certainty of the evidence for interventions with motivational interviewing promoting physical activity was low for total physical activity and very low for MVPA and reduction in sedentary time, with few high quality studies.
Effect sizes for studies using device-measured total physical activity and sendentary time outcomes were more modest, and for MVPA were higher than for studies using self-reported outcomes. This finding is consistent with reports that self-reported measures can over-report physical activity levels.125
We found no evidence of an effect when interventions with motivational interviewing were compared with comparator interventions of similar intensity. Most studies were judged to be at high risk of bias, and when the","Objective: To evaluate the effectiveness of behavioural interventions that include motivational interviewing on physical activity outcomes in adults.
Design: Systematic review and meta-analysis.
Study selection: A search of seven databases for randomised controlled trials published from inception to 1 March 2023 comparing a behavioural intervention including motivational interviewing with a comparator without motivational interviewing on physical activity outcomes in adults. Outcomes of interest were differences in change in quantitative measures of total physical activity, moderate to vigorous physical activity (MVPA), and sedentary time.
Data extraction and synthesis: Two reviewers extracted data and assessed risk of bias. Population characteristics, intervention components, comparison groups, and outcomes of studies were summarised. For overall main effects, random effects meta-analyses were used to report standardised mean differences (SMDs) and 95% confidence intervals (CIs). Differential effects based on duration of follow-up, comparator type, intervention duration, and disease or health condition of participants were also examined.
Results: 129 papers reporting 97 randomised controlled trials totalling 27 811 participants and 105 comparisons were included. Interventions including motivational interviewing were superior to comparators for increases in total physical activity (SMD 0.45, 95% CI 0.33 to 0.65, equivalent to 1323 extra steps/day; low certainty evidence) and MVPA (0.45, 0.19 to 0.71, equivalent to 95 extra min/week; very low certainty evidence) and for reductions in sedentary time (−0.58, −1.03 to −0.14, equivalent to −51 min/day; very low certainty evidence). Evidence for a difference in any outcome compared with comparators of similar intensity was lacking. The magnitude of effect diminished over time, and evidence of an effect of motivational interviewing beyond one year was lacking. Most interventions involved patients with a specific health condition, and evidence of an effect of motivational interviewing to increase MVPA or decrease sedentary time was lacking in general population samples.
Conclusions: Certainty of the evidence using motivational interviewing as part of complex behavioural interventions for promoting total physical activity in adults was low, and for MVPA and sedentary time was very low. The totality of evidence suggests that although interventions with motivational interviewing increase physical activity and decrease sedentary behaviour, no difference was found in studies where the effect of motivational interviewing could be isolated. Effectiveness waned over time, with no evidence of a benefit of motivational interviewing to increase physical activity beyond one year.
Systematic review registration: PROSPERO CRD42020219881.
"
Trends in cardiovascular disease incidence among 22 million people in the UK over 20 years,"Introduction
Since the 1970s, the prevention of coronary disease, both primary and secondary, has improved considerably, largely attributable to public health efforts to control risk factors, such as antismoking legislation, and the widespread use of drugs such as statins.12
Improvements in mortality due to heart disease have, however, stalled in several high income countries,3and reports suggest that the incidence of heart disease might even be increasing among younger people.456Conversely, along with coronary heart disease, other cardiovascular conditions are becoming relatively more prominent in older people, altering the profile of cardiovascular disease (CVD) in ageing societies. The importance of non-traditional risk factors for atherosclerotic diseases, such as socioeconomic deprivation, has also been increasingly recognised. Whether socioeconomic deprivation is as strongly associated with other CVDs as with atherosclerosis is uncertain, but it is important to understand as many countries have reported an increase in socioeconomic inequalities.7
Large scale epidemiological studies are therefore needed to investigate secular trends in CVDs to target future preventive efforts, highlight the focus for future clinical trials, and identify healthcare resources required to manage emerging problems. Existing comprehensive efforts, such as statistics on CVD from leading medical societies or the Global Burden of Diseases studies, have helped toward this goal, but reliable age standardised incidence rates for all CVDs, how these vary by population subgroups, and changes over time are currently not available.8910
We used a large longitudinal database of linked primary care, secondary care, and death registry records from a representative sample of the UK population1112to assess trends in the incidence of 10 of the most common CVDs in the UK during 2000-19, and how these differed by sex, age, socioeconomic status, and region.
Methods
Data source and study population
We used anonymised electronic health records from the GOLD and AURUM datasets of Clinical Practice Research Datalink (CPRD). CPRD contains information on about 20% of the UK population and is broadly representative of age, sex, ethnicity, geographical spread, and socioeconomic deprivation.1112It is also one of the largest databases of longitudinal medical records from primary care in the world and has been validated for epidemiological research for a wide range of conditions.11We used the subset of CPRD records that linked information from primary care, secondary care from Hospital Episodes Statistics (HES admitted patient care and HES outpatient) data, and death certificates from the Office for National Statistics (ONS). Linkage was possible for a subset of English practices, covering about 50% of the CPRD records. Data coverage dates were 1 January 1985 to 31 December 2019 for primary care data (including drug prescription data), 1 April 1997 to 30 June 2019 for secondary care data, and 2 January 1998 to 30 May 2019 for death certificates.
Included in the study were men and women registered with a general practice for at least one year during the study period (1 January 2000 to 30 June 2019) whose records were classified by CPRD as acceptable for use in research and approved for HES and ONS linkage.
Study endpoints
The primary endpoint was the first presentation of CVD as recorded in primary or secondary care. We investigated 10 CVDs: acute coronary syndrome, aortic aneurysm, aortic stenosis, atrial fibrillation or flutter, chronic ischaemic heart disease, heart failure, peripheral artery disease, second or third degree heart block, stroke (ischaemic, haemorrhagic, or unspecified), and venous thromboembolism (deep vein thrombosis or pulmonary embolism). We defined incident diagnoses as the first record of that condition in primary care or secondary care regardless of its order in the patient’s record.
Diseases were considered individually and as a composite outcome of all 10 CVDs combined. For the combined analyses, we calculated the primary incidence (considering only the first recorded CVD in each patient, reflecting the number of patients affected by CVDs) and the total incidence (considering all incident CVD diagnoses in each patient, reflecting the cumulative number of CVD diagnoses). We performed sensitivity analyses including diagnoses recorded on death certificates.
To identify diagnoses, we compiled a list of diagnostic codes based on the coding schemes in use in each data source following previously established methods.131415We used ICD-10 (international classification of diseases, 10th revision) codes for diagnoses recorded in secondary care, ICD-9 (international classification of diseases, ninth revision) (in use until 31 December 2000) and ICD-10 codes for diagnoses recorded on death certificates (used in sensitivity analyses only), the UK Office of Population Censuses and Surveys classification (OPCS-4) for procedures performed in secondary care settings, and a combination of Read, SNOMED, and local EMIS codes for diagnoses recorded in primary care records (see supplementary table S1).16Supplementary texts S1, S2, and S3 describe our approach to the generation of the diagnostic code list as well as considerations and sensitivity analyses into the validity of diagnoses recorded in UK electronic health records.
Covariates
We selected covariates to represent a range of known cardiovascular risk factors. For clinical data, including systolic and diastolic blood pressure, smoking status, cholesterol (total:high density lipoprotein ratio), and body mass index (BMI), we abstracted data from primary care records as the most recent measurement within two years before the incident CVD diagnosis. BMI was categorised as underweight (<18.5), normal (18.5-24.9), overweight (25-29.9), and obesity (≥30). Information on the prevalence of chronic kidney disease, dyslipidaemia, hypertension, and type 2 diabetes was obtained as the percentage of patients with a diagnosis recorded in their primary care or secondary care record at any time up to and including the date of a first CVD diagnosis. Patients’ socioeconomic status was described using the index of multiple deprivation 2015,17a composite measure of seven dimensions (income, employment, education, health, crime, housing, living environment) and provided by CPRD. Measures of deprivation are calculated at small area level, covering an average population of 1500 people, and are presented in fifths, with the first 20% and last 20% representing the least and most deprived areas, respectively. We extracted information on ethnicity from both primary and secondary care records, and we used secondary care data when records differed. Ethnicity was grouped into four categories: African/Caribbean, Asian, white, and mixed/other. Finally, we extracted information on cardiovascular treatments (ie, aspirin and other antiplatelets, alpha adrenoceptor antagonists, aldosterone antagonists/mineralocorticoid receptor antagonists, angiotensin converting enzyme inhibitors, angiotensin II receptor antagonists, beta blockers, calcium channel blockers, diuretics, nitrates, oral anticoagulants, and statins) as the number of patients with at least two prescriptions of each drug class within six months after incident CVD, among patients alive and registered with a general practitioner 30 days after the diagnosis. Supplementary table S2 provides a list of substances included in each drug class. Prescriptions were extracted from primary care records up to 31 December 2019.
Statistical analyses
Categorical data for patient characteristics are presented as frequencies (percentages), and continuous data are presented as means and standard deviations (SDs) for symmetrically distributed data or medians and interquartile ranges (IQRs) for non-symmetrically distributed data, over the whole CVD cohort and stratified by age, sex, socioeconomic status, region, and calendar year of diagnosis. For variables with missing entries, we present numbers and percentages of records with missing data. For categorical variables, frequencies refer to complete cases.
Incidence rates of CVD were calculated by dividing the number of incident diagnoses by the number of patient years in the cohort. Category specific rates were computed separately for subgroups of age, sex, socioeconomic status, region, and calendar year of diagnosis. Age calculations were updated for each calendar year. To ensure calculations referred to incident diagnoses, we excluded individuals, from both the numerator and the denominator populations, with a disease of interest diagnosed before the study start date (1 January 2000), or within the first 12 months of registration with their general practice. Time at risk started at the latest of the patient’s registration date plus 12 months, 30 June of their birth year, or study start date; and stopped at the earliest of death, transfer out of practice, last collection date of the practice, incidence of the disease of interest, or linkage end date (30 June 2019). Disease incidence was standardised for age and sex18using the 2013 European standard population19in five year age bands up to age 90 years.
Negative binomial regression models were used to calculate overall and category specific incidence rate ratios and corresponding 95% confidence intervals (CIs).20Models were adjusted for calendar year of diagnosis, age (categorised into five years age bands), sex, socioeconomic status, and region. We chose negative binomial models over Poisson models to account for potential overdispersion in the data. Sensitivity analyses comparing Poisson and negative binomial models showed similar results.
Study findings are reported according to the RECORD (reporting of studies conducted using observational routinely collected health data) recommendations.21We performed statistical analyses in R, version 4.3.3 (R Foundation for Statistical Computing, Vienna, Austria).
Patient and public involvement
No patients or members of the public were directly involved in this study owing to constraints on funding and time.
Results
A total of 22 009 375 individuals contributed data between 1 January 2000 and 30 June 2019, with 146 929 629 patient years of follow-up. Among those we identified 2 906 770 new CVD diagnoses, affecting 1 650 052 patients. Mean age at first CVD diagnosis was 70.5 (SD 15.0) years, 47.6% (n=784 904) of patients were women, and 11.6% (n=191 421), 18.0% (n=296 554), 49.7% (n=820 892), and 14.2% (n=233 833) of patients had a history of chronic kidney disease, dyslipidaemia, hypertension, and type 2 diabetes, respectively, at the time of their first CVD diagnosis (table 1).
During 2017-19, the most common CVDs were atrial fibrillation or flutter (age-sex standardised incidence 478 per 100 000 person years), heart failure (367 per 100 000 person years), and chronic ischaemic heart disease (351 per 100 000 person years), followed by acute coronary syndrome (190 per 100 000 person years), venous thromboembolism (183 per 100 000 person years), and stroke (181 per 100 000 patient years) (fig 1).
Temporal trends
The primary incidence of CVDs (ie, the number of patients with CVD) decreased by 20% during 2000-19 (age-sex standardised incidence rate ratio 2017-19v2000-02: 0.80 (95% CI 0.73 to 0.88)). However, the total incidence of CVD (ie, the total number of new CVD diagnoses) remained relatively stable owing to an increasing number of subsequent diagnoses among patients already affected by a first CVD (incidence rate ratio 2017-19v2000-02: 1.00 (0.91 to 1.10)).
The observed decline in CVD incidence was largely due to declining rates of atherosclerotic diseases, in particular acute coronary syndrome, chronic ischaemic heart disease, and stroke, which decreased by about 30% during 2000-19. The incidence of peripheral artery disease also declined, although more modestly (incidence rate ratio 2017-19v2000-02: 0.89 (0.80 to 0.98)) (fig 1).
The incidence of non-atherosclerotic heart diseases increased at varying rates, with incidence of aortic stenosis and heart block more than doubling over the study period (2017-19v2000-02: 2.42 (2.13 to 2.74) and 2.22 (1.99 to 2.46), respectively) (fig 1). These increasing rates of non-atherosclerotic heart diseases balanced the reductions in ischaemic diseases so that the overall incidence of CVD across the 10 conditions appeared to reach a plateau and to remain relatively stable from 2007-08 (incidence rate ratio 2017-19v2005-07: 1.00 (0.91 to 1.10)) (fig 2).
Age stratified analyses further showed that the observed decrease in incidence of chronic ischaemic heart disease, acute coronary syndrome, and stroke was largely due to a reduced incidence in those aged >60 years, whereas incidence rates in those aged <60 years remained relatively stable (fig 3andfig 4).
Age at diagnosis
CVD incidence was largely concentrated towards the end of the life span, with a median age at diagnosis generally between 65 and 80 years. Only venous thromboembolism was commonly diagnosed before age 45 years (fig 5). Over the study period, age at first CVD diagnosis declined for several conditions, including stroke (on average diagnosed 1.9 years earlier in 2019 than in 2000), heart block (1.3 years earlier in 2019 than in 2000), and peripheral artery disease (1 year earlier in 2019 than in 2000) (see supplementary figure S1). Adults with a diagnosis before age 60 years were more likely to be from lower socioeconomic groups and to have a higher prevalence of several risk factors, including obesity, smoking, and high cholesterol levels (see supplementary table S3).
Incidence by sex
Age adjusted incidence of all CVDs combined was higher in men (incidence rate ratio for womenvmen: 1.46 (1.41 to 1.51)), with the notable exception of venous thromboembolism, which was similar between men and women. The incidence of aortic aneurysms was higher in men (3.49 (3.33 to 3.65)) (fig 2). The crude incidence of CVD, however, was similar between men and women (1069 per 100 000 patient years and 1176 per 100 000 patient years, respectively), owing to the higher number of women in older age groups. Temporal trends in disease incidence were generally similar between men and women (fig 2).
Incidence by socioeconomic status
The most deprived socioeconomic groups had a higher incidence of any CVDs (incidence rate ratio most deprivedvleast deprived: 1.37 (1.30 to 1.44)) (fig 6). A socioeconomic gradient was observed across almost every condition investigated. That gradient did not decrease over time, and it was most noticeable for peripheral artery disease (incidence rate ratio most deprivedvleast deprived: 1.98 (1.87 to 2.09)), acute coronary syndrome (1.55 (1.54 to 1.57)), and heart failure (1.50 (1.41 to 1.59)). For aortic aneurysms, atrial fibrillation, heart failure, and aortic stenosis, socioeconomic inequalities in disease incidence appeared to increase over time.
Regional differences
Higher incidence rates were seen in northern regions (north west, north east, Yorkshire and the Humber) of England for all 10 conditions investigated, even after adjusting for socioeconomic status. Aortic aneurysms and aortic stenosis had the strongest regional gradients, with incidence rates about 30% higher in northern regions compared with London. Geographical variations remained modest, however, and did not appear to change considerably over time (see supplementary figure S2).
Sensitivity analyses
In sensitivity analyses that used broader disease definitions, that included diagnoses recorded on death certificates, that relied on longer lookback periods for exclusion of potentially prevalent diagnoses, or that were restricted to diagnoses recorded during hospital admissions, temporal trends in disease incidence appeared similar (see supplementary figures S3-S6).
Secondary prevention treatments
The proportion of patients using statins and antihypertensive drugs after a first CVD diagnosis increased over time, whereas the use of non-dihydropyridines calcium channel blockers, nitrates, and diuretics decreased over time. Non-vitamin K antagonist oral anticoagulants increasingly replaced vitamin K anticoagulants (see supplementary figure S7).
Discussion
The findings of this study suggest that important changes occurred in the distribution of CVDs during 2000-19 and that several areas are of concern. The incidence of non-atherosclerotic heart diseases was shown to increase, the decline in atherosclerotic disease in younger people was stalling, and socioeconomic inequalities had a substantial association across almost every CVD investigated.
Implications for clinical practice and policy
Although no causal inference can be made from our data, the decline in rates of ischaemic diseases coincided with reductions in the prevalence of risk factors such as smoking, hypertension, and raised cholesterol levels in the general population over the same period,22and this finding suggests that efforts in the primary and secondary prevention of atherosclerotic diseases have been successful. The decline in stroke was not as noticeable as that for coronary heart disease, which may reflect the rising incidence of atrial fibrillation. The variation in trends for peripheral artery disease could be due to differences in risk factors (eg, a stronger association with diabetes), the multifaceted presentations and causes, and the introduction of systematic leg examinations for people with diabetes.2324
All the non-atherosclerotic diseases, however, appeared to increase during 2000-19. For some conditions, such as heart failure, the observed increase remained modest, whereas for others, such as aortic stenosis and heart block, incidence rates doubled. All analyses in this study were standardised for age and sex, to illustrate changes in disease incidence independently of changes in population demographics. Whether these trends solely reflect increased awareness, access to diagnostic tests, or even screening (eg, for abdominal aortic aneurysm25) and coding practices, is uncertain. Reductions in premature death from coronary heart disease may have contributed to the emergence of these other non-atherosclerotic CVDs. Regardless, the identification of increasing numbers of people with these problems has important implications for health services, especially the provision of more surgical and transcatheter valve replacement, pacemaker implantation, and catheter ablation for atrial fibrillation. Importantly, these findings highlight the fact that for many cardiovascular conditions such as heart block, aortic aneurysms, and non-rheumatic valvular diseases, current medical practice remains essentially focused on the management of symptoms and secondary prevention and that more research into underlying causes and possible primary prevention strategies is needed.2627
These varying trends also mean that the contribution of individual CVDs towards the overall burden has changed. For example, atrial fibrillation or flutter are now the most common CVDs in the UK. Atrial fibrillation is also a cause (and consequence) of heart failure, and these two increasingly common problems may amplify the incidence of each other. Venous thromboembolism and heart block also appeared as important contributors to overall CVD burden, with incidence rates similar to those of stroke and acute coronary syndrome, yet both receive less attention in terms of prevention efforts.
The stalling decline in the rate of coronary heart disease in younger age groups is of concern, has also been observed in several other high income countries, and may reflect rising rates of physical inactivity, obesity, and type 2 diabetes in young adults.4628The stalled decline suggests prevention approaches may need to be expanded beyond antismoking legislation, blood pressure control, and lipid lowering interventions to include the promotion of physical activity, weight control, and use of new treatments shown to reduce cardiovascular risk in people with type 2 diabetes.29Although CVD incidence is generally low in people aged <60 years, identifying those at high risk of developing CVD at a young age and intervening before problems occur could reduce premature morbidity and mortality and have important economic implications.
Our study further found that socioeconomic inequalities may contribute to CVD burden, and that this association is not restricted to selected conditions but is visible across most CVDs. The reasons behind the observed increase in risk in relation to socioeconomic inequalities are likely to be multifactorial and to include environmental, occupational, psychosocial, and behavioural risk factors, including established cardiovascular risk factors such as smoking, obesity, nutrition, air pollution, substance misuse, and access to care.30How these findings apply to different countries is likely to be influenced by socioeconomic structures and healthcare systems, although health inequalities have been reported in numerous countries.30One important factor in the present study is that access to care is free at the point of care in the UK,31and yet socioeconomic inequalities persist despite universal health coverage and they did not appear to improve over time. Independently of the specificities of individual countries, our findings highlight the importance of measuring and considering health inequalities and suggest that dealing with the social determinants of health—the conditions under which people are born, live, work, and age—could potentially bring substantial health improvements across a broad range of chronic conditions.
Finally, our results reflect disease incidence based on diagnostic criteria, screening practices, availability, and accuracy of diagnostic tests in place at a particular time and therefore must be interpreted within this context.32Several of the health conditions investigated are likely to being sought and detected with increased intensity over the study period. For example, during the study period the definition of myocardial infarction was revised several times,333435and high sensitivity troponins were progressively introduced in the UK from 2010. These more sensitive markers of cardiac injury are thought to have increased the detection rates for less severe disease.3637Similarly, increased availability of computed tomography may have increased detection rates for stroke.38These changes could have masked an even greater decline in these conditions than observed in the present study. Conversely, increased use of other biochemical tests (such as natriuretic peptides) and more sensitive imaging techniques might have increased the detection of other conditions.394041The implementation of a screening programme for aortic aneurysm and incentive programmes aimed at improving coding practices, including the documentation of CVD, associated risk factors and comorbidities, and treatment of these, are also likely to have contributed to the observed trends.254243As a result, the difference in incidence estimates and prevalence of comorbidities over time may not reflect solely changes in the true incidence but also differences in ascertainment of people with CVD.44Nonetheless, long term trends in large and unconstrained populations offer valuable insights for healthcare resource planning and for the design of more targeted prevention strategies that could otherwise not be answered by using smaller cohorts, cross sectional surveys, or clinical trials; and precisely because they are based on routinely reported diagnoses they are more likely to capture the burden of disease as experienced by doctors and health services.
Strengths and limitations of this study
A key strength of this study is its statistical power, with >140 million person years of data. The large size of the cohort allowed us to perform incidence calculations for a broad spectrum of conditions, and to examine the influence of age, sex, and socioeconomic status as well as trends over 20 years. One important limitation of our study was the modest ethnic diversity in our cohort and the lack of information on ethnicity for the denominator population, which precluded us from stratifying incidence estimates by ethnic group. Our analyses were also limited by the unavailability or considerable missingness of additional variables potentially relevant to the development of CVD, such as smoking, body mass index, imaging data, women specific cardiovascular risk factors (eg, pregnancy associated hypertension and gestational diabetes), and blood biomarkers. Further research may also need to consider an even wider spectrum of CVDs, including individual types of valve disease, pregnancy related conditions, and infection related heart diseases. Research using databases with electronic health records is also reliant on the accuracy of clinical coding input by doctors in primary care as part of a consultation, or in secondary care as part of a hospital admission. We therefore assessed the validity of diagnoses in UK electronic health records data and considered it to be appropriate in accordance with the >200 independent validation studies reporting an average positive predictive value of about 90% for recorded diagnoses.45Observed age distributions were also consistent with previous studies and added to the validity of our approach. Nevertheless, our results must be interpreted within the context and limitations of routinely collected data from health records, diagnostic criteria, screening practices, the availability and accuracy of diagnostic tests in place at that time, and the possibility that some level of miscoding is present or that some bias could have been introduced by restricting the cohort to those patients with at least 12 months of continuous data.
Conclusions
Efforts to challenge the notion of the inevitability of vascular events with ageing, and evidence based recommendations for coronary heart disease prevention, have been successful and can serve as a model for other non-communicable diseases. Our findings show that it is time to expand efforts to improve the prevention of CVDs. Broadening research and implementation efforts in both primary and secondary prevention to non-atherosclerotic diseases, tackling socioeconomic inequalities, and introducing better risk prediction and management among younger people appear to be important opportunities to tackle CVDs.
Recent data show that despite decades of declining rates of cardiovascular mortality, the burden from cardiovascular disease (CVD) appears to have stalled in several high income countries
This observational study of a representative sample of 22 million people from the UK during 2000-19 found reductions in CVD incidence to have been largely restricted to ischaemic heart disease and stroke, and were paralleled by a rising number of diagnoses of cardiac arrhythmias, valve disease, and thromboembolic events
Venous thromboembolism and heart block were important contributors to the overall burden of CVDs, with incidence rates similar to stroke and acute coronary syndromes
Improvements in rates of coronary heart disease almost exclusively appeared to benefit those aged >60 years, and the CVD burden in younger age groups appeared not to improve
","Objective: To investigate the incidence of cardiovascular disease (CVD) overall and by age, sex, and socioeconomic status, and its variation over time, in the UK during 2000-19.
Design: Population based study.
Setting: UK.
Participants: 1 650 052 individuals registered with a general practice contributing to Clinical Practice Research Datalink and newly diagnosed with at least one CVD from 1 January 2000 to 30 June 2019.
Main outcome measures: The primary outcome was incident diagnosis of CVD, comprising acute coronary syndrome, aortic aneurysm, aortic stenosis, atrial fibrillation or flutter, chronic ischaemic heart disease, heart failure, peripheral artery disease, second or third degree heart block, stroke (ischaemic, haemorrhagic, and unspecified), and venous thromboembolism (deep vein thrombosis or pulmonary embolism). Disease incidence rates were calculated individually and as a composite outcome of all 10 CVDs combined and were standardised for age and sex using the 2013 European standard population. Negative binomial regression models investigated temporal trends and variation by age, sex, and socioeconomic status.
Results: The mean age of the population was 70.5 years and 47.6% (n=784 904) were women. The age and sex standardised incidence of all 10 prespecified CVDs declined by 19% during 2000-19 (incidence rate ratio 2017-19v2000-02: 0.80, 95% confidence interval 0.73 to 0.88). The incidence of coronary heart disease and stroke decreased by about 30% (incidence rate ratios for acute coronary syndrome, chronic ischaemic heart disease, and stroke were 0.70 (0.69 to 0.70), 0.67 (0.66 to 0.67), and 0.75 (0.67 to 0.83), respectively). In parallel, an increasing number of diagnoses of cardiac arrhythmias, valve disease, and thromboembolic diseases were observed. As a result, the overall incidence of CVDs across the 10 conditions remained relatively stable from the mid-2000s. Age stratified analyses further showed that the observed decline in coronary heart disease incidence was largely restricted to age groups older than 60 years, with little or no improvement in younger age groups. Trends were generally similar between men and women. A socioeconomic gradient was observed for almost every CVD investigated. The gradient did not decrease over time and was most noticeable for peripheral artery disease (incidence rate ratio most deprivedvleast deprived: 1.98 (1.87 to 2.09)), acute coronary syndrome (1.55 (1.54 to 1.57)), and heart failure (1.50 (1.41 to 1.59)).
Conclusions: Despite substantial improvements in the prevention of atherosclerotic diseases in the UK, the overall burden of CVDs remained high during 2000-19. For CVDs to decrease further, future prevention strategies might need to consider a broader spectrum of conditions, including arrhythmias, valve diseases, and thromboembolism, and examine the specific needs of younger age groups and socioeconomically deprived populations.
"
Colchicine in patients with acute ischaemic stroke or transient ischaemic attack,"Introduction
Patients with an acute minor-to-moderate ischaemic stroke or transient ischaemic attack remain at considerable risk of subsequent stroke, particularly within the first few days.1234An estimate suggests that two thirds of recurrences of stroke within three months occurred within seven days of symptom onset.56More importantly, the three month high risk period accounts for approximately 70% of subsequent strokes within one year and 40% within five years.78Exploration of novel treatments are needed to reduce the early recurrent risk of stroke and the overall burden of stroke.
Patients with increased inflammation in the acute phase had even higher risk of a new stroke after ischaemic stroke or transient ischaemic attack,9101112suggesting the potential implications of early anti-inflammatory treatment for reducing the residual recurrent risk of stroke.1314Colchicine has broad anti-inflammatory effects.151617The Colchicine Cardiovascular Outcomes Trial (COLCOT) found that colchicine reduced the risk of cardiovascular events and stroke as one component of primary endpoints in patients with acute coronary artery disease,18especially in patients who had treatment initiation within three days of myocardial infarction onset.19Together with other secondary prevention trials in cardiovascular disease, collective evidence suggests that colchicine slowed the progression of atherosclerosis by limiting plaque growth, reducing the risk of plaque instability and the risk of in-stent restenosis.2021The subsequent meta-analyses of the trials involving colchicine has shown that the risk of stroke was almost halved in patients treated with colchicine who had coronary artery disease.222324However, whether colchicine can prevent early subsequent stroke in patients with acute stroke or transient ischaemic attack is still unknown.
In this trial, we aimed to evaluate the efficacy and safety of low dose colchicine versus placebo initiating within 24 h of symptom onset on reducing subsequent stroke within three months in patients with acute non-cardioembolic minor-to-moderate ischaemic stroke or transient ischaemic attack and who have concentration of at least 2 mg/L of high sensitivity C-reactive protein.25
Methods
Study design
CHANCE-3 (colchicine in high risk patients with acute minor-to-moderate ischaemic stroke or transient ischaemic attack) was an investigator initiated, randomised, double blind, placebo controlled trial conducted at 244 centres in China. The trial rationale, design, and methods have been described in the trial protocol.26Full details of the protocol, statistical analysis plan, committees, sites, investigators, and definitions regarding acute infection, endpoints, and subgroups, such as symptomatic intracranial artery stenosis and symptomatic extracranial artery stenosis,2728are available in the appendix.
The study was approved by the ethics committee at Beijing Tiantan Hospital (No. KY2022-093-02) and each participating site. Written informed consent was provided by all patients or their representatives before enrolment. The trial was overseen by a trial steering committee. An independent data and safety monitoring committee monitored the progress of the trial, with regular assessment of cumulative safety data to safeguard the wellbeing of the patients.
Participants
Participants were eligible if they were 40 years of age or older; had either an acute minor-to-moderate ischaemic stroke with a National Institutes of Health Stroke Scale score of 5 or less (range 0-42, with higher scores indicating more severe stroke)29or a high risk transient ischaemic attack with a score of 4 or higher on the ABCD2scale (stroke risk score based on age, blood pressure, clinical features, duration of transient ischaemic attack, and the presence or absence of diabetes mellitus; range 0-7, with higher scores indicating higher risk of stroke);730had a high sensitivity C-reactive protein level of ≥2 mg/L at baseline; and could start the trial drug within 24 h from the time at which the patient’s condition was last reported to be normal.
Patients were not eligible if they had a presumed cardiac source of embolus based on medical history, including but not limited to, atrial fibrillation and prosthetic cardiac valve and electrocardiograph at baseline; inflammatory bowel disease or chronic diarrhoea; symptomatic peripheral neuropathy or pre-existing progressive neuromuscular disease; or a non-transient creatine kinase level that was greater than three times the upper limit of the normal range, creatinine level exceeding 1.5 times of the upper limit of normal range, or estimated glomerular filtration rate less than 50 mL/min; severe hepatic disease; clinically significant non-transient haematological abnormalities; acute infection, including respiratory tract infection, urinary tract infection, or gastro-enteritis, or currently using or planning to receive oral or intravenous anti-infective treatment for any other infection; current or planned long term use of systemic glucocorticoid treatment; or planned use of medications that may interact with colchicine. Additional information on inclusion and exclusion criteria is provided in the appendix.
Randomisation and masking
Eligible participants were randomly assigned in a 1:1 ratio with a block length of four to receive colchicine or placebo colchicine. The randomisation code list, which the trial drug was packaged in accordance with, was generated centrally by a contract research organisation (appendix page 1). The treatment number was allocated using a centralised treatment allocation system at randomisation.
Masking included removing the manufacturer’s label and replacing it with the clinical trial label and randomisation number. Apart from the randomisation number, the pack label text was identical for colchicine and placebo. Patients, care givers, and those assessing outcomes were masked to allocation.
Procedures
The treatment was initiated within 24 h of symptom onset. Patients randomly received colchicine at a dose of 0.5 mg twice daily or placebo on days 1-3, followed by 0.5 mg per day on days 4 to 90 on a background of standard treatment. Standard treatment included, but was not limited to, antiplatelet drugs, lipid lowering treatment, and control of hypertension and diabetes as applicable at the discretion of the treating physician, according to Chinese Stroke Association guideline for clinical management of cerebrovascular disorders.31
Study visits collected outcome data at discharge from the randomising hospital, and at day 90 (plus or minus seven days) via face-to-face interview at each site by trained and certified evaluators who were unaware of the trial group assignments. The exact number of tablets of study drug taken was also recorded at each visit for evaluating treatment adherence.
Outcomes
The primary efficacy outcome was any new stroke (ischaemic or haemorrhagic) within 90 days after randomisation. Secondary outcomes at 90 days were a vascular event (a composite of stroke, transient ischaemic attack, myocardial infarction, or vascular death), ischaemic stroke, stroke, or transient ischaemic attack, incidence of modified Rankin scale of more than 1 (ranges from 0 to 6, 0 indicating no symptoms; 5 indicating severe disability; and 6 indicating death),332and the severity of any recurrent stroke or transient ischaemic attack by means of a six level ordinal scale that combined stroke or transient ischaemic attack events with a score on modified Rankin scale as follows433: fatal stroke (with subsequent score on the modified Rankin scale of 6), severe stroke (with subsequent score of 4 or 5), moderate stroke (with subsequent score of 2 or 3), mild stroke (with subsequent score of 0 or 1), transient ischaemic attack, and no stroke or transient ischaemic attack.
The primary safety outcome was any serious adverse event within 90 days. Secondary safety outcomes were any adverse event within 90 days and adverse events of special interest as described in the protocol.
The endpoint events were adjudicated by an independent clinical event adjudication committee whose members were unaware of the trial group assignments.
Statistical analysis
We determined that a total of 8238 patients would provide 90% statistical power to detect a 25% reduction in the risk of any new stroke, with a significance level (α) of 0.05, assuming a 7.1% event risk of the primary outcome in the control group.11134
All efficacy analyses were based on the intention-to-treat principle and included all the patients who underwent randomisation. All patients in the intention-to-treat set received at least one dose of the study drug, therefore, the safety analyses were also conducted in this dataset. The crude cumulative incidence of the primary and secondary outcomes was estimated using the Kaplan-Meier method. When multiple events of the same type occurred, the time to the first event was used. Patients without a primary outcome were censored at the time of death, last known contact, or at 90 days, whichever occurred first. The heterogeneity of the treatment effect across the trial centre was visually verified using the forest plot by the trial centres. We compared the difference in event risk between the two trial groups by a shared frailty model, which incorporated the random effect of the trial centre on the baseline hazard of Cox proportional-hazards model35and were reported by hazard ratios and 95% confidence intervals (95% CIs). Proportional hazard assumptions were assessed using log-cumulative hazard plots and by incorporating an interaction term of treatment and time in the Cox model. Absolute event risks and subdistribution hazard ratios from the Fine-Gray mode, which accounts for competing risks of death, were also calculated. For functional outcomes, the incidence of modified Rankin scale scores of more than 1 was reported, generalised linear mixed models were fitted, and an odds ratio was reported. A shift analysis was conducted using ordinal logistic regression and proportional odds assumption was assessed by the χ2score test. The common odds ratio and 95% CI were calculated for the severity of stroke or transient ischaemic attack outcome events measured by the ordinal scale.433The common odds ratio was for a one step shift towards the outcome of incidence of ordinal stroke or transient ischaemic attack in the colchicine group compared with the placebo group. All analyses of secondary efficacy outcomes were not adjusted for multiple comparisons and were considered exploratory. Analysis of efficacy outcomes were also performed among the per-protocol set using the same strategy. Prespecified subgroup analyses were done using the shared frailty models as previously specified. The interaction effect between treatment and subgroup variables was tested. We also examined whether any treatment effect differed with age using a continuous scale of age. Safety analysis was conducted by χ2or Fisher’s exact test. A crude two sided P<0.05 was considered as statistical significance. Statistical analyses were done using SAS software, version 9.4 (SAS Institute).
An interim analysis for efficacy was initially planned to be conducted when 60% of patients completed their 90 day follow-up.26However, both steering committee and the independent data and safety monitoring committee recommended to waive the interim analysis because of the high recruitment rate of the trial and because no safety concerned determined by the data and safety monitoring committee .
Patient and public involvement
The China Guidelines for patient involvement was not available till 21 November 2022 by China Center for Drug Evaluation,36approximately three months after the start of patient enrolment of the CHANCE-3 trial. Patient and public were not involved in design and conduct of this study. The study protocol and manuscript have been widely discussed among physicians and neurologists, who will be involved in disseminating the study findings to healthcare professionals, patients, and members of the public.
Results
Patient
Between 11 August 2022 and 13 April 2023, 14 449 patients with ischaemic stroke or transient ischaemic attack were screened at 244 clinical sites; 8343 patients (57.7%) were enrolled, with 4176 randomly assigned to receive colchicine and 4167 receive placebo(fig 1and table S1 in the appendix, page 21). During the trial, 198 (4.7%) individuals in the colchicine group and 206 (4.9%) participants in the placebo group permanently discontinued their study treatments. All patients who underwent randomisation completed the 90 day (with time window of seven days) follow-up. The primary outcome was available for all patients, including individuals who discontinued their study treatments (fig 1).
Baseline patient characteristics were similar between the two treatment groups (table 1). The median age of the patients was 66.3 years, and 37.6% were women. 7411 patients (88.8%) presented with an ischaemic stroke, and 932 (11.2%) presented with a transient ischaemic attack. The median level of high sensitivity C-reactive protein was 4.8 mg/L. The median time from symptom onset to randomisation was 14.6 h.
The usages of secondary prevention medications (table 1and table S2, appendix page 23) and prohibited medications during the trial (table S3, appendix pages 24-25) were also similar between the two trial groups.
Primary and secondary outcomes
A primary outcome event of any new ischaemic or haemorrhagic stroke within 90 days occurred in 264 patients (6.3%) in the colchicine group and in 270 patients (6.5%) in the placebo group (hazard ratio 0.98 (95% CI 0.83 to 1.16); P=0.79) (fig 2andtable 2). Results from the Fine-Gray model, accounting for the competing risk of death, showed similar outcomes (event risk: 6.0%v6.0%; subdistribution hazard ratio 1.00 (95% CI 0.84 to 1.19); P=0.97) (table 2).
With respect to secondary outcomes, a vascular event occurred in 298 patients (7.1%) in the colchicine group and in 309 patients (7.4%) in the placebo group (hazard ratio 0.96 (95% CI 0.82 to 1.13); P=0.64). Ischaemic stroke occurred in 257 patients (6.2%) in the colchicine group and in 263 patients (6.3%) in the placebo group. A total of 284 patients (6.8%) had stroke or transient ischaemic attack in the colchicine group and 290 (7.0%) in the placebo group. Sensitivity analyses by accounting for competing risk of death showed similar results (table 2). At 90 day follow-up, 435 patients (10.4%) had modified Rankin scale >1 in the colchicine group and 440 patients (10.6%) in the placebo group (odds ratio 0.99 (95% CI 0.86 to 1.14); P=0.86,table 2and table S4 in appendix page 26). No difference was noted in the incidence and severity of stroke or transient ischaemic attack (using the ordered categorical scale) between two treatment groups (common odds ratio 1.03 (95% CI 0.88 to 1.21), P=0.74;table 2).
When the primary outcome was assessed in prespecified subgroups, treatment effects might be different between the two trial groups according to ages (crude interaction P=0.02 when age is dichotomised, and P=0.018 when age is treated as a continuous variable) (fig 3, figure S1 in appendix page 36), and among the three levels of the status for symptomatic intracranial artery stenosis (crude interaction P=0.03) (fig 3). We did not find any potential heterogenetic treatment effect of colchicine in other prespecified subgroups (fig 3). The forest plot by the trial centres justified using shared frailty models to address treatment effect heterogeneity across centres (figure S2 in appendix page 37).
The results of the per protocol analysis were similar to those of the primary intention-to-treat analysis (table S5, appendix pages 27-28).
Safety and adverse events
Serious adverse events occurred in 91 (2.2%) patients in the colchicine group and in 88 (2.1%) patients in the placebo group (table 3). Fifteen patients (0.4%) in the colchicine group and 18 patients (0.4%) in the placebo group died from non-cardiovascular causes. During the trial, 910 patients (21.8%) had any adverse event in the colchicine group and 888 (21.3%) in the placebo group (table 3).
Despite the use of statins within 90 days in 7964 (95.5%) patients, no myopathy was reported in both trial groups (table 3and table S6, appendix page 29). Twelve (0.3%) patients had abnormal hepatic function within 90 days in the colchicine group compared with three (0.1%) in the placebo group (P=0.03;table 3and table S6, appendix page 29). Diarrhoea occurred in 71 patients (1.7%; six patients had a stroke within 90 day) in the colchicine group and in 30 patients (0.7%; one patient had a stroke within 90 day) in the placebo group (P<0.001;table 3and table S6, appendix page 29). The risk of serious adverse events and adverse events classified by system organ class were similar between the two trial groups (table S7, appendix page 30, and table S8, appendix page 31).
Sixty eight patients in the colchicine group (1.6%) and 62 patients in the placebo group (1.5%) prematurely discontinued study treatments owing to serious adverse events or adverse events (table S9, appendix page 32). The rates of use of any or dual antiplatelet treatment, statin, hypoglycaemic, and antihypertensive drugs within 90 days among patients with serious adverse events, adverse events, or premature discontinuation of study treatments, were similar between two trial groups (table S10-12, appendix pages 33-35).
Discussion
Principal findings
In this randomised, double blind, placebo controlled trial in participants with acute minor-to-moderate ischaemic stroke or high risk transient ischaemic attack and a high sensitivity C-reactive protein concentration of at least 2 mg/L at baseline, low dose colchicine initiated within 24 h of symptom onset was not superior to placebo in reducing the risk of subsequent stroke within 90 days.
Comparison with other studies
Our previous study of patients with acute ischaemic stroke from independent cohorts showed that risk of recurrent stroke increased by continuous increase in high sensitivity C-reactive protein concentration or by use of a cut-off of 2 mg/L at baseline.1125Moreover, the association of high sensitivity C-reactive protein with recurrence of stroke was observed when this protein was tested within 24 h, but the association disappeared if detecting concentrations of the protein between 72 h and 8 days.37The measurement of high sensitivity C-reactive protein is common in clinics and its concentration can usually be obtained rapidly. The median time from symptom onset to obtaining test results for this protein was 10.4 h in CHANCE-3, suggesting no delay for initiating the trial drugs caused by the assay. Taken together, patients who were at high risk were intended to be recruited for anti-inflammatory treatment by using a high sensitivity C-reactive protein cut-off of 2 mg/L. However, the necessity of this measurement for screening patients still needs further investigation.
Use of low-dose colchicine as an anti-inflammatory drug resulted in lower cardiovascular event rates over a median of 22.6 months in the COLCOT trial, which included patients in an acute condition after myocardial infarction and had completed planned percutaneous revascularisation procedures.1819Our results differ from that of the COLCOT trial and various reasons might explain these differential findings. Of note, the purpose of our study was not identical to COLCOT. As the time course analyses of the CHANCE-2 and POINT trials showed, which was also indicated in this study, most recurrences occurred within seven days of symptom onset.56We focused on this highest risk period for stroke and tested the effect of colchicine on preventing very early recurrent stroke. The 90 day duration of colchicine treatment in CHANCE-3 was thus different from COLCOT,1819which concentrated on long term risk of cardiovascular events. Similarly, another ongoing phase 3 randomised trial of CONVINCE (Colchicine for prevention of vascular inflammation in non-cardioembolic stroke), which included patients 72 h after ischaemic stroke and transient ischaemic attack, aims to determine the long term effectiveness of colchicine as a secondary prevention strategy.38In addition, important pathophysiological differences exist in early recurrent events between coronary heart disease and ischaemic stroke.3940How inflammation is involved in early prognosis of acute ischaemic stroke and high risk transient ischaemic attack is unclear. Therefore, low-dose colchicine may not be the appropriate drug for the short term secondary preventive treatment in this study population. Premature permanent discontinuation of colchicine might influence the evaluation of its treatment effect. In the per protocol analysis, however, the conclusion of non-effectiveness of colchicine was sustained after removal of the patients with premature discontinuation of study drugs from the analysis. We found that six of 71 patients (with diarrhoea) in colchicine group and one of 30 (with diarrhoea) in the control group had an event of stroke during the trial. Such a small difference could not explain the findings in this trial.
A meta-analysis of 15 randomised controlled trials in patients with coronary heart disease showed that colchicine significantly reduced the risk of cardiovascular events in patients up to 65 years of age, but not in those older than 65 years.24In this study, we observed a similar trend in patients younger than 65 years old as compared with that in individuals older than 65 years. Systemic inflammation increased with the advancement of age.41Inflammation in older adults might be more complex with more contributing factors, such as atherosclerosis, infection, and comorbidity. Whether colchicine affected a specific factor and what that was is unclear, as is the different inflammatory pathways that might be involved in cerebrovascular ageing. In the subgroup analyses, crude P values were reported for treatment by prespecified factor interaction effect individually without adjustment for multiple testing. Therefore, we could not rule out the possibility of chance findings. In addition, all the subgroup analyses were exploratory in nature, and these findings warrant further investigation.
We found no evidence of safety concerns on serious adverse events with colchicine during the trial. Consistent with previous colchicine trials,1842a difference was noted in the rates of diarrhoea, flatulence, and abnormal hepatic function between patients receiving colchicine and those receiving placebo. However, the overall incidences were lower in this study population, partially due to short duration of treatment.
Limitations
The CHANCE-3 trial has some limitations. In this study, we tested only high sensitivity C-reactive protein at baseline to screen patients and did not collect patients’ biosamples at their 90 day follow up. Therefore, we were not able to evaluate the biological effects of low-dose colchicine during the 90 day treatment period. The absence of follow up of concentrations for high sensitivity C-reactive protein also made the interpretation of the results challenging. The benefit of long term treatment of colchicine in reducing the risk of stroke were shown to be higher than that of short term treatment in patients with coronary disease.24Our findings in this study are from a 90 day treatment period, which was relatively short, and therefore did not imply that a long term duration of colchicine would not be beneficial. Additionally, the 95% CI of the hazard ratio for the primary efficacy outcome contained clinically relevant values, especially the lower limit of 0.83, and uncertainty still exists as to the treatment effect. It warrants further validation studies. In this study, the general categories of stroke secondary prevention medication were available without collecting their exact drug types, such as antihypertensive and antidiabetic drugs. Therefore, we were not able to access whether any drugs used for secondary prevention might influence the colchicine effect. Furthermore, the relative short time window for enrolment in the study limited routine assessment of cardio-embolism, including 24 h Holter electrocardiograph monitoring, and evaluations to rule out alternative causes for strokes before randomisation. We did not collect the data for these assessments during the study period either, which may provide better insights into the absence of colchicine effect. Although commonly seen in previous secondary stroke prevention trials, women were under-represented in this study. To promote equal sex representation, stratified sampling design may be considered in future studies. In addition, this study recruited patients from 11 August 2022 and completed recruiting on 13 April 2023. Fewer patients were included in summer, which might also limit the interpretation of the trial’s findings. Both intracranial and extracranial atherosclerotic diseases are known risk factors for recurrent ischaemic stroke.4344In Asian populations, the incidence of intracranial atherosclerosis is generally high without features of complicated plaques.45464748We did not collect the data regarding intracranial and extracranial atherosclerosis or histology of either the index events or the new stroke events at 90 day follow-up. We, therefore, were not able to evaluate atherosclerotic situations and its potential impact on the effect of colchicine for the patients participated in this study. Due to lack of information for colchicine treatment in patients with stroke in the literature, we adopted a suboptimal approach to calculate the sample size for this study as described previously,26which might lead to either an under or over powered study. Our findings of this study may not be generalisable to populations other than Asian patients.
Conclusions
This trial of participants with non-cardioembolic minor-to-moderate ischaemic stroke or high risk transient ischaemic attack and with a baseline concentration for high sensitivity C-reactive protein of at least 2 mg/L, did not provide sufficient evidence that low-dose colchicine initiated within 24 h of symptom onset could reduce the risk of subsequent stroke or increase the risk of serious adverse events as compared with placebo within 90 days.
Inflammation has been associated with incidence and recurrence of stroke, and risk of stroke was reduced in patients who have coronary artery disease and who were treated with colchicine
However, evidence is lacking regarding efficacy and safety of acute use of colchicine for the prevention of early recurrent stroke
No differences were noted in treatment effects on subsequent stroke between the low dose colchicine and the placebo groups
Additionally, no difference was noted in the primary safety outcome for any serious adverse event
","Objectives: To assess the efficacy and safety of colchicine versus placebo on reducing the risk of subsequent stroke after high risk non-cardioembolic ischaemic stroke or transient ischaemic attack within the first three months of symptom onset (CHANCE-3).
Design: Multicentre, double blind, randomised, placebo controlled trial.
Setting: 244 hospitals in China between 11 August 2022 and 13 April 2023.
Participants: 8343 patients aged 40 years of age or older with a minor-to-moderate ischaemic stroke or transient ischaemic attack and a high sensitivity C-reactive protein ≥2 mg/L were enrolled.
Interventions: Patients were randomly assigned 1:1 within 24 h of symptom onset to receive colchicine (0.5 mg twice daily on days 1-3, followed by 0.5 mg daily thereafter) or placebo for 90 days.
Main outcome measures: The primary efficacy outcome was any new stroke within 90 days after randomisation. The primary safety outcome was any serious adverse event during the treatment period. All efficacy and safety analyses were by intention to treat.
Results: 4176 patients were assigned to the colchicine group and 4167 were assigned to the placebo group. Stroke occurred within 90 days in 264 patients (6.3%) in the colchicine group and 270 patients (6.5%) in the placebo group (hazard ratio 0.98 (95% confidence interval 0.83 to 1.16); P=0.79). Any serious adverse event was observed in 91 (2.2%) patients in the colchicine group and 88 (2.1%) in the placebo group (P=0.83).
Conclusions: The study did not provide evidence that low-dose colchicine could reduce the risk of subsequent stroke within 90 days as compared with placebo among patients with acute non-cardioembolic minor-to-moderate ischaemic stroke or transient ischaemic attack and a high sensitivity C-reactive protein ≥2 mg/L.
Trial registration: ClinicalTrials.gov,NCT05439356.
"
"SGLT-2 inhibitors, GLP-1 receptor agonists, and DPP-4 inhibitors and risk of hyperkalemia among people with type 2 diabetes","Introduction
People with type 2 diabetes are prone to developing hyperkalemia, especially those with comorbid conditions such as heart failure and chronic kidney disease.123However, several drugs that improve clinical outcomes in people with type 2 diabetes and related comorbidities increase serum potassium levels, such as inhibitors of the renin-angiotensin-aldosterone system.456789Hyperkalemia is associated with a risk of life threatening cardiac arrhythmias and increased mortality,10and the occurrence of hyperkalemia frequently leads to dose reduction or discontinuation of cardiorenal protective drugs. Stopping these drugs is associated with increased risk of adverse cardiovascular outcomes.11121314Therefore, strategies that reduce the risk of hyperkalemia in this population are urgently needed.
Sodium-glucose cotransporter-2 (SGLT-2) inhibitors and glucagon-like peptide-1 (GLP-1) receptor agonists have become cornerstone drug classes in the treatment of type 2 diabetes1516owing to their cardiovascular and kidney benefits.17181920Post hoc analyses of randomized trials have recently shown that SGLT-2 inhibitors also lower the risk of hyperkalemia compared with placebo, an outcome that was not defined as primary or secondary in those trials.212223However, we do not know whether these benefits are also observed outside the highly controlled setting of randomized trials, and whether all agents within the SGLT-2 inhibitor class similarly reduce the risk of hyperkalemia. Furthermore, large scale epidemiological studies are needed that investigate the effects of GLP-1 receptor agonists on the risk of hyperkalemia in people with type 2 diabetes, with only a few small clinical studies suggesting plausible mechanisms for increased potassium excretion.2425GLP-1 receptor agonists might lead to increased potassium secretion owing to enhancement in sodium delivery to the cortical collecting duct and altered tubular electronegativity.2526Additionally, long term kidney preservation by SGLT-2 inhibitors or GLP-1 receptor agonists might contribute to reduced hyperkalemia risks. Notably, a recent study found that GLP-1 receptor agonist use was associated with lower hyperkalemia risk in patients with chronic kidney disease, but whether these benefits extend to the broader population with type 2 diabetes is unknown.27The aim of this study was to investigate the comparative effectiveness of SGLT-2 inhibitors, GLP-1 receptor agonists, and dipeptidyl peptidase-4 (DPP-4) inhibitors in lowering the risk of hyperkalemia among adults with type 2 diabetes.
Methods
Data sources
We used data from Medicare fee-for-service (parts A, B, and D) and two commercial insurance databases: Optum’s deidentified Clinformatics Data Mart Database (CDM) and MarketScan. All three databases contain deidentified longitudinal information on patient demographics, healthcare use, inpatient and outpatient medical diagnoses and procedures, prescription dispensing records, and outpatient laboratory test results (available for approximately 45% of the population in CDM and 5-10% of patients in MarketScan). This study was approved by the Mass General Brigham institutional review board and granted waiver of informed consent because only deidentified claims data were used. Data use agreements were in place.
Study design and study population
We identified three study cohorts of patients who started SGLT-2 inhibitors versus DPP-4 inhibitors (cohort 1), GLP-1 receptor agonists versus DPP-4 inhibitors (cohort 2), and SGLT-2 inhibitors versus GLP-1 receptor agonists (cohort 3) from April 2013 to the end of available data (December 2019 in Medicare, December 2020 in MarketScan, and April 2022 in CDM). Cohort entry was the date of a newly filled prescription of SGLT-2 inhibitors, GLP-1 receptor agonists, or DPP-4 inhibitors. We chose DPP-4 inhibitors as comparator because they were commonly used as second or third line diabetes drugs during our study period, similar to SGLT-2 inhibitors or GLP-1 receptor agonists. In contrast, patients using metformin or insulin probably have less or more advanced diabetes, which would increase the risk of unmeasured confounding by diabetes severity and baseline risk of hyperkalemia. We restricted the study cohorts to patients with a diagnosis of type 2 diabetes and without use of any of the two drug classes being compared for the past 365 days, aged ≥18 years (≥65 years for Medicare), and with at least 12 months of continuous insurance enrollment before cohort entry. We excluded patients who had a history of type 1 diabetes, secondary or gestational diabetes, chronic kidney disease stage 5 or end stage kidney disease, nursing home admission, or a history of organ transplantation, pancreatitis, cirrhosis, acute hepatitis, or multiple endocrine neoplasia type 2 within 365 days before cohort entry. To decrease the risk of reverse causation bias (ie, that early outcomes would be related to a previous hyperkalemia diagnosis before starting the drug and therefore not related to the treatments under study), we further excluded people who had a hyperkalemia diagnosis in the inpatient or outpatient setting or potassium binder use in the 90 days before cohort entry. Supplemental table 1 provides definitions for inclusion and exclusion criteria and supplemental figure 1 gives an overview of the longitudinal design.
Outcomes and follow-up
The primary outcome was the occurrence of a diagnosis code for hyperkalemia in the inpatient or outpatient setting (supplemental table 2 gives definitions). Secondary outcomes were the occurrence of serum potassium ≥5.5 mmol/L during follow-up in the outpatient setting, and hyperkalemia diagnosis in the inpatient or emergency department setting. The laboratory based hyperkalemia outcome definition (serum potassium ≥5.5 mmol/L) was only assessed in CDM because Medicare and MarketScan contain no or too few laboratory test results. For this analysis, we restricted the study population to people who had at least two serum potassium measurements in the 365 days before cohort entry.
To test the specificity and sensitivity of the claims based hyperkalemia definitions, an internal validation study was performed in CDM. Briefly, we included all 12.3 million adults with serum potassium measurements (logical observation identifiers names and codes (LOINC) 6298-4, 77142-8, 12812-4, 12813-2, 42569-4). Then, we assessed whether there was a hyperkalemia diagnosis in the three months after the serum potassium test. For the primary outcome definition (ie, hyperkalemia diagnosis in inpatient or outpatient setting), specificity was 99.5% and sensitivity was 22.3% when we used serum potassium ≥5.5 mmol/L to define hyperkalemia; specificity was 99.3% and sensitivity was 37.1% when serum potassium ≥6.0 mmol/L was used as the gold standard. Relative risk estimates will be unbiased when specificity is high and non-differential, even if sensitivity is low.28However, absolute rate differences will be biased towards the null when sensitivity is low.
We started follow-up on the day after cohort entry and continued until outcome occurrence or until any of the following occurred: treatment discontinuation or starting a drug in the comparator class, death, end of continuous health plan enrollment, or end of available data. We did not censor participants when they started other diabetes drugs (eg, sulfonylureas) during follow-up. We defined discontinuation as no prescription refill for the index exposure in the 30 days after the end of the days’ supply for the most recent prescription.
Confounders
We measured potential confounders during the 365 days before and including cohort entry date. We identified covariates that were confounders, confounder proxies or predictors for the outcome based on subject matter knowledge and previous studies that evaluated outcomes associated with drug use in people with type 2 diabetes.29These included age, sex, race (race was only available in CDM and Medicare), and geographical region; comorbidities, such as heart failure and chronic kidney disease; diabetes specific complications, such as diabetic nephropathy, neuropathy, and retinopathy; use of drugs used to treat diabetes and cardiovascular disease, for example, insulin and renin-angiotensin system inhibitors; use of other drugs; measures of healthcare use, such as number of emergency department visits, hospital admissions, endocrinologist and internist visits, and laboratory tests; healthy behavior markers, such as screening and vaccinations; and calendar year. We also adjusted for a claims based frailty index30to address potential confounding by frailty and for a claims based combined comorbidity score.31Comorbidities and drug use were assessed in the 365 days before and including the cohort entry date and based on international classification of diseases (version 9 and 10) diagnosis and procedure codes, and generic drug names, respectively. In the subset of patients who had creatinine measurements available, we calculated estimated glomerular filtration rate using the race-free 2021 CKD-EPI (chronic kidney disease epidemiology collaboration) equation.32
Statistical analysis
To adjust for confounding, we used 1:1 propensity score matching with the nearest neighbor method and a caliper of 0.01 of the propensity score.33We used multivariable logistic regression models to estimate the propensity scores. These models were fitted separately for each of the data sources (ie, CDM, MarketScan, and Medicare) and for each drug comparison (SGLT-2 inhibitorsvDPP-4 inhibitors, GLP-1 receptor agonistsvDPP-4 inhibitors, and SGLT-2 inhibitorsvGLP-1 receptor agonists), for a total of nine propensity score models. All covariates listed in supplemental table 3 were included in the propensity score models, except for the laboratory test results, which were only available for a subset of patients. Because race was only available in CDM and Medicare, it was only used in the six propensity scores developed in the CDM and Medicare cohorts. Continuous covariates (eg, age) were entered as main terms and quadratic terms. We assessed covariate balance before and after propensity score matching with standardized mean differences, with a standardized mean difference <0.10 indicating sufficient balance.3435Because laboratory test results were not included in the propensity score, we considered their balance after propensity score matching to reflect residual unmeasured confounding. Hazard ratios were estimated with Cox regression models, and incidence rate differences were estimated with generalized linear regression models using an identity link function and normal error distribution.36Effect estimates and their standard errors were estimated separately in each of the three data sources, and then pooled with fixed effects meta-analysis. Cumulative incidence curves were estimated with the Aalen-Johansen estimator in the propensity score matched cohort, which accounts for the competing risk of death.37Absolute risks and risk differences at six month intervals were obtained from the cumulative incidences. There were no missing data for covariates other than the laboratory measurements. Analyses were performed using R version 3.6.2 and the Aetion Evidence Platform version 4.53.38
Subgroup and sensitivity analyses
To investigate potential treatment effect modification, we performed a number of subgroup analyses in the following prespecified strata: age (<65 yearsv≥65 years), sex, race (whitevblack, based on Medicare data only, where the race variable has been validated against self-reported race39), heart failure, cardiovascular disease, chronic kidney disease, use of renin-angiotensin-aldosterone system inhibitors, mineralocorticoid receptor antagonists, loop diuretics and insulin on the cohort entry date, and by baseline hemoglobin A1c level (<7.5%v7.5-9.0%v≥9.0%). We re-estimated propensity scores and reperformed matching for each subgroup stratum.40
To examine the robustness of our findings, we performed the following sensitivity analyses: treatment discontinuation was defined as no prescription refill for the index drug within 60 days rather than 30 days; to investigate the potential influence of informative censoring, we followed patients for a maximum of 180 and 365 days, regardless of treatment discontinuation or starting a drug in the comparator class; finally, we excluded patients with a history of hyperkalemia or potassium binder use in the previous 365 days.
Individual agents in SGLT-2 inhibitor and GLP-1 receptor agonist classes
We investigated potential differences in the risk of hyperkalemia for individual agents in the SGLT-2 inhibitor or GLP-1 receptor agonist classes by constructing separate cohorts for empagliflozin, canagliflozin, dapagliflozin, liraglutide, dulaglutide, exenatide, and semaglutide versus DPP-4 inhibitors, re-estimated the propensity scores and reperformed the matching, and calculated effect estimates for the primary outcome. The SGLT-2 inhibitor cohorts were restricted to the dates when both drugs under comparison were on the market (April 2013 for canagliflozinvDPP-4 inhibitors, January 2014 for dapagliflozinvDPP-4 inhibitors, and August 2014 for empagliflozinvDPP-4 inhibitors).
Patient and public involvement
There were no funds or time allocated for patient and public involvement, so we were unable to involve patients. Nevertheless, this study was inspired by conversations with patients in clinical practice. We also asked a member of the public to provide feedback on the article before resubmission. To be compliant with our data use agreements, we are not allowed to reidentify and contact patients who were included in the study dataset to share the results of this research.
Results
Baseline characteristics of study populations
Figure 1reports patient inclusion flowcharts. After 1:1 propensity score matching, there were 389 454 propensity score matched pairs in the SGLT-2 inhibitor versus DPP-4 inhibitor cohort, 364 910 pairs in the GLP-1 receptor agonist versus DPP-4 inhibitor cohort, and 436 730 matched pairs in the SGLT-2 inhibitor versus GLP-1 receptor agonist cohort. After matching, all baseline characteristics in the three cohorts were well balanced, with standardized mean differences <0.10. Laboratory test results, including potassium, were also balanced, despite not being included in propensity score models (table 1, supplemental tables 3-5).
In the SGLT-2 inhibitor versus DPP-4 inhibitor cohort, the mean age was 63 years, 54% were male, and 30% had a history of cardiovascular disease. Commonly used drugs included metformin (81%), angiotensin converting enzyme inhibitors or angiotensin II receptor blockers (72%), statins (71%), and β blockers (35%). Mean estimated glomerular filtration rate was 79 mL/min/1.73 m2and mean serum potassium level was 4.4 mmol/L among the subset with available laboratory test results. Baseline characteristics were comparable in the GLP-1 receptor agonist versus DPP-4 inhibitor cohort, and the SGLT-2 inhibitor versus GLP-1 receptor agonist cohort. In the SGLT-2 inhibitor versus DPP-4 inhibitor cohort, 40.7% started empagliflozin, 38.7% started canagliflozin, and 20.3% started dapagliflozin (supplemental table 6). The most commonly used GLP-1 receptor agonists were liraglutide (37.2%), dulaglutide (31.8%), exenatide (15.7%), and semaglutide (13.0%).
Risk of hyperkalemia after starting SGLT-2 inhibitors, GLP-1 receptor agonists, and DPP-4 inhibitors
Mean on-treatment follow-up ranged between 8.1 and 8.8 months, reflecting the large rate of discontinuation in routine clinical practice (supplemental table 7). Use of SGLT-2 inhibitors versus DPP-4 inhibitors was associated with a lower rate of hyperkalemia in the propensity score matched cohort, with an adjusted hazard ratio of 0.75 (95% confidence interval (CI) 0.73 to 0.78). Incidence rates were 25.3 versus 18.5 events per 1000 person years, corresponding to an incidence rate difference of −6.88 (95% CI −7.65 to −6.11) events per 1000 person years (table 2). Similarly, use of GLP-1 receptor agonists versus DPP-4 inhibitors was associated with a lower rate of hyperkalemia, with an adjusted hazard ratio of 0.79 (0.77 to 0.82). Incidence rates were 28.5 versus 22.1 events per 1000 person years, corresponding to an incidence rate difference of −6.36 (−7.24 to −5.48) per 1000 person years. The adjusted hazard ratio for SGLT-2 inhibitors versus GLP-1 receptor agonists was 0.92 (0.89 to 0.95). Incidence rates were 22.1 versus 19.8 events per 1000 person years, corresponding to an incidence rate difference of −2.31 (−3.05 to −1.57).Figure 2shows cumulative incidence curves for all three cohorts and supplemental table 8 reports corresponding absolute risks and risk differences at six month intervals. The lower risk of hyperkalemia for SGLT-2 inhibitors and GLP-1 receptor agonists versus DPP-4 inhibitors appeared within six months of follow-up. At three years of follow-up, the absolute risk was 2.4% (95% CI 2.1% to 2.7%) lower for SGLT-2 inhibitors than DPP-4 inhibitors (4.6%v7.0%), and 1.8% (1.4% to 2.1%) lower for GLP-1 receptor agonists than DPP-4 inhibitors (5.7%v7.5%).
When using serum potassium ≥5.5 mmol/L as the outcome definition, hazard ratios were 0.86 (0.78 to 0.95) for SGLT-2 inhibitors versus DPP-4 inhibitors, 0.82 (0.73 to 0.91) for GLP-1 receptor agonists versus DPP-4 inhibitors, and 1.01 (0.91 to 1.12) for SGLT-2 inhibitors versus GLP-1 receptor agonists (supplemental table 9). Furthermore, when using hyperkalemia diagnosis in the inpatient or emergency department setting, adjusted hazard ratios were 0.77 (0.69 to 0.85), 0.65 (0.59 to 0.72), and 0.96 (0.86 to 1.06), respectively (supplemental table 10).
Subgroup and sensitivity analyses
SGLT-2 inhibitors and GLP-1 receptor agonists showed protective associations for hyperkalemia across all subgroups compared with DPP-4 inhibitors (fig 3,fig 4). Benefits for SGLT-2 inhibitors and GLP-1 receptor agonists on the absolute scale were largest for those with heart failure, chronic kidney disease, or those using mineralocorticoid receptor antagonists. Findings for the SGLT-2 inhibitor versus GLP-1 receptor agonist cohort were consistent, with absence of large differences in hyperkalemia rate between the two drug classes across subgroups (fig 5). Findings were also consistent across sensitivity analyses (supplemental table 11).
Effectiveness of individual agents in SGLT-2 inhibitor and GLP-1 receptor agonist classes compared with DPP-4 inhibitors
Compared with DPP-4 inhibitors, the lower rate of hyperkalemia was consistent for single agents within the SGLT-2 inhibitor class: hazard ratios were 0.76 (0.72 to 0.80) for canagliflozin, 0.85 (0.79 to 0.91) for dapagliflozin, and 0.75 (0.71 to 0.78) for empagliflozin (table 3). Hazard ratios were consistent among individual GLP-1 receptor agonist agents compared with DPP-4 inhibitors, with hazard ratios of 0.80 (0.76 to 0.84) for dulaglutide, 0.78 (0.73 to 0.84) for exenatide, 0.79 (0.75 to 0.83) for liraglutide, and 0.74 (0.68 to 0.80) for semaglutide (table 4).
Discussion
Statement of principal findings
In this cohort study using three nationwide administrative claims databases in the United States, we found a lower rate of hyperkalemia in people with type 2 diabetes who started SGLT-2 inhibitors or GLP-1 receptor agonists compared with DPP-4 inhibitors. These observations were consistent in subgroups and several sensitivity analyses, and across comparisons of single agents within the SGLT-2 inhibitor and GLP-1 receptor agonist classes.
Novelty and comparison with previous studies
Our study provides several new findings and builds upon current evidence. An individual participant meta-analysis using data from six randomized clinical trials and comprising 49 875 patients found that SGLT-2 inhibitors reduced the risk of hyperkalemia compared with placebo.21Our study provides additional evidence by extending these results to a broader group of >750 000 people with type 2 diabetes in routine clinical practice. Additionally, our study provides evidence of the association between GLP-1 receptor agonists and hyperkalemia, which has been lacking in large scale epidemiological studies or trial analyses. The relative rate reduction observed for GLP-1 receptor agonists versus DPP-4 inhibitors (21% reduction) was similar to the reduction observed for SGLT-2 inhibitors versus DPP-4 inhibitors (25% relative reduction in hazard). In head-to-head comparisons of SGLT-2 inhibitors versus GLP-1 receptor agonists, we only observed small differences (hazard ratio 0.92 in the primary analysis), and in several secondary and sensitivity analyses we observed no association. We interpret these findings to indicate that no large differences exist in the rate of hyperkalemia between SGLT-2 inhibitors and GLP-1 receptor agonists, although the subgroup with chronic kidney disease showed a larger effect size on the relative scale. However, these subgroup findings should be considered hypothesis generating and interpreted with caution because many subgroup analyses were performed. Finally, our large study population allowed us to investigate associations with a precision sufficient to exclude the presence of clinically meaningful treatment effect heterogeneity by relevant patient subgroups. We were also able to exclude the presence of large differences in the reduction of hyperkalemia risk across individual SGLT-2 inhibitor and GLP-1 receptor agonist agents compared with DPP-4 inhibitors.
Possible explanations and clinical implications
There are several potential mechanisms by which SGLT-2 inhibitors and GLP-1 receptor agonists might lower the risk of hyperkalemia. SGLT-2 inhibitors and GLP-1 receptor agonists could increase the delivery of sodium and water to the cortical collecting duct of the kidney. Increased absorption of sodium by the principal cells might increase the electronegative charge, leading to increased potassium secretion.25264142A small randomized trial of 35 participants with type 2 diabetes showed increased fractional and absolute excretion of potassium after eight weeks of treatment with the GLP-1 receptor agonist lixisenatide.24Furthermore, both drug classes have been shown to slow progression of kidney function decline and albuminuria, and the preserved kidney function might contribute to the prevention of hyperkalemia in the long term.434445464748
Our findings have important clinical implications. Hyperkalemia is a common electrolyte disorder among patients with type 2 diabetes, especially in those with concurrent heart failure or decreased kidney function, and who use guideline recommended treatments that increase potassium levels, such as angiotensin converting enzyme inhibitors, angiotensin II receptor blockers, or mineralocorticoid receptor antagonists.10The occurrence of hyperkalemia frequently leads to dose reduction or discontinuation of these drugs, and this discontinuation is associated with adverse cardiovascular and kidney outcomes.111213Although newer potassium binders such as patiromer and sodium zirconium cyclosilicate might allow the use of renin-angiotensin system inhibitors,495051they add to the pill burden, and their benefits on hard clinical outcomes are unknown. Identifying additional strategies that prevent hyperkalemia is therefore a key priority. Our findings suggest that SGLT-2 inhibitors and GLP-1 receptor agonists are associated with lower risk of hyperkalemia. This ancillary benefit further supports the use of SGLT-2 inhibitors and GLP-1 receptor agonists in people with type 2 diabetes.
Unanswered questions and future research
In our analyses, we focused on hyperkalemia as an outcome. A recent post hoc analysis of the CREDENCE (canagliflozin and renal events in diabetes with established nephropathy clinical evaluation) and DAPA-CKD (dapagliflozin and prevention of adverse outcomes in chronic kidney disease) trials found that SGLT-2 inhibitor use was associated with a lower rate of discontinuation of renin-angiotensin-aldosterone system inhibitors compared with placebo during follow-up in patients with albuminuric chronic kidney disease. Future studies should investigate whether these effects are also observed for GLP-1 receptor agonists, and whether this is mediated by a lower risk of hyperkalemia. Similarly, studies could investigate whether SGLT-2 inhibitors or GLP-1 receptor agonists have an effect on the use of loop diuretics.
Strengths and weaknesses of the study
The strengths of our study include its large sample size, more than 15-fold larger than the individual participant meta-analysis of randomized trials previously discussed,21which allowed investigation of important subgroups and individual agents, and rich adjustment for >140 potential confounders. Furthermore, we applied rigorous methods, including the use of an active comparator and new user cohort design, which reduces confounding and mitigates time related and selection bias caused by prevalent users.5253
Our study has several limitations. We cannot rule out the presence of unmeasured confounding. However, our analysis accounted for a wide set of confounders,53and balance was achieved even among the laboratory test results that were not included in the adjustment. Furthermore, confounding by indication is less likely because hyperkalemia is an unintended effect of glucose lowering drugs and currently not an indication that would drive a choice of one of the three investigated drug classes.5455We also used a claims based definition for our primary outcome, with excellent specificity (>99%), but low sensitivity (22.3%). Therefore, although relative risk estimates are not expected to be biased under the assumption of non-differential measurement error, differences on the absolute scale are probably an underestimate of the true benefit of SGLT-2 inhibitors and GLP-1 receptor agonists. We believe non-differential measurement error might be a plausible assumption in our study because hyperkalemia has not been a safety concern for either of these drug classes. Furthermore, we adjusted for a wide number of measures of healthcare use (eg, number of outpatient visits and number of laboratory tests) to ensure patients were comparable at baseline with respect to healthcare surveillance and would have a similar opportunity for potassium monitoring during follow-up.
Mean follow-up in our study was relatively short (around eight to nine months) owing to high rates of treatment discontinuation. Nevertheless, this represents the reality of routine clinical practice in which many patients discontinue their treatment during follow-up. Therefore, our results reflect the outcomes that could be expected in patients from clinical practice after starting these drugs. We believe this timeframe should be sufficient to show the effects of GLP-1 receptor agonists and SGLT-2 inhibitors because mechanistic studies have found rapid effects of GLP-1 receptor agonists on potassium handling,2425and post hoc analyses of randomized trials of SGLT-2 inhibitors have shown separation of survival curves within one year for hyperkalemia.2223Finally, our findings are representative of the insured population in the United States, but might not be generalizable to uninsured patients.
Conclusion
In this analysis of three nationwide US databases, use of SGLT-2 inhibitors and GLP-1 receptor agonists was associated with a lower rate of hyperkalemia compared with DPP-4 inhibitors. This study further supports the use of these agents in a broad range of people with type 2 diabetes.
Hyperkalemia is associated with increased mortality and limits the use of guideline recommended drugs such as renin-angiotensin system inhibitors among people with type 2 diabetes
Sodium-glucose cotransporter-2 (SGLT-2) inhibitors, glucagon-like peptide-1 (GLP-1) receptor agonists, and dipeptidyl peptidase-4 (DPP-4) inhibitors are increasingly being used in the treatment of type 2 diabetes
The comparative effectiveness of these drugs in preventing hyperkalemia in routine clinical practice is unclear
In this population based cohort study of people with type 2 diabetes in the United States, starting SGLT-2 inhibitors or GLP-1 receptor agonists was associated with a lower risk of hyperkalemia compared with DPP-4 inhibitors
Benefits were consistent among demographic and clinical subgroups, and among single agents within the SGLT-2 inhibitor and GLP-1 receptor agonist classes
In addition to improving cardiovascular and kidney outcomes, the potential benefit of preventing hyperkalemia further solidifies the use of SGLT-2 inhibitors and GLP-1 receptor agonists in people with type 2 diabetes
","Objectives: To evaluate the comparative effectiveness of sodium-glucose cotransporter-2 (SGLT-2) inhibitors, glucagon-like peptide-1 (GLP-1) receptor agonists, and dipeptidyl peptidase-4 (DPP-4) inhibitors in preventing hyperkalemia in people with type 2 diabetes in routine clinical practice.
Design: Population based cohort study with active-comparator, new user design.
Setting: Claims data from Medicare and two large commercial insurance databases in the United States from April 2013 to April 2022.
Participants: 1:1 propensity score matched adults with type 2 diabetes newly starting SGLT-2 inhibitors versus DPP-4 inhibitors (n=778 908), GLP-1 receptor agonists versus DPP-4 inhibitors (n=729 820), and SGLT-2 inhibitors versus GLP-1 receptor agonists (n=873 460).
Main outcome measures: Hyperkalemia diagnosis in the inpatient or outpatient setting. Secondary outcomes were hyperkalemia defined as serum potassium levels ≥5.5 mmol/L and hyperkalemia diagnosis in the inpatient or emergency department setting.
Results: Starting SGLT-2 inhibitor treatment was associated with a lower rate of hyperkalemia than DPP-4 inhibitor treatment (hazard ratio 0.75, 95% confidence interval (CI) 0.73 to 0.78) and a slight reduction in rate compared with GLP-1 receptor agonists (0.92, 0.89 to 0.95). Use of GLP-1 receptor agonists was associated with a lower rate of hyperkalemia than DPP-4 inhibitors (0.79, 0.77 to 0.82). The three year absolute risk was 2.4% (95% CI 2.1% to 2.7%) lower for SGLT-2 inhibitors than DPP-4 inhibitors (4.6%v7.0%), 1.8% (1.4% to 2.1%) lower for GLP-1 receptor agonists than DPP-4 inhibitors (5.7%v7.5%), and 1.2% (0.9% to 1.5%) lower for SGLT-2 inhibitors than GLP-1 receptor agonists (4.7%v6.0%). Findings were consistent for the secondary outcomes and among subgroups defined by age, sex, race, medical conditions, other drug use, and hemoglobin A1c levels on the relative scale. Benefits for SGLT-2 inhibitors and GLP-1 receptor agonists on the absolute scale were largest for those with heart failure, chronic kidney disease, or those using mineralocorticoid receptor antagonists. Compared with DPP-4 inhibitors, the lower rate of hyperkalemia was consistently observed across individual agents in the SGLT-2 inhibitor (canagliflozin, dapagliflozin, empagliflozin) and GLP-1 receptor agonist (dulaglutide, exenatide, liraglutide, semaglutide) classes.
Conclusions: In people with type 2 diabetes, SGLT-2 inhibitors and GLP-1 receptor agonists were associated with a lower risk of hyperkalemia than DPP-4 inhibitors in the overall population and across relevant subgroups. The consistency of associations among individual agents in the SGLT-2 inhibitor and GLP-1 receptor agonist classes suggests a class effect. These ancillary benefits of SGLT-2 inhibitors and GLP-1 receptor agonists further support their use in people with type 2 diabetes, especially in those at risk of hyperkalemia.
"
"Nab-paclitaxel, cisplatin, and capecitabine versus cisplatin and gemcitabine as first line chemotherapy in patients with nasopharyngeal carcinoma","Introduction
Nasopharyngeal carcinoma is a common malignant tumour in the head and neck cancer spectrum and is particularly prevalent in regions such as southern China, southeast Asia, and north Africa.12Despite the improved overall survival rate of patients with localised nasopharyngeal carcinoma recently, clinical outcome is still not satisfactory in patients with recurrent or metastatic nasopharyngeal carcinoma.345
A platinum based chemotherapy regimen is generally recommended as the first line standard of care option for patients with recurrent or metastatic nasopharyngeal carcinoma. A phase 3 study has shown that gemcitabine plus cisplatin prolongs progression-free survival and overall survival in comparison with 5-fluorouracil plus cisplatin, which led to the establishment of gemcitabine plus cisplatin as the front line regimen for recurrent or metastatic nasopharyngeal carcinoma.678However, median progression-free survival derived from the gemcitabine plus cisplatin regimen remains limited, ranging from 7.0 to 7.7 months according to data from several prospective clinical studies.679Hence, innovative strategies are needed to enhance clinical benefits while maintaining an acceptable toxicity profile.
Recently, our phase 3 trial reported that capecitabine maintenance after a triplet regimen with taxanes, cisplatin, and capecitabine has tolerable toxicities in patients with metastatic nasopharyngeal carcinoma, as well as enhanced tumour control and prolonged survival.10Compared with traditional solvent based paclitaxel, albumin binding paclitaxel is a water soluble paclitaxel linked to an albumin nanoparticle, which increases the concentration and uptake rate of nab-paclitaxel in tumour cells and reduces cytotoxicity and solvent induced anaphylaxis.1112The combination of nab-paclitaxel and cisplatin has been approved for the treatment of metastatic nasopharyngeal carcinoma on the basis of satisfactory efficacy (median progression-free survival 9 months) and moderate toxicity.13We designed this phase 3, open label, randomised trial to evaluate the efficacy and safety of nab-paclitaxel, cisplatin, and capecitabine (nab-TPC) as first line treatment for recurrent or metastatic nasopharyngeal carcinoma compared with a gemcitabine plus cisplatin regimen.
Methods
Study design
This was a multicentre, open label, randomised, controlled, phase 3 study conducted in eligible patients at four hospitals located in China (supplementary table A).Figure 1shows a flowchart illustrating the study design. All patients signed written informed consent before enrolment.
Participants
Patients were eligible if they had histological or cytological confirmation of nasopharyngeal carcinoma. Other key inclusion criteria included primary metastatic disease or metastatic disease after curative radiotherapy, no previous receipt of systemic therapy for metastatic disease, Eastern Cooperative Oncology Group performance status of 0 or 1, at least one measurable lesion according to Response Evaluation Criteria in Solid Tumors version 1.1 (RECIST v1.1), adequate organ function, an estimated life expectancy of at least 12 weeks, and age ≥18 years. We excluded patients with a history of other malignancies, central nervous system metastases, severe coexisting illness, or pregnancy. The complete eligibility criteria are given in the trial protocol, available as an online supplement.
Randomisation and masking
A computer generated random number code was provided for randomisation of enrolled patients at the Sun Yat-sen University Cancer Center. Random distribution sequences were generated using a permuted block of flexible size (2, 4, 6, or 8), with no stratification factors. Randomisation details were provided in sequentially numbered, opaque, sealed envelopes prepared by a data management team unaware of the patient assignment process. The therapy assignment was not masked to investigators, patients, and other treating oncologists, but the central evaluation of the imaging data by an independent review committee was done in a masked manner.
Procedures
Before randomisation, patients received a comprehensive screening assessment within four week intervals between enrolment and start of treatment. Eligible patients were randomly assigned (1:1) to the nab-TPC or gemcitabine plus cisplatin cohort. Patients assigned to the nab-TPC cohort received nab-paclitaxel (200 mg/m2) on day 1, cisplatin (60 mg/m2) on day 1, and capecitabine (1000 mg/m2twice daily) on days 1-14 of each three week cycle for up to six cycles, followed by capecitabine maintenance for a maximum of two years. Patients assigned to the gemcitabine plus cisplatin cohort received gemcitabine (1g/m2) on days 1 and 8 and cisplatin (80 mg/m2) on day 1 of every three week cycle for up to six cycles, followed by best supportive care. Trial assigned chemotherapy was continued until progression of disease, intolerable toxicity, investigator’s decision, or withdrawal of consent, whichever occurred first. Crossover between the two chemotherapy regimens was permitted at disease progression. Details of the pretreatment screening evaluations and chemotherapy dose modifications during the trial are available in the trial protocol.
We assessed radiological responses of tumours every six weeks during the course of chemotherapy and every 12 weeks thereafter, according to RECIST v1.1, with 18F-fluorodeoxyglucose positron emission tomography-computed tomography or enhanced magnetic resonance imaging or computed tomography scan. We used the National Cancer Institute Common Terminology Criteria, version 4.0, to evaluated adverse events during treatment and for a period of 90 days after the last dose of study medication.
Endpoints
Progression-free survival served as the primary endpoint, which we assessed as the time from randomisation to disease progression according to RECIST v1.1 evaluated by the independent review committee, or death, whichever occurred first. Secondary endpoints consisted of overall survival, objective response rate, and safety. We defined overall survival as the time from randomisation to death from any cause and the objective response rate as the proportion of patients showing a best overall response, including complete and partial response.
Statistical analysis
We derived the sample size from the previously reported median progression-free survival (7.0 months) in a phase 3 trial in the setting of patients with recurrent or metastatic nasopharyngeal carcinoma treated with a gemcitabine plus cisplatin regimen as the first line option.7We calculated that we would need to randomise approximately 134 patients (67 patients per group), providing 80% power to detect the underlying hazard ratio of 0.42 for the primary analysis of progression-free at a two sided α level of 5% with one interim analysis. An interim analysis of progression-free survival, as prespecified, was conducted approximately three years after the first patient was randomised.
We did all analyses within the intention-to-treat population. Safety analyses included all patients who received at least one dose of the study treatment. We presented continuous variables as medians with interquartile ranges and compared them by using the Mann-Whitney test. We assessed categorical variables by using either the χ2test or Fisher’s exact test for comparison. We estimated survival curves and rates for time-to-event endpoints (overall survival, progression-free survival, and duration of response) by using the Kaplan-Meier method. We used stratified log-rank tests to compare survival between the two treatment cohorts. We used a Cox proportional hazards model to calculate hazard ratios and their corresponding 95% confidence intervals, with stratification according to the study design. We verified the assumption of proportional hazards by examining the Schoenfeld residuals. In addition, we did post hoc subgroup analyses stratified by various factors, including age (<50v≥50 years), gender (malevfemale), smoking history (yesvno), cancer stage (primary metastasesvrecurrent), number of metastatic organs (1v≥2), presence of liver metastasis (presentvabsent), presence of lung metastasis (presentvabsent), presence of bone metastasis (presentvabsent), and levels of Epstein-Barr virus DNA (<2000v≥2000 copies/mL). We tested treatment-by-covariate interaction by using the Cox proportional hazards model as the basis for the interaction study.
The progress of trial was overseen by an independent data monitoring committee, empowered to determine whether early termination of the trial was warranted. To uphold an overall type I error rate of 0.05 throughout the trial, early termination followed the O’Brien-Fleming type boundary (with an α level of 0.005). An interim analysis done on 31 October 2022 led the independent data monitoring committee to terminate the trial on the basis of the findings. This report presented the data of survival and adverse events.
We used R software (version 4.0.5) and SPSS (version 24.0) for statistical analyses. The codes used for analysis are provided as an online supplement. We deemed a significance threshold of 0.05 or lower for two sided P values to indicate statistical significance.
Patient and public involvement
Although not initially involved in designing the trial, all participants were informed of the trial’s objectives and contents on recruitment. Patients were not involved in the study’s execution or subsequent report preparation. When we began the research, patient and public participation was not a routine practice in our specialty in this region.
Results
Patients
Between October 2019 and August 2022 a total of 81 eligible patients with recurrent or metastatic nasopharyngeal carcinoma across four hospitals were randomly assigned to treatment with either nab-TPC (n=41) or gemcitabine plus cisplatin (n=40) (fig 1). Baseline demographics and disease characteristics were balanced between the treatment groups (table 1). The median age was 48 (interquartile range 36-57) years, and 79% of patients were male. Non-keratinising carcinoma was predominant among the patients (98%). Approximately two thirds of patients had previously undergone induction and concurrent chemoradiotherapy, and all patients had distant metastases. Thirty three (80%) patients in the nab-TPC cohort and 33 (83%) patients in the gemcitabine plus cisplatin cohort received six cycles of chemotherapy. The initial cost for each group is shown in supplementary table B.
After progression was documented, more than half of the patients received second line or third line chemotherapy, with 23 (56%) patients in the nab-TPC cohort and 26 (65%) patients in the gemcitabine plus cisplatin cohort receiving such treatment (supplementary table D). The predominant regimen used was anti-programmed cell death protein-1 inhibitor, administered to 21 (51%) patients in the nab-TPC cohort and 20 (50%) patients in the gemcitabine plus cisplatin cohort. Additionally, a portion of patients in both cohorts received crossover treatment—12 (29%) in the nab-TPC cohort and 12 (30%) in the gemcitabine plus cisplatin cohort.
Efficacy
As of the interim analysis cut-off date (31 October 2022), the median follow-up period was 15.8 (interquartile range 11.6-24.9) months. For the primary endpoint of progression-free survival, as evaluated independently, 27 (66%) of 41 patients in the nab-TPC cohort and 32 (80%) of 40 patients in the gemcitabine plus cisplatin cohort had disease progression or died. The median progression-free survival was 11.3 (95% confidence interval 9.7 to 12.9) months for the nab-TPC cohort and 7.7 (6.5 to 9.0) months for the gemcitabine plus cisplatin cohort. The difference in median progression-free survival between the two cohorts was statistically significant (hazard ratio 0.43, 95% confidence interval 0.25 to 0.73; P=0.002) (fig 2). The proportional hazards assumption for progression-free survival was unmet (supplementary figure A). In multivariate analyses, treatment group was an independent prognostic factor for progression-free survival (supplementary table F). For patients whose pre-treatment Epstein-Barr virus DNA was ≥2000 copies/mL, the nab-TPC regimen significantly improved their progression-free survival (hazard ratio 0.42, 0.21 to 0.84; P=0.01, supplementary figure B). However, this survival benefit was not observed for patients whose pre-treatment Epstein-Barr virus DNA was <2000 copies/mL (hazard ratio 0.45, 0.17 to 1.19; P=0.11). Improvement in progression-free survival was consistent in patients with or without liver metastases (supplementary figure C).Figure 3shows the outcomes of the primary endpoint, independently assessed progression-free survival, in the primary analyses for post hoc subgroups.
In the updated analysis done on 3 June 2023, we found that 31 (76%) patients in the nab-TPC cohort and 35 (88%) patients in the gemcitabine plus cisplatin cohort had experienced disease progression or died according to an independent review committee. The median progression-free survival, as assessed by the independent review committee, was 11.9 (10.0 to 13.8) months in the nab-TPC cohort and 7.6 (6.6 to 8.7) months in the gemcitabine plus cisplatin cohort. The hazard ratio was 0.39 (0.24 to 0.65; P<0.001) (supplementary figure D). Sixteen deaths were reported, with eight (20.0%) deaths in the gemcitabine plus cisplatin cohort and eight (20%) deaths in the nab-TPC cohort. The overall survival data were still immature for both cohorts at the time of analysis.
Tumour responses, as assessed by the independent review committee during the interim analysis, are outlined in supplementary table E. We observed complete responses in five (12%) patients in the nab-TPC cohort and four (10%) patients in the gemcitabine plus cisplatin cohort, as well as partial responses in 29 (71%) patients in the nab-TPC cohort and 21 (53%) patients in the gemcitabine plus cisplatin cohort. The nab-TPC cohort had a significantly higher objective response rate than the gemcitabine plus cisplatin cohort (34/41 (83%)v25/40 (63%); P=0.05). Among responders, the median duration of response was notably longer in the nab-TPC cohort (10.8 (8.8 to 12.8) months) than in the gemcitabine plus cisplatin cohort (6.9 (5.5 to 8.2) months), with a hazard ratio of 0.42 (0.22 to 0.81; P=0.009) (fig 2). However, the disease control rate was similar in the two treatment cohorts (1/41 (98%)v3/40 (92.5%); P=0.36). Tumour burden and response in patients with baseline positive or negative plasma EBV DNA levels are shown in supplementary table C.
Safety
Table 2shows the treatment related adverse events that occurred in ≥5% of patients. The most common adverse events were haematological and gastrointestinal. However, the overall incidence of grade 3 or 4 adverse events was low in both of cohorts (table 2). Treatment related grade 3 or 4 adverse events that differed significantly between the nab-TPC and gemcitabine plus cisplatin cohorts were leucopenia (10%v33%); P=0.02), neutropenia (15%v40%; P=0.01), and anaemia (2%v20%; P=0.01). The rate of discontinuation due to drug related adverse events was 2% (1/41) in the nab-TPC cohort and 3% (1/40) in the gemcitabine plus cisplatin cohort. Dose reductions occurred in five (12%) patients in the nab-TPC cohort and 10 (25%) patients in the gemcitabine plus cisplatin cohort. No treatment related deaths were reported during the trial.
Discussion
The nab-TPC regimen showed a substantial extension in progression-free survival compared with the conventional gemcitabine plus cisplatin chemotherapy regimen. This difference reached statistical significance in the pre-planned interim analysis focused on progression-free survival. The adverse events associated with the nab-TPC regimen were found to be manageable. Notably, this study represents the first prospective, multicentre, randomised clinical trial illustrating the progression-free survival benefits and objective response rate of the nab-TPC regimen in the first line treatment of recurrent or metastatic nasopharyngeal carcinoma.
Comparison with other studies
In a landmark phase 3 trial conducted in 2016 in patients with recurrent or metastatic nasopharyngeal carcinoma, gemcitabine plus cisplatin showed superior progression-free survival compared with a fluorouracil plus cisplatin regimen, thereby establishing gemcitabine plus cisplatin as a preferred first line regimen option.67However, the study found a median progression-free survival of seven months with the gemcitabine plus cisplatin regimen. In our recently reported prospective phase 3 trial, using capecitabine maintenance following paclitaxel, cisplatin, and capecitabine (TPC) chemotherapy for disease control in metastatic nasopharyngeal carcinoma, the median progression-free survival extended to 35.5 months. Interestingly, we noticed that the response rate with TPC was higher than for the gemcitabine plus cisplatin regimen before capecitabine maintenance was initiated (74%v64%), strongly indicating superior tumour control efficacy with TPC chemotherapy.10Nevertheless, nab-paclitaxel has shown a higher complete remission rate than conventional taxanes, with anaphylaxis being rare.14Owing to the promising antitumour effects of the nab-TPC regimen, we conducted a controlled, randomised, phase 3 trial to assess its antitumour efficacy compared with a standard gemcitabine plus cisplatin regimen for recurrent or metastatic nasopharyngeal carcinoma.
The median progression-free survival was 7.7 months for the gemcitabine plus cisplatin group in this study, which was similar to that reported in the previous studies in recurrent or metastatic nasopharyngeal carcinoma.915Notably, the nab-TPC treatment cohort showed a significant improvement, with a median increase of 3.6 months in progression-free survival among the intention-to-treat population. This improvement represents a 57% reduction in immediate risk of disease progression or mortality compared with the gemcitabine plus cisplatin treatment group. We note that median progression-free survival of nab-TPC regimen in this trial was shorter than the previously reported result. Several reasons contributed to this difference. First of all, the results from a post hoc subgroup analysis showed that patients without liver metastasis would more likely to achieve improved survival through capecitabine maintenance therapy.10More than half of patients (63.4%) enrolled in this trial had liver metastasis, compared with 38.5% of patients in the previous study.1016Secondly, and importantly, the method of calculating progression-free survival is different. In this trial, we assessed it from randomisation (before chemotherapy started) to disease progression, whereas in the previous study it was counted from the time of randomisation (after achievement of disease control with the TPC regimen) to disease progression. In addition, patients who progressed during TPC treatment were excluded from capecitabine maintenance therapy in the previous study.
In our study, the nab-TPC regimen achieved a significant 20% improvement in objective response rate compared with the gemcitabine plus cisplatin group during the initial phase of induction chemotherapy in patients with recurrent or metastatic nasopharyngeal carcinoma (82.9%v62.5%; P=0.05). Thus, this nab-TPC regimen represents an important option for patients in need of clinically meaningful tumour shrinkage. The objective response is commonly regarded as an independent predictor of survival in patients with solid tumours and may serve as a potential indicator for overall survival and progression-free survival. Meanwhile, objective response rate is strongly correlated with progression-free survival in metastatic nasopharyngeal carcinoma.1718Our data are consistent with previous findings regarding the association between objective response rate and progression-free survival. Furthermore, we observed that the progression-free survival curve in the two groups started to separate at the end of the induction chemotherapy phase (fig 2), indicating the superior efficacy of nab-TPC versus the gemcitabine plus cisplatin regimen even in the induction phase. In addition, the significantly improved duration of response was also observed in the nab-TPC group (median duration of response 10.8v6.9 months; P=0.009). Likewise, as part of the nab-TPC regimen, maintenance capecitabine therapy may also play a role in improving progression-free survival. These results suggest that the nab-TPC regimen possesses more potent and enduring anticancer effects than the gemcitabine plus cisplatin regimen and could be considered as a front line treatment option for patients with recurrent or metastatic nasopharyngeal carcinoma.
Recently, three phase 3 trials reported a significant improvement in progression-free survival for gemcitabine and cisplatin plus programmed cell death protein 1 inhibitors compared with gemcitabine and cisplatin alone.91519These findings suggest that the combination of programmed cell death protein 1 inhibitors with gemcitabine plus cisplatin has emerged as the new standard treatment for patients with recurrent or metastatic nasopharyngeal carcinoma in the front line setting. Furthermore, investigating whether the use of nab-TPC as chemotherapy backbone, combined with programmed cell death protein 1 inhibitors, improves efficacy compared with gemcitabine plus cisplatin with programmed cell death protein 1 inhibitors would be worthwhile
The adverse events profiles of nab-TPC and gemcitabine plus cisplatin in this trial were as expected. No new toxicity signals were noted during the trial. All the adverse events were generally manageable. Treatment related haematological adverse events were more frequent with the gemcitabine plus cisplatin regimen, whereas hand-foot syndrome was associated with nab-TPC regimen. However, the observed rate of hand-foot syndrome is comparable to that reported in previous clinical trials.10The proportion of patients discontinuing the study drugs was similar between the two cohorts.
Limitations of study
This trial has some inevitable limitations. Firstly, as all participants were sourced from endemic areas in China where non-keratinising nasopharyngeal carcinoma comprises more than 95% of cases, whether the findings can be extrapolated to non-endemic regions without further validation remains uncertain. Secondly, although the trial has achieved its primary endpoint of progression-free survival, the overall survival data have not yet fully matured. A meta-analysis of individual patient data suggested that progression-free survival was strongly correlated with overall survival in locoregional nasopharyngeal carcinoma.20However, measurement of overall survival is potentially diluted by administration of crossover treatment in our study (29% of patients in the nab-TPC group and 30% of patients in the gemcitabine plus cisplatin group). Furthermore, with an increasing number of treatment options for recurrent or metastatic nasopharyngeal carcinoma and considering that regimens containing programmed cell death protein 1 inhibitor are effective in subsequent treatment, whether the same correlation exists is unclear.21Further follow-up is needed to ascertain any potential benefit in overall survival. Thirdly, a thorough analysis of potential predictive biomarkers is essential to identify patients who may derive benefit from the nab-TPC regimen as a first line treatment.
Conclusion
In summary, our findings show, for the first time, that the nab-TPC regimen offers a statistically significant enhancement in progression-free survival compared with the current standard-of-care gemcitabine plus cisplatin regimen in recurrent or metastatic nasopharyngeal carcinoma as a first line treatment. The nab-TPC regimen has a manageable safety profile. These results advocate for the inclusion of the nab-TPC regimen as a standard treatment option for patients with recurrent or metastatic nasopharyngeal carcinoma and propose its suitability as a control arm in future randomised clinical trials.
","Objective: To compare the effectiveness and safety of nab-paclitaxel, cisplatin, and capecitabine (nab-TPC) with gemcitabine and cisplatin as an alternative first line treatment option for recurrent or metastatic nasopharyngeal carcinoma.
Design: Phase 3, open label, multicentre, randomised trial.
Setting: Four hospitals located in China between September 2019 and August 2022.
Participants: Adults (≥18 years) with recurrent or metastatic nasopharyngeal carcinoma.
Interventions: Patients were randomised in a 1:1 ratio to treatment with either nab-paclitaxel (200 g/m2on day 1), cisplatin (60 mg/m2on day 1), and capecitabine (1000 mg/m2twice on days 1-14) or gemcitabine (1 g/m2on days 1 and 8) and cisplatin (80 mg/m2on day 1).
Main outcome measures: Progression-free survival was evaluated by the independent review committee as the primary endpoint in the intention-to-treat population.
Results: The median follow-up was 15.8 months in the prespecified interim analysis (31 October 2022). As assessed by the independent review committee, the median progression-free survival was 11.3 (95% confidence interval 9.7 to 12.9) months in the nab-TPC cohort compared with 7.7 (6.5 to 9.0) months in the gemcitabine and cisplatin cohort. The hazard ratio was 0.43 (95% confidence interval 0.25 to 0.73; P=0.002). The objective response rate in the nab-TPC cohort was 83% (34/41) versus 63% (25/40) in the gemcitabine and cisplatin cohort (P=0.05), and the duration of response was 10.8 months in the nab-TPC cohort compared with 6.9 months in the gemcitabine and cisplatin cohort (P=0.009). Treatment related grade 3 or 4 adverse events, including leukopenia (4/41 (10%)v13/40 (33%); P=0.02), neutropenia (6/41 (15%)v16/40 (40%); P=0.01), and anaemia (1/41 (2%)v8/40 (20%); P=0.01), were higher in the gemcitabine and cisplatin cohort than in the nab-TPC cohort. No deaths related to treatment occurred in either treatment group. Survival and long term toxicity are still being evaluated with longer follow-up.
Conclusion: The nab-TPC regimen showed a superior antitumoural efficacy and favourable safety profile compared with gemcitabine and cisplatin for recurrent or metastatic nasopharyngeal carcinoma. Nab-TPC should be considered the standard first line treatment for recurrent or metastatic nasopharyngeal carcinoma. Longer follow-up is needed to confirm the benefits for overall survival.
Trial registration: Chinese Clinical Trial Registry ChiCTR1900027112.
"
Lateral episiotomy or in vacuum assisted delivery in nulliparous women,"Introduction
Obstetric anal sphincter injury is a serious complication to vaginal birth causing anal incontinence1and reduced quality of life.2Primiparity and instrumental birth are two major risk factors of obstetric anal sphincter injury.3The obstetric anal sphincter injury rate in primiparous women varies between countries, with rates of 0.1-4% for spontaneous births and 6-24% for instrumental births in Europe, Canada, and the United States.45
The preventive effect of an episiotomy, an incision made in the tissue between the vaginal opening and the anus during childbirth, on obstetric anal sphincter injury is not clear. A Cochrane review of randomised controlled trials has concluded that routine episiotomy may increase the risk of obstetric anal sphincter injury in non-instrumental birth,6while lateral or mediolateral episiotomy might prevent obstetric anal sphincter injury in vacuum extraction in nulliparous women, based on results from pooled observational studies.78910The most recent meta-analysis, published in 2022, which included 23 observational studies and two underpowered randomised controlled trials, reported an adjusted odds ratio of 0.51 (95% confidence interval (CI) 0.42 to 0.84) for obstetric anal sphincter injury when a lateral or mediolateral episiotomy was performed compared with no episiotomy.7However, other observational studies reported no effect,11or the opposite effect.12Episiotomy has also been associated with an increased risk of postpartum haemorrhage and pain.13As such, some national guidelines state that a lateral or mediolateral episiotomy should be considered in nulliparous women requiring vacuum extraction,1415but also that the decision should be tailored to the circumstances,14making the decision to do an episiotomy provider dependent. This uncertainty regarding treatment effect size and adverse effects is reflected in the internationally varying rates of episiotomy in instrumental births, from 17.1% in Denmark to 97.2% in Poland.4
Observational studies come with limitations. For instance, the differences in effect can be due to lack of standardisation regarding the type of episiotomy, where the angle and incision point have been deemed the most important traits.161718To date, no adequately sized randomised controlled trial on the protective effect of episiotomy on obstetric anal sphincter injury in vacuum extraction has been published.1920Hence, the Cochrane Collaboration and the National Institute for Health and Care Excellence’s Evidence Search, among others, have stated that the protective effect of lateral or mediolateral episiotomy in vacuum extraction should be investigated in an adequately sized randomised controlled trial.6813Accordingly, we hypothesised that a routine lateral episiotomy reduces the risk of obstetric anal sphincter injury in nulliparous women requiring vacuum extraction. We performed a randomised controlled trial to assess the effect of lateral episiotomy compared with no episiotomy on obstetric anal sphincter injury in nulliparous women requiring vacuum extraction.
Methods
Study design
The Episiotomy in Vacuum Assisted delivery (EVA) trial was a randomised, parallel, open label, controlled trial comparing the effect of lateral episiotomy with no episiotomy (1:1), on obstetric anal sphincter injury in nulliparous women requiring vacuum extraction. Eight hospitals in Sweden conducted the study between 1 July 2017 and 15 February 2023. The study protocol has been published previously.21The trial was approved by the regional ethical review board of Stockholm before the start (2015/1238-31/2) with amendments to approve additional participating hospitals (2017/1005-32, 2018/775-32, 2018/2291-32, 2019-02758, 2019-02758, and 2019-04427) and retrieval of data from the Swedish pregnancy register and the Swedish neonatal quality register (2023-02301-02). The trial was monitored by the Karolinska Trial Alliance 2017-20 and by two independent monitors 2021-23. The trial was registered inwww.clinicaltrials.gov(NCT02643108) on 30 December 2015.
Results are reported according to CONSORT 2010 guidelines22and TIDieR checklist.23
Participants
Inclusion criteria were nulliparous women with a singleton, live, cephalic presenting fetus at 34 gestational weeks or more, requiring vacuum extraction. Exclusion criteria were previous surgery for urinary or anal incontinence or for genital prolapse. Women here refers to people of female sex. All genders were eligible. Informed written and oral consent was obtained by attending midwives or physicians after gestational week 18, including during labour if the woman had adequate pain relief and time to reflect, based on the healthcare provider’s judgement.
Randomisation and masking
The decision to perform a vacuum extraction was made by the attending physician on obstetric indications independent of trial participation. This instruction was clearly made to physicians during trial education and to patients in the consent form, to avoid biased trial inclusion. Randomisation took place after the decision. The randomisation sequence was produced by Karolinska Trial Alliance using computer-based random permuted blocks of two to eight stratified by site. The allocation was done at a 1:1 ratio using consecutive opaque sealed envelopes to facilitate inclusion in medically urgent situations. After informed consent was confirmed, the sealed envelope was opened by the assisting nurse, midwife, or physician, and the allocation was read out loud in the delivery room. No masking was possible.
Procedures
In all women, the vacuum extraction procedure was prepared according to clinical routine, including intermittent bladder catheterisation, and ensuring pain relief in the form of epidural, pudendal, or local anaesthesia, or a combination of these. The extraction was done by the attending physician synchronously with maternal contractions and pushing until the fetal head was crowning, when a lateral episiotomy was done by the physician or midwife. The trial intervention was a standardised lateral episiotomy, beginning 1-3 cm from the posterior fourchette, at a 60° (45-80°) angle from the midline, and 4 cm (3-5 cm) long (fig 1), based on consensus16and suggested protective trigonometric properties.1718The clinical staff at all sites received education on several occasions on how to perform the standardised lateral episiotomy before trial start. The comparison was no episiotomy unless considered indispensable. All women received verbal guidance and manual perineal support to prevent obstetric anal sphincter injury according to clinical routine, including intended slow delivery of the fetal head, hands-on perineal support, and warm compresses. Examination and suturing of vaginal and perineal injuries were managed according to clinical routine. Postnatal care was supplied according to clinical routine. In addition, perineal pain was assessed once between day one and seven after childbirth using a numerical rating scale (from 0=no pain to 10=worst possible pain) and a questionnaire covering complications was sent out at two months after childbirth (supplementary table S3).
Outcomes
The primary outcome was obstetric anal sphincter injury, defined as a third or fourth degree perineal injury involving the external or internal anal sphincter muscles, or both, as defined by the diagnoses O702 and O703, requiring surgical repair, in the Swedish version of the International Classification of Diseases 10th edition. Obstetric anal sphincter injury was the primary outcome in the published meta-analyses,78and carries well known risks of both short and long term pelvic floor sequelae.24Obstetric anal sphincter injury rate is also an established marker of quality of care, comparable between hospitals and countries.4625The diagnosis was made clinically by the attending physician on site through visual inspection and digital vaginal and rectal examination immediately after childbirth, according to Swedish guidelines.15Exploratory maternal outcomes were vaginal or perineal injury other than obstetric anal sphincter injury (ie, intact perineum, first degree injury, and second degree perineal injury including episiotomy, allocated or not), duration of hospital stay (days), perineal pain (once between days one and seven after delivery reported on a numerical rating scale from 0=no pain to 10=worst imaginable pain), and birth experience (once between days one and seven after delivery reported on a numerical rating scale from 1=worst possible overall experience to 10=best possible overall experience). Perineal pain of 7 or more was used as an arbitrary indicator of severe pain. Childbirth experience of 3 or less was used as an indicator of a negative childbirth experience.26From the two month questionnaire, questions regarding perineal pain, surgical wound problems, infections, and re-admission were included (supplementary material). Secondary outcomes are patient reported outcomes collected at 12 months, with results anticipated in 2025.21
Exploratory neonatal outcomes were an Apgar score of less than 7 at 5 mins, metabolic acidosis defined as umbilical artery pH of less than 7.05 or base deficit of 12 mmol/L or more, admission to neonatal intensive care, shoulder dystocia, scalp haematoma, fetal fracture, obstetric brachial plexus palsy, hypoxic ischaemic encephalopathy, and neonatal seizures. Safety maternal outcomes were postpartum haemorrhage (blood loss of ≥1000 mL) and severe perineal pain (numerical rating scale of ≥7, measured once between days one and seven).
Demographics, maternal and childbirth characteristics, and outcomes were retrieved from the Swedish pregnancy register.27Data not available in the register were registered in an electronic case report form for each participant. Second stage duration was defined as the time between full cervical dilatation and birth. Neonatal data were retrieved from the Swedish pregnancy register and the Swedish neonatal quality register.28Data for vaginal and perineal injuries were registered in the electronic case report form. The primary outcome of obstetric anal sphincter injury was cross checked with data from the Swedish pregnancy register. Two individuals had discrepant results. In both cases, an erroneous procedure code in the Swedish pregnancy register did not match the diagnostic codes or text in the medical record. Therefore, the outcome obstetric anal sphincter injury was based on the electronic case report form.
Adverse and serious adverse events
An adverse event was defined as a complication to the trial intervention or perineal injury including perineal wound infection, dehiscence, granuloma or symptomatic scarring, severe perineal pain (requiring opioids), or fistula formation in the vagina, anus, or perineum during the first eight weeks postpartum. A serious adverse event was any event resulting in maternal death within 42 days postpartum or neonatal death within 28 days after birth, or that was life threatening, required admission to intensive care unit, or that resulted in persistent or significant disability. Participants were instructed to contact the hospital where they gave birth in case of symptoms of adverse events. Adverse events were thus identified through self-referral. In addition to this, medical records were screened for adverse events up to two months after childbirth and self-reported complications were collected from the questionnaire at two months.
Statistical analyses
The original sample size was based on the meta-analysis by Lund and colleagues,8reporting a 50% reduction of obstetric anal sphincter injury in vacuum extraction in nulliparous women, when a lateral or mediolateral episiotomy was performed. We used mean rate of obstetric anal sphincter injury in vacuum extraction in Sweden in 2015, according to the Swedish medical birth register, to hypothesise that a 50% reduction of obstetric anal sphincter injury from 12.4% to 6.2% could be detected with 80% power and a P<0.05 with 344 women in each group using a two sided χ2test. With an estimated 3% loss to follow-up, 355 women in each group (total n=710) was needed. After the Swedish network for national clinical studies in obstetrics and gynaecology (www.snaks.se) advised that a smaller difference could be clinically important, we applied for ethical approval for an amendment in 2017, after the trial had started, to include 1400 women to show a 30% reduction from 12.4% to 7.8%. In 2019, we aimed for the initial sample size (710 women) due to slow recruitment. An interim analysis was done in 2020 according to prespecified criteria to detect a difference in line with van Bavel and colleagues,2129reporting a reduction from 14.0% to 2.5% of obstetric anal sphincter injury with episiotomy in nulliparous women requiring vacuum extraction. To show this difference with 80% power and P<0.01, 350 women were needed, a total which was attained in 2020. In case this difference was found, we had a priori decided to stop the trial. Stopping criteria were not met.
For the main comparative analyses, we used a modified intention-to-treat population, defined as all consenting randomised women with vacuum extraction or vacuum extraction attempt. A vacuum extraction attempt was defined as having the vacuum cup applied to the fetal head. One woman withdrew consent before the vacuum cup was applied and was excluded. Women who had been randomised but had a spontaneous birth before the vacuum cup was applied were also excluded (fig 2). This modification to the intention-to-treat population was motivated by the primary objective of the trial, because women with spontaneous birth were deemed to have lost the main inclusion criterium (vacuum assisted delivery). The primary analysis was based on allocation regardless of received treatment. We also analysed the outcome of obstetric anal sphincter injury on the total intention-to-treat (all consenting women), per protocol (consenting women delivered with vacuum extraction who received the assigned treatment), as treated (consenting women with attempted or successful vacuum extraction), and safety populations (all consenting women as treated).
Baseline characteristics are presented with medians and ranges or numbers and proportions. The prespecified primary efficacy analysis was the unadjusted comparison of the primary outcome obstetric anal sphincter injury in the lateral episiotomy group compared with the no episiotomy group on the modified intention-to-treat population with a two sided χ2test. Due to the performed interim analysis, P<0.04 was deemed necessary for the primary outcome of obstetric anal sphincter injury to account for multiple testing. The risk difference and risk ratio between the groups were calculated with 96% CI using the method of Miettinen and Nurminen.30Risk differences of less than 0 and risk ratios of less than 1 indicated lower risks for the outcome in the intervention group versus the control group. Since the randomisation was stratified by site, adjustment for site effects was done by using a mixed effects Poisson regression model to estimate the risk ratio with 96% CI, with site and site × treatment as random effects. The interaction term was included to account for treatment effect heterogeneity across sites. Site specific treatment effects were calculated using the best linear unbiased predictions of the random effects and visualised in a forest plot. In a prespecified sensitivity analysis, we also adjusted for maternal age, height, body mass index, country of birth, fetal position, and operator skills (categorised into resident, specialist gynaecologist, or specialist obstetrician), by inclusion as covariates in the mixed effects Poisson model. Number needed to treat were calculated with 96% CI and number needed to harm were calculated with 95% CI.
Maternal and neonatal categorical exploratory outcomes were compared with two sided χ2test, or Fisher’s exact test for rare events, and t test for continuous variables. Maternal safety outcomes, including adverse events and serious adverse events, were calculated for the modified intention-to-treat population based on allocated treatment and for the safety population based on received treatment. All analyses except the primary outcome were unadjusted, P<0.05 was considered significant, and risk difference and risk ratio with 95% CI were calculated. A detailed statistical analysis plan was set before data lock on 27 June 2023 and is available as supplementary information. Changes from the protocol included the use of Poisson regression to estimate the relative risk instead of logistic regression to estimate the odds ratio. Analyses were performed by independent, endpoint masked statisticians using SAS 9.4 (SAS Institute Inc, Cary, NC, USA).
Patient and public involvement
At the initiation of this trial, patient or public involvement was not a compulsory requirement and, at the time, no relevant patient organisations existed. Therefore, layperson patients or the public were not directly involved in the design or conduct of this trial. However, in 2015 the Swedish government commissioned the healthcare regions, research councils, and authorities to improve prevention of obstetric anal sphincter injury. The Swedish Agency for Health Technology Assessment and Assessment of Social Services (SBU), with the contribution of patient representatives, published a report in 2018 of prioritised research areas within the field of maternal birth injuries. The report stated that preventative measures against obstetric anal sphincter injury was one of the most important research questions.31In addition, along the conduct of this trial, a qualitative research project assessed the views of consenting and non-consenting women regarding the recruitment process.32To increase the impact of the results from this trial, both for care providers and researchers, as well as for patients and public, the results will be disseminated in conference presentations, posters, and mass media, and shared across social media, pregnancy podcasts, and companion blogs to involve pregnant women and their families.
Results
During the recruitment period of 1 July 2017 to 15 February 2023, 61 150 women were eligible at the eight study sites, of which 6218 women consented to participate if vacuum extraction was required. Of these, a total of 717 women required vacuum extraction and were randomly assigned by 255 different physicians. In all, 354 (49%) women were allocated to lateral episiotomy and 363 (51%) women were allocated to no episiotomy (fig 2). One woman from the intervention group withdrew consent before the vacuum extraction and was excluded. Spontaneous birth occurred before the vacuum was applied in nine women allocated to lateral episiotomy and in five women allocated to no episiotomy. These women were also excluded from the modified intention-to-treat population. Therefore, the modified intention-to-treat population included 344 (49%) women allocated to lateral episiotomy and 358 (51%) women allocated to no episiotomy (fig 2).
Of the 344 women allocated to lateral episiotomy, 310 (90%) received a lateral episiotomy and 34 (10%) received no episiotomy. Of 358 women allocated to no episiotomy, 291 (81%) received no episiotomy, while 67 (19%) received a lateral episiotomy. The two allocation groups were similar regarding maternal (table 1) and childbirth characteristics (table 2). Among women allocated to lateral episiotomy, 22 (6%) were delivered by caesarean section after vacuum extraction was unsuccessful, of which four received an episiotomy before conversion. Among women allocated to no episiotomy, five (1%) were delivered by caesarean section after vacuum extraction was unsuccessful. None of these women received an episiotomy.
Obstetric anal sphincter injury occurred in 21 (6%) of the women allocated to lateral episiotomy and in 47 (13%) of the women allocated to no episiotomy (P=0.002), with a risk difference of −7.0% (96% CI −11.7% to −2.5%) and risk ratio of 0.46 (96% CI 0.28 to 0.78). The adjusted risk ratio (adjusted for study site) was 0.47 (96% CI 0.23 to 0.97) (table 3). Results were largely consistent across sites, although the crude event rates were numerically larger in the episiotomy group for two of eight sites (supplementary figure S1). Similar results were also observed in a prespecified sensitivity analysis adjusting for maternal age, height, body mass index, country of birth, fetal position, and operator skills (adjusted risk ratio 0.49 (96% CI 0.24 to 0.99)). Number needed to treat with episiotomy was 14.3 (96% CI 8.6 to 40.0) to avoid one obstetric anal sphincter injury. In the per protocol population, obstetric anal sphincter injury occurred in 20 (6%) of 311 women who received lateral episiotomy and 38 (13%) of 295 women with no episiotomy (risk difference −6.5% (95% CI −11.4% to −1.8%); risk ratio 0.50 (95% CI 0.30 to 0.84)). Obstetric anal sphincter injury for the total intention-to-treat population had consistent results with the modified intention-to-treat and per protocol analyses. In the as treated population, obstetric anal sphincter injury occurred in 29 (8%) of 377 women with lateral episiotomy and 39 (12%) of 325 women with no episiotomy, which did not reach statistical significance (risk difference −4.3% (95% CI −8.9% to 0.2%)) (supplementary table S1).
Intact perineum was rare and most of these participants gave birth by caesarean section. In the intervention group, first degree injuries were significantly less frequent and second degree injuries including episiotomy (allocated or not) were significantly more frequent, compared with the comparison group. No statistically significant differences were noted for postpartum haemorrhage, postpartum perineal pain, birth experience, hospital stay, or neonatal outcomes (table 3).
Self-referral of wound infection and dehiscence were significantly more common in the intervention group than in the control group. Number needed to harm was 21.7 (95% CI 12.5 to 125) for infection and 16.9 (10.2 to 41.7) for wound dehiscence. Total self-referred adverse events did not differ significantly between the groups and no significant difference was noted in surgical treatment such as wound re-suturing or extirpation of granuloma. Serious adverse events and persistent incapacity were rare and did not differ significantly between groups (table 4). The differences in wound complications increased in the safety population (supplementary table S2).
For self-reported complications within two months after childbirth, no significant difference in pain assessment was noted but women in the intervention group significantly more often used analgesics after discharge from the hospital. No significant difference was noted in duration of analgesics use. Similar to the results from self-referral, the intervention group reported more wound complications, including wound infection and wound re-suturing (table 5).
Discussion
Principal findings
In this randomised trial of 702 nulliparous women requiring vacuum extraction, the rate and risk of obstetric anal sphincter injury was more than halved in women allocated to lateral episiotomy compared with no episiotomy. No significant differences were noted in blood loss, perineal pain, birth experience, hospital stay duration, short term neonatal outcomes, or total adverse events. Women allocated to lateral episiotomy had more wound infections, dehiscence, and re-suturing, but when including extirpation of granulomas, surgical treatment did not significantly differ between groups.
Strengths and limitations
This study is the first adequately sized randomised controlled trial to assess the effect of episiotomy in nulliparous women requiring vacuum extraction, filling the previous knowledge gap.6813Other strengths of this trial are the small differences between results in the modified intention-to-treat, intention-to-treat, per protocol, and as treated analyses, the excellent protocol adherence, and the representative population and staff from eight hospitals evenly distributed over Sweden.
A limitation of this trial is that the obstetric anal sphincter injury diagnosis was not masked for the allocation because this was deemed impossible as an episiotomy would be apparent. Instead, the sites followed Swedish guidelines recommending a joint assessment of two care providers when diagnosing perineal injuries.15One of these providers was often the same physician who performed the vacuum extraction. This reflects clinical practice but could infer a risk of detection bias, especially if the provider had a strong opinion of the effect of episiotomy. Since vacuum extraction was performed unplanned at all hours, an independent investigator was not deemed feasible. Ultrasound immediately after childbirth to objectively assess obstetric anal sphincter injury was neither considered feasible,33nor is it recommended.34Nevertheless, to check the diagnostic accuracy, as is recommended in the postnatal period in high risk births,34an ongoing substudy blinded for allocation in four sites using 3D endoanal and endovaginal ultrasound at 6-12 months postpartum is expected to have results in 2025.21
Comparison with other studies
Two previous randomised controlled trials have assessed the effect of episiotomy in operative vaginal delivery.1920Neither of these was adequately sized to confirm or refute an effect. The level of risk reduction and number needed to treat in our trial confirm the results of pooled observational studies.78Nevertheless, the confidence interval for number needed to treat was wide, which may be due to a limited sample size or a variation in effect also seen in observational studies.824The average number needed to treat may be difficult to interpret because the average does not refer to a particular individual, which may carry a higher or lower baseline risk of obstetric anal sphincter injury. However, the average number needed to treat should be acceptable also in settings where episiotomy is used restrictively.35The rate of obstetric anal sphincter injury in the total study population is also consistent with the current rate among Swedish primiparous women delivered with vacuum extraction.36
Episiotomy has been associated with greater blood loss and perineal pain,13although this was not confirmed in our trial. Likewise, no clear evidence suggests that lateral episiotomy significantly affected birth experience, hospital stay duration, or short term neonatal outcomes. However, the confidence intervals were wide which makes equipoise for these outcomes uncertain. Women allocated to episiotomy reported more wound infection, coherent with previous observational studies.3738This complication might have been prevented if prophylactic antibiotics had been given, as shown in the ANODE trial from 2019.38During our trial, Swedish guidelines recommended prophylactic antibiotics only for obstetric anal sphincter injury,15which potentially could have imbalanced wound complications in the groups. Given the results from the ANODE trial and our trial, prophylactic antibiotics should be given in vacuum extraction.38
The major objection against routine episiotomy is the risk of an unnecessary cut, perhaps causing a larger injury than needed. Our study supports that routine lateral episiotomy will result in a higher proportion of women sustaining a second degree injury, instead of a first degree injury or obstetric anal sphincter injury. The proportion of women with intact perineum was similar in both groups, partly due to more conversions to caesarean section in the intervention group. Conversion to caesarean section was in most cases done before the intervention, therefore, this difference was deemed to result from chance. It is also not plausible that episiotomy would increase the risk of failed vacuum extraction.
A potential pitfall in this trial is the uneven non-adherence to allocated treatment, which was more common in the no episiotomy group. Episiotomy was in these cases usually motivated by fetal distress or imminent tearing, as restrictive practice implies. In comparison with previous trials,1920the protocol adherence in the no episiotomy group was excellent. Notably, the clinical judgement of who did and did not need an episiotomy did not improve the reduction of obstetric anal sphincter injury as seen in the as treated analysis. This effect is consistent with a previous meta-analysis reporting that an episiotomy rate of over 75% had a greater protective effect.8Admittedly, it seems possible to maintain a low rate of episiotomy and a low rate of obstetric anal sphincter injury in some settings,20which suggests that other factors, such as operator skills, may contribute.39
Conclusions and policy implications
Until now, guidelines have advised clinicians to consider episiotomy in nulliparous women requiring vacuum extraction, without a clear recommendation. With the results of our study, a lateral episiotomy can be recommended for nulliparous women requiring vacuum extraction to significantly lower the risk of obstetric anal sphincter injury. However, before recommending routine lateral episiotomy in nulliparous women requiring vacuum extraction, more evidence is needed regarding long term patient reported outcomes. For example, outcomes such as quality of life and pelvic floor function after vacuum extraction with and without lateral episiotomy. These outcomes will be assessed in the planned follow-up of the EVA trial. The follow-up results can inform the design of an optimal prophylactic strategy.
Obstetric anal sphincter injury is a serious complication to vaginal birth, leading to anal incontinence and reduced quality of life, and is more common in nulliparous women delivered with vacuum extraction
Lateral or mediolateral episiotomy might reduce obstetric anal sphincter injury in nulliparous women delivered with vacuum extraction by approximately 50%
The effect of a lateral or mediolateral episiotomy in instrumental births in nulliparous women has been called on to be investigated in an adequately sized randomised controlled trial
This trial provides evidence that the rate of obstetric anal sphincter injury in nulliparous women requiring vacuum extraction can be significantly reduced with a lateral episiotomy
No differences were noted between groups in postpartum pain, blood loss, neonatal outcomes, or total adverse events, but episiotomy was significantly associated with an increase in wound infection and dehiscence
Lateral episiotomy can be recommended in nulliparous women requiring vacuum extraction to reduce the rate of obstetric anal sphincter injury
","Objective: To assess the effect of lateral episiotomy, compared with no episiotomy, on obstetric anal sphincter injury in nulliparous women requiring vacuum extraction.
Design: A multicentre, open label, randomised controlled trial.
Setting: Eight hospitals in Sweden, 2017-23.
Participants: 717 nulliparous women with a single live fetus of 34 gestational weeks or more, requiring vacuum extraction were randomly assigned (1:1) to lateral episiotomy or no episiotomy using sealed opaque envelopes. Randomisation was stratified by study site.
Intervention: A standardised lateral episiotomy was performed during the vacuum extraction, at crowning of the fetal head, starting 1-3 cm from the posterior fourchette, at a 60° (45-80°) angle from the midline, and 4 cm (3-5 cm) long. The comparison was no episiotomy unless considered indispensable.
Main outcome measures: The primary outcome of the episiotomy in vacuum assisted delivery (EVA) trial was obstetric anal sphincter injury, clinically diagnosed by combined visual inspection and digital rectal and vaginal examination. The primary analysis used a modified intention-to-treat population that included all consenting women with attempted or successful vacuum extraction. As a result of an interim analysis at significance level P<0.01, the primary endpoint was tested at 4% significance level with accompanying 96% confidence interval (CI).
Results: From 1 July 2017 to 15 February 2023, 717 women were randomly assigned: 354 (49%) to lateral episiotomy and 363 (51%) to no episiotomy. Before vacuum extraction attempt, one woman withdrew consent and 14 had a spontaneous birth, leaving 702 for the primary analysis. In the intervention group, 21 (6%) of 344 women sustained obstetric anal sphincter injury, compared with 47 (13%) of 358 women in the comparison group (P=0.002). The risk difference was −7.0% (96% CI −11.7% to −2.5%). The risk ratio adjusted for site was 0.47 (96% CI 0.23 to 0.97) and unadjusted risk ratio was 0.46 (0.28 to 0.78). No significant differences were noted between groups in postpartum pain, blood loss, neonatal outcomes, or total adverse events, but the intervention group had more wound infections and dehiscence.
Conclusions: Lateral episiotomy can be recommended for nulliparous women requiring vacuum extraction to significantly reduce the risk of obstetric anal sphincter injury.
Trial registration: ClinicalTrials.govNCT02643108.
"
Global burden of type 1 diabetes in adults aged 65 years and older,"Introduction
Type 1 diabetes mellitus (T1DM) was traditionally considered a disease that manifested during childhood and adolescence and could have a profound effect on life span.12Variable evidence from recent studies suggests that the life expectancy of an increasing number of older and elderly people with T1DM has improved as has diabetes care and the management of complications.34Current and comprehensive data on the burden of T1DM is, however, lacking in most countries and territories worldwide.56Nearly all of the existing clinical practices and guidelines lack targeted content for the management of T1DM among older people.7
As the global population ages, substantial gaps in data related to T1DM need to be bridged urgently. A modelling study estimated that a diagnosis of T1DM was missed in 3.7 million people in 2021, and it predicted a 60-107% increase in the number of people with a diagnosis between 2021 and 2040 globally, highlighting the opportunity to raise the standard of care and save millions of lives in the coming decade.8This increasing prevalence could represent an increasing number of people with T1DM living to elderly age. Importantly, based on current evidence, the number of older people with T1DM has already likely increased owing to an overall increase in life expectancy.4Continuous T1DM care of people in older age, diabetes related complications, and impaired cognitive and physiological function from ageing are challenging for both people with T1DM and healthcare resources.9Understanding the changes in mortality and DALYs among older people (≥65 years) with T1DM is critical for their care.
We investigated T1DM associated prevalence, mortality, and disability adjusted life years (DALYs) in adults aged ≥65 years at global, regional, and national level during 1990-2019 by social developmental level and age and sex of the older population. We also assessed factors that might have an influence on DALYs among older people with T1DM.
Methods
Study population and data collection
In our analysis of the Global Burden of Disease Study 2019, we accessed repeated cross sectional data from the Global Health Data Exchange, encompassing the global burden of 369 diseases and injuries and 87 risk factors, including T1DM, across 21 regions and 204 countries and territories from 1990 to 2019. The study population comprised people with T1DM aged 65 years and older, including those with diabetes diagnosed before age 65 years.10111213From the Global Burden of Disease Study 2019 we extracted information on T1DM in people aged ≥65 years; location, age, and sex specific prevalence; mortality; numbers and rates for DALYs; and DALYs attributable to each risk factor (with corresponding 95% uncertainty intervals (UIs)). Attributable DALYs are a metric for quantifying the contribution of specific risk factors to the burden of disease. DALYs signify the changes and reduction in current burden of disease if a change occurs to population level exposure to a particular risk factor. After adjusting for comorbidity, we used micro-simulation to obtain the final estimate of years lived with disability. Years of life lost were calculated by multiplying the estimated number of deaths from T1DM by the standard life expectancy at the age of death. DALYs were equal to the sum of years lived with disability and years of life lost. The methodology employed in the Global Burden of Disease Study 2019 is described elsewhere (also see supplementary file, methods section).1415
In the Global Burden of Disease Study, T1DM is defined as doctor diagnosed disease identified through a diabetic registry or hospital records. To estimate non-fatal burdens of T1DM and complications from T1DM, a bayesian meta-regression modelling tool, DisMod-MR 2.1, was used to analyse 1527 location years of data from the scientific literature, survey microdata, and insurance claims.14Estimates of both non-fatal and fatal outcomes in people with T1DM include those attributed to complications from the disease. The Global Burden of Disease Study extrapolates and models data based on the reported T1DM, juvenile onset diabetes, and insulin dependent diabetes to develop a database applicable to most countries, with no age restrictions on the input raw data. To allow for smoothing over age, time, and location in areas without complete datasets, spatiotemporal Gaussian process regression was used to model the Global Burden of Disease Study 2019 input data.14The Global Burden of Disease Study analysed the spatiotemporal Gaussian process regression.16The methods section in the supplementary file provides detailed information on the input data and methodology for T1DM.
In this study, we collected data on T1DM from 21 regions of countries that are geographically proximate and have similar epidemiological profiles, encompassing seven age groups (65-69 years, 70-74 years, 75-79 years, 80-84 years, 85-89 years, 90-94 years, ≥95 years) for both men and women. We also calculated the sociodemographic index for each country, which is a composite indicator of the social and economic conditions influencing health outcomes in each locality. The sociodemographic index ranges from 0.005 to 1. In this study 1 represented the highest education level, highest per capita income, and lowest fertility rate. The sociodemographic index is divided into five categories: low, low-middle, middle, high-middle, and high.
Statistical analysis
A descriptive analysis was performed to characterise the burden of T1DM among adults aged ≥65 years on a global scale. We compared the age standardised prevalence (per 100 000 population), age standardised mortality (per 100 000 population), and age standardised DALYs (per 100 000 population) of T1DM across different age groups, sexes, regions, and countries. Based on the data on T1DM and associated risk factors obtained from the Global Burden of Disease Study, we further calculated the age standardised rates and corresponding 95% confidence intervals (CIs) based on the world standard population reported in the Global Burden of Disease Study 2017 for comparison between regions, and further estimated average annual percentage changes (AAPCs) by joinpoint regression to measure the temporal trend.14Our estimates are shown per 100 000 population using the top equation infigure 1.
AAPCs are used to represent the average increase or rate of change of a specific variable over a specified period. In this study, it is the annual change percentage transformed from the weighted average of the slope coefficients of the underlying joinpoint regression model from 1990 to 2019.17The AAPC value denotes the percentage annual change (increase, decrease, or no change). If annual percentage change estimates and 95% CIs were both >0 (or both <0), we considered the corresponding rate to be in an upward (or downward) trend. AAPC was calculated using the lower equation infigure 1.
The measures we chose are correlated epidemiologically, including prevalence, DALYs (one DALY represents the loss of one year of full health owing to premature death or disability), and mortality. For example, the reduction in mortality from a chronic disease could lead to an increase in both prevalence and DALYs, in terms of more patients living longer with that disease. An increased number of people with incident disease or a longer disease duration can both contribute to increased prevalence. The increasing prevalence of T1DM among people aged ≥65 years could be explained by longer life with disease because of improved medical care.
All statistical analyses were conducted using GraphPad Prism (version 8.0), Joinpoint Regression program (version 5.0.2), and R (version 4.2.3).
Patient and public involvement
The Global Burden of Disease Study is a collaborative scientific effort allowing the effects of different health conditions to be compared and replicated between age, sex, and geographical locations for specific points in time. The study has generated substantial scientific, policy, and public interest worldwide. Our study used the secondary data from this collaborative work, and we did not have direct access to the participants. No patients were involved in setting the research question or the outcome measures, nor were they involved in the design and implementation of the study.The BMJinvited a patient reviewer as a member of the public to read our manuscript after submission.
Results
Global trends
Globally, the prevalence of T1DM among older people aged ≥65 years increased by 180% between 1990 and 2019, from 1.3 million to 3.7 million. The age standardised prevalence rate of T1DM among this age group increased by 28%, from 400 per 100 000 population in 1990 to 514 per 100 000 population in 2019, with an average annual trend of 0.86% (table 1). Furthermore, the proportion of older people with T1DM has shown a consistent upward trend relative to the overall number of people with T1DM, increasing from 12% in 1990 to 17% in 2019 (see supplementary figure 1). Compared with the overall number of people with T1DM, the increasing trend in prevalence of T1DM among those aged ≥65 years was noticeable from 1990 to 2019 (see supplementary figure 2). The disease burden (all cause DALYs) from T1DM has been increasing over the past 30 years in the population aged ≥65 years (see supplementary figure 3). The age standardised mortality from T1DM among this age group significantly decreased by 25%, from 4.7 per 100 000 population in 1990 to 3.5 per 100 000 population in 2019, with an average annual trend of −1.00% (see supplementary table 1).
Compared with prevalence and mortality, the trend of DALYs from T1DM among people aged ≥65 years was less noticeable during the same period. Age standardised DALYs decreased by 8.9%, from 113 per 100 000 population in 1990 to 103 per 100 000 population in 2019, with an average annual trend of −0.33% (see supplementary table 1).
Global trends by sex
From 1990 to 2019, the age standardised prevalence of T1DM among people aged ≥65 years increased for both men and women worldwide (men: from 0.6 million to 1.7 million; women: from 0.7 million to 2 million). The increase in T1DM prevalence was more rapid among men than among women (AAPC 1.00%v0.74%) (table 1). During the same period, the age standardised mortality from T1DM decreased for both men and women aged ≥65 years, although the reduction was smaller in men (AAPC −0.58%v−1.29%) (see supplementary table 1).
From 1990 to 2019, the reduction in age standardised DALYs from T1DM among older people was less pronounced in men than in women. The decrease in DALYs was more substantial in women aged ≥65 years, with an average annual trend of −0.58%. In 1990, among people aged ≥65 years, women had higher DALYs from T1DM per 100 000 population than men (118v106). In 2019, however, women aged ≥65 years had fewer DALYs from T1DM per 100 000 population than men (100v106) (see supplementary table 1). This sex difference remained consistent regardless of changes in sociodemographic index, with the burden of diseases higher in men compared with women, especially in countries with a low-middle sociodemographic index (see supplementary figures 4 and 5).
Global trends by age subgroup
Globally, during 1990-2019 the prevalence of T1DM at least tripled in every age subgroup of people aged ≥65 years (65-69 years: from 0.46 million to 1.2 million; 70-74 years: from 0.32 million to 0.94 million; 75-79 years: from 0.25 million to 0.67 million; 80-84 years: from 0.15 million to 0.46 million; 85-89 years: from 0.07 million to 0.26 million) and even increased 5-6 times for those >90 years (90-94 years: from 0.02 million to 0.11 million; ≥95 years: from 0.005 million to 0.03 million). Notably, among all age subgroups, the upward trend for men consistently surpassed that for women, especially among men aged 90-94 years (AAPC 1.18%) (see supplementary figure 6). The age standardised prevalence of T1DM across all age subgroups increased at a noticeable rate during 1990-2019, especially among those aged 70-74 years (AAPC 0.94%) (table 1). The age standardised mortality of T1DM decreased across all age subgroups among older people worldwide, especially among those younger than 79 years. In 2019, the mortality rates for T1DM per 100 000 population among people aged ≥65 years increased with age, from 2.8 among those aged 65-69 years to 10 among those aged ≥95 years (see supplementary table 1).
The age standardised DALYs associated with T1DM among older people decreased in all age subgroups at various rates. The most significant decrease was observed among those aged <79 years, including 65-69 years (AAPC −0.44%), 70-74 years (−0.34%), and 75-79 years (−0.42%). In 2019, the DALYs attributable to T1DM were highest in those aged 75-79 years (111 per 100 000 population) and ≥95 years (111 per 100 000 population) (see supplementary table 1).
Global trends by sociodemographic index
The age standardised prevalence of T1DM among people aged ≥65 years increased across all subgroups of sociodemographic index during 1990-2019, especially in countries with a middle sociodemographic index (AAPC 1.73%). Regardless of sociodemographic level, the increase in prevalence among those aged ≥65 years has consistently been higher than in the general population (see supplementary figures 7 and 8). In 2019, among older people the highest prevalence of T1DM was in countries with a high sociodemographic index (913 per 100 000 population) (table 1). While T1DM related mortality among older people decreased across all sociodemographic subgroups during the same period, the largest decrease was attributed to countries with a high sociodemographic index (AAPC −2.17%), which was more than 13 times faster than that in countries with a low-middle sociodemographic index (AAPC −0.16%). In 2019, the highest mortality rate was in countries with a low sociodemographic index (6.4 per 100 000 population), which was more than two times the mortality in countries with a high sociodemographic index (2.2 per 100 000 population) (see supplementary table 1 and supplementary figures 4 and 5).
The age standardised DALYs from T1DM among older people significantly decreased across all sociodemographic subgroups, except in countries with a low-middle sociodemographic index (AAPC 0.01%) (see supplementary table 1). The downward trend in DALYs for T1DM became more pronounced after surpassing a sociodemographic index threshold of 0.7 (see supplementary figure 9). In 2019, the DALYs were highest in countries with a low sociodemographic index (141 per 100 000 population) and lowest in countries with a high-middle sociodemographic index (86 per 100 000 population) (see supplementary table 1).
Regional trends
From 1990 to 2019, none of the 21 regions showed a decrease in prevalence of T1DM among people aged ≥65 years. The most rapid increase in age standardised prevalence for older with T1DM was observed in North Africa and the Middle East (AAPC 2.40%), East Asia (2.17%), and western Europe (1.95%), and the slowest increase in prevalence was in high income North America (0.60%) during the same period (see supplementary figure 10). In 2019, among the 21 regions, the highest age standardised prevalence for T1DM among people aged ≥65 years was in high income North America (1248 per 100 000 population), Australasia (1080 per 100 000 population), and western Europe (1077 per 100 000 population) (see supplementary table 2). No difference was observed after sex stratification (see supplementary table 3).
Most regions experienced a reduction in DALYs from T1DM among older people at various rates during 1990-2019. However, a large increase in DALYs was found in central Asia (AAPC 1.79%). The largest reduction in DALYs from T1DM among older people was in Central Latin America (AAPC −1.30%) (see supplementary figure 10). In 2019, the highest DALYs from T1DM among older people were in southern sub-Saharan Africa (178 per 100 000 population), Oceania (178 per 100 000 population), and the Caribbean (177 per 100 000 population). The lowest DALYs were in East Asia (32 per 100 000 population), the high income Asia Pacific region (51 per 100 000 population), and eastern Europe (77 per 100 000 population) (see supplementary table 4). After stratifying by sex, no significant differences were found (see supplementary figure 11).
National trends
At the national level, from 1990 to 2019, United Arab Emirates had the highest increase in age standardised prevalence of T1DM among people aged ≥65 years, with an average annual trend of 3.61%, followed by Libya (AAPC 3.28%) and Greece (3.16%). Over the same period, Cuba showed the most substantial decrease in age standardised DALYs for older people with T1DM (AAPC −2.40%), followed by the Republic of Korea (−2.29%). The country with the greatest increase in age standardised DALYs for older people with T1DM was Uzbekistan (AAPC 3.60%). In 2019, Finland had the highest age standardised prevalence of T1DM among older people (1693 per 100 000 population), while Oman had the highest age standardised DALYs for T1DM among older people (418 per 100 000 population) (fig 2,fig 3,fig 4, and supplementary table 5).
Risk factors
A detailed analysis of global data from 1990 to 2019 revealed three primary risk factors associated with DALYs for T1DM among people aged ≥65 years, including high fasting plasma glucose levels, low temperature, and high temperature. In 2019, these factors accounted for 103 per 100 000 people, 3 per 100 000 people, and 1 DALY per 100 000 people, respectively. From 1990 to 2019, the corresponding AAPCs for these factors were −0.33%, −2.27%, and 1.92%. In countries with a high sociodemographic index, the most substantial reduced burden from 1990 to 2019 was associated with a high fasting plasma glucose level (AAPC −0.37%) and low temperature (−2.65%). In contrast, countries with a low sociodemographic index contributed the most to the burden attributed to the three risk factors (table 2).
Discussion
Worldwide during 1990-2019, the age standardised prevalence of T1DM increased among older people aged ≥65 years, concomitant with a substantial decrease in associated mortality. The results suggest that T1DM is no longer a contributory factor in decreased life expectancy owing to improvements in medical care over the three decades. DALYs from T1DM among older people decreased at a much slower rate, with inequality identified between countries with different sociodemographic levels. Optimal blood glucose control remains challenging among older people. Our study extended the current understanding of the increasing global burden of T1DM by focusing on trends in older people (≥65 years) with T1DM. Our study also advocates for urgent attention to coping strategies for ageing populations and older people with T1DM, rational allocation of health resources, and the provision of targeted guidelines. The findings are important for health practice and future research, and provide optimistic evidence for all people with T1DM, especially those with a diagnosis at a young age.
Age differences in burden of T1DM among older people
Our study found that in 2019, possibly owing to notable advances in medicine, mortality from T1DM significantly decreased and the life expectancy of affected people improved. These findings may be related to recent achievements in development goals aimed at improving accessibility and coverage of healthcare services, as well as progress in economic growth, reduced poverty, and social protection efforts.18We found that the prevalence of T1DM substantially increased in every age subgroup among those aged ≥65 years. Populations worldwide are ageing, and caring for older people with T1DM involves both healthy ageing and management of T1DM. In recent years, with the increasing prevalence and accessibility of scientific technologies, an increasing number of older people are turning to technology to improve the management of their diabetes.19The widespread adoption of insulin analogues and the improved utilisation of insulin pumps, along with improved education about diabetes and its management, in countries such as France and Sweden, have contributed to the improvement of the disease burden associated with T1DM.202122Although no cure exists for T1DM, the disease is manageable.23
Sociodemographic differences in burden of older people with T1DM
The DALYs from T1DM among people aged ≥65 years decreased slower than mortality and unequally between countries. Firstly, against a background of increasing T1DM prevalence, a small decrease in DALYs is not necessarily harmful because of the huge improvement in survival for this population, and after accounting for more morbidity. Secondly, most of the prevalent cases of T1DM occurred among older people in high income regions and countries and the highest DALYs occurred in the least developed regions and countries. As the sociodemographic index decreased, the burden of T1DM among older people appeared to gradually increase. This aligns with previous studies, which identified limited access to insulin and testing equipment as crucial reasons why people with T1DM in countries with a low sociodemographic index do not receive timely management, which leads to low awareness, poor treatment, low control rates, and silent progression and deterioration among patients.24Although this unequal distribution of T1DM associated DALYs between countries with different sociodemographic index reduced in the past three decades, it was still substantial in 2019.
Sex difference in burden of T1DM among older people
In people aged ≥65 years, we observed a significantly increased overall disease burden from T1DM among men compared with women. T1DM is an autoimmune disease that results from activation of immune cells that destroy insulin producing pancreatic beta cells, and sex hormones are major contributors to this sex difference in the onset and progression of T1DM. Evidence suggests that the prevalence of T1DM is higher in girls during prepuberty, but high in men after puberty, findings that have also been observed in animal models.25This sex inequality, however, is attributed not only to physiological differences between the sexes in terms of anatomy and metabolic processes but also to lifestyle, educational attainment, socioeconomic factors, and cultural factors.26Studies have suggested that women exhibit a more proactive approach to healthcare utilisation and disease management, showing a higher awareness of their condition and greater adherence to treatment. These factors could mitigate the risk of diabetes in women and reduce the incidence of complications.27Additionally, high risk factors such as smoking and lack of physical activity may, to some extent, account for this trend shift. These risk factors are also considered important prognostic factors for T1DM. Population based interventions aimed at reducing the presence of these risk factors are therefore crucial for managing and mitigating the disease burden of T1DM.28Consequently, consideration of further sex specific treatment strategies is necessary to derive enhanced treatment benefits, and treatment plans can be more targeted.
Risk factors in burden of T1DM among older people
A high fasting plasma glucose level was identified as the major contributor to DALYs from T1DM among people aged ≥65 years, indicating that blood glucose control is still suboptimal and a challenge among this population. On the one hand, most people with T1DM are unable to remain within normal glucose range for a large part of the day.29The difficulty of optimal blood glucose control for people lies in the dynamic adjustment of insulin dose shots, which depends on factors such as daily nutrient intake, dietary patterns, insulin sensitivity factors, and daily activity. Difficulties, such as adjustment and calculation of insulin sensitivity factors according to dietary patterns, become even more challenging for older people owing to changes in metabolism and other factors associated with ageing. On the other hand, doctors might prefer to deal with hyperglycaemia rather than hypoglycaemia when caring for older people with diabetes, because complications from hyperglycaemia take longer to manifest than hypoglycaemia, the latter leading to immediate adverse outcomes such as unconsciousness, falls, brain damage, and cardiovascular events. Tighter control measures also increase stress for older patients at home and their caregivers. The objective should be to aim for normoglycaemia without putting people at risk of hypoglycaemia.30We therefore suggest active, but not tight, control of blood glucose, especially in people aged ≥65 years with T1DM. The development of self-management skills, education about nutrition, and training are crucial for people with T1DM and their caregivers, all of which should be highlighted in guidelines for managing T1DM in older people. The latest advances in a new hybrid close loop insulin delivery system have been found to be safe and efficient for glucose instability among young people with T1DM,31but evidence for its use in older people with T1DM is lacking.
Additionally, we observed that extreme temperatures were major contributors to the burden of T1DM among older people. A previous study suggested a positive correlation between high and low temperatures and diabetes incidence and mortality, particularly among older people.32Temperature is one of the main environmental factors, and changes in temperature can influence insulin sensitivity.3334Another explanation might be that temperature affects fasting plasma glucose levels through alterations in dietary and exercise patterns, heightened susceptibility to infections, activation of the sympathetic nervous system, and the triggering of catecholamine secretion, leading to faster onset of diabetes.35Also, not only does high or low temperature result in physiological complications but it also affects people’s daily routines, activities, and dietary habits—all important factors in the wellbeing of people with T1DM. Notably, we found harm to be more noticeable during extreme temperatures in countries with a low sociodemographic index. This finding could be attributed to the management of T1DM, as extreme temperatures can impair insulin storage and reduce insulin efficacy.36The capacity to adapt to variations in temperature is closely associated with the economic status of individual countries.37Despite forecasts indicating that the global economic burden of diabetes in the adult population will escalate to $20tn (£16tn; €18tn) by 2030, the unmet demands for diabetes care in middle and low income countries persist.3839
Strengths and limitations of this study
For older people with T1DM and their families worldwide, the decreasing mortality and DALYs associated with this disease is encouraging. For policy makers, health resource preparedness is needed for the growing number of people with both T1DM and ageing related problems, especially in countries with a middle and low sociodemographic index. In clinical practice, older people with T1DM are encouraged to take active control of their blood glucose levels, with the development of self-management skills and education about nutrition, and this should be reinforced in guidelines. Suitable training is also required for doctors and caregivers. For future research, intervention for glycaemic control that proved effective among adolescents and young adults with T1DM needs to be examined for older people too, with evidence based strategies. More innovative studies are needed in the area of healthy ageing in people with T1DM, and the cost effective delivery of treatment and care.
This study has several limitations. Firstly, the data were extrapolated from countries with existing epidemiological data. For our T1DM model, we used DisMod-MR 2.1 to generate estimates for older age groups based on age pattern. Interpreting the results in the real world requires caution. Secondly, despite using rigorous statistical methods in our study, variations in health information systems and reporting mechanisms across countries and regions, particularly in low and middle income countries and in areas experiencing conflict, could lead to incomplete data and bias, potentially affecting the accuracy of the results. Thirdly, the data on disease burden includes a time lag. Fourthly, we primarily relied on modelling processes for the estimates in this study, and the choice of models and parameter settings could have influenced the results. Finally, the diagnosis of T1DM in older people presents challenges,4041and variations in healthcare systems, healthcare policies, and medical practices across countries may have an affect on disease burden. As a result, further high quality real world research is needed to validate the findings of this study.
Conclusions
Mortality and DALYs among older people (≥65 years) with T1DM decreased considerably from 1990 to 2019. Both were lower in women, those living in countries or regions with a high sociodemographic index, and those younger than 79 years. Management of high fasting plasma glucose levels remains a major challenge for older people with T1DM, and targeted clinical guidelines are needed.
Type 1 diabetes mellitus (T1DM) has been considered a disease to have a deleterious effect on life expectancy
Diabetes care has improved substantially since the 1990s, with an increasing number of older people with T1DM reported in studies
In most countries and regions worldwide, comprehensive and current data on the burden of T1DM are lacking
Using the Global Burden of Disease Study model, this study found that the age standardised prevalence of T1DM among people aged ≥65 years worldwide increased during 1990-2019 concomitant with a substantial decrease in associated mortality
A high fasting plasma glucose level was the major contributor to disability adjusted life years, indicating that hyperglycaemia management remained challenging for the older people with T1DM
Mortality fell 13 times faster in countries with a high sociodemographic index versus countries with a low-middle sociodemographic index (−2.17% per yearv−0.16% per year)
","Objectives: To estimate the burden, trends, and inequalities of type 1 diabetes mellitus (T1DM) among older adults at global, regional, and national level from 1990 to 2019.
Design: Population based study.
Population: Adults aged ≥65 years from 21 regions and 204 countries and territories (Global Burden of Disease and Risk Factors Study 2019)from 1990 to 2019.
Main outcome measures: Primary outcomes were T1DM related age standardised prevalence, mortality, disability adjusted life years (DALYs), and average annual percentage change.
Results: The global age standardised prevalence of T1DM among adults aged ≥65 years increased from 400 (95% uncertainty interval (UI) 332 to 476) per 100 000 population in 1990 to 514 (417 to 624) per 100 000 population in 2019, with an average annual trend of 0.86% (95% confidence interval (CI) 0.79% to 0.93%); while mortality decreased from 4.74 (95% UI 3.44 to 5.9) per 100 000 population to 3.54 (2.91 to 4.59) per 100 000 population, with an average annual trend of −1.00% (95% CI −1.09% to −0.91%), and age standardised DALYs decreased from 113 (95% UI 89 to 137) per 100 000 population to 103 (85 to 127) per 100 000 population, with an average annual trend of −0.33% (95% CI −0.41% to −0.25%). The most significant decrease in DALYs was observed among those aged <79 years: 65-69 (−0.44% per year (95% CI −0.53% to −0.34%)), 70-74 (−0.34% per year (−0.41% to −0.27%)), and 75-79 years (−0.42% per year (−0.58% to −0.26%)). Mortality fell 13 times faster in countries with a high sociodemographic index versus countries with a low-middle sociodemographic index (−2.17% per year (95% CI −2.31% to −2.02%)v−0.16% per year (−0.45% to 0.12%)). While the highest prevalence remained in high income North America, Australasia, and western Europe, the highest DALY rates were found in southern sub-Saharan Africa, Oceania, and the Caribbean. A high fasting plasma glucose level remained the highest risk factor for DALYs among older adults during 1990-2019.
Conclusions: The life expectancy of older people with T1DM has increased since the 1990s along with a considerable decrease in associated mortality and DALYs. T1DM related mortality and DALYs were lower in women aged ≥65 years, those living in regions with a high sociodemographic index, and those aged <79 years. Management of high fasting plasma glucose remains a major challenge for older people with T1DM, and targeted clinical guidelines are needed.
"
Antiplatelet therapy after coronary artery bypass surgery,"Introduction
Aspirin monotherapy is recommended after coronary artery bypass graft surgery to improve graft patency and reduce major adverse cardiovascular events.1234Findings from randomised and observational studies suggest that dual antiplatelet therapy is more effective than aspirin monotherapy in preventing saphenous vein graft failure.56789However, the evidence is inconclusive with regard to the effect of dual antiplatelet therapy on clinical outcomes,47101112131415161718and current clinical guidelines recommend dual antiplatelet therapy only in selected patients at high ischaemic risk after coronary artery bypass grafting.319202122
The Different Antiplatelet Therapy Strategy After Coronary Artery Bypass Graft Surgery (DACAB)23trial was a multicentre, randomised, open label trial comparing ticagrelor dual antiplatelet therapy, ticagrelor monotherapy, and aspirin monotherapy for one year after coronary artery bypass grafting. Vein graft patency (primary outcome) at one year was significantly higher in patients randomised to dual antiplatelet therapy compared with patients randomised to aspirin monotherapy (88.7%v76.5%; P<0.001) and numerically higher compared with patients randomised to ticagrelor monotherapy (88.7%v82.8%; P=0.068).
The DACAB trial was not powered for clinical outcomes. At one year after coronary artery bypass grafting, the rate of major adverse cardiovascular events (cardiovascular death, non-fatal myocardial infarction, or non-fatal stroke) was 1.8% in the dual antiplatelet therapy group, 2.4% in the ticagrelor monotherapy group, and 5.4% in the aspirin monotherapy group. The rate of major bleeding was 1.8% in the dual antiplatelet therapy group and 1.2% in the ticagrelor monotherapy group; no major bleeding event was reported in the aspirin monotherapy group.
We have now extended the clinical follow-up of patients included in the DACAB trial to five years (DACAB-Follow-up Extension (DACAB-FE) study), with the aim of investigating the effect of different antiplatelet strategies on clinical outcomes at five years after coronary artery bypass grafting.
Methods
Study design
The DACAB trial was a prospective, multicentre, open label, evaluator blind, randomised, controlled trial that compared the effect of dual antiplatelet therapy with ticagrelor plus aspirin, ticagrelor monotherapy, and aspirin monotherapy on saphenous vein graft patency in patients aged 18 to 80 years who had elective coronary artery bypass graft surgery in six Chinese tertiary hospitals.23The study design and full inclusion/exclusion criteria have been described previously.23Briefly, eligible patients were enrolled from 31 July 2014 through 3 November 2015 and randomised 1:1:1 to ticagrelor 90 mg twice daily plus aspirin 100 mg once daily (dual antiplatelet therapy), ticagrelor monotherapy 90 mg twice daily, or aspirin monotherapy 100 mg once daily for one year after coronary artery bypass graft surgery.23After the first year, antiplatelet therapy was based on the recommendation of the individual treating physician (including aspirin monotherapy, adenosine diphosphate receptor inhibitor monotherapy (ticagrelor or clopidogrel), and dual antiplatelet therapy).
All patients in the DACAB trial were included in the DACAB-FE study. This study was conducted in accordance with the Declaration of Helsinki and Guidelines for Good Clinical Practice.
Data collection
Annual clinical follow-up at local sites was recommended according to clinical status. A face-to-face or telephone visit with the central clinical trial unit was scheduled for all surviving patients at five years (within three months either side) after coronary artery bypass grafting, at which the original medical records of clinical events, related laboratory test results, electrocardiograms, and ultrasonic cardiography results and the use of concomitant medications were collected. Electrocardiograms were read centrally by the Clinical Endpoint Committee, which independently adjudicated the outcome. For deceased patients, the cause of death was adjudicated using the national China Centre for Disease Control database.
Outcomes and endpoints
The primary outcome was major adverse cardiovascular events (a composite of all cause death, myocardial infarction, stroke, and coronary revascularisation). Secondary outcomes included an extended major adverse cardiovascular events outcome (a composite of all cause death, myocardial infarction, stroke, coronary revascularisation, and hospital admission for unstable angina) and a restricted major adverse cardiovascular events outcome (a composite of cardiovascular death, myocardial infarction, and stroke), as well as their individual components. All outcomes were measured at the time to first occurrence from randomisation to last visit. Detailed outcome definitions are provided in the supplementary appendix.
The safety outcome, analysed post hoc, was major bleedings events based on the Thrombolysis in Myocardial Infarction (TIMI) risk criteria (combination of coronary artery bypass grafting related bleeding and non-coronary artery bypass grafting related major bleeding, such as intracranial bleeding, clinically overt signs of haemorrhage with haemoglobin drop ≥5 g/dL, and fatal bleeding). The secondary safety outcome was serious adverse events resulting in hospital admission or emergency department visits. We added net adverse clinical events, a composite of major adverse cardiovascular events and major bleeding events, as a post hoc outcome.
Statistical analysis
The sample size for the DACAB trial was based on a calculated statistical power of 80% to detect a significant difference in the primary outcome of vein graft patency.23For the DACAB-FE study, we did no formal power calculation. In the DACAB-FE primary analysis, patients were analysed according to randomised treatment groups in the DACAB trial (intention to treat).
We summarised baseline characteristics by using counts and percentages for discrete variables and mean with standard deviation and median with interquartile range for continuous variables. We reported outcomes as frequencies, incidence per 1000 patient months, and cumulative incidence.
We used Kaplan-Meier curves to describe the event-free survival in the groups and univariate Cox regression models to compare the risk of outcomes between treatment groups. We tested the Cox proportionality assumption by means of scaled Schoenfeld residuals. We reported estimates as hazard ratios and 95% confidence intervals. We used competing risk analysis with a Fine-Gray framework for all outcomes except major adverse cardiovascular events, extended major adverse cardiovascular events, and all cause death.24Patients with no events were right censored at their last visit. For patients who were lost to follow-up, we used the date of the last visit or the most recent known clinical outcome in the primary analysis.
We used a univariate Cox regression model to analyse treatment effects in key patient subgroups (age group; sex; coronary syndrome status; history of myocardial infarction, hypertension, diabetes, hyperlipidaemia, and peripheral arterial disease; pump use; and bleeding risk based on the CRUSADE score25). Additional subgroup analyses were based on history of stroke, chronic pulmonary disease, chronic kidney disease, smoking, SYNTAX score, EuroSCORE, Lp(a) lipoprotein concentrations, left main disease, internal mammary artery use, and completeness of revascularisation. We tested treatment effect modification by using interaction terms.
Sensitivity analyses
We did multiple post hoc sensitivity analyses. To evaluate the effect of one year of dual antiplatelet therapy on late events, we did landmark analysis starting at one year after randomisation by using univariate Cox proportional hazard models. We did this analysis in participants who remained in the trial and had not had a primary endpoint event during the first year. We repeated the main analysis including only patients who received the planned dose of study drug without interruption for more than 60 days during the first year (per protocol).
In addition, we did two as-treated analyses. In the first analysis, we compared patients on the basis of the antiplatelet treatment that they received in year 1 and in years 2-5 by using an adjusted multivariable Cox regression model. We identified the covariates included in the model by using a causal directed acylic graph (supplementary figure S1). Considering the limitations of conditional models and the likelihood of reverse causation bias, we fitted a marginal structural model through stable inverse probability weighting in the second analysis.26We divided the entire follow-up period into seven time segments (0-2 months, 2-4 months, 4-6 months, 6-8 months, 8-10 months, 10-12 months, and 12-60 months) and regarded antiplatelet treatment at different time segments as time dependent exposure. The covariates included in the effect estimation were the same as in the first as-treated analysis (see as-treated analysis methods section of supplementary appendix for more details).
In another post hoc analysis, we compared major adverse cardiovascular events, all cause death, cardiovascular death, and myocardial infarction between patients with all patent grafts and patients with at least one failed graft at one year imaging. To investigate a potential dependency of the treatment effect by the enrolling centres, we used a multivariable Cox regression model with an interaction term between treatment and study centre.
We applied a two sided significance level of 0.05 to all statistical analyses without multiplicity adjustment. We used SAS version 9.4 for statistical analyses.
Patient and public involvement
Although patients and the public were not directly involved in this paper owing to the lack of funding and covid-19, we spoke to patients about the study and asked one representative of the patients to read the manuscript and give us advice before submission.
Results
Patient characteristics
The primary analysis included 500 patients whose demographics and baseline characteristics have been previously published.23Among these patients, 168 received dual antiplatelet therapy during the first year after coronary artery bypass grafting, 166 received ticagrelor monotherapy, and 166 received aspirin monotherapy. Most of the patients enrolled were men (81.8%), and the mean age was 63.1 years.Table 1summarises the baseline characteristics of the included patients.
Follow-up
The first five year visit occurred in August 2019 and the last in June 2021. Completeness of follow-up for the primary outcome of major adverse cardiovascular events at five years was 95.4% (477/500 patients;fig 1). Median follow-up time was 61.1 months (supplementary table S1). For the five year assessment, 197 (39.4%) patients were followed up in person and 280 (56.0%) by telephone interview. Electrocardiographic or echocardiographic assessment was obtained in 428 (85.6%) patients at one year and in 402 (80.4%) patients at five years.
Antiplatelet and other concomitant medications
Within the first year from randomisation, 17 (3.4%) patients discontinued the allocated study drug for longer than 60 days, of whom five (3%) had been randomised to dual antiplatelet therapy, eight (5%) to ticagrelor monotherapy, and four (2%) to aspirin monotherapy. At the five year follow-up, 324 (64.8%) of 500 patients were receiving aspirin monotherapy: 103 (61%) in the dual antiplatelet therapy group, 102 (61%) in the ticagrelor monotherapy group, and 119 (72%) in the aspirin monotherapy group. Full details of concomitant medications at one year and at five years are shown in supplementary figure S2 and supplementary tables S2 and S3.
Primary outcome
Major adverse cardiovascular events occurred in 148 patients: 39 in the dual antiplatelet therapy group, 54 in the ticagrelor monotherapy group, and 55 in the aspirin monotherapy group. At the five year follow-up, the rate of major adverse cardiovascular events was 22.6% in the dual antiplatelet therapy group, 32.9% in the ticagrelor monotherapy group, and 29.9% in the aspirin monotherapy group (table 2). Patients in the dual antiplatelet therapy group had a significantly lower risk of major adverse cardiovascular events compared with patients in the aspirin monotherapy group (hazard ratio 0.65, 95% confidence interval 0.43 to 0.99; P=0.04) and in the ticagrelor monotherapy group (0.66, 0.44 to 1.00; P=0.05); we found no significant difference between patients in the ticagrelor monotherapy group and those in the aspirin monotherapy group (0.99, 0.68 to 1.44; P=0.97) (table 3andfig 2). The global Schoenfeld residuals test verified the Cox proportionality assumption (P=0.63).
Sensitivity analyses
Results were consistent in all the sensitivity analyses. In the landmark analysis restricted to patients who were event-free at one year, patients in the dual antiplatelet therapy group had a numerically lower risk of late (2-5 years) major adverse cardiovascular events compared with patients in the aspirin monotherapy group (hazard ratio 0.76, 0.43 to 1.34; P=0.34) and patients in the ticagrelor monotherapy group (0.74, 0.42 to 1.30; P=0.31) (supplementary figure S3). In the per protocol analyses, patients in the dual antiplatelet therapy group had a significantly lower risk of major adverse cardiovascular events compared with patients in the aspirin monotherapy group (hazard ratio 0.63, 0.42 to 0.96; P=0.03) and patients in the ticagrelor monotherapy group (0.62, 0.41 to 0.94; P=0.03).
The baseline characteristics and the incidence of primary and secondary outcomes in the first as-treated analysis are summarised in supplementary tables S4 and S5. Patients who received dual antiplatelet therapy in the first year and aspirin monotherapy in years 2-5 had a significantly lower risk of major adverse cardiovascular events compared with those who received aspirin monotherapy during the first year and in years 2-5 (hazard ratio 0.54, 0.33 to 0.88; P=0.01) and those who received adenosine diphosphate receptor inhibitor monotherapy in the first year and aspirin monotherapy in years 2-5 (0.56, 0.34 to 0.93; P=0.03) (supplementary table S6). In the marginal structural model with stable inverse probability weighting (mean weight was 1.006, minimum weight was 0.2489, and maximum weight was 1.3255), use of dual antiplatelet therapy in the first year and aspirin monotherapy in years 2-5 was associated with a significantly lower risk of major adverse cardiovascular events compared with the use of aspirin monotherapy for the whole study period (hazard ratio 0.45, 0.28 to 0.72; P<0.001) and the use of adenosine diphosphate receptor inhibitor monotherapy for the first year followed by aspirin monotherapy in years 2-5 (0.65, 0.44 to 0.95; P=0.04).
The rate of major adverse cardiovascular events and myocardial infarction was higher in patients with at least one failed graft compared with patients with all grafts patent at one year (44.1%v24.7% (P<0.001) and 31.5%v14.0% (P<0.001), respectively; supplementary table S7). We found no significant differences in the rates of major adverse cardiovascular events between participating centres (supplementary table S8).
Secondary outcomes
The extended major adverse cardiovascular events outcome occurred in 39 patients in the dual antiplatelet therapy group, 55 patients in the ticagrelor monotherapy group, and 56 patients in the aspirin monotherapy group; the rates at five years were 22.6%, 32.9%, and 30.5%, respectively (table 2). The risk of extended major adverse cardiovascular events was lower in patients in the dual antiplatelet therapy group compared with patients in the aspirin monotherapy group (hazard ratio 0.64, 0.43 to 0.96; P=0.03) and in the ticagrelor monotherapy group (0.65, 0.43 to 0.98; P=0.04) (table 3andfig 3).
The restricted major adverse cardiovascular events outcome occurred in 27 patients in the dual antiplatelet therapy group, 44 patients in the ticagrelor monotherapy group, and 45 patients in the aspirin monotherapy group; the rates at five years were 15.8%, 26.2%, and 26.6%, respectively (table 2). The risk of restricted major adverse cardiovascular events was lower in patients in the dual antiplatelet therapy group compared with patients in the aspirin monotherapy group (hazard ratio 0.56, 0.35 to 0.90; P=0.02) and in the ticagrelor monotherapy group (0.57, 0.35 to 0.91; P=0.02) (table 3andfig 4).
The rates of all cause mortality and cardiovascular mortality at five year were numerically higher in the patients in dual antiplatelet therapy group compared with patients in the aspirin monotherapy group (9.3%v6.2% and 5.1%v4.0%, respectively) and in the ticagrelor monotherapy group (9.3%v6.3% and 5.1%v3.4%, respectively) (table 2). The individual clinical outcomes are summarised intable 2and supplementary tables S9 to S13.
Safety outcomes
Nineteen patients experienced major bleeding events, including eight in the dual antiplatelet therapy group, four in the ticagrelor monotherapy group, and seven in the aspirin monotherapy group; the rates at five years were 4.9%, 2.5%, and 4.3%, respectively (table 2). The risk of major bleeding was not significantly different between patients in the dual antiplatelet therapy group and patients in the aspirin monotherapy group (hazard ratio 1.14, 0.42 to 3.14; P=0.80) and in the ticagrelor monotherapy group (1.99, 0.60 to 6.61; P=0.26) (table 3). The other adverse events were similar between groups (supplementary table S14).
Net adverse clinical outcomes (post hoc analysis)
The risk of net adverse clinical events was significantly lower in patients in the dual antiplatelet therapy group compared with patients in the aspirin monotherapy group (24.4%v31.7% at five years; hazard ratio 0.67, 0.45 to 1.00; P=0.05) (table 2andtable 3). We found no significant difference in net adverse clinical events between patients in the dual antiplatelet therapy and ticagrelor monotherapy groups (24.4%v33.5% at five years; hazard ratio 0.70, 0.47 to 1.05; P=0.09) (tables 2andtable 3).
Subgroup analysis
Subgroup treatment effect interactions are reported intable 4and supplementary table S15.
Discussion
In this extended follow-up study of the DACAB trial, the risk of major adverse cardiovascular events up to five years after coronary artery bypass grafting was lower with dual antiplatelet therapy with ticagrelor plus aspirin for one year compared with aspirin monotherapy or ticagrelor monotherapy. The beneficial effect of ticagrelor dual antiplatelet therapy seemed to continue after the first year and was consistent across key clinical subgroups, including different baseline bleeding risks.
Strengths and limitations of study
This study provides evidence of clinical benefit with the use of ticagrelor dual antiplatelet therapy for one year after coronary artery bypass grafting. Given the substantial rate of cardiovascular events after coronary artery bypass grafting despite standard aspirin therapy, evidence for the clinical effect of ticagrelor dual antiplatelet therapy is of critical importance to inform clinical practice. Strengths of this study include the evaluation of three different antiplatelet strategies, the high adherence to the randomised treatment during the first year, the high rate of complete five year follow-up, and the combined evaluation of clinical events and imaging.
The main limitation is that the randomised allocation to antiplatelet therapy was for the first year only. However, coronary artery bypass graft thrombosis typically occurs within the first year after surgery,2728and the prevention of graft occlusion in the first postoperative year could conceivably have long term effects on reducing the risk of major adverse cardiovascular events29; in addition, the long term benefit of one year of ticagrelor dual antiplatelet therapy was confirmed in all the sensitivity analyses.
In clinical practice, patients with coronary artery bypass grafts are generally followed by a specialist early after surgery and then referred to their general physician, and changes in medical therapy are frequent during this transition and also driven by the patient’s health status. The change in antiplatelet treatment could be seen as an intercurrent event, and we presented the intention-to-treat analysis and two as-treated analyses to describe the effect of the interventions both from a treatment policy estimand perspective and from a hypothetical strategy perspective. However, the intention-to-treat analysis is of principal interest for inference as it describes the practical consequences of the specific antiplatelet treatment over the first year (and collider stratification, confounding by indication, and unmeasured bias may be present in the as-treated analyses).
Other limitations include the fact that owing to restrictions associated with the covid-19 pandemic, more than half of the patients completed the five year follow-up remotely, rather than attending an in-person visit, and this may have affected in particular the ascertainment of silent myocardial infarctions. Moreover, given the small number of events, the analysis of the individual outcomes may be underpowered. Finally, the use of the internal mammary artery for coronary artery bypass grafting was lower compared with that in Western registries, which may affect the generalisability of results, but this is consistent with contemporary Chinese cardiac surgical practice.30
Comparison with other studies
Previous studies on the clinical effect of dual antiplatelet therapy, aspirin monotherapy, and ticagrelor monotherapy after coronary artery bypass graft surgery reported mixed results.71213143132In the CASCADE study, at a median follow-up of 7.6 years, treatment with clopidogrel plus aspirin for one year was associated with a numerical reduction in major adverse cardiovascular events compared with aspirin monotherapy.13In the prematurely terminated TICAB trial, no significant differences in the incidence of major adverse cardiovascular events one year after coronary artery bypass grafting were observed in patients who were treated with ticagrelor monotherapy and those treated with aspirin monotherapy, although the study may have been underpowered to detect even moderate treatment effects.32In a subgroup analysis of the PLATO trial, among patients with acute coronary syndromes who had coronary artery bypass grafting, ticagrelor plus aspirin for one year was superior to clopidogrel plus aspirin in reducing the risk of major adverse cardiovascular events at 12 months after surgery.31However, this was an observational analysis in a subgroup of patients enrolled in a prospective randomised trial.31
Clinical implications
In the one year imaging analysis of the DACAB trial, ticagrelor dual antiplatelet therapy was associated with a significantly higher saphenous vein graft patency rate compared with aspirin monotherapy, but too few events occurred to allow meaningful intergroup comparisons of clinical outcomes, although the absolute rate of major adverse cardiovascular events was lower in the ticagrelor dual antiplatelet therapy group than in the aspirin monotherapy group.23The attrition rate of coronary bypass grafts is known to be higher in the first year after surgery (mostly owing to early thrombosis and reactive intimal hyperplasia) and relatively lower during postoperative years 2-5.2833Our analysis suggests that ticagrelor dual antiplatelet therapy during the first postoperative year may protect coronary bypass grafts from early occlusion, leading to sustained postoperative clinical benefit. We hypothesise that this benefit is due to the protective effect of patent surgical grafts against coronary artery disease progression and acute coronary events.34
We observed an absolute increase in mortality in patients who received dual antiplatelet therapy. This increased incidence of mortality was based on a low number of events and was not accompanied by an increase in bleeding events; further investigation is needed. An increase in the risk of major bleeding events was noted in a recent meta-analysis comparing ticagrelor dual antiplatelet therapy with aspirin monotherapy in coronary artery bypass grafting patients,9but this was not seen in our study, in which a significant net clinical benefit was seen with the use of ticagrelor dual antiplatelet therapy for one year after surgery; however, our sample size may have been underpowered for safety outcomes. Treatment decisions must be based on clinical and surgical characteristics of the individual patient.
Future directions
Future studies should investigate the effect of short term ticagrelor dual antiplatelet therapy after coronary artery bypass grafting, as well as its optimal duration. Furthermore, research comparing the efficacy and safety of ticagrelor dual antiplatelet therapy with aspirin monotherapy or ticagrelor monotherapy in specific patient populations, such as people with diabetes or older people, or in patients taking concomitant drugs, would provide additional guidance to clinicians treating patients after coronary artery bypass graft surgery.
Conclusions
The DACAB-FE study provides evidence of a significant five year clinical benefit of one year of ticagrelor dual antiplatelet therapy after coronary artery bypass grafting compared with aspirin monotherapy or ticagrelor monotherapy.
","Objective: To assess the effect of different antiplatelet strategies on clinical outcomes after coronary artery bypass grafting.
Design: Five year follow-up of randomised Different Antiplatelet Therapy Strategy After Coronary Artery Bypass Grafting (DACAB) trial.
Setting: Six tertiary hospitals in China; enrolment between July 2014 and November 2015; completion of five year follow-up from August 2019 to June 2021.
Participants: 500 patients aged 18-80 years (including 91 (18.2%) women) who had elective coronary artery bypass grafting surgery and completed the DACAB trial.
Interventions: Patients were randomised 1:1:1 to ticagrelor 90 mg twice daily plus aspirin 100 mg once daily (dual antiplatelet therapy; n=168), ticagrelor monotherapy 90 mg twice daily (n=166), or aspirin monotherapy 100 mg once daily (n=166) for one year after surgery. After the first year, antiplatelet therapy was prescribed according to standard of care by treating physicians.
Main outcome measures: The primary outcome was major adverse cardiovascular events (a composite of all cause death, myocardial infarction, stroke, and coronary revascularisation), analysed using the intention-to-treat principle. Time-to-event analysis was used to compare the risk between treatment groups. Multiple post hoc sensitivity analyses examined the robustness of the findings.
Results: Follow-up at five years for major adverse cardiovascular events was completed for 477 (95.4%) of 500 patients; 148 patients had major adverse cardiovascular events, including 39 in the dual antiplatelet therapy group, 54 in the ticagrelor monotherapy group, and 55 in the aspirin monotherapy group. Risk of major adverse cardiovascular events at five years was significantly lower with dual antiplatelet therapy versus aspirin monotherapy (22.6%v29.9%; hazard ratio 0.65, 95% confidence interval 0.43 to 0.99; P=0.04) and versus ticagrelor monotherapy (22.6%v32.9%; 0.66, 0.44 to 1.00; P=0.05). Results:  were consistent in all sensitivity analyses.
Conclusions: Treatment with ticagrelor dual antiplatelet therapy for one year after surgery reduced the risk of major adverse cardiovascular events at five years after coronary artery bypass grafting compared with aspirin monotherapy or ticagrelor monotherapy.
Trial registration: NCT03987373ClinicalTrials.govNCT03987373.
"
Mailed feedback to primary care physicians on antibiotic prescribing,"Introduction
Antimicrobial resistance is a rising global public health crisis with an estimated 1.27 million attributable deaths per year worldwide.1Overuse and misuse of antibiotics are important modifiable drivers of rising drug resistant infections. Most antibiotics are prescribed by primary care physicians.
Peer comparison audit and feedback on antibiotic prescribing is a potentially scalable and effective intervention. Effective audit and feedback incorporates behavioural science principles to drive behaviour change and improve the quality of patient care. Previously, a three arm trial of an antibiotic feedback letter mailed to the highest antibiotic prescribing physicians in Ontario, Canada, led to a 4% to 5% reduction in antibiotic prescriptions.2The effect size of other trials evaluating antibiotic audit and feedback to reduce antibiotic prescribing resulted in variable effect sizes.34567Various unanswered questions remain regarding the optimal design of audit and feedback, with relatively few trials comparing different audit and feedback designs in an attempt to improve the effect of these interventions.89Results from a trial in Australia showed that feedback including peer comparison and a graph performed best and reduced antibiotic prescription rates by 12%.10A trial from Scotland investigated the effect of behavioural change messaging and showed no effect of sending three compared with two reports over the year.11
The drivers of inappropriate antibiotic prescribing are complex and multifactorial. A knowledge gap is generally not the primary driver of over-prescribing, whereas, physician habit, perceived patient expectations, and fear of consequences, predominate as themes in the medical literature. Physicians tend to overestimate the potential benefits of antibiotics and underestimate the potential harms. Another common theme in qualitative studies of audit and feedback is that physicians articulate that the data do not reflect their unique practice and patient characteristics.12131415
Our primary objective was to evaluate whether providing family physicians with feedback on their antibiotic prescribing, using routinely collected data, compared with their peers, reduces antibiotic use. Our secondary objectives were to test whether further incremental reductions could be made in antibiotic prescribing by providing two more forms of feedback. Firstly, we assessed case-mix adjustment in feedback reports, to address physicians’ perception that the data do not reflect their unique patient populations. Secondly, we assessed emphasising harms associated with antibiotics, to address physicians’ tendency to underestimate antibiotic harms. We hypothesised that there would be no interaction between these modifications and therefore chose a factorial design to efficiently evaluate our secondary objectives.
Methods
Design
We conducted a pragmatic randomised controlled trial of mailed antibiotic audit and feedback reports to primary care physicians with an embedded 2x2 factorial experiment in Ontario, Canada.16The trial was considered pragmatic because the design was created to directly inform policy and decision making.17We evaluated the effectiveness of our audit and feedback intervention under usual conditions across the province. Eligible physicians and outcomes were identified through existing, routinely collected administrative data. The study protocol has been previously published and registered (NCT04594200).16
Participants and setting
Ontario is Canada’s most populous province (population 14.5 million in 2022). Residents can see a physician without incurring out of pocket costs for the visit. However, medications are only publicly funded for some patients, including all patients 65 years and older. Our data only includes publicly funded medications; therefore, this study was limited to community dwelling patients aged 65 years and older. We have previously shown a strong correlation between physician prescribing for patients of this age and their overall antibiotic prescribing (Spearman’s r for men were 0.80 and women were 0.84).16
Ontario Health is the government agency that oversees administration of the provincial healthcare system. As part of their quality improvement activities, primary care physicians can voluntarily sign up for MyPractice reports, which, since 2021, have included antibiotic prescribing indicators. At the time of this trial, approximately 4000 primary care physicians received these reports. To avoid duplication, physicians who had previously signed up for MyPractice reports were excluded from this trial. We also excluded physicians who had opted out of a previous feedback trial in Ontario,2physicians with fewer than 100 unique patient visits to patients 65 years or older in the most recent year, or in two of the three prior years, and physicians with fewer than 10 antibiotic prescriptions to patients 65 years or older in the most recent year, or two of the three prior years. Data from these physicians would not have been sufficient for meaningful feedback. Finally, one month before the trial, an introductory letter was sent to all eligible physicians with the opportunity to opt out.
Interventions
We developed the intervention audit and feedback reports using an iterative process. Team members with expertise in audit and feedback, design science, behavioural science, trial design, patient care, and antibiotic prescribing met to develop prototype reports. Prototype reports were then informally reviewed by physician colleagues who belonged to the target audience. Physicians' comments and reactions were brought back to the group and informed iterative revisions of the reports.
Physicians in the control group were not notified that they were in a study, aside from the opt-out letter, and did not receive any audit and feedback on antibiotics. All eligible physicians in the intervention group were randomly assigned to receive case-mix adjusted feedback, harms messaging, neither, or both. Physicians in the intervention group received a mailed letter to their primary office location in January 2022. The same letter was sent out again in February 2022 in an attempt to increase engagement with the intervention. The letters included three years of data (from 1 March 2018 to 28 February 2021) on total antibiotic prescribing to their patients aged 65 years or older. The letter also provided the median antibiotic prescribing rate and the lowest quartile met by other primary care physicians in Ontario, which was described in the report as an achievable target. Our previous work identified that the average primary care physician could safely reduce their antibiotic prescribing by at least 24%.18Antibiotic audit and feedback interventions are discouraged from using the mean as the sole comparator to avoid regression to the mean.19We also provided physicians with their data for the proportion of their antibiotic prescriptions that were more than seven days and a table of recommended antibiotic durations for common infections. These elements were based on the success of a previous trial.2The report included a graph, education on appropriate antibiotic prescribing and durations, evidence informed communication strategies as well as tools from Choosing Wisely Canada to help to improve antibiotic prescribing. The letter was co-signed by Ontario’s Chief Medical Officer of health, the president of the Ontario College of Family Physicians, and the chair of Choosing Wisely Canada (supplementary appendix 1).
We embedded a 2x2 factorial trial in the intervention arm to efficiently evaluate the independent effects of each factor in the absence of any hypothesised interaction. For physicians randomly assigned to the case-mix adjusted letter, we standardised their antibiotic prescribing rate using hierarchical regression modelling, which incorporated their number of patient visits per year, as well as patient age, sex, socioeconomic status, comorbidities, and practice setting. On the letter’s first page, it was emphasised to physicians that their data were adjusted to represent a fair comparison to physicians with similar patients and practice characteristics. Physicians not in the adjusted group received feedback on their raw antibiotic prescribing rate compared with that of their peers. We anticipated that this modification would address physician’s lack of acceptance of audit and feedback that did not adequately capture their patient and practice complexity.15For physicians randomly assigned to receive harms messaging, we included an infographic highlighting the frequency of side effects and harms associated with antibiotics. This infographic highlighted the 30% risk of side effects from antibiotic use, the doubling of bacterial resistance rates, and predicted rising mortality from drug resistant infections in the future (supplementary appendix 1). The physicians who were randomly assigned to the non-harms group only received an infographic on the lack of benefits from unnecessary antibiotic prescribing. We anticipated that this modification would address the perceived imbalance of risks from unnecessary antibiotic prescribing.13The various versions of the intervention are available in supplementary appendix 1.16
Allocation
An epidemiologist who was not otherwise involved in the study generated the allocation sequence. Randomisation was a simple random sampling method without replacement, with no block size specification or clustering. Randomisation was done simultaneously for all eligible physicians, stratified by participation in a previous trial from Ontario in 2018, which was limited to high antibiotic prescribers only.2Physicians were randomly assigned 4:1 (intervention:control), and 1:1:1:1 within the intervention arm to each of the two factors described above (adjusted or unadjusted and harms or no harms), giving four experimental conditions.
Outcomes and data sources
The primary outcome was antibiotic prescribing rate defined as the total number of systemic oral antibiotic prescriptions per 1000 patient visits in patients aged 65 years or older from the time of mailing the intervention to six months after intervention. The antibiotic prescribing rate was selected as the primary outcome because accurate measurement was possible, the rate is known to drive antimicrobial resistance, and over prescribing of antibiotics is present in this population; therefore, an opportunity exists to safely reduce overall antibiotic use.1820Secondary outcomes included the number of likely unnecessary antibiotic prescriptions per 1000 patient visits (table S1), the number of antibiotic prescriptions of more than seven days per 1000 patient visits, and the number of broad-spectrum antibiotic prescriptions per 1000 patients visits (table S2). Unnecessary antibiotic prescriptions were defined as having a physician International Classification of Diseases 9th edition billing claim in the Ontario Health Insurance Plan database for one or more codes for a condition that rarely or never requires antibiotics (eg, asthma, common cold, and bronchitis). This list was derived from previous research (table S1).18We initially planned to include total antibiotic days of treatment as an outcome, however, since this metric is a combination of antibiotic prescribing rate and duration we omitted it because we felt no new information would be gained. In a secondary analysis, outcomes were measured from letter mailing to 12 months of follow-up. The data for all outcomes were derived from routinely collected administrative data at ICES (formerly, the Institute for Clinical Evaluative Sciences), an independent, non-profit research institute whose legal status under Ontario’s health information privacy law allows it to collect and analyse health care and demographic data without consent for health system evaluation and improvement. Antibiotic prescription data were from the Ontario Drug Benefit database which is more than 99% accurate.21Additional ICES databases used include the Ontario Health Insurance Plan database to identify physician visits, the Registered Person Database to identify patient demographics, the Canadian Institutes for Health Information Discharge Abstract Database to identify patient admission to hospital and comorbidities, and the ICES Physician Database to identify prescriber characteristics. These datasets were linked using unique encoded identifiers and analysed at ICES. We evaluated the intervention fidelity through phone calls to a random sample of 3% (ie, 135) of physicians in the intervention group (up to two phone call attempts per physician) after February 2023 when data collection ended.
Sample size
We anticipated that we would have data for approximately 6000 eligible physicians and an average of 784 patient visits per physician over six months. Assuming in the control arm an antibiotic prescribing rate of 40 per 1000 patient visits and a between-cluster coefficient of variation of 75%, we would meet at least 80% power to detect a 7.5% relative reduction in the antibiotic prescribing rate comparing the intervention to usual care in a 4:1 allocation, using a two sided test for the difference between two Poisson rates.
Statistical analysis
We conducted a modified intention-to-treat analysis. We excluded outliers at the 99th percentile for antibiotic prescribing rate at baseline after randomisation from each arm to eliminate data errors from implausibly high numbers of antibiotic prescriptions attributed to a small number of physicians.25We analysed the data at the level of the physician using Poisson regression. The dependent variable was the number of antibiotic prescriptions and the model was offset by the log of the physician’s total patient visits. To improve power and efficiency the models were further adjusted for the following prespecified covariates: the log of the baseline antibiotic prescribing rate (15 January 2021 to 14 January 2022), physician’s sex, years since medical school graduation, and our stratification variable of whether the physician was enrolled in a previous Ontario trial. A second exploratory model included terms for the presence or absence of each factor (case-mix adjusted data or harms messaging).22We hypothesised a priori that no interaction would be shown between factors and to avoid an increased risk of type I error, we did not perform a two stage analysis evaluating for interaction effects.23We conducted an at-the-margins analysis by estimating the effects of each factor compared with physicians not receiving that factor. We added a post-hoc analysis that included an interaction term between factors to test our hypothesis of no multiplicative interaction. We then added a post-hoc inside-the-table sensitivity analysis of the embedded trial as four separate groups. Intervention effects were expressed using relative risks and 95% confidence intervals (CIs). Prespecified subgroup analyses of the primary outcome were conducted stratified on physician years in practice (<11 years, 11-24 years, or ≥25 years), sex, neighbourhood income quintile of practice, tertiles of the number of patient visits, tertiles of proportions of rostered patient visits (the number of patient visits for the physicians’ own patients divided by all patient visits), tertiles of the proportion of patient visits of people aged greater than 85 years, rural versus urban practice location, tertiles of baseline antibiotic prescribing rates, and tertiles of baseline antibiotic prescribing rates from virtual patient visits. The significance of subgroup differences was assessed by including the interaction between the subgroup variable and the treatment effect in the statistical model. We did not account for multiple hypothesis testing because these subgroup analyses were considered exploratory. The data were analysed using SAS Enterprise Guide 9.4.
Patient and public involvement
This trial was funded through the Canadian Institutes for Health Research’s strategy for patient oriented research. Patient representatives were engaged at the grant development stage and patient insights were solicited on the overall study and intervention design (as described in our protocol for this trial).16Patients were involved in providing feedback during the iterative process of designing the feedback reports. For example, patient representatives helped to refine the language used in the section of the feedback reports that emphasised communication tips that physicians could use to explain to patients why antibiotics are not recommended. The experiences of our patients engaged through this project were previously detailed in a podcast.24
Results
We identified 15 438 physicians within our administrative datasets with the specialty of general practice or family medicine. After excluding physicians who were inactive, with low numbers of visits or antibiotic prescriptions to the patient population, and those already signed up for feedback from Ontario Health, 6466 physicians were eligible. We were unable to include 1016 of these due to missing identification numbers or address information. An opt out introductory letter was mailed to 5450 physicians and 353 (6.5%) opted out of the trial or had an invalid address. 5097 physicians were randomly assigned to a group and 4076 assigned to the intervention group were mailed the intervention. After excluding 1% of outliers based on baseline antibiotic prescribing rate, data for 5046 physicians were analysed (fig 1).
Overall, the physician characteristics and baseline prescribing practices in each group were well balance (table 1). The average physician’s age was 50.6 years in the intervention group and 51.2 years in the control group. More than half of physicians were male and almost half were in practice for 25 years or more. The average baseline prescribing rate by antibiotic prescriptions per 1000 patient visits was 63.5 (standard deviation 36.3) in the intervention group and 64.6 (38.9), in the control group.
The antibiotic prescribing rate was lower in the intervention group compared with the control group at 0-6 months and 0-12 months post intervention (fig 2). After six months of follow-up, the mean antibiotic prescribing rate of the intervention group compared with the control group was significantly lower (56.0 (39.2)v59.4 (42.0) per 1000 patient visits; relative rate 0.95 (95% CI 0.94 to 0.96))(table 2). Mean patient visits per physician during the six month intervention period were 612.7 (527.4) for the intervention group and 626.6 (547.4) for the control group. The intervention also was significantly lower for antibiotic prescribing that was likely unnecessary (ie, for viral illnesses) (7.5v8.6 per 1000 patient visits, 0.89 (0.86 to 0.92)), antibiotic prescribing rate for long duration prescriptions (13.7v16.5, 0.85 (0.83 to 0.87)), and antibiotic prescribing rate for broad-spectrum prescriptions (26.0v28.4, 0.94 (0.92 to 0.95)). We observed similar results when including antibiotic prescriptions dispensed between baseline and 12 months of follow-up (0.96 (0.95 to 0.97)) (table S3).
In our prespecified subgroup analyses, no significant differences were reported in the intervention effectiveness among subgroups of physician’s sex, neighbourhood income group, patient visit volume, rural versus urban practice setting, and antibiotic prescribing for virtual visits (fig 3and table S4). Physicians with high baseline antibiotic prescribing had significantly larger reductions in antibiotic prescribing effect sizes compared with lower antibiotic prescribing physicians (pinteraction=0.018). In the sensitivity analysis that stratified the cohort by whether the patient was rostered to the prescribing physician, we observed similar results in both groups (table S5). We broke down the analysis by quarter post-hoc. The intervention effect in quarter one for relative risk was 0.94 (95% CI 0.93 to 0.96) and in quarter four was 0.98 (0.96 to 0.99) (table S6).
In the embedded factorial trial, our primary analysis assuming no interaction found that physicians receiving adjusted data had a 1% relative increase in antibiotic prescribing (relative risk 1.01 (95% CI 1.00 to 1.03)) (table 2) but those receiving harms messaging had similar outcomes to those who did not (1.00 (0.99 to 1.01)). A significant interaction was noted between factors in the factorial trial (P<0.001). The results from the sensitivity analysis as a four arm trial are presented in table S7. The results were consistent with our primary finding of no significant decreases in antibiotic prescribing outcomes for either intervention (table S7).
Of 135 randomly sampled physicians in the intervention group that we called after sending the debrief letter, 76 (56%) could not be reached, 18 (13%) confirmed receipt of the feedback letter, and 41 (30%) either did not receive or were unsure if they received the intervention.
Discussion
Principal findings
In this large, pragmatic randomised controlled trial with more than 5000 primary care physicians, a mailed letter to physicians led to a significant 5% relative reduction in overall antibiotic prescribing rate compared with physicians in the control group. We observed improvements in antibiotic prescribing on all outcomes evaluated including an 11% relative reduction in unnecessary antibiotic prescriptions, 15% relative reduction in antibiotic durations more than seven days, and a 6% relative reduction on broad-spectrum antibiotic prescribing.
Comparison with other studies
These results are consistent with a previous trial from Ontario,2as well as most other audit and feedback interventions on antibiotic prescribing in primary care.4510112526Some audit and feedback interventions in Europe have been ineffective with no significant change in antibiotic prescribing.367Two main explanations are likely for this discrepancy. Firstly, some jurisdictions with negative studies had significantly lower baseline antibiotic prescribing rates, suggesting less opportunity for change.27The second reason is likely related to the design of the intervention. A theory put forward by Linder and Fox is that feedback designed to be more subtle and avoid offending physicians may be too vague to elicit the desired behaviour change.28Best practice recommendations have been published on optimizing audit and feedback of antibiotics in primary care.29Antibiotic audit and feedback should be simple and include a single central figure.10Prescribers should be able to understand the data within seconds and connect the data directly to a desired action. Data that are too subtle, complex, provide multiple metrics or multiple comparators are less likely to be used by physicians and will not drive behaviour change.282930
Our attempts to maximize the effects of audit and feedback through design modifications did not lead to measurable differences in the embedded factorial trial. A common concern of audit and feedback recipients is that the data do not adequately reflect their specific patient and practice characteristics.1215Our attempt to address this common concern through case-mix adjustment did not reduce antibiotic prescribing and paradoxically may have led to small increases in antibiotic use. Providing more complex data could possibly have led to lower engagement with the intervention and less behaviour change. Another possible explanation is that including the case-mix adjustment paradoxically reinforced the cognitive bias that their patients’ may be different, which led to increased prescribing. These results should be interpreted cautiously as the effect sizes were very small and of uncertain clinical significance.
Regarding harms information, data show that physicians frequently perceive an imbalance in risks by underestimating the harms of unnecessary antibiotic prescribing and overestimating the risks of not prescribing antibiotics.1231A previous trial emphasising harms related to opioid prescribing led to significant reductions in opioid use.32However, despite additional emphasis on antibiotic harms, further improvements in antibiotic prescribing were not observed in our study. A possible explanation for this finding is that our infographic did not adequately address this risk perception to promote behaviour change beyond the feedback of data and other messaging within the letter. A theory informed process will explore the possible reasons that behaviour did not change and will be published separately. We observed an unanticipated statistically significant interaction effect in the factorial trial between case-mix adjusted feedback and harms messaging emphasis. The clinical significance of this observation is unclear and post-hoc analyses were similar; however, results from the factorial analysis should be interpreted cautiously.
Policy implications
Based on this work, as well as other published literature, we believe antibiotic audit and feedback should be a required routine quality improvement initiative in primary care funded by governments or insurers responsible for funding healthcare systems. All prescribers can benefit from access to their antibiotic prescribing data, if implemented thoughtfully using audit and feedback best practice recommendations to optimize patient care.29Our stratified analyses support that audit and feedback is effective across different physician groups (eg, age and sex), and practice types (eg, urgent care centres where physicians tend to see non-rostered patients as well as those with high and low baseline prescribing levels of rostered patients). Encouragingly, this intervention also appeared effective across neighbourhood income levels, supporting the role of audit and feedback to improve equitable access to appropriate antibiotic use.
Strengths and limitations of this study
Our observed effect size was modest given the limited engagement from clinicians in reviewing the feedback reports. In a previous study from Ontario, approximately a third of physicians opened the letter.2In this study, we sent the same letter twice, one month apart, in an attempt to increase engagement. Unfortunately, we were unable to engage more than one third of mailed recipients. Previous studies that have increased the number of mailings, or modified introductory emails, have had no or minimal impact.1133However, our significant findings were observed despite this low engagement with the intervention. Further research on ways to improve engagement with feedback reports should be a priority because this change may result in larger effect sizes.
Further research is needed into implementing co-interventions such as public education, communication interventions, point-of-care testing, and other quality improvement initiatives implemented and evaluated alongside audit and feedback to further reduce unnecessary antibiotic prescribing in primary care. A previous trial demonstrated the effectiveness of a simple table on antibiotic durations to reduce the duration of antibiotic prescriptions.2Prolonged antibiotic durations are likely very amenable to change because these are a knowledge gap for some physicians.34However, the reasons driving antibiotic initiation for respiratory tract infections are substantially more complex,12and will require more complex solutions through engagement of multiple stakeholders and prescribers. Future studies should focus on assessing the cost-effectiveness of antibiotic audit and feedback interventions in primary care. This research will aid in reducing unnecessary antibiotic use and improve resource allocation in publicly funded healthcare systems.
This trial has some notable limitations. The intervention was implemented during the covid-19 pandemic with substantial changes in the burden and timing of respiratory illnesses and healthcare seeking behaviours. The intervention was delivered soon after the early winter peak in antibiotic prescribing, which subsequently increased during the intervention, likely due to the resurgence of respiratory infections following a large decrease during the covid-19 pandemic.3536The generalizability to a post-pandemic setting is uncertain. However, our findings are consistent with several studies from different times and jurisdictions. We excluded primary care physicians who have already volunteered for electronic prescribing feedback (which includes antibiotic indicators) making these results particularly generalizable to primary care physicians not already engaged in audit and feedback. The pragmatic nature of our trial make it generalizable to other countries with primary care systems that, like Canada’s, offer care that is free at point of access. We chose not to cluster randomise by practice and contamination was possible across study arms. However, physicians in the control group for the primary analysis had no access to their data and therefore any contamination would have biased the results towards the null. Physicians within one practice may have been randomised to different factors with the potential for contamination. Our study was not specifically powered for the factorial analysis and the results from this embedded trial should be considered exploratory and interpreted cautiously. Our primary outcome was overall antibiotic prescribing rate because we were unable to accurately determine the appropriateness of all prescriptions with administrative data. Previous research from Ontario identified that approximately 25% of antibiotic prescriptions written by primary care physicians are unnecessary, suggesting substantial opportunity for safe reduction in use.18It is less likely that necessary antibiotic would be withheld and our secondary analysis supports that the observed reduction in use was likely driven by less unnecessary antibiotic use. A previous study did not identify any harms with a 12% reduction in antibiotic use in primary care.37Our data were limited to patients 65 years or older and may not apply to all patient populations. However, we have previously showed strong correlations between antibiotic prescribing by family physicians in Ontario to patients 65 years or older and overall prescribing.16Finally, the results of previous literature, including this study, are largely limited to high income countries and further research in low and middle income countries on antibiotic audit and feedback is needed.
Conclusions
Mailed antibiotic audit and feedback letters led to significant reductions in antibiotic prescribing by primary care physicians. This intervention is scalable to large populations of prescribers with the potential to reduce inappropriate antibiotic use, improve the quality of patient care, and slow the emergence of drug resistant infections. Antibiotic prescribing feedback should be a routine quality improvement expectation in primary care.
Antibiotic audit and feedback in primary care can be effective, however, previous literature has produced variable effect sizes
Which design components of a feedback intervention can improve the effectiveness of this intervention is largely unknown
A mailed antibiotic feedback letter with peer comparison is effective at reducing antibiotic prescribing by primary care physicians
Including adjusted metrics for the feedback, or antibiotic harms messaging, did not result in further reductions in antibiotic prescribing
Antibiotic audit and feedback reports should be a routine quality improvement initiative for all primary care physicians
","Objectives: To evaluate whether providing family physicians with feedback on their antibiotic prescribing compared with that of their peers reduces antibiotic prescriptions. To also identify effects on antibiotic prescribing from case-mix adjusted feedback reports and messages emphasising antibiotic associated harms.
Design: Pragmatic, factorial randomised controlled trial.
Setting: Primary care physicians in Ontario, Canada
Participants: All primary care physicians were randomly assigned a group if they were eligible and actively prescribing antibiotics to patients 65 years or older. Physicians were excluded if had already volunteered to receive antibiotic prescribing feedback from another agency, or had opted out of the trial.
Intervention: A letter was mailed in January 2022 to physicians with peer comparison antibiotic prescribing feedback compared with the control group who did not receive a letter (4:1 allocation). The intervention group was further randomised in a 2x2 factorial trial to evaluate case-mix adjusted versus unadjusted comparators, and emphasis, or not, on harms of antibiotics.
Main outcome measures: Antibiotic prescribing rate per 1000 patient visits for patients 65 years or older six months after intervention. Analysis was in the modified intention-to-treat population using Poisson regression.
Results: 5046 physicians were included and analysed: 1005 in control group and 4041 in intervention group (1016 case-mix adjusted data and harms messaging, 1006 with case-mix adjusted data and no harms messaging, 1006 unadjusted data and harms messaging, and 1013 unadjusted data and no harms messaging). At six months, mean antibiotic prescribing rate was 59.4 (standard deviation 42.0) in the control group and 56.0 (39.2) in the intervention group (relative rate 0.95 (95% confidence interval 0.94 to 0.96). Unnecessary antibiotic prescribing (0.89 (0.86 to 0.92)), prolonged duration prescriptions defined as more than seven days (0.85 (0.83 to 0.87)), and broad spectrum prescribing (0.94 (0.92 to 0.95)) were also significantly lower in the intervention group compared with the control group. Results:  were consistent at 12 months post intervention. No significant effect was seen for including emphasis on harms messaging. A small increase in antibiotic prescribing with case-mix adjusted reports was noted (1.01 (1.00 to 1.03)).
Conclusions: Peer comparison audit and feedback letters significantly reduced overall antibiotic prescribing with no benefit of case-mix adjustment or harms messaging. Antibiotic prescribing audit and feedback is a scalable and effective intervention and should be a routine quality improvement initiative in primary care.
Trial registration: ClinicalTrials.govNCT04594200
"
Tislelizumab plus chemotherapy versus placebo plus chemotherapy as first line treatment for advanced gastric or gastro-oesophageal junction adenocarcinoma,"Introduction
Gastric cancer is one of the most common types of cancer (>1 million new diagnoses in 2020) and one of the leading causes of cancer related deaths globally (>768 000 deaths in 2020).1Gastric cancer is more common in Asia (~75% of new diagnoses worldwide)2than in Europe and North America (~16% of new diagnoses),134and is of particular concern in China, South Korea, and Japan.567
Before the introduction of immunotherapy, the standard of care as first line (primary) treatment for advanced human epidermal growth factor receptor 2 (HER2) negative (ie, without HER2 overexpression or amplification) gastric or gastro-oesophageal junction adenocarcinoma was platinum plus fluoropyrimidine chemotherapy,8910with median overall survival rarely exceeding 12 months.811Immune checkpoint inhibitors, such as antiprogrammed cell death protein 1/programmed death-ligand 1 (PD-1/PD-L1) antibodies, have led to a paradigm shift in the standard of care of many types of solid tumours. Although several countries have approved the use of anti-PD-1 antibodies, such as nivolumab, pembrolizumab, and sintilimab, plus chemotherapy for this indication, the results from global and Asian phase 3 clinical trials in advanced gastric or gastro-oesophageal junction adenocarcinoma with these inhibitors have been inconsistent,121314151617prompting a debate around the survival benefit of anti-PD-1 antibodies in gastric or gastro-oesophageal junction adenocarcinoma and its magnitude according to PD-L1 expression status.
Tislelizumab is an anti-PD-1 monoclonal antibody with high affinity for PD-118and was specifically engineered to minimise Fcγ receptor binding on macrophages.1920In a phase 2 study in Chinese patients, tislelizumab showed promising antitumour activity when added to chemotherapy as treatment for advanced gastric or gastro-oesophageal junction adenocarcinoma (objective response rate 47% and median duration of response not reached, with a median study follow-up of 15.4 months) and had an acceptable safety and tolerability profile,21supporting further investigation in a phase 3 study in this setting. Here, we report the primary results of the RATIONALE-305 study, which evaluated the efficacy and safety of tislelizumab plus chemotherapy compared with placebo plus chemotherapy as primary treatment for locally advanced unresectable or metastatic gastric or gastro-oesophageal junction adenocarcinoma.
Methods
Study design and participants
RATIONALE-305 was a randomised, double blind, global, phase 3 clinical trial conducted at 146 medical centres across Asia, Europe, and North America (see supplementary appendix). Patients aged 18 years or older with histologically confirmed, locally advanced unresectable or metastatic gastric or gastro-oesophageal junction adenocarcinoma and no previous systemic therapy for advanced disease were recruited. If patients had received previous neoadjuvant or adjuvant therapy, a disease progression-free interval of at least six months was required. Patients were also required to have an Eastern Cooperative Oncology Group performance status of 0 or 1 and at least one measurable or non-measurable lesion according to the Response Evaluation Criteria in Solid Tumors (RECIST) version 1.1, as determined by investigator assessment. Patients were enrolled regardless of PD-L1 expression status. Key exclusion criteria were HER2 positive tumours, active leptomeningeal disease or uncontrolled brain metastasis, active autoimmune diseases, medical conditions requiring systemic corticosteroids or immunosuppressants, or previous treatment with anti-PD-1/PD-L1 therapies or other checkpoint inhibitors. The supplementary appendix provides a list of the eligibility criteria.
An independent data monitoring committee monitored safety and oversaw the unblinded results of a protocol specified interim overall survival analysis. The supplementary appendix provides the full protocol.
Randomisation and masking
Patients were randomly assigned (1:1) using an interactive response technology system with permuted block randomisation (block size of four) to either tislelizumab plus investigator chosen chemotherapy or placebo plus investigator chosen chemotherapy. Randomisation was stratified according to region (China (including Taiwan)vJapan and South KoreavEurope/North America), PD-L1 expression (PD-L1 tumour area positivity (TAP) score ≥5% or <5%), peritoneal metastases (yesvno), and investigator’s choice of chemotherapy (capecitabine and oxaliplatin, or 5-fluorouracil and cisplatin). Investigators, participants, site staff, and funder staff were masked to group assignment.
Procedures
Patients were given a 200 mg fixed dose of tislelizumab or matching placebo intravenously every three weeks, in combination with investigator’s choice of chemotherapy every three weeks: capecitabine 1000 mg/m2twice daily on days 1-14 and oxaliplatin 130 mg/m2on day 1 or 5-fluorouracil 800 mg/m2on days 1-5 and cisplatin 80 mg/m2on day 1 for up to six cycles. Thereafter, patients continued treatment with either tislelizumab or placebo, with optional maintenance capecitabine (only permitted for patients who initially received capecitabine and oxaliplatin), until disease progression or unacceptable toxicity. After two years of study treatment, if complete response, partial response, or stable disease were achieved, tislelizumab or placebo treatment could be stopped based on the investigator’s evaluation of a patient’s clinical benefit and risk. Crossover between the treatment arms or between chemotherapy regimens was prohibited.
Investigators conducted assessments of radiological tumour response by computed tomography or magnetic resonance imaging per RECIST version 1.1 about every six weeks during the first 48 weeks of the study and every nine weeks thereafter. Adverse events were assessed throughout the study period and up to 30 days after the last dose of study treatment (including chemotherapy) or before initiation of new anticancer therapy, whichever occurred first. Safety was assessed by monitoring treatment emergent adverse events and serious adverse events, mapped to terms from the Medical Dictionary for Regulatory Activities version 24.0 and graded according to National Cancer Institute Common Terminology Criteria for Adverse Events (version 5.0). Treatment related adverse events included adverse events that were considered by investigators to be related to any study drug. Immune related adverse events were reported until 90 days after the last dose of tislelizumab or placebo, regardless of initiation of new anticancer therapy.
A central laboratory prospectively assessed PD-L1 expression using the TAP score, defined as total percentage of tumour area (tumour and any desmoplastic stroma) covered by tumour cells with PD-L1 membrane staining (any intensity), and tumour associated immune cells with PD-L1 staining (any intensity), visually estimated by pathologists using an investigational use only version of the Ventana PD-L1 (SP263) assay (Roche Diagnostics).22In addition, for exploratory purposes, pathologists in the central laboratory scored the same stained samples according to combined positive score (defined as the number of PD-L1-expressing tumour cells, lymphocytes, and macrophages divided by total number of viable tumour cells, multiplied by 100).
Outcomes
The primary endpoint was overall survival, defined as the time from randomisation to death due to any cause, assessed in patients with a PD-L1 TAP score of ≥5% and in all randomised patients (intention-to-treat population). Secondary endpoints (assessed by investigators) were progression-free survival, confirmed objective response rate, disease control rate, clinical benefit rate, time to response, and duration of response, evaluated in patients with a PD-L1 TAP score of ≥5% and in all randomised patients; safety and tolerability, assessed based on the incidence and severity of adverse events; and health related quality of life, which will be reported in a separate publication. The supplementary appendix and protocol provide a complete list of study endpoints and their definitions, together with a summary of any changes during study conduct, and the corresponding rationale.
Statistical analysis
Efficacy analyses were conducted in randomised patients with a PD-L1 TAP score of ≥5% and in all randomised patients. For the primary endpoint, a hierarchical, sequential testing method was used to control type I error at 0.025 (one sided). If the null hypothesis in the overall survival analysis in patients with a PD-L1 TAP score of ≥5% was rejected, only then was overall survival tested in all randomised patients. After a rejection of the null hypothesis for the primary endpoint, testing was shifted sequentially to secondary endpoints of progression-free survival and objective response rate in patients with a PD-L1 TAP score of ≥5%, and then in all randomised patients (see supplementary appendix for description of statistical considerations related to hierarchical testing). The inferential test was to be stopped at the first non-significant endpoint.
Assumed hazard ratios for overall survival were 0.75 in patients with a PD-L1 TAP score of ≥5%, corresponding to a median overall survival of 15.3 months with tislelizumab plus chemotherapy versus 11.5 months with placebo plus chemotherapy, and 0.80 in all randomised patients, corresponding to a median overall survival of 14.4 months versus 11.5 months, respectively. It was expected that about 384 deaths in patients with a PD-L1 TAP score of ≥5% and 768 deaths in all randomised patients at final analysis (estimated to be 48 months after the first patient was randomised) would provide a power of 80% and 87%, respectively, for superiority testing at a one sided significance level of 0.025. One protocol-specified interim analysis of overall survival was planned to take place after 70% of the target number of deaths in each population. We predefined the superiority boundary at the interim overall survival analysis using the O’Brien-Fleming boundary approximated using the Hwang-Shih-DeCani spending function. The independent data monitoring committee confirmed that at the interim analysis (data cut-off: 8 October 2021) the study met the primary endpoint of overall survival in patients with a PD-L1 TAP score of ≥5% (one sided P value boundary for superiority is 0.0092 based on 291 deaths) but not in all randomised patients (one sided P value boundary for superiority is 0.0081 based on 559 deaths), and the study continued in a blinded manner towards the final analysis. The one sided P value boundary for superiority of overall survival in all randomised patients at final analysis (data cut-off: 28 February 2023) was updated to 0.0226 based on 776 actual observed deaths.
The overall survival and progression-free survival analyses were performed using a stratified log-rank test, stratified by region (AsiavEurope/North America), PD-L1 expression (only for analyses in all randomised patients, PD-L1 TAP score <5%v≥5%), and presence of peritoneal metastasis (yesvno). The stratified overall survival and progression-free survival hazard ratios and associated two sided 95% confidence intervals were estimated using a Cox proportional hazard regression model, including treatment arm as a covariate, and using stratification factors region, PD-L1 expression, and presence of peritoneal metastasis as strata. The median and cumulative probabilities of time-to-event endpoints were estimated using the Kaplan-Meier method. Overall survival was assessed in prespecified subgroups by PD-L1 expression status using TAP score, region, and several other baseline personal and disease characteristics (see supplementary appendix), as well as in post hoc subgroups by PD-L1 expression status using a combined positive score.
We calculated and compared objective response rates along with the Clopper-Pearson two sided 95% confidence intervals between treatment groups. Odds ratios were calculated using the Cochran-Mantel-Haenszel method, stratified by TAP score, region, and several other baseline personal and disease characteristics (see supplementary appendix). Safety was assessed in all patients who received at least one dose of study treatment (safety population). Safety data were analysed descriptively.
More detailed statistical considerations are provided in the statistical analysis plan (see supplementary appendix) and supplementary statistical considerations (see supplementary appendix). All calculations and analyses were done using SAS (version 9.2 or higher).
Patient and public involvement
Study participants were aware of the purpose of the trial during recruitment, although they were not involved in formulating the research question, outcome measures, or study design. Most of the participants were recruited from study countries where patient and public involvement in study design was uncommon at the time the study was planned, and for those participants who were recruited in study countries where patient and public involvement was more widely practised, the sponsor was still in the process of establishing relationships with suitable patient advocacy groups. However, the results were communicated to patients who expressed an interest during visits to participating clinics.
Results
Patients
Overall, 1657 patients were screened between 13 December 2018 and 9 February 2021, of whom 997 were randomly assigned to receive either tislelizumab plus chemotherapy (n=501) or placebo plus chemotherapy (n=496;fig 1). Supplementary appendix table S1 presents the primary reasons for screening failure. Minimum study follow-up time (defined as time from the date of the last patient randomised to the data cut-off) was 7.9 months at the interim analysis (data cut-off: 8 October 2021) and 24.6 months at the final analysis (data cut-off: 28 February 2023). In total, 498 (99%) of 501 patients in the tislelizumab plus chemotherapy arm and 494 (>99%) of 496 in the placebo plus chemotherapy arm received at least one dose of the study drug. Personal and baseline characteristics were balanced between treatment arms (table 1). Eight hundred (80%) of 997 patients had gastric cancer and 196 (20%) had gastro-oesophageal junction adenocarcinoma. Five hundred and forty six (55%) of 997 patients had a PD-L1 TAP score of ≥5%.
Median study follow-up duration (defined as time from randomisation to data cut-off, death, or study discontinuation owing to other reasons, whichever came first for all patients) was 11.8 months (interquartile range (IQR) 7.4-15.9 months) at the interim analysis and 13.2 (IQR 7.1-24.6) months at the final analysis. At the final analysis, 459 (92%) of 501 patients in the tislelizumab plus chemotherapy arm and 470 (95%) of 496 in the placebo plus chemotherapy arm had discontinued treatment (fig 1). The median duration of exposure to chemotherapy was 5.9 (IQR 3.3-11.9) months for tislelizumab plus chemotherapy and 5.7 (IQR 3.0-9.8) months for placebo plus chemotherapy; the median durations of exposure were similar between the two arms (see supplementary appendix table S2). Overall, 276 (61%) of 453 patients in the tislelizumab plus chemotherapy arm and 250 (56%) of 450 patients in the placebo plus chemotherapy arm received capecitabine maintenance treatment (safety population). After study treatment discontinuation, 265 (53%) of 501 patients in the tislelizumab plus chemotherapy arm and 294 (59%) of 496 patients in the placebo plus chemotherapy arm received subsequent systemic anticancer therapies (see supplementary appendix table S3).
Efficacy in population with PD-L1 TAP score ≥5%
At the interim analysis, tislelizumab plus chemotherapy met the criteria for superiority versus placebo plus chemotherapy for overall survival in patients with a PD-L1 TAP score of ≥5% (median 17.2 months (95% confidence interval (CI) 13.9 to 21.3)v12.6 months (12.0 to 14.4), respectively; stratified hazard ratio 0.74 (95% CI 0.59 to 0.94); P=0.006) (fig 2). Significant improvement in investigator assessed progression-free survival was observed in the tislelizumab plus chemotherapy arm versus placebo plus chemotherapy arm in patients with a PD-L1 TAP score of ≥5% (stratified hazard ratio 0.67 (0.55 to 0.83); P<0.001) (fig 3) along with higher objective response rate (non-significant) and longer duration of response (see supplementary appendix table S4 and fig S1). After an additional 17 months of follow-up (minimum 24.6 months) at the final analysis, the updated results in the tislelizumab plus chemotherapy arm versus placebo plus chemotherapy arm for overall survival (median 16.4 months (95% CI 13.6 to 19.1)v12.8 months (12.0 to 14.5), respectively; stratified hazard ratio 0.71 (95% CI 0.58 to 0.86)) and for progression-free survival (0.68 (0.56 to 0.83)) in patients with a PD-L1 TAP score of ≥5% were consistent with the primary results at the interim analysis (see supplementary appendix fig S2, fig S3, and table S5).
Efficacy in intention-to-treat population
At the final analysis, tislelizumab plus chemotherapy also met the criteria for superiority versus placebo plus chemotherapy for overall survival (median 15.0 months (95% CI 13.6 to 16.5)v12.9 months (12.1 to 14.1), respectively; stratified hazard ratio 0.80 (95% CI 0.70 to 0.92); P=0.001) in all randomised patients (fig 2). In the tislelizumab plus chemotherapy arm versus placebo plus chemotherapy arm, the overall survival rate at 18 months was 42%v33%, respectively, and at 24 months was 33%v23%, respectively (fig 2). Investigator assessed progression-free survival was also improved in the tislelizumab plus chemotherapy arm versus placebo plus chemotherapy arm in all randomised patients (stratified hazard ratio 0.78 (0.67 to 0.90)) (fig 3), along with tumour response and duration of response at the final analysis (see supplementary appendix tables S4 and S5 and fig S4).
Subgroup and exploratory analyses
Overall survival results at the final analysis consistently favoured patients in the tislelizumab plus chemotherapy arm versus the placebo plus chemotherapy arm across multiple prespecified subgroups in patients with a PD-L1 TAP score of ≥5% (see supplementary appendix table S6 and fig S5) and in all randomised patients (see supplementary appendix table S7 andfig 4). In the prespecified analysis of patients with a PD-L1 TAP score of <5%, median overall survival in the tislelizumab plus chemotherapy arm was 14.1 months (95% CI 11.9 to 15.6)v12.9 months (11.3 to 14.7) in the placebo plus chemotherapy arm, with a stratified hazard ratio of 0.92 (95% CI 0.75 to 1.13). Supplementary appendix table S5 summarises the outcomes in the population with a PD-L1 TAP score of <5%. In the post hoc analysis, similar results to those seen with TAP score based subgroups were observed in patients with a PD-L1 combined positive score of ≥5 (unstratified hazard ratio 0.72 (95% CI 0.59 to 0.88)) and <5 (0.89 (0.72 to 1.09)) (see supplementary appendix table S8). Supplementary appendix tables S9 and S10 report the prevalence by PD-L1 combined positive score status and concordance between TAP score and combined positive score from the exploratory biomarker analysis.
Safety
Treatment related adverse events were reported in 483 (97%) of 498 patients in the tislelizumab plus chemotherapy arm and 476 (96%) of 494 patients in the placebo plus chemotherapy arm (see supplementary appendix table S11). Grade 3 or worse treatment related adverse events were reported in 268 (54%) of 498 patients in the tislelizumab plus chemotherapy arm and 246 (50%) of 494 patients in the placebo plus chemotherapy arm. The most common grade 3 or grade 4 treatment related adverse events were decreased neutrophil count (59 patients (12%) in the tislelizumab plus chemotherapy armv57 (12%) in the placebo plus chemotherapy arm), decreased platelet count (56 (11%)v57 (12%), respectively), neutropenia (33 (7%)v34 (7%), respectively), and anaemia (25 (5%)v37 (7%), respectively) (table 2). Serious treatment related adverse events occurred in 113 (23%) of 498 patients in the tislelizumab plus chemotherapy arm versus 72 (15%) of 494 patients in the placebo plus chemotherapy arm (see supplementary appendix table S11). Treatment related adverse events led to treatment discontinuation in 80 (16%) of 498 patients in the tislelizumab plus chemotherapy arm versus 40 (8%) of 494 patients in the placebo plus chemotherapy arm, and dose modification in 355 (71%) versus 354 patients (72%), respectively. Treatment related adverse events that led to death occurred in six of 498 patients (1%) in the tislelizumab plus chemotherapy arm (unspecified death (n=4), colitis (n=1), and sepsis (n=1)) and two of 494 patients (<1%) in the placebo plus chemotherapy arm (pneumonia (n=2)). A total of 154 (31%) of 498 patients in the tislelizumab plus chemotherapy arm versus 58 (12%) of 494 patients in the placebo plus chemotherapy arm experienced immune mediated adverse events, with 38 (8%)v10 (2%) patients, respectively, experiencing grade 3 or worse events. Supplementary appendix table S12 lists the most common immune mediated adverse events reported.
Discussion
Primary treatment of advanced gastric or gastro-oesophageal junction adenocarcinoma with tislelizumab plus chemotherapy provided significant overall survival benefit versus placebo plus chemotherapy in patients with previously untreated HER2 negative advanced gastric or gastro-oesophageal junction adenocarcinoma. At the final analysis, overall survival rates at 18 and 24 months were higher with tislelizumab plus chemotherapy in patients with a PD-L1 TAP score of ≥5%, and in all randomised patients. With risk reductions for death of 29% and 20% in patients with a PD-L1 TAP score of ≥5% and in all randomised patients, respectively, the magnitude of improvement was considered clinically meaningful.23The overall survival benefit was observed across multiple prespecified patient subgroups and was accompanied by improvements in progression-free survival, objective response rates, and duration of response. With a minimum study follow-up time of 24.6 months and death events reported in 78% of all randomised patients at the final analysis, the current study provides robust data for efficacy and safety.
Comparison with other studies
During the conduct of the RATIONALE-305 study, other phase 3 trials of anti-PD-1 antibodies for primary treatment of advanced gastric or gastro-oesophageal junction adenocarcinoma (in both global and Asian populations) have been reported, with variable observed results for overall survival. Nivolumab plus chemotherapy showed a significant overall survival benefit versus chemotherapy alone in patients with a PD-L1 combined positive score of ≥5 (hazard ratio 0.71) and in all randomised patients (hazard ratio 0.80) in a global population in the CheckMate 649 trial,13but not in all randomised patients in an Asian population in the ATTRACTION-4 (nivolumab plus chemotherapy versus chemotherapy alone in patients with HER2 negative, untreated, unresectable advanced or recurrent gastric or gastro-oesophageal junction cancer) trial (hazard ratio 0.90).14Pembrolizumab plus chemotherapy failed to show superiority versus chemotherapy alone in patients with high expression of PD-L1 in the KEYNOTE-062 (efficacy and safety of pembrolizumab or pembrolizumab plus chemotherapy versus chemotherapy alone for patients with first line, advanced gastric cancer) trial (combined positive score ≥1 or combined positive score ≥10),15although significant overall survival benefit was shown versus chemotherapy alone in patients with a PD-L1 combined positive score of ≥10 (hazard ratio 0.65) or ≥1 (hazard ratio 0.74), and in all randomised patients (hazard ratio 0.78) in the KEYNOTE-859 (pembrolizumab plus chemotherapy versus chemotherapy alone for HER2 negative advanced gastric cancer) trial.16Another anti-PD-1 antibody, sintilimab, has shown significantly improved overall survival in combination with chemotherapy versus chemotherapy alone in Chinese patients with PD-L1 combined positive scores of ≥5 (hazard ratio 0.66) and in all randomised patients (hazard ratio 0.77) in the ORIENT-16 (sintilimab plus chemotherapy for unresectable gastric or gastro-oesophageal junction cancer) trial.17The variability in overall survival observed among these studies could potentially be attributed to differences in study design, in the proportion of Asian compared with European/North American patients of the included populations, in chemotherapy regimens, or in the studies’ statistical considerations, as well as differences in use of subsequent therapies. With the inclusion of a similar study population and use of a similar chemotherapy regimen, but with a slightly higher number of patients receiving subsequent immunotherapy, the overall survival benefit of tislelizumab plus chemotherapy versus placebo plus chemotherapy in RATIONALE-305 (hazard ratios 0.74 in the population with a PD-L1 TAP score ≥5% and 0.80 in all randomised patients) was consistent with the observed benefits in the global studies, with the addition of nivolumab or pembrolizumab to chemotherapy in CheckMate 64913and KEYNOTE-859,16respectively. These data for tislelizumab reinforce the survival benefit seen with anti-PD-1 antibodies in combination with chemotherapy as primary treatment for patients with gastric or gastro-oesophageal junction adenocarcinoma.
The overall survival benefit of tislelizumab plus chemotherapy appeared greater in the population with a PD-L1 TAP score of ≥5% compared with patients with a score of <5%, suggesting an enrichment of survival benefit in patients with higher PD-L1 expression than in patients with lower PD-L1 expression. In patients with a PD-L1 TAP score of <5%, the hazard ratio for overall survival seemed to favour the tislelizumab plus chemotherapy arm over the placebo plus chemotherapy arm (ie, hazard ratio <1); a similar trend in the hazard ratio for progression-free survival and a numerical increase in objective response rate were also observed. This prespecified exploratory analysis was not powered for hypothesis testing, however, and should be interpreted with caution. The observed difference in the magnitude of survival benefit between patients with high and low PD-L1 expression was similar to findings in studies of other anti-PD-1 antibodies plus chemotherapy in the setting of primary treatment of patients with advanced gastric or gastro-oesophageal junction adenocarcinoma.131617We acknowledge that challenges in data interpretation across studies may arise when different PD-L1 scoring methods are used. The prevalence of patients with a PD-L1 TAP score of ≥5% was similar to that for a combined positive score of ≥5 in other studies in gastric or gastro-oesophageal junction adenocarcinoma.1317Reassuringly, we observed an acceptable concordance rate between TAP scores of ≥5% and combined positive scores of ≥5 in the exploratory analysis in our study, and similar overall survival results with tislelizumab plus chemotherapy in subgroups defined by TAP score or combined positive score in the exploratory analysis.
The RATIONALE-305 study population was representative of the global distribution of gastric or gastro-oesophageal junction adenocarcinoma, with 75% of patients enrolled from Asia and 25% from Europe and North America; the data therefore provide valuable clinical evidence relevant to the management of patients with gastric or gastro-oesophageal junction adenocarcinoma globally. Subgroup analyses showed that overall survival benefit with tislelizumab plus chemotherapy was consistently seen across the Asia and Europe/North America regions in the population with a PD-L1 TAP score of ≥5%, and in all randomised patients. A consistent overall survival benefit among different regional subgroups was also observed with other anti-PD-1 antibodies in the CheckMate 649 and KEYNOTE-859 studies,1316despite numerical differences in the magnitude of treatment effect across subgroups, which should be interpreted with caution owing to the exploratory nature of subgroup analyses. Similar to results observed with other anti-PD-1 antibodies,1316survival benefit with tislelizumab plus chemotherapy was observed regardless of microsatellite instability status. Results suggested that the magnitude of benefit was greater in patients with high microsatellite instability/mismatch repair-deficient tumours than those with low microsatellite instability/mismatch repair-proficient tumours, although the wide confidence interval, likely due to the small sample size, precludes a definitive conclusion (fig 4).
The safety profile of tislelizumab plus chemotherapy was manageable when being used as primary treatment in patients with locally advanced unresectable or metastatic gastric or gastro-oesophageal junction adenocarcinoma. The duration of treatment with tislelizumab or placebo, along with chemotherapy, was similar in both arms. The most common treatment related adverse events (any grade and grade ≥3) in both groups were consistent with the known safety profile of the individual study treatment components. Most immune mediated adverse events were grade 1 or grade 2 in severity, and the profile of commonly reported immune mediated adverse events was generally similar to those seen with other anti-PD-1 antibodies.131415161724Overall, the toxicity of chemotherapy did not appear to worsen with the addition of tislelizumab.
Limitation of this study
A potential limitation of the current study was the lack of independent review committee assessment of tumour responses. As the investigators and site staff were blinded to group assignment and PD-L1 expression, however, the potential for bias in investigator assessed responses using standard criteria was expected to be minimal.
Conclusions
Primary treatment of advanced gastric or gastro-oesophageal junction adenocarcinoma with tislelizumab plus chemotherapy provided significant and clinically meaningful overall survival benefit versus placebo plus chemotherapy in patients with a PD-L1 TAP score of ≥5%, and in all randomised patients. The safety profile of tislelizumab in combination with chemotherapy was manageable and acceptable. These data suggest that tislelizumab plus chemotherapy represents a potential new primary treatment option for patients with advanced gastric or gastro-oesophageal junction adenocarcinoma.
Survival outcomes with platinum plus fluoropyrimidine chemotherapy alone as primary treatment for advanced gastric or gastro-oesophageal junction adenocarcinoma are poor
Results from studies with anti-programmed cell death protein 1 (PD-1) antibodies added to chemotherapy in this setting show inconsistent overall survival benefits
Debate is ongoing about the survival benefit of anti-PD-1 therapy in patients with gastric or gastro-oesophageal junction adenocarcinoma, and its magnitude according to PD-L1 expression status
Addition of the anti-PD-1 antibody tislelizumab to chemotherapy provided significant and clinically meaningful overall survival benefit versus placebo plus chemotherapy in patients with previously untreated HER2 negative advanced gastric or gastro-oesophageal junction adenocarcinoma
This finding was further enriched in patients with a PD-L1 tumour area positivity score of ≥5%
Tislelizumab plus chemotherapy could present a new primary treatment option for patients with advanced gastric or gastro-oesophageal junction adenocarcinoma
","Objective: To evaluate the efficacy and safety of tislelizumab added to chemotherapy as first line (primary) treatment for advanced gastric or gastro-oesophageal junction adenocarcinoma compared with placebo plus chemotherapy.
Design: Randomised, double blind, placebo controlled, phase 3 study.
Setting: 146 medical centres across Asia, Europe, and North America, between 13 December 2018 and 28 February 2023.
Participants: 1657 patients aged ≥18 years with human epidermal growth factor receptor 2 negative locally advanced unresectable or metastatic gastric or gastro-oesophageal junction adenocarcinoma, regardless of programmed death-ligand 1 (PD-L1) expression status, who had not received systemic anticancer therapy for advanced disease.
Interventions: Patients were randomly (1:1) assigned to receive either tislelizumab 200 mg or placebo intravenously every three weeks in combination with chemotherapy (investigator’s choice of oxaliplatin and capecitabine, or cisplatin and 5-fluorouracil) and stratified by region, PD-L1 expression, presence or absence of peritoneal metastases, and investigator’s choice of chemotherapy. Treatment continued until disease progression or unacceptable toxicity.
Main outcome measures: The primary endpoint was overall survival, both in patients with a PD-L1 tumour area positivity (TAP) score of ≥5% and in all randomised patients. Safety was assessed in all those who received at least one dose of study treatment.
Results: Of 1657 patients screened between 13 December 2018 and 9 February 2021, 660 were ineligible due to not meeting the eligibility criteria, withdrawal of consent, adverse events, or other reasons. Overall, 997 were randomly assigned to receive tislelizumab plus chemotherapy (n=501) or placebo plus chemotherapy (n=496). Tislelizumab plus chemotherapy showed statistically significant improvements in overall survival versus placebo plus chemotherapy in patients with a PD-L1 TAP score of ≥5% (median 17.2 monthsv12.6 months; hazard ratio 0.74 (95% confidence interval 0.59 to 0.94); P=0.006 (interim analysis)) and in all randomised patients (median 15.0 monthsv12.9 months; hazard ratio 0.80 (0.70 to 0.92); P=0.001 (final analysis)). Grade 3 or worse treatment related adverse events were observed in 54% (268/498) of patients in the tislelizumab plus chemotherapy arm versus 50% (246/494) in the placebo plus chemotherapy arm.
Conclusions: Tislelizumab added to chemotherapy as primary treatment for advanced or metastatic gastric or gastro-oesophageal junction adenocarcinoma provided superior overall survival with a manageable safety profile versus placebo plus chemotherapy in patients with a PD-L1 TAP score of ≥5%, and in all randomised patients.
Trial registration: ClinicalTrials.govNCT03777657
"
Epidural analgesia during labour and severe maternal morbidity,"Introduction
The rising incidence of severe maternal morbidity (SMM) constitutes a pressing global issue, compromising the wellbeing of mothers and their children, and resulting in potentially devastating short term and long term consequences.12SMM is defined by the US Centers for Disease Control and Prevention (CDC) as encompassing 21 indicative conditions or procedures, such as myocardial infarction, eclampsia, and hysterectomy occurring during admission to hospital for delivery.3In the UK, the incidence of SMM almost doubled between 2009 and 2018, from 0.9% to 1.7% of deliveries, likely reflecting the trend of mothers being older, more obese, and with increasing comorbidities, along with a rising incidence of previous caesarean delivery.4SMM can be conceptualised as an indicator of increased risk for maternal mortality, providing crucial opportunities to identify and implement interventions to improve the health of mothers and their offspring.5
Epidural analgesia is commonly advised for safety reasons in pregnant women considered at higher risk of SMM, such as those with multiple births, morbid obesity (body mass index (BMI) ≥40), or certain comorbidities, owing to its advantageous physiological effects and capacity to provide expedient anaesthesia if required in an emergency.6Women with these factors can be considered as having a medical indication for epidural analgesia during labour. Women giving birth preterm also carry a higher risk of SMM, although epidural analgesia is seldom recommended for preterm labour alone.7Despite the assumed benefits of epidural analgesia during labour to prevent SMM, the evidence base for this is limited. We identified just two observational studies that attempted to delineate the association between epidural analgesia during labour and SMM.89One, a US study (n=574 525), indicated a 14% risk reduction in SMM in women who received epidural analgesia, but it only included vaginal births and excluded the six week postnatal period, during which about 15% of SMM events occur.810The other study, from France (n=4550), reported a 47% decreased risk of severe postpartum haemorrhage in women with epidural analgesia who gave birth vaginally, but it did not assess other constituents of SMM.9Neither of these studies explored whether the association differed between women with a medical indication and those without, or between women who delivered preterm and those who did not. In these two studies from countries with private healthcare systems, the use of epidural analgesia was 47%8and 78%,9respectively, whereas in the UK, the use of epidural analgesia during labour is around 22-30%, despite healthcare being free at the point of access.1112
Notwithstanding that clinicians may advise mothers with medical indications about epidural analgesia during labour, the final decision is up to the woman. The lack of robust evidence on whether benefits exist beyond the provision of epidural analgesia might affect the discussions clinicians have with women and their decisions. Women from minority ethnic groups and areas of socioeconomic deprivation are at higher risk of maternal morbidity and mortality, and they are more likely to have medical indications for epidural analgesia, but are less likely to have one.131415Stronger evidence on the effects of epidural analgesia might contribute to reducing these inequalities. The importance of improving this evidence base is highlighted by the priority setting exercises undertaken by the James Lind Alliance, which identified the effect of epidural analgesia on obstetric outcomes as a research priority.16The James Lind Alliance brings patients, carers, and clinicians together to identify research priorities.
In this population based cohort analysis of all births in Scotland over a 13 year period, we estimated the causal effect of the use of epidural analgesia during labour on SMM in all mothers, except those undergoing planned caesarean section delivery. Additionally, we explored whether this effect was more pronounced among pregnant women who according to clinical guidelines are at increased risk of SMM (ie, women with a medical indication for epidural analgesia during labour), and in those with preterm labour.
Methods
Our methods are reported in accordance with Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) guidance.17
Data sources and study population
We linked six Scotland-wide administrative databases: the Scottish Morbidity Record-2 (SMR02), the Scottish Morbidity Record-1 (SMR01), the Scottish Birth Record, the National Records of Scotland, the Scottish Stillbirth Infant Death Survey, and the Scottish Intensive Care Society Audit Group. The SMR02 documents all obstetric inpatient and day case admissions during pregnancy and the postnatal period and includes maternal and infant characteristics. The SMR02 is subject to regular quality assurance checks, with data more than 99% complete since the late 1970s.1819The SMR01 records all non-obstetric inpatient and day case admissions according to ICD-9 and ICD-10 (international classification of diseases, ninth revision and 10th revision, respectively) codes and UK NHS OPCS-4 (Office of Population Censuses and Surveys classification of interventions and procedures).2021All neonatal care is recorded in the Scottish Birth Record. The National Records of Scotland registers all births, stillbirths, and infant deaths, and the Scottish Stillbirth Infant Death Survey collects additional information from the relevant coordinator of the survey (obstetrician, paediatrician, or midwife) at each hospital. The database of the Scottish Intensive Care Society Audit Group records admission data for all Scottish intensive care and high dependency units, with regular data validation.22
Inclusion and exclusion criteria
We analysed all women in labour in Scotland between 1 January 2007 and 31 December 2019 with gestation between 24+0 and 42+6 weeks. Births were excluded after this period to remove any potential confounding influence of the covid-19 pandemic. We also excluded births when mode of delivery, child identity, or data for analgesia during labour were not recorded (n=38 705, 5.5% of all 697 981 pregnancies considered); see supplementary eFigure 1), as well as births by elective caesarean section as these women knew their mode of delivery in advance and would not experience labour, and therefore by definition could not have chosen to have epidural analgesia (n=92 060, 13.2%; see supplementary eFigure 1).
Epidural analgesia
We defined epidural analgesia during labour as conventional lumbar epidural sited at any time during labour. This definition is consistent with standard medical practices in the UK, where epidural drugs are generally administered only after labour has commenced. We were unable to identify use of combined spinal epidural (spinal injection plus insertion of an epidural catheter), as SMR02 classifies the procedure as spinal anaesthesia. Combined spinal epidural is used infrequently in Scotland, representing only 1% of epidural use during labour.23Women recorded as having no epidural could have delivered without additional analgesia or anaesthesia or have required spinal or general anaesthesia for operative delivery, reflecting the unpredictability of labour outcomes and the resultant different potential pathways care may take. Since recording of anaesthetic intervention is hierarchical, we could not identify if women who had a spinal or general anaesthetic also had epidural analgesia at an earlier point. Conversion of epidural analgesia to spinal or general anaesthesia occurs in around 5% of women.24
Outcomes
The primary outcome was SMM, defined as a composite outcome of ≥1 of 21 conditions according to the US CDC criteria for SMM or a critical care admission, with either occurring at any point from the date of delivery to 42 days post partum (described as SMM). In keeping with other published data, we incorporated critical care admission as an SMM indicator because the CDC’s definition does not cover all SMM events (eg, asthma attack, status epilepticus).4We identified conditions using ICD-9, ICD-10, and OPCS codes from SMR01, SMR02, and Scottish Intensive Care Society Audit Group datasets (see supplementary eTable 1 for table of codes).3The CDC’s definition of SMM has a sensitivity of 77% and specificity of 99% in identifying SMM compared with medical records.25
Secondary outcomes aimed to capture more severe morbidity and included ≥1 of the 21 CDC conditions when that condition resulted in admission for critical care (described as SMM plus critical care), and respiratory morbidity (ventilation, tracheostomy, acute respiratory distress syndrome, or respiratory complications of anaesthesia), as diagnosed from the date of delivery to 42 days post partum (see supplementary eTable 1).
Minor modifications were made to the CDC SMM criteria to accommodate data recording practices in Scotland (see supplementary eTable 1). In line with other UK studies,426we found that the UK definition for postpartum haemorrhage (≥500 mL blood loss) resulted in over-reporting of major obstetric haemorrhage (ICD-10 code O72), and therefore we included postpartum haemorrhage only if it occurred in association with a critical care admission, indicating a clinically significant haemorrhage event. Alternative metrics such as volume of blood loss and blood transfusion are not reliably recorded in SMR02. Similar to a previous Scottish study, we found the incidence of sepsis had increased exponentially from 2012 (see supplementary eFigure 2).4This might reflect different coding practices and changes in guidance with the publication of the 2012 Surviving Sepsis recommendations resulting in increased awareness of the condition.2728Because sepsis is defined as the presence of an infection and evidence of acute organ dysfunction, we included it only if associated with admission to a critical care unit.
Given that in our analyses, as in any risk analyses, we censored at the first SMM condition, the difference between the primary outcome and the first secondary outcome is illustrated by considering a mother with eclampsia diagnosed on the day of delivery and acute heart failure diagnosed on postnatal day 22. In the primary analysis, that woman would be censored on the day of delivery. In contrast, a woman with eclampsia diagnosed on the day of delivery who experienced heart failure resulting in critical care admission at 22 days postnatally when heart failure was diagnosed, would be censored at postnatal day 22. Conversely, a woman with the same conditions at the same time points but who was not admitted to critical care for either would not be considered at risk for the secondary outcome of SMM plus critical care admission (and would contribute to the comparator group—no SMM plus critical care).
Confounders and other variables used in analyses
To determine confounding variables before analyses, we used the established definition of a confounder—something that is a known or plausible reason for having both epidural analgesia during labour and SMM, and we considered all potential plausible pathways between these variables.29We included these confounders (irrespective of whether they were available in our data) in directed acyclic graphs drawn using the R package “DAGGitty,”30to highlight sources of unmeasured confounding and how these might be captured by other measured confounders on the same confounding path (see supplementary eFigures 3a and 3b). We included socioeconomic status and ethnicity as these factors are increasingly recognised as influencing poor maternal outcomes and epidural analgesia use during labour.131415Ethnicity was defined using NHS Scotland 2011 census categories.31As we did not have information on individual socioeconomic status, we used residential area deprivation according to the Scottish index for multiple deprivation as a proxy; the first 10% of deprivation denoting the most deprived areas and the last 10% the least deprived.32Pre-existing comorbidities that plausibly influence the use of epidural analgesia and SMM were defined for each mother by calculating a Bateman index score, an extensively validated, weighted, risk prediction tool including 20 conditions plus maternal age that is specific to obstetric patients and more accurately predicts SMM than other generic comorbidity indices (see supplementary eTable 2)33To avoid conflating comorbid conditions with the outcome of SMM, we applied strict criteria, restricting these diagnoses to the period between 180 days before the estimated date of conception (as described in the original paper by Bateman et al)33and the day before delivery. This approach ensured the validity of our findings by accurately reflecting the impact of comorbidities on risk of SMM. Using ICD-9 and ICD-10 codes from SMR02, we obtained information on maternal height, weight, and smoking status plus obstetric indices of previous caesarean section, parity, and induction of labour. Gestational age at birth was based on ultrasound assessment in the first half of pregnancy. Smoking status at booking was defined as current, former, or never. Birth location was categorised into obstetric unit, freestanding midwifery unit, or home birth. Obstetric units were defined as hospitals with on-site obstetric and anaesthetic services, inclusive of epidural analgesia provision, or midwifery led units co-located with an obstetric unit. Freestanding midwifery units were defined as midwifery led units without direct access to obstetric or anaesthetic services.34
In exploratory analyses we assessed whether associations differed by the presence of a medical indication for epidural analgesia and by gestational age. We classified births as preterm if they occurred before 37 weeks’ gestation and as term or post term if they occurred at ≥37+0 weeks. Births were further classified using World Health Organization (WHO) criteria as extremely preterm (<28 weeks), very preterm (28 to <32 weeks), and moderate to late preterm (≥32 to 36+6 weeks), and by whether labour occurred spontaneously or was commenced iatrogenically.35
We defined medical indications for epidural analgesia as any of serious cardiovascular or respiratory disease (congestive heart failure, congenital heart disease, pulmonary hypertension, ischaemic heart disease, asthma); pre-eclampsia; previous caesarean section; breech presentation; multiple pregnancy; and morbid obesity (BMI ≥40), diagnosed before the date of delivery and with no contraindication to epidural insertion (see supplementary eTable 3).6363738394041These indications are easily identified by obstetric, anaesthesia, and midwifery staff, reflect criteria that drive common decision making processes, and are in widespread use in clinical practice. These conditions were included if recorded up to the day pre-delivery to ensure they occurred before the decision to have an epidural and any episodes of SMM.
Statistical analysis
As this was a whole population study, we did not perform sample size calculations. We report baseline characteristics by epidural status. Continuous variables are expressed as medians with interquartile range (IQR), and categorical variables as counts and percentages. For group comparison, we used standardised differences.
To adjust for confounders, we used multivariable Poisson regression models with cluster robust sandwich estimators under the generalised estimation equation framework (see supplementary eFigures 3a and 3b). These models were chosen in place of log-binomial models to avoid problems with convergence. The robust estimator was used to correct the inflated variance found from the standard Poisson model, and to account for more than one birth in some women.42We also assessed a zero inflated Poisson model using a single zero inflation parameter applied to all observations to account for any excess of zeros in the model. This indicated no excess of zeros (P>0.9), further supporting the use of a multivariable Poisson regression model with cluster robust errors. In the modelling of risk analyses, we censored at the first SMM condition (ie, a mother with two SMM conditions was only counted once in the analysis). These models were used to determine adjusted relative risks and absolute risks. As we a priori assumed that outcomes might differ depending on gestational age, we included this as an interaction and adjusted for all of the other previously defined confounders. To explore potential residual confounding from confounders that we did not consider because evidence was lacking to suggest they would affect epidural use and SMM, we calculated an E-value.43The E-value was defined as the minimum strength of association that one confounder or several unmeasured confounders would need to have with both epidural analgesia and SMM, conditional on the confounders we adjusted for, to fully explain a specific exposure-outcome association. This was calculated using the EValue package (version 4.1.3).
We repeated the same adjusted Poison regression modelling cluster robust sandwich estimators as described for the main analyses in three sets of subgroup analyses: Women with a medical indication and those without a medical indication, women delivering pre-term (<37 completed weeks of gestation) and those delivering at term or post term (≥37 completed weeks), and women with a medical indication and delivering preterm and those with no medical indication and delivering at term or post term.
In each of these analyses we tested statistical evidence for a difference between the two related subgroups by comparing a model with an interaction term (eg, interaction term between epidural analgesia during labour and medical indication—yesvno) using a likelihood ratio test comparing these two models. As analyses between subgroups are often under-powered, we considered a P value <0.01 to provide statistical evidence of a difference.
As our definition of medical indication for epidural analgesia included some components of the Bateman index score and BMI, we removed Bateman index score and maternal height and weight as confounding variables in the models of subgroup analyses that included medical indication (see supplementary eFigure 3b). Finally, to further model the effect of epidural analgesia on women with different underlying risk profiles for SMM, we analysed the association between epidural analgesia and SMM in women with and without an indication for epidural throughout the continuum of gestational ages using robust Poisson regression with non-linear splines.
Given that epidural analgesia is only available to women delivering in an obstetric unit, we repeated the analyses restricted to births occurring within an obstetric unit (n=541 389, 95.4% of eligible women) and compared the results to our main analyses. We also provided additional subgroup analyses using WHO criteria of preterm births, and by iatrogenic or spontaneous preterm birth.35
All eligible women (see supplementary eFigure 1) had complete data on epidural analgesia and outcome. Missing data on confounders varied, with the least for maternal age (0 missing) and most for maternal ethnicity (n=222 213, 39.2%) and illicit drug use (n=179 284, 31.6%) (table 1). In total, 257 713 (45.4%) of eligible participants had missing data on ≥1 confounders. We imputed missing data for confounders using multiple imputations through chained equations to form 10 imputed datasets employing a predictive mean matching methodology.44Ten iterations assured data output stability, and 10 imputations guaranteed the accuracy of pooled variable effect size estimates.
We also presented results from non-imputed, complete case analyses (n=309 503) and compared these with our main imputed analyses. In accordance with data regulation guidelines, we redacted any outcome or variable with five or fewer values, or any data that could be used to derive these redacted values.
Patient and public involvement
This study used anonymised data from national registries, focusing on the analysis of existing information without necessitating new direct contact with participants. Despite the inherent limitations of our approach, including the lack of allocated funding for direct patient involvement, we recognised the importance of incorporating public perspectives into our research. While direct involvement in designing the research question, the outcome measures, and study implementation was not feasible, our motivation was strongly influenced by discussions with members of the public and specific concerns highlighted by patients about maternal morbidity rates. These conversations, along with a priority setting exercise by the James Lind Alliance on the impact of epidural analgesia during labour, shaped our research focus.16Although formal patient and public involvement was not integrated into the study’s design, we engaged with the public by inviting a patient to review our manuscript, whose insights contributed to refining our presentation and interpretation of findings.
Results
Study population and baseline characteristics
After exclusions, 567 216 women presented in labour in Scotland between 1 January 2007 and 31 December 2019 (table 1, see supplementary eFigure 1), of whom 39 601 (7.0%) delivered prematurely. Epidural analgesia was administered to 125 024 (22.0%) women. Of the 77 439 women with a medical indication for treatment, epidural analgesia was administered to 19 061 (24.6%) (see supplementary eFigure 1). Mothers who received epidural analgesia during labour were more likely to be primiparous, be from a less deprived socioeconomic group, be a former or non-smoker, be undergoing labour induction, give birth in an obstetric unit, and have a multiple birth, ≥1 comorbidities, a higher birthweight baby, and operative delivery (table 1). SMM occurred in 2412 women (0.43%) and was more commonly observed in those with a medical indication for epidural analgesia (819/77 439, 1.06%) and in women delivering preterm (581/39 601, 1.47%) (table 2and supplementary eTable 4).
Temporal trends in SMM
The overall incidence of SMM (irrespective of epidural analgesia status) did not change annually during the study period (relative risk per year 1.00 (95% confidence interval (CI) 0.99 to 1.02, P=0.7) (see supplementary eTables 5 and 6).
Association between epidural analgesia and SMM and related outcomes
Epidural analgesia during labour was associated with a reduction in SMM (adjusted relative risk 0.65, 95% CI 0.50 to 0.85), SMM plus critical care admission (0.46, 0.29 to 0.73), and respiratory morbidity (0.42, 0.16 to 1.15), although the last of these had limited power with wide confidence intervals (table 2).
In subgroup analyses, epidural analgesia was associated with a greater risk reduction in SMM in women with a medical indication for epidural analgesia (0.50, 0.34 to 0.72) versus those without a medical indication (0.67, 0.43 to 1.03); likelihood ratio of difference between subgroups, P<0.001 (table 3). Similarly, we found a greater risk reduction in SMM in women receiving epidural analgesia and delivering prematurely (0.53, 0.37 to 0.76) compared with women delivering at term or post term (1.09, 0.98 to 1.21); likelihood ratio of difference between subgroups, P<0.001, and in women with a medical indication and delivering prematurely (0.36, 0.24 to 0.53) compared with women with no medical indication and delivering at term or post term (1.14, 0.99 to 1.31); likelihood ratio of difference between subgroups, P<0.001 (table 3). The reduced risk of SMM with epidural analgesia seen in the whole cohort and in women with a medical indication for epidural analgesia was more pronounced as gestational age at birth decreased (fig 1).
Robustness of results and sensitivity analysis
E-values suggest our findings are not likely to be solely due to residual confounding (see supplementary eTable 7). Consistent results were observed in analyses limited to births in obstetric units with 24 hour access to obstetric and anaesthetic services (see supplementary eTables 8 and 9). Epidural analgesia was associated with reduced risk of SMM across all categories of preterm birth: extremely preterm (<28 weeks) gestations (0.36, 0.21 to 0.62), very preterm (28 to <32 weeks) gestations (0.48, 0.32 to 0.72), and moderate to late preterm (≥32 to 37 weeks) gestations (0.71, 0.56 to 0.88) (see supplementary eTable 10). This effect was irrespective of whether the reason for the preterm birth was spontaneous or iatrogenic (see supplementary eTable 10). Similar results were seen in both complete case and unimputed datasets (table 2,table 3, and supplementary eTable 11).
Discussion
In this population based cohort study encompassing 567 216 births in Scotland, epidural analgesia during labour was associated with a 35% risk reduction in SMM and 54% risk reduction in SMM plus critical care admission across all births. These benefits were more pronounced in women with a medical indication for epidural analgesia compared with those without an indication, and in those who delivered preterm compared with those who did not deliver preterm. Women with a higher pre-existing morbidity risk, stemming from either medical or obstetric conditions, spontaneous preterm delivery, or conditions necessitating iatrogenic preterm delivery, face increased risks of adverse events related to their chronic comorbidities, diseases related to preterm birth, haemorrhage, and surgical complications.4454647Our results suggest that these risks might be effectively mitigated by use of epidural analgesia.
Comparison with other studies
Our findings enhance the limited existing literature,89and respond to a research priority identified by patients and clinical providers.16Given that mode of birth is unknown when the decision to use labour epidural analgesia is made, and that around 15% of SMM events will occur in the postnatal period,10our study provided a more accurate portrayal of the clinical situation than in the previous US study, which did not include postnatal SMM.8As few known modifiable risk factors for SMM exist, and as the incidence of SMM continues to rise, with this increase contributing to the global plateauing of maternal mortality, our findings provide a means to reduce SMM and maternal mortality.1445That a large portion of women in whom epidural analgesia would generally be considered medically indicated did not receive one highlights a potential area for intervention.
The latest UK Mothers and Babies: Reducing Risk through Audits and Confidential Enquiries report underlines the uneven distribution of maternal morbidity and mortality, with deaths in women from black ethnic groups four times higher than in women from white ethnic groups, and the mortality risk twofold higher in women from the most deprived areas compared with least deprived areas.13Recent UK based studies have shown that women from ethnic minority groups and socioeconomically deprived areas are less likely to receive epidural analgesia, although the underlying reasons remain unclear.1415
Policy implications
Misinformation and misconceptions about epidural analgesia, particularly the effect on delivery mode and neonatal wellbeing, might contribute to inequities in epidural use during labour.48Existing research, including a Cochrane review of 40 randomised controlled trials and two Scottish population based studies, found that epidural analgesia was not causally linked to an increased risk of operative births and did not adversely affect neonatal or long term childhood outcomes, but these studies did not examine SMM or mortality.114950Although a randomised controlled trial would be ideal for confirming our results, the global prevalence of epidural analgesia during labour, its established safety, and the urgency of this research make a strong case for applying our results in clinical practice. Our study offers valuable insights that can potentially reduce inequalities in maternal healthcare by providing robust evidence for individualised, person centred, and informed decision making. To maximise this effect, it is crucial to develop strategies that ensure women from diverse backgrounds, including those in preterm labour, have access to comprehensive information and support about the use of epidural analgesia.
The mechanism by which epidural analgesia could diminish SMM is likely multifaceted, involving closer medical oversight and haemodynamic monitoring, established intravenous access, fluid administration, blunting of physiological stress responses to labour, avoidance of the need for spinal or general anaesthesia for caesarean section, and faster escalation to definitive obstetric interventions. In essence, using epidural analgesia during labour alters the care pathway to one that enhances the capacity to manage adverse events. From these data it is not possible to separate the direct influence of epidural analgesia from the accompanying comprehensive care package. In the UK, implementing epidural analgesia inherently includes this bundle of enhanced care, which could be particularly advantageous for women at heightened risk of SMM.
Strengths and limitations of this study
Our study was undertaken in a large, unselected population cohort of linked mother-infant data over a 13 year period reflecting contemporary obstetric and anaesthetic practices. We adjusted for confounding variables that were defined before analyses started, used imputation for missing confounder data, and showed consistency between the confounder imputed and complete case analyses. The E-value suggested that bias due to unknown confounders was unlikely to have made a major contribution to our results, and additional sensitivity analyses support the robustness of our findings. We had too few cases of respiratory morbidity to provide precise estimates, highlighting the need for larger studies to explore this outcome. As other forms of anaesthesia may be used in more urgent clinical scenarios, such as major haemorrhage, this could have resulted in more favourable results in the epidural analgesia group. Nevertheless, our analysis aimed to reflect the divergent management pathways and outcomes depending on womens’ choice about epidural analgesia during labour. For instance, a woman with a functioning epidural is potentially more likely to undergo an assisted vaginal delivery than a caesarean section. In line with other UK based studies, we only accounted for postpartum haemorrhage when it necessitated critical care admission, potentially underestimating this morbidity. As a result, our findings might have been attenuated towards the null and strengthens our confidence in the effect seen between epidural analgesia and SMM. Our study excluded elective caesarean births, acknowledging that women undergo this procedure before labour starts and therefore by definition will not receive epidural analgesia during labour. While this analysis was not within our study’s scope, we recognise the importance of investigating anaesthetic choices in elective caesarean deliveries in future research, given the different risk profiles. We used widely validated area deprivation indices to indicate socioeconomic status.32However, we acknowledge that this may not always reflect individual socioeconomic positions (eg, well educated or wealthy women living in an area with a high deprivation score). As the population of Scotland is predominantly white, our results might not be generalisable to more diverse populations; however, the similarity of our results to those of a US study with an ethnically diverse population increases confidence in our findings.8We lacked data on systemic opioid use and maternal haemodynamics, both of which would have been valuable in elucidating the mechanisms by which epidural analgesia during labour could reduce the risk of SMM. Additionally, we did not have information on individual care providers and factors influencing maternal decision making about epidural analgesia. These aspects are crucial for understanding and dealing with potential barriers to the adoption of epidural analgesia during labour.
Conclusions
Our analysis of 567 216 births in Scotland indicates that epidural analgesia during labour is associated with a 35% risk reduction in SMM in all women. This effect was more pronounced in specific groups, showing a 50% risk reduction in women with predefined risk factors, and a 47% reduction in those delivering prematurely. These findings substantiate the current practice of recommending epidural analgesia during labour to women with known risk factors, underscores the importance","Objectives: To determine the effect of labour epidural on severe maternal morbidity (SMM) and to explore whether this effect might be greater in women with a medical indication for epidural analgesia during labour, or with preterm labour.
Design: Population based study.
Setting: All NHS hospitals in Scotland.
Participants: 567 216 women in labour at 24+0 to 42+6 weeks’ gestation between 1 January 2007 and 31 December 2019, delivering vaginally or through unplanned caesarean section.
Main outcome measures: The primary outcome was SMM, defined as the presence of ≥1 of 21 conditions used by the US Centers for Disease Control and Prevention (CDC) as criteria for SMM, or a critical care admission, with either occurring at any point from date of delivery to 42 days post partum (described as SMM). Secondary outcomes included a composite of ≥1 of the 21 CDC conditions and critical care admission (SMM plus critical care admission), and respiratory morbidity.
Results: Of the 567 216 women, 125 024 (22.0%) had epidural analgesia during labour. SMM occurred in 2412 women (4.3 per 1000 births, 95% confidence interval (CI) 4.1 to 4.4). Epidural analgesia was associated with a reduction in SMM (adjusted relative risk 0.65, 95% CI 0.50 to 0.85), SMM plus critical care admission (0.46, 0.29 to 0.73), and respiratory morbidity (0.42, 0.16 to 1.15), although the last of these was underpowered and had wide confidence intervals. Greater risk reductions in SMM were detected among women with a medical indication for epidural analgesia (0.50, 0.34 to 0.72) compared with those with no such indication (0.67, 0.43 to 1.03; P<0.001 for difference). More marked reductions in SMM were seen in women delivering preterm (0.53, 0.37 to 0.76) compared with those delivering at term or post term (1.09, 0.98 to 1.21; P<0.001 for difference). The observed reduced risk of SMM with epidural analgesia was increasingly noticeable as gestational age at birth decreased in the whole cohort, and in women with a medical indication for epidural analgesia.
Conclusion: Epidural analgesia during labour was associated with a 35% reduction in SMM, and showed a more pronounced effect in women with medical indications for epidural analgesia and with preterm births. Expanding access to epidural analgesia for all women during labour, and particularly for those at greatest risk, could improve maternal health.
"
Exposure to antibiotics during pregnancy or early infancy and risk of neurodevelopmental disorders,"Introduction
Neurodevelopmental disorders are emerging as a critical public health problem among children, given the long lasting effect of these disorders on individuals’ lives and society.12The global prevalence of neurodevelopmental disorders, including autism spectrum disorders, has been steadily increasing.345Although the causes of neurodevelopmental disorders are not yet fully understood, several potential risk factors include advanced maternal age, preterm birth, and environmental factors.16In recent years, a growing body of evidence is also highlighting that alterations in the microbiome may play a significant role in the development of neurodevelopmental disorders.67
Antibiotics, which are known to disturb the composition of the microbiome, are commonly used during pregnancy and infancy to treat infections.8910Given that fetal and early life is the critical period for the extensive development of the gut microbiome, concern is growing about antibiotic use during these periods.11Few epidemiological studies have investigated an association between prenatal or infant antibiotic use and neurodevelopmental disorders such as autism spectrum disorder and epilepsy.12131415However, the evidence remains limited and inconclusive, possibly owing to insufficient control for confounding in some studies. Because infection, in both severe and less severe forms, has been linked with neurodevelopmental consequences, confounding by indication is of particular concern in investigating the role of antibiotics in neurodevelopmental disorders.161718In addition, familial confounding presents another potential source of bias, as the pathophysiology of neurodevelopmental disorders involves both genetic and environmental factors. Therefore, comprehensive investigation on this topic based on real world data is warranted, while controlling for these potential confounding, as pregnant women and infants are generally excluded from randomised trials.
In this study, we aimed to evaluate whether exposure to antibiotics during pregnancy or early infancy is associated with subsequent development of autism spectrum disorder, intellectual disorder, language disorder, and epilepsy in children by using a large nationwide database in South Korea. Although intellectual disorder and language disorder are recognised as another common type of neurodevelopmental disorder, no study to date has thoroughly evaluated whether these disorders are associated with antibiotic use. To account for confounding by indication and unmeasured familial factors, we implemented two designs: a propensity score matched cohort study and a sibling analysis.
Methods
Data source and study design
We conducted a nationwide retrospective cohort study using the National Health Insurance Service (NHIS) mother-child linked database from 2008 to 2021.19The NHIS database contains claims data of the entire population (>50 million) in South Korea, and construction of the mother-child linkage has been described previously.2021This database comprises comprehensive information on socioeconomics, healthcare utilisation (for example, diagnosis, drug prescription, and medical procedures) from both inpatient and outpatient settings, health examination records for mothers and infants, and vital statistics data. We estimated the start of pregnancy on the basis of a previously validated algorithm developed using administrative data.22This study followed the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) reporting guideline.23
Study cohorts
We constructed two separate cohorts among all children born between 1 April 2009 and 31 December 2020 (fig 1). The first cohort (hereafter referred to as the pregnancy cohort), for analysing antibiotic exposure during pregnancy, consisted of all children after exclusion of children with chromosomal abnormalities and those whose mothers took antibiotics one month before the start of pregnancy but not during pregnancy to avoid classifying those who actually took their antibiotics or had available antibiotics after the start of pregnancy in the unexposed group. The second cohort (hereafter referred to as the infant cohort), for analysing antibiotic exposure during early infancy, comprised all children after exclusion of those who died during the first six months of life, children with chromosomal abnormalities, and those who received a diagnosis of one of the study outcomes during the first six months of life.
Antibiotic exposure
We defined exposure as the presence of one or more prescriptions for a systemic antibiotic (Anatomical Therapeutic Chemical classification system code J01). For the pregnancy cohort, we defined the exposed group as children whose mothers had at least one prescription for antibiotics at any time during pregnancy and the unexposed group as children whose mothers had no history of prescriptions for antibiotics from 30 days before pregnancy to the end of the pregnancy. For the infant cohort, we defined the exposed group as children who had at least one prescription for antibiotics during the first six months of life and the unexposed group as children with no history of prescriptions for antibiotics during this period.
Outcomes
Study outcomes were autism spectrum disorder (ICD-10 (international classification of diseases, 10th revision) code F84), intellectual disorder (F70-F73), language disorder (F80), and epilepsy (G40, G41, F803, R56, and prescription for antiepileptic medication) (supplementary table A). Autism spectrum disorder, intellectual disorder, and language disorder are typically diagnosed by psychiatrists or paediatricians in South Korea on the basis of clinical evaluation and standardised tests and recorded with ICD-10 codes. A previous study found a high positive predictive value of diagnostic codes in Korea’s claims data,24and our definition of epilepsy has been validated with high positive predictive values especially among children (1-4 years old: 84.7%; 5-9 years old: 92.9%).25We followed up all children from birth (pregnancy cohort) or six months after birth (infant cohort) until the occurrence of an outcome, death, or the end of the study period (31 December 2021), whichever came first.
Covariates
For the analyses of both pregnancy and infant cohorts, we considered a broad range of potential confounders including demographics (for example, maternal age, insurance type, and income level at delivery), indications for antibiotic use (for example, respiratory infection, urinary infection), infection related healthcare utilisation as a proxy for severity of infection (for example, number of hospital visits for infection diagnoses, number of distinct infection diagnoses), maternal conditions (for example, attention deficit/hyperactivity disorder, autoimmune diseases, diabetes mellitus), medication use (for example, paracetamol, antidepressants, antipsychotics), obstetric conditions (for example, nulliparity, multiple gestations), measures of healthcare utilisation (for example, obstetric comorbidity index),2627smoking status, and body mass index. For the analyses of the infant cohort, we additionally considered maternal use of antibiotics, sex of child, preterm birth, caesarean section, birth weight, and type of feeding. We considered maternal antibiotic use during pregnancy as a covariate in the infant cohort as it may influence the infant’s exposure and the study outcomes, whereas we did not consider maternal antibiotic use during early infancy as a covariate in the pregnancy cohort as the infant’s exposure occurs after the prenatal exposure. Details of the covariates and their assessment windows are presented in supplementary table A.
Statistical analyses
To control for potential confounding, we did one-to-one propensity score matching using the greedy nearest neighbour matching algorithm without replacement.28We estimated the propensity score, or the probability of antibiotic exposure in our study, by using a logistic regression model based on all pre-defined covariates except for smoking status and body mass index given their high proportion of missing data. We evaluated the distribution of covariates between exposed and unexposed groups by using the standardised mean difference, considering a value less than 0.1 on the absolute scale to indicate well balanced groups. In the propensity score matched cohort, we calculated the incidence of outcomes per 1000 person years and absolute rate differences with 95% confidence intervals on the basis of Poisson regression. We also estimated hazard ratios with 95% confidence intervals by using Cox proportional hazard regression models, while applying robust standard errors to account for data clustering. In each outcome model, we additionally adjusted for covariates that were unbalanced after propensity score matching. We considered a P value of <0.05 to be statistically significant. We used SAS Enterprise Guide, version 7.1, for data management and analysis.
Sibling analyses
Given the concerns about unmeasured confounding from familial factors, we additionally did sibling controlled analyses by restricting the population to children who had at least one sibling during the study period. Sibling analyses, by design, can control for time invariant shared factors at the family level. Using a stratified Cox proportional hazard regression model, we calculated hazard ratios with 95% confidence intervals for each outcome that were adjusted for all pre-defined covariates and birth order of sibling. Only siblings with discordant exposure and outcome status would contribute to the estimates; thus, informative pairs are reported. Additional details on the sibling analyses, including the assumption test for carryover effects, are available in supplementary appendix 1.29
Subgroup and sensitivity analyses
We did several subgroup analyses. Firstly, we analysed the risk of study outcomes by antibiotic groups (broad spectrum or narrow spectrum antibiotics; supplementary table B), given that broad spectrum antibiotics are reported to have a greater effect than narrow spectrum antibiotics on the gut microbiota. Secondly, we assessed whether the risk differed by specific timing of exposure (first, second, third trimester for pregnancy cohort; 0 to <2, 2 to <4, 4 to <6 months for infant cohort). Thirdly, as boys are more likely to receive a diagnosis of neurodevelopmental disorders than girls, we evaluated the association by sex of the infants.30Fourthly, we analysed the duration-response relation and the risk of the top three most frequently prescribed antibiotic classes in each cohort. Lastly, we investigated the risk of study outcomes stratified by birth year (minimum follow-up period). As a post hoc analysis, we also estimated the risk of joint antibiotic exposure during pregnancy and early infancy.
We did multiple sensitivity analyses to test the robustness of our main findings. We accounted for potential exposure misclassification, outcome misclassification, and residual confounding, and we also evaluated whether our main findings from the propensity score matched cohorts were robust across different propensity scoring methods. Detailed information on subgroup and sensitivity analyses is given in supplementary appendices 2 and 3.
Patient and public involvement
The NHIS privacy policy and our institutional policy were not equipped to support patients or members of the public as partners. As study investigators, we honoured this policy. As a result, no patients were involved in this research although we are grateful for their data, which made the research possible.
Results
Cohort characteristics
We identified 3 665 246 children eligible for the pregnancy cohort, of whom 1 649 300 (45.0%) were exposed to antibiotics in utero. For the infant cohort, we identified 3 944 731 children, of whom 1 976 472 (50.1%) were exposed to antibiotics during early infancy (fig 1). After one-to-one propensity score matching, we identified 980 872 and 804 887 pairs for the pregnancy and infant cohorts, respectively. We observed substantial baseline differences between the exposed and unexposed groups (for example, indication) before propensity score matching, but all characteristics were well balanced (absolute standardised mean difference <0.1) after matching in both cohorts (table 1) All characteristics before and after propensity score matching are shown in supplementary tables C and D.
Antibiotic exposure during pregnancy and early infancy: propensity score matched analyses
The median follow-up time for each outcome ranged around seven years in both pregnancy and infancy cohorts (supplementary table E). For the pregnancy cohort, the rate difference was 0.15 per 1000 person years for autism spectrum disorder (exposed versus unexposed: 1.25v1.10), 0.09 for intellectual disorder (0.62v0.53), 0.21 for language disorder (2.59v2.38), and 0.09 for epilepsy (1.12v1.03) (table 2). Exposure to antibiotics during pregnancy was associated with increased risks of all study outcomes, with adjusted hazard ratios ranging from 1.08 for epilepsy to 1.17 for intellectual disorder (fig 2, top).
We observed similar incidence rates and rate differences in the infant cohort (autism spectrum disorder: 0.05 (1.25v1.20); intellectual disorder: 0.13 (0.68v0.55); language disorder: 0.13 (2.71v2.58); epilepsy: 0.22 (1.01v0.80)) (table 2). Likewise, antibiotic exposure during early infancy was associated with increased risks of all study outcomes, with adjusted hazard ratios ranging from 1.04 for autism spectrum disorder to 1.27 for epilepsy (fig 2, top).
Antibiotic exposure during pregnancy and early infancy: sibling analyses
The sibling analyses comprised 843 412 and 1 082 417 exposure discordant children for the pregnancy and infant cohorts, respectively. Their baseline characteristics and informative exposure-outcome discordant pairs are shown in supplementary tables F and G. In contrast to the propensity score matched analyses, we found that all estimates were attenuated in the sibling analysis and showed no substantial associations (hazard ratio for autism spectrum disorder 1.06, 95% confidence interval 1.01 to 1.12; intellectual disorder 1.00, 0.93 to 1.07; language disorder 1.05, 1.02 to 1.09; epilepsy 1.03, 0.98 to 1.08) in the pregnancy cohort (fig 2, bottom). Similarly, in the infant cohort, we observed no associations (hazard ratio for autism spectrum disorder 1.00, 0.96 to 1.03; intellectual disorder 1.07, 0.98 to 1.15; language disorder 1.04, 1.00 to 1.08), except with epilepsy, which remained slightly increased (1.13, 1.09 to 1.18) (fig 2, bottom). Additional results from the sibling analyses are described in supplementary appendix 1 and tables H-K.
Subgroup and sensitivity analyses
We observed no notable differences in the association between antibiotic exposure during pregnancy or early infancy and all outcomes in the subgroup analyses, except for a slightly higher risk among infants who used antibiotics in the first two months of life and those who used antibiotics for longer than 15 days (fig 3;fig 4; supplementary tables L-S). In the post hoc analyses, we observed increased risks of autism spectrum disorder and epilepsy among children who were exposed to antibiotics during both pregnancy and early infancy (supplementary table T).
Sensitivity analyses yielded estimates generally consistent with those from the main findings (supplementary tables U-X). Additional analyses to account for potential outcome misclassification also showed similar results (supplementary appendix 3 and table Y). The E value for the point estimate of epilepsy associated with antibiotic exposure during early infancy was 1.51.
Discussion
In this large nationwide cohort study, exposure to antibiotics during pregnancy or early infancy was not associated with increased risks of autism spectrum disorder, intellectual disorder, or language disorder in children. Although we observed small increased risks in the overall population, these associations were all attenuated and pointed towards the null in the sibling analysis, suggesting that the observed associations may have been confounded by shared familial factors. However, the risk of epilepsy associated with antibiotic use during infancy remained slightly elevated even after we accounted for the shared familial factors. Results from various subgroup and sensitivity analyses were largely consistent with our main findings, except for slightly increased risks observed among children who used antibiotics during very early life and those who used antibiotics for more than 15 days.
Comparison with other studies
A systematic review reported that available data on the association between prenatal or infant antibiotic exposure and autism spectrum disorder are conflicting and inconclusive.12A later cohort study based on the Swedish population found an increased risk of autism spectrum disorder after use of antibiotics both during pregnancy and during infancy, with an odds ratio of 1.16 (95% confidence interval 1.09 to 1.23) and 1.46 (1.38 to 1.55), respectively.13However, the authors noted that confounding by indication or genetics could not be ruled out. Likewise, a recent meta-analysis showed that an increased risk of autism (pooled odds ratio 1.13, 1.07 to 1.21) associated with antibiotic use in early life was no longer apparent when only the sibling matched studies were pooled (1.04, 0.97 to 1.11).31In line with previous evidence, our study further supports a lack of association between antibiotic use during pregnancy or infancy and autism spectrum disorder after control for many confounders including indications and genetic/familial factors.
Our study did not observe an association between prenatal or infant antibiotic use and intellectual disorder or language disorder, which to our knowledge have not been investigated to date. Given that intellectual disorder and language disorder are increasingly recognised as another common type of neurodevelopmental disorder,32further studies on the evaluation of these disorders are needed. Meanwhile, on the basis of the estimates observed in the comparison of the siblings, our study suggests no substantial association between prenatal or infant antibiotic use and intellectual disorder or language disorder.
By contrast, antibiotic use during infancy was modestly associated with epilepsy, even after control for familial factors in the sibling analysis, which indicates that familial confounding could not fully explain the observed increased risk. This increased risk was consistently observed across all subgroup analyses. Although several studies have investigated the risk of seizure or epilepsy in children associated with exposure to antibiotics during pregnancy, no previous studies have assessed this risk after antibiotic use in infants.141533One hypothesised mechanism explaining the link between antibiotics and epilepsy is interference with gut microbiota, which can influence the interaction between the gut and the nervous system.34In support, emerging evidence from case reports are indicating intestinal dysbiosis in patients with epilepsy.35Because gut microbiota compositions are known to develop extensively during early life, antibiotic use during infancy may play a role in subsequent development of epilepsy.11Another plausible mechanism is central nervous system toxicity of antibiotics. Several antibiotics have been suspected to provoke acute seizure, and animal studies have also shown that β-lactam associated seizures may arise from its γ-aminobutyric acid A receptor binding property.3637Although the acute symptomatic seizures provoked by antibiotics do not necessarily result in chronic epilepsy, a potential negative effect on the developing brain may exist. Nevertheless, our study is the first epidemiological study to have investigated the association between antibiotics and epilepsy in infant populations and report increased risk; thus, additional studies are needed to confirm our findings. While awaiting future studies, the small but potential risk of epilepsy should be taken into account when weighing the benefits and risks of using antibiotics in infants.
Strengths and limitations of study
Antibiotics are increasingly used during pregnancy and infancy, with parallel concerns indicating that alterations in the microbiome may be associated with neurodevelopmental disorders; thus, our study tackles an important question. The major concern in investigating the potential role of antibiotics in neurodevelopmental outcomes is confounding by indication and familial factors. To minimise the possibility of these confounding, we included a wide range of covariates in the propensity score model, such as indications and related healthcare utilisation (for example, proxies for the severity of indication), and also implemented sibling designs to disentangle the effects of underlying infection and familial factors. Another strength of our study is the large sample size, which enabled us to have sufficient statistical power and also to do various clinically relevant subgroup analyses. In addition, by using the nationwide longitudinal database, adequate follow-up was possible to identify the risk of neurodevelopmental disorders along with no risk of selection or recall bias.
This study also has several limitations. Firstly, exposure misclassification is possible as we defined exposure on the basis of prescription of an antibiotic not the actual use. Secondly, outcome misclassification is possible. Although the algorithm for defining epilepsy in our study has been validated with high positive predictive values in young children,25other outcomes have not been validated. Thus, we did a sensitivity analysis in which we redefined the outcomes by incorporating data from Korean Developmental Screening Test for Infants and Children,38a validated tool to detect neurodevelopmental disorders in Korea, and the results were consistent with those of our main analyses (supplementary appendix 3). Thirdly, although we considered a broad range of covariates, we cannot rule out the possibility of residual confounding. For instance, paternal characteristics (for example, paternal age and comorbidities), which may be associated with the development of neurodevelopmental disorders, were unavailable in our database. The data on maternal smoking status and body mass index were also incomplete and thus not included in the propensity score model or adjustment. Fourthly, the indicators that we included in our study as proxies for the severity of infection are not direct measures of severity. The inclusion of the number of inpatient visits or emergency department visits with infection diagnoses and the number of distinct infection diagnoses was based on the assumption that the inpatient and emergency department visits typically necessitate a higher level of medical care (for example, close monitoring and intensive treatment) and that multiple infections suggest a greater level of severity compared with a single infection. However, they do not provide a direct assessment of the severity itself; thus, future studies incorporating direct measures of severity would be valuable. Fifthly, although sibling analyses are useful in controlling for familial factors that are not easily captured in the administrative database, time variant confounders are still of concern. To tackle this concern, we additionally adjusted for time variant characteristics; however, potential residual confounding from non-shared factors may still exist. Sibling comparison designs are also susceptible to carryover effects29; however, our analyses ruled out potential types of carryover effects. Lastly, as our study period included the early covid-19 era (2020-21), potential information bias on exposure, outcome, and covariate assessment due to delayed healthcare encounters may exist during this period.
Implications
In this study, the prevalence of antibiotic use during pregnancy and early infancy was 45.0% and 50.1%, respectively. Although antibiotics are essential for treating bacterial infections, concern is growing about their inappropriate use, which can contribute to antibiotic resistance.3940Many countries, including South Korea and the UK, are actively working to reduce unnecessary prescription of antibiotics, especially for respiratory tract infections.414243Despite the fact that many respiratory tract infections are viral and do not require antibiotics, studies have shown a substantial number of antibiotic prescriptions for such cases.4445The challenge lies in the difficulty of distinguishing between bacterial and viral respiratory infections in primary care settings, a limitation also present in this study.46Nevertheless, a considerable number of antibiotics were prescribed with respiratory infections as the primary diagnosis during pregnancy and infancy in our study. Furthermore, our additional analysis indicates that 117 572 pregnancies and 742 823 infants were treated with antibiotics for 15 days or longer for respiratory tract infections, of which 21 118 pregnancies and 167 404 infants had no other diagnoses of infection. Of note, the recommended treatment durations for most respiratory tract infections are below 15 days.44Overall, clinicians should adhere to the recommended guidelines when prescribing antibiotics to pregnant women and infants to improve antibiotic stewardship.
Our study suggested no substantial association between prenatal or infant antibiotic exposure and autism spectrum disorder, intellectual disorder, and language disorder, but elevated estimates observed in several subgroups warrant further attention. Specifically, long term use of antibiotics (≥15 days) and antibiotic use during the first two months of life showed slightly increased risk even after control for familial factors. Moreover, in the post hoc analysis, antibiotic exposure both during pregnancy and during infancy was associated with autism spectrum disorder and epilepsy. Although residual confounding due to unfavourable health conditions cannot be completely ruled out, further investigation may be warranted. In the meantime, clinicians should carefully weigh the benefits and potential harms of antibiotics when prescribing antibiotics to pregnant women and infants.
Conclusions
In this large cohort study, exposure to antibiotics during pregnancy or early infancy was not associated with an increased risk of autism spectrum disorder, intellectual disorder, and language disorder in children. However, elevated risks were observed in several subgroups such as antibiotics use during very early life and long term antibiotic use, which warrants attention and further investigation. Moreover, antibiotic use during early infancy was modestly associated with epilepsy, even after control for indications and familial factors. When prescribing antibiotics to pregnant women and infants, clinicians should carefully balance the benefits of their use against potential risks.
","Objective: To evaluate the association between antibiotic use during pregnancy or early infancy and the risk of neurodevelopmental disorders in children.
Design: Nationwide population based cohort study and sibling analysis.
Setting: Korea’s National Health Insurance Service mother-child linked database, 2008-21.
Participants: All children live born between 2009 and 2020, followed up until 2021 to compare those with and without antibiotic exposure during pregnancy or early infancy (first six months of life).
Main outcomes measures: Autism spectrum disorder, intellectual disorder, language disorder, and epilepsy in children. After 1:1 propensity score matching based on many potential confounders, hazard ratios with 95% confidence interval were estimated using Cox proportional hazard models. A sibling analysis additionally accounted for unmeasured familial factors.
Results: After propensity score matching, 1 961 744 children were identified for the pregnancy analysis and 1 609 774 children were identified for the early infancy analysis. Although antibiotic exposure during pregnancy was associated with increased risks of all four neurodevelopmental disorders in the overall cohort, these estimates were attenuated towards the null in the sibling analyses (hazard ratio for autism spectrum disorder 1.06, 95% confidence interval 1.01 to 1.12; intellectual disorder 1.00, 0.93 to 1.07; language disorder 1.05, 1.02 to 1.09; and epilepsy 1.03, 0.98 to 1.08). Likewise, no association was observed between antibiotic exposure during early infancy and autism spectrum disorder (hazard ratio 1.00, 0.96 to 1.03), intellectual disorder (1.07, 0.98 to 1.15), and language disorder (1.04, 1.00 to 1.08) in the sibling analyses; however, a small increased risk of epilepsy was observed (1.13, 1.09 to 1.18). The results generally remained consistent across several subgroup and sensitivity analyses, except for slightly elevated risks observed among children who used antibiotics during very early life and those who used antibiotics for more than 15 days.
Conclusions: In this large cohort study, antibiotic exposure during pregnancy or early infancy was not associated with an increased risk of autism spectrum disorder, intellectual disorder, or language disorder in children. However, elevated risks were observed in several subgroups such as children using antibiotics during very early life and those with long term antibiotic use, which warrants attention and further investigation. Moreover, antibiotic use during infancy was modestly associated with epilepsy, even after control for indications and familial factors. When prescribing antibiotics to pregnant women and infants, clinicians should carefully balance the benefits of use against potential risks.
"
Clinical and healthcare use outcomes after cessation of long term opioid treatment due to prescriber workforce exit,"Introduction
Chronic pain affects more than one in four American adults over 65 years old and is commonly managed using long term opioid treatment (LTOT).123However, the general shift away from prescribing opioids has meant that millions of patients in chronic pain are also being removed from LTOT, often at faster taper speeds than recommended by guidelines.4563789Many have raised concerns that poor adherence to guideline-suggested tapering may lead to undertreated pain, mental health crises, and suicide.1011
Work examining the association between tapering or discontinuation of LTOT and health outcomes has important limitations. Systematic reviews on the reduction or discontinuation of LTOT find little high quality evidence, although studies generally indicate improvement in pain and quality of life after discontinuation or tapering.1213By contrast, large scale observational studies find both increased and decreased risk for death or addiction related adverse events.71415161718192021These studies typically use statistical techniques adjusting for observable, but not unobservable, differences between users of LTOT who taper or discontinue versus those who do not, populations whose clinical profiles can diverge around the time of LTOT changes.32223For instance, a patient may be discontinued because prescribers suspect that patients are bordering on behavioral, substance use, or mental health disorders, whereas patients continued on LTOT are maintaining baseline. Without robust evidence that accounts for selection and confounding, clinical knowledge on the association between LTOT discontinuation and patient outcomes is incomplete, impeding both therapeutic management of LTOT and the development of opioid policy.
In this study, we investigated the evidence gap by leveraging prescriber exit from the workforce, a common event,242526as an external shock to prescribing patterns. Previous work found that primary care physician exit was associated with substantial shifts in patient prescribing patterns.2728We hypothesized that prescriber market exit would lead to an increase in discontinuation of LTOT unrelated to observed or unobserved patient clinical factors. Patients receiving LTOT who discontinued or tapered from opioids are likely different from those who have not tapered, therefore, we addressed selection bias and confounding by defining the exposure as prescriber workforce exit, an event plausibly not driven by clinical events leading to LTOT discontinuation.18This method of experimentation enabled testing for the independent effect of discontinuation of LTOT on patient outcomes.
Methods
Data source and study population
The cohort study used a 20% random sample of Medicare fee-for-service and Medicare Advantage beneficiaries from 1 January 2011 to 31 December 2018. Medicare is a public insurance program that enrolls 65 million Americans who are age 65 years or older, receiving social security disability income, or diagnosed with amyotrophic lateral sclerosis or end stage renal disease. Our primary study cohort captured all clinical and healthcare use outcomes for fee-for-service beneficiaries enrolled in fee-for service Medicare parts A, B, and D. For outcomes related to prescriptions, we also included Medicare Advantage beneficiaries with Medicare part D prescription claims. We excluded data with missing racial status accounting for less than 1% of the sample. Additionally, we excluded patients diagnosed with cancer at any point over the sample period because the role of LTOT may differ between cancer and non-cancer indications (see appendix figure 1 for cohort flow diagrams).
Identification of prescribers who exited Medicare
The main study exposure occurred when a prescriber stopped providing office based patient care or exited, as in the case of retirement or death. Patients attributed to such an exiting prescriber were considered exposed to this discontinuation. A prescriber’s exit date was defined as the last date the prescriber billed Medicare for an office based service with no subsequent services observed, as defined in prior research.2930Prescribers were considered exiting if they had at least one office visit 6-12 months before their last observed office visit and had a last office visit between 1 January 2012 and 31 December 2017. These restrictions allowed us to observe patients prescribed an opioid at least four quarters before and after prescriber workforce exit (fig 1). Patients attributed to prescribers with an exit date were considered exposed while those without prescriber exit were considered unexposed.
Study sample and LTOT definition
The study sample included Medicare beneficiaries receiving LTOT who were exposed and unexposed to prescriber exit. To be included, the beneficiary had to be at least 18 years old and continuously enrolled in Medicare. LTOT was defined as receipt of at least 60 days’ supply of opioids at a dosage of 25 daily morphine milligram equivalents or more on average per quarter for at least four consecutive quarters. The first four or more quarter period meeting this definition was the initial LTOT episode for patients.
Patients in the exposed group were limited to those with an LTOT episode beginning at least five quarters before prescriber exit (meaning the first quarter before exit was excluded when capturing eligible LTOT episodes) to avoid bias from anticipation of prescriber exit that could affect patterns of LTOT and subsequent outcomes (fig 1).3Unexposed patients were comprised of those meeting the definition for having an LTOT episode. Patients were attributed to the prescriber providing the plurality of opioid prescriptions over the initial four quarter LTOT episode.
Matching and exit date assignment
To control for observed patient differences, we used propensity score matching to match patients of exiting prescribers (exposed patients) to patients who did not lose their prescriber (unexposed patients). The propensity score, which estimates the likelihood that a given patient would be in the exposed group, was estimated using patient covariates measured the first year they began LTOT. We only matched patients who were unexposed and exposed with an initial LTOT episode in the same calendar year to account for nationwide changes in opioid prescribing over time (appendix methods 1).31
After propensity score matching, we assigned the patient who was unexposed to the same prescriber exit date of the matched patient who was exposed. For example, consider patient A who was exposed to LTOT in January 2011, subsequently losing their prescriber in August 2013. The closest propensity score match is patient B who was not exposed and began LTOT in January 2011 but did not lose their prescriber. We therefore assigned the exit date of August 2013 from exposed patient A’s prescriber to be the synthetic exit date for unexposed patient B. This assignment of exit dates to unexposed patients enabled us to model changes in exposed patients’ outcomes in response to the loss of a prescriber compared with observably similar unexposed patients before and after prescriber exit in a difference-in-differences design (fig 1).
Defining opioid discontinuation
Discontinuation from opioid treatment occurred when a patient had no resumption of opioid treatment at the end of the prescription’s days supplied for at least one year after discontinuation (see appendix figure 2 for the allocation of prescriptions, as well as by provider type). Discontinuation was defined as equal to one in the quarter of discontinuation and zero in the quarters before and after the discontinuation event.
Outcome measures
Our main outcomes were adverse clinical events that could plausibly be a clinical outcome in response to discontinuation of LTOT: all drug overdoses, mental health crises, opioid or alcohol withdrawal, gastrointestinal bleeding or kidney failure due to substitution to non-steroidal anti-inflammatory drugs, or all cause mortality. We measured all cause hospital use as admission to hospital or emergency department visits identified by claims in the inpatient file or outpatient claims. We categorized emergency department visits for pain by using claims with a primary diagnosis of pain (appendix table 1). Mental health crises included hospital visits with a primary diagnosis of depression, anxiety, or a suicide attempt (appendix table 1). We captured mortality using the Medicare beneficiary summary file. We also quantified beneficiaries’ annual rate of office visits to any type of provider, including specialist and primary care providers. Medicare spending included all charges listed on beneficiaries’ claims for the study period. For prescription outcomes, we used prescription data from Medicare part D (which also captures patients using Medicare Advantage) to estimate all filled prescription quarterly as well as mutually exclusive groups of opioid, buprenorphine, naloxone, and non-opioid prescription drugs.
Covariates
We collected information on patients’ age, gender, race or ethnicity, state of residence, dual eligibility for Medicaid and Medicare coverage, Medicare Advantage enrollment, disability as the original reason for Medicare enrollment, diagnosis of chronic non-cancer pain (appendix table 1), and morphine milligram equivalent daily dose in the period before prescriber exit, so none of the covariates was causally influenced by future exposure or outcomes.32We quantified patients’ opioid total morphine milligram equivalent, average daily dose, and the total number of days supplied during the first four consecutive quarters a patient met the definition of LTOT. We calculated the total morphine milligram equivalent and days supplied from prescriptions’ generic ingredients for opioid compounds.33We calculated the average daily dose of opioids as the total morphine milligram equivalent of opioids supplied divided by the total number of days supplied during the initial LTOT episode.
We classified whether the patient’s county of residence was rural or urban,34and included 27 chronic conditions classified following prior work: acquired hyperthyroidism, acute myocardial infarction, Alzheimer’s disease, Alzheimer’s disease and related senile dementia disorders, anemia, asthma, atrial fibrillation, benign prostatic hyperplasia, cataract, chronic kidney disease, chronic obstructive pulmonary disease, diabetes, depression, heart failure, glaucoma, hip or pelvic fracture, hyperlipidemia, hypertension, ischemic heart disease, osteoporosis, rheumatoid arthritis or osteoarthritis, stroke or transient ischemic attack, breast cancer, colorectal cancer, prostate cancer, lung cancer, endometrial cancer.3
Statistical analysis
We compared outcomes for matched exposed and unexposed patients receiving LTOT before and after prescriber exit in a difference-in-differences framework. The framework allowed us to estimate the average treatment effect of physician exit on exposed patients. The difference-in-differences design required two key assumptions in our context. We first assumed that outcomes would trend similarly for unexposed and exposed patients in the absence of treatment. We visually tested this assumption infigure 1andfigure 2by observing whether outcomes for exposed and unexposed patients moved in parallel before treatment (visual inspection of pre-trends is another advantage of propensity score matching exposed and unexposed groups).We then assumed the exit of a patient’s assigned prescriber was independent of baseline patient outcomes.Table 1shows that matching unexposed and exposed patients through a propensity score match effectively limited baseline differences between exposed and unexposed patients, supporting this assumption.
We used linear regression at the level of quarter per patient to estimate a set of interaction terms between indicators for being an exposed patient and indicators for eight quarters relative to prescriber exit (four before exit and four after exit; details in appendix methods 3). The interaction terms describe the mean differential change in the outcome between unexposed and exposed patients by quarter relative to prescriber exit, using quarter −5 as the baseline period. All regression models also included patient and prescriber fixed effects (except for the outcome of mortality, which only contained prescriber fixed effects; appendix methods 3)2935and clustered standard errors at the matched level of the prescriber pair.30The use of fixed effects controlled for time invariant differences among patients and prescribers, such as baseline age, race, sex, living in a rural area, reason and type of Medicare enrollment, and baseline chronic conditions.
In a separate set of models, we used the same regression approach, but estimated the differential change between unexposed and exposed patients by year relative to prescriber exit (one year before exit, one year after exit). All regression analyses at the year level defined the period before prescribe exit as quarters −5 to −2 before a prescriber’s exit, excluding quarter −1 to account for potential anticipation (appendix methods 2)
We assessed the robustness of findings with several alternative specifications, including examining patients qualifying for Medicare because of social security disability income as a separate subgroup; repeating adjusted analyses over the entire sample but excluding Medicare Advantage patients; and including patient with cancer. We also examined changes in outcomes attributable to prescriber exit among patients not receiving LTOT with the same prescribers as patients who were receiving LTOT to determine the effect of the exit alone.30Additionally, we compared treatment effects by whether the patient lost a primary care physician or specialist. We tested the sensitivity of results by adjusting for multiple comparisons and compared treated patients who discontinued to treated patients who did not discontinue to quantify how the provider’s exit alone affected estimates. The 95% confidence intervals (CI) reflected 0.025 in each tail or P≤0.05. Analyses were performed in Stata, version 16 (StataCorp LLC).
Patient and public involvement
No patients were involved in setting the research question or the outcome measures, nor were they involved in developing plans to design or implement the study. No patients advised the interpretation or writing up of results. We used previously collected, de-identified data purchased from the Centers for Medicare and Medicaid Services that is restricted use. The institutional review board at the Harvard TH Chan School of Public Health approved the study, waived informed consent, and did not require us to involve patients and the public in the research process.
Results
Study sample
Before matching, the full study sample consisted of 80 158 exposed and 322 970 unexposed patients who received LTOT. Propensity score matching led to the exclusion of 32 079 exposed patients and 274 891 unexposed patients, leaving 48 079 patients assigned to 15 713 exiting prescribers (exposed) and 48 079 patients assigned to 28 150 stable prescribers (unexposed (appendix figure 1)). Propensity score matching improved balance on observable characteristics (appendix figures 3 and 4). After matching, patients in both exposed and not exposed groups had similar demographic and clinical characteristics, with almost all standardized mean differences of 0.05 or less (table 1). Comparisons of exiting versus stable prescribers and patients receiving versus not receiving LTOT are in appendix tables 2 and 3.
LTOT discontinuation and prescription outcomes
In the first quarter after prescriber exit, the opioid discontinuation rate for exposed patients receiving LTOT increased from 132 to 229 per 10 000 patients per quarter, compared with 97 to 100 per 10 000 unexposed patients (fig 2; adjusted difference of 1.22 percentage points ((95% CI 1.02 to 1.42), 160% increase from a baseline of 0.77%, appendix table 4). The adjusted yearly rate of discontinuation differentially increased 2.08 percentage points (1.66 to 2.50), or a 56% increase from the baseline 3.70% rate of discontinuation, for exposed patients relative to unexposed patients (table 2). In the overall post-exit period, the yearly number of opioid prescriptions declined by 1.01 prescriptions ((95% CI −1.11 to −0.91) or −6% off the baseline mean of 15.71 prescriptions), total days’ supply of opioids declined by 29 days ((95% CI −31 to −26), or −7% off the baseline mean of 414 days’ supply), and total morphine milligram equivalent of opioids declined by 5311 morphine milligram equivalent ((95% CI −5759 to −4864), or −15% off baseline mean of 35 336 morphine milligram equivalent). Further, non-opioid prescriptions declined by 0.66 ((95% CI −1.03 to −0.3) or −1% off baseline mean of 59.68 prescriptions), and buprenorphine prescriptions increased by 0.03 ((0.01 to 0.05), or 25% off baseline mean of 0.12 prescriptions). Naloxone prescriptions increased by 0.0025 ((0.0002 to 0.0047), or 40% off baseline mean of 0.0047 prescriptions).
Clinical and healthcare use outcomes
From quarter −5 to −1 before provider exit, unadjusted trends in outcomes were similar between exposed and unexposed patients receiving LTOT, supporting the parallel trends assumption needed for the differences-in-difference research design (fig 3). Some outcomes showed potential anticipation of an upcoming prescriber exit, most notably mortality, which motivated our exclusion of quarter −1 from year-level regressions.
In the first quarter after prescriber exit (denoted quarter 0), when discontinuation rates were highest, a significant increase was noted in the rate of suicide attempts (0.05 absolute percentage points (95% CI 0.01 to 0.09); 122% increase off baseline 0.04% suicide attempt), opioid or alcohol withdrawals (0.14 absolute percentage points (0.004 to 0.28); 50% increase off baseline 0.28% withdrawal rate), and emergency department visits or admissions to hospital (0.04 visits (0.01 to 0.06); 9% increase off baseline 0.45 visits), including emergency department visits with a pain diagnosis (0.011 visits (0.002 to 0.02); 10% increase off baseline 0.11 visits with a pain diagnosis), compared with patients who had a prescriber that exited the workforce and those who had a continuous prescriber (fig 3and appendix table 4). Mortality declined by −0.15 percentage points ((95% CI −0.29 to −0.02); 52% decline off base of 0.5%). Significant differences receded by quarter two except for emergency department visits and admissions to hospital, which were 0.02 visits ((0.001 to 0.04); increase of 4.4% from baseline mean) higher among patients with a provider exit but then receded by quarter three. No significant change in the rate of overdose was noted across all quarters of the study period.
In adjusted analyses averaging across the whole post prescriber exit period, a significant increase was noted in opioid or alcohol withdrawal (0.31 percentage points (95% CI 0.041 to 0.58), or 31% increase from baseline mean of 0.99%) and mental health crises (0.39 (0.08 to 0.69), or a 24% increase from a 1.6% baseline mean) comparing patients who had prescriber workforce exit versus those who did not. Additionally, a significant decrease in mortality was recorded (−0.50 (−0.77 to −0.23), or −23% from baseline mean of 2.18%) (table 2). No significant difference in percentage points was noted between patients who were exposed and unexposed in annual rates of drug overdose (−0.12 (−0.41 to 0.18), or 9% decrease from 1.37% baseline rate), suicide attempt (0.02 (−0.07 to 0.11), or 13% from 0.15% increase from baseline rate), gastrointestinal bleeding (0.33 (−0.07 to 0.73), or 11% increase from 2.87% baseline rate), or kidney failure (0.16 (−0.40 to 0.71), or 2% increase from 7.61% baseline rate).
To quantify the effect of only losing a prescriber on results, we examined changes in outcomes associated with prescriber exit among all patients who were not receiving LTOT but lost the same prescriber (appendix table 3, appendix table 5). The unadjusted differences reflect differences in the main results intable 2that may be attributable to prescriber exit instead of to LTOT discontinuation. After the loss of a prescriber, increases among patients not receiving LTOT were noted for overdose (0.12 percentage points, or a 27% increase from baseline mean of 0.45%), anxiety (0.10, or a 59% increase from baseline mean of 0.17%), opioid or alcohol withdrawal (0.21, or a 47% increase from baseline mean of 0.45%), and mortality (0.90, or 16% increase from baseline rate of 5.68%). Additionally, differences in mental health crises (0.08, or 15% increase from the baseline mean of 0.55%) were small among patients not receiving LTOT when compared with estimates among patients receiving LTOT (table 2).
Additional analyses
In sensitivity analyses, we focused on: beneficiaries qualifying for social security disability income (people with disabilities); excluding patients in Medicare Advantage; including patients with cancer; and separately, patients above or below median morphine milligram equivalent (median 54.36 average daily), which were all similar to the main analysis (appendix tables 6 and 7). Appendix table 8 replicates the main results focusing on patients who were alive after a prescriber’s exit. Appendix table 9 compares treatment effects modeled intable 2by whether the patient’s main prescriber was a primary care physician or specialist, showing that effects are similar across the two groups. Appendix table 10 indicates that effects maintain significance when adjusting for multiple comparisons. Appendix table 11 compares exposed LTOT patients who did versus did not discontinue LTOT in response to prescriber exit, showing standardized mean differences of 0.12 or less.
Discussion
Principal findings
The loss of a prescriber was associated with increased discontinuation of LTOT and transient, but significant, increases in adverse outcomes among patients, including suicide attempts, withdrawal, and admissions to hospital or emergency department visits. Rates of adverse outcomes among patients reverted to baseline rates within four to seven months after prescriber exit. However, a significant increase in mental health crises and opioid or alcohol withdrawal was noted on average across the full four quarters after the exit period. Despite these outcomes, drug overdose rates did not change. Our findings suggest that discontinuation of LTOT may be associated with a temporary period of negative health effects, not including overdose or mortality, after accounting for unobserved confounding.
While a small proportion of patients discontinued LTOT in both groups, a substantial increase in discontinuation was reported in the quarter after prescriber exit. As seen infigure 2, the increase in the first quarter after a prescriber’s exit was not associated with any visible or regression estimated change in overdose rates, despite large increases that might be expected based on standard observational models.71521The difference in results is likely because discontinuation is a clinical event associated with other health changes, and patients with discontinuation differ from others, preventing the estimation of a causal effect.32223The small or null results over the post prescriber exit period for outcomes such as overdose suggest that despite potential harm, discontinuation of LTOT may have counteracting benefits, such as reduced overdose risk, for some patients. The overall reduction in mortality among patients receiving LTOT who had a prescriber exit supports the potential longer term benefit of shorter duration and lower dosage to LTOT, although we interpret our mortality results with caution given pre-exit period trends that diverged prior to prescriber exit (fig 2).
The harm we do observe associated with discontinuation of LTOT could be related to low quality management of transitioning patients across prescribers. Most discontinuations have excessively rapid tapers,3and patients receiving LTOT are a population at high risk with many comorbidities who must frequently navigate substantial stigma in the health care system.36For instance, discontinuation may accompany distressing clinician abandonment of patients with LTOT and cause opioid withdrawal, emotional harm, and undertreated pain, events that were unobservable in claims data unless they result in diagnoses that we captured.
One question is whether the observed associations were attributable to prescriber exit rather than the accompanying rise in overdose. To address this, we focused on patients receiving LTOT and those who were not on this treatment but who lost the same prescriber. If the prescriber exit explains the effects, patients not receiving LTOT should be impacted similarly to those who were receiving LTOT after prescriber exit. Instead, analyses show that clinical outcomes for patients not on LTOT were either null or opposite to those observed for patients receiving LTOT after the loss of a prescriber. Therefore, the loss of the physician is unlikely to explain the observed results among patients on LTOT and, if anything, may lead to an underestimation of the effects of discontinuation.
Comparison with other studies
One clear conclusion is that the observed clinical effect of LTOT discontinuation is highly dependent on the methods used. Prior research on tapering or discontinuation of LTOT finds a doubling in the rate of overdose and mental health crises comparing populations with discontinuation directly to those without.715161719By contrast, the absence of change in overdose rates in our analysis is closer to other observational studies using techniques to control for unobserved confounding, which have found opioid discontinuation to have a small or null impact on rates of addiction related adverse events.141822This discrepancy suggests that overdose risk is more likely to be misestimated in conventional approaches.
Policy implications
While the choice of methods is a technical issue, it has great relevance to patients and policymakers struggling with the clinical and public health challenges of LTOT. Some mechanisms are plausible by which LTOT discontinuation could lead to either benefit, harm, or a mixture of both. Both observational research37and randomized trials38describe that LTOT, especially at a high dose, is associated with a multitude of adverse outcomes. On the one hand, discontinuing LTOT without replacing the clinical role of LTOT could lead to untreated pain, withdrawal, or worsening of mental health issues as described previously. On the other hand, discontinuing LTOT could mitigate these risks and promote patient safety in some circumstances. The clinical and scientific uncertainty around this question suggests clinical equipoise to justify ongoing394041and future randomized interventions that promote patient centered, clinically appropriate LTOT discontinuation or tapering to investigate how tapering LTOT can be done safely and respectfully while prioritizing quality of life.
Limitations
This study has several limitations. Firstly, the findings may not generalize to the entire Medicare population or to populations outside of Medicare beneficiaries.32Our sample was a younger, Medicare qualifying population: the average patient in our sample was 58.0 years old, and only 33% of the population was over age 65 years (table 1). Additionally, the study may not generalize to discontinuations outside of those caused by losing a prescriber. Secondly, we cannot observe the reason for a prescriber’s exit, which may be associated with patient outcomes in certain circumstances. We attempt to circumvent this issue by controlling for prescriber specific factors by comparing patients receiving LTOT with patients not receiving LTOT with the same assigned, exiting prescriber, showing that effects are directionally opposed between the two groups. Thirdly, our findings may not apply to patients on LTOT not meeting the threshold of the restrictive definition used in this study, such as those receiving lower doses of opioids or those on LTOT for less than a year. Fourthly, our statistical power, as assessed by the size of confidence intervals in the adjusted results, does not enable us to rule out a low magnitude of harm in response to discontinuation. However, across multiple outcomes, point estimates were consistently close to zero, and they did not change in any consistent pattern with the timing of a large change in discontinuation associated with prescriber exit. Fifthly, we chose what we believe to be the best strategy to handle confounding, but other strategies might be equally valid.
An additional limitation is that, while our analysis overcomes the confounding in prior work, our statistical strategy leverages a specific group of patients: those who have their opioid prescriptions discontinued in response to the loss of a prescriber. For instance, the increase in buprenorphine and naloxone prescriptions may suggest that patients receive replacement prescribers who initiated treatment of opioid use disorder in response to the loss of their prescriber. Also, not all patients had their opioid prescriptions discontinued after losing their main prescriber, suggesting that our results do not generalize to all patients who are discontinued or tapered from opioids. Since we followed up patient outcomes for only four quarters, our study does not consider the effects on patients losing a prescriber beyond four quarters after prescriber exit. We also do not observe prescriptions not billed to Medicare. Finally, our definition of LTOT discontinuation follows prior work to support comparability across studies. However, our results might not generalize to all alternative definitions of LTOT and discontinuation.
Conclusion
This study finds a complex association between the discontinuation of LTOT coinciding with prescriber exit and subsequent health effects. The cessation of LTOT was linked to a short term increase in negative health events, such as suicide attempts and admissions to hospital, indicating a potential need for heightened mental health support during the transition. Despite this, we found no effect of discontinuation on overdose rates or mortality. These findings differ from prior evidence that did not control unobserved confounding, implying that the observed consequences of LTOT discontinuation may vary considerably depending on the methods used. This variation underscores the importance of randomized interventions to better understand how LTOT discontinuation can be managed safely and effectively.
Much research shows that discontinued versus continued long term opioid treatment (LTOT) is associated with an increased rate of overdoses and mental health crises
Uncertainty remains because studies used observational models comparing individuals that discontinue LTOT to those that do not, populations whose clinical profiles diverge around the time of LTOT changes
Unobserved confounding was accounted for by leveraging prescriber workforce exit as an external shock increasing LTOT discontinuation and quantify outcomes in a difference-in-differences analysis
Prescriber workforce exit significantly changed opioid prescriptions and short term increases in adverse events of opioid or alcohol withdrawal, suicide attempts, and admission to hospital, but overdose rates changed little
LTOT discontinuation may be associated with a temporary period of adverse health impacts after accounting for unobserved confounding
","Objective: To examine the association between prescriber workforce exit, long term opioid treatment discontinuation, and clinical outcomes.
Design: Quasi-experimental difference-in-differences study
Setting: 20% sample of US Medicare beneficiaries, 2011-18.
Participants: People receiving long term opioid treatment whose prescriber stopped providing office based patient care or exited the workforce, as in the case of retirement or death (n=48 079), and people whose prescriber did not exit the workforce (n=48 079).
Main outcomes: Discontinuation from long term opioid treatment, drug overdose, mental health crises, admissions to hospital or emergency department visits, and death. Long term opioid treatment was defined as at least 60 days of opioids per quarter for four consecutive quarters, attributed to the plurality opioid prescriber. A difference-in-differences analysis was used to compare individuals who received long term opioid treatment and who had a prescriber leave the workforce to propensity-matched patients on long term opioid treatment who did not lose a prescriber, before and after prescriber exit.
Results: Discontinuation of long term opioid treatment increased from 132 to 229 per 10 000 patients who had prescriber exit from the quarter before to the quarter after exit, compared with 97 to 100 for patients who had a continuation of prescriber (adjusted difference 1.22 percentage points, 95% confidence interval 1.02 to 1.42). In the first quarter after provider exit, when discontinuation rates were highest, a transient but significant elevation was noted between the two groups of patients in suicide attempts (adjusted difference 0.05 percentage points (95% confidence interval 0.01 to 0.09)), opioid or alcohol withdrawal (0.14 (0.01 to 0.27)), and admissions to hospital or emergency department visits (0.04 visits (0.01 to 0.06)). These differences receded after one to two quarters. No significant change in rates of overdose was noted. Across all four quarters after prescriber exit, an increase was reported in the rate of mental health crises (0.39 percentage points (95% confidence interval 0.08 to 0.69)) and opioid or alcohol withdrawal (0.31 (0.014 to 0.58)), but no change was seen for drug overdose (−0.12 (−0.41 to 0.18)).
Conclusions: The loss of a prescriber was associated with increased occurrences of discontinuation of long term opioid treatment and transient increases in adverse outcomes, such as suicide attempts, but not other outcomes, such as overdoses. Long term opioid treatment discontinuation may be associated with a temporary period of adverse health impacts after accounting for unobserved confounding.
"
Effect of the HPV vaccination programme on incidence of cervical cancer by socioeconomic deprivation in England,"Introduction
Human papillomavirus (HPV) comprises a family of viruses, a subset of which are responsible for virtually all cervical and some anogenital and oropharyngeal cancers.1More than 100 countries worldwide have introduced prophylactic HPV vaccination as part of routine immunisation schedules.2One important outcome yet to be reported is whether vaccination has reduced or increased the inequalities seen for cervical disease in the UK and elsewhere.
In England, the national HPV vaccination programme started in 2008 using the bivalent Cervarix vaccine to prevent infections due to HPV types 16 and 18, which are estimated to cause around 80% of all cervical cancers in the UK.3Vaccination was offered routinely to 12-13 year old (school year 8) girls and as part of a catch-up campaign to those aged <19 years.4In September 2012 the programme switched to the quadrivalent vaccine (Gardasil), which additionally protects against HPV types 6 and 11 (responsible for genital warts), and in 2019 the programme was extended to 12-13 year old boys. Those who are eligible but not vaccinated can receive the vaccine free of charge from their general practitioner until their 25th birthday.5
The introduction and implementation of HPV immunisation in this way means that noticeable discontinuities exist in the proportion of women vaccinated by date of birth, enabling a rigorous evaluation of the effectiveness of the programme.6For example, women born in August 1990 are unlikely to have received HPV vaccination, whereas among those born in the year from 1 September 1990 nearly 70% have received at least one dose of the vaccine.
Findings on the early effect of national HPV vaccination programmes have been encouraging. A wealth of real world evidence for the effect of vaccination on HPV prevalence exists7891011and evidence is growing for its effectiveness in reducing high grade cervical intraepithelial neoplasia (CIN)12131415and cervical cancer in vaccinated women.1416171819For instance, we found that in England rates of grade 3 CIN (CIN3) and of cervical cancer were greatly reduced among those who were offered HPV vaccination, and that the magnitude of the reduction was greatest in the cohorts with the highest uptake and younger age at vaccination.14We estimated that by mid-2019 the immunisation programme had prevented cervical cancer in nearly 450 women and CIN3 in around 17 000 women.
Along with preventing ill health, a key aim of the NHS is to reduce health inequalities.20To this end, we investigated whether the effect of immunisation against HPV has resulted in a reduction in inequalities in cervical disease or a widening. Concern has been expressed that if the uptake of HPV vaccination is lower in those at greatest risk of cervical cancer, as has been seen in the US,21this could accentuate health inequalities. One study found that the introduction of HPV immunisation in England might initially have increased inequities in HPV related cancer incidence among ethnic minority groups because of the differential effect of herd protection in subpopulations with dissimilar vaccination coverage.22Previous studies have suggested that white people have a higher awareness of HPV and acceptance of the immunisation23and that vaccination uptake is lower in women from ethnic minority groups and more deprived areas.24Using data on HPV vaccination coverage by local area, however, a study found little variation by deprivation score in women offered routine vaccination (83%v86% for most and least deprived areas, respectively) and only a small negative correlation between deprivation and vaccine uptake in those offered catch-up vaccination (47%v53% for most and least deprived areas, respectively).25A full understanding of the effect of HPV vaccination across different socioeconomic groups is complicated by the poor uptake of cervical screening observed among younger women in the most deprived areas, leading to lower rates of screen detected cervical cancer and CIN3 at age 25 years compared with women in less deprived areas.2627
We replicated results from an analysis of population based cancer registry data to evaluate if the high vaccination effectiveness seen previously continued during an additional year of follow-up. The combined data were also used to investigate the effect of the vaccination programme by socioeconomic deprivation.
Methods
To represent socioeconomic deprivation, we used the index of multiple deprivation, a small area measure based on several domains of deprivation, such as income, employment, and health. The index is determined by using a standard statistical geographical unit, called lower super output area, which divides England into small areas of similar sized populations (on average about 1500 residents, or 650 households).28The lower super output areas are then ranked from the most to the least deprived and divided into five equal groups. The first and fifth groups correspond to the 20% most deprived and 20% least deprived lower super output areas in England, respectively.
We retrieved the records of all women aged 20-64 years resident in England with a diagnosis of invasive cervical cancer (ICD-10 (international classification of diseases, 10th revision) code C53) or CIN3 (ICD-10 code D06) between 1 January 2006 and 30 June 2020. These records are stored in the database managed by NHS England’s National Disease Registration Service,29and for each patient included information on index of multiple deprivation derived from the patient’s home postcode at the time of diagnosis. To convert these counts into rates, we used mid-year estimates of the female population for England by single year of age, calendar year (January 2006 to June 2020), and index of multiple deprivation (five groups). These estimates were retrieved from multiple tables publicly available on the website of the UK’s Office for National Statistics (ONS).30The supplementary material provides more details about the index of multiple deprivation versions used by the National Disease Registration Service and ONS, along with information on how we derived the population estimates required in our statistical analysis.
Statistical analysis
We separately analysed incidence rates of cervical cancer and CIN3 by using extensions of our previously described age-period-cohort Poisson model.143132Data on women with cancer or CIN3 were aggregated by single month of age, calendar time (period), and date of birth (cohort). We derived the corresponding population risk time by subdividing the mid-year ONS population estimates into one month intervals for age, period, and cohort. For the analysis of the effectiveness by deprivation, we further split both the data on women with cancer or CIN3 and the population estimates by deprivation group (fifths). We then used the population risk time as the denominator for calculating rates (formally, the subdivided population estimates were log transformed and included in the Poisson regression model as an offset). Confidence intervals were computed using robust standard errors.3334
The code for the analysis was written and tested on synthetic data (extending the Simulacrum dataset)35by a statistician (MF) at King’s College London and then run on the real dataset by an analyst (BN) at the National Disease Registration Service.
We started by considering a core model where we included the main effects for age, period, and birth cohort, along with selected age by cohort and age by period interactions (see supplementary table S1). The interaction terms were included to account for variations in screening policy and historical events that affected cervical cancer rates. Specifically, we defined seven birth cohorts to capture differences in the age at first invitation to screening and the school years in which HPV vaccination was offered (seetable 1). We added terms for seasonality and for events that may have affected registrations for cervical cancer and CIN3, such as the covid-19 lockdown, the “Jade Goody effect,”3637and the 2019 cervical screening awareness campaign. In our previous paper,14we used several similar regression models to study the sensitivity of results to the precise way in which we adjusted for potential confounding factors. Because we found that the estimates of the cohort specific incidence rate ratios changed little across the various models, here we report on only a single model adjustment for confounders.
Using the core model described, we investigated if the high effectiveness of the HPV immunisation programme reported previously14continued during an additional 12 months of follow-up. To do this we split the main effect of each cohort offered vaccination into two subgroup effects depending on whether the data related to the periods 1 January 2006 to 30 June 2019 or 1 July 2019 to 30 June 2020; this approach corresponded to adding three cohort by period interaction terms.
To evaluate the impact of socioeconomic deprivation on incidences of cervical cancer and CIN3, we extended the core model by adding main effects for deprivation and deprivation by cohort interactions. Specifically, we allowed the effect of each deprivation level to vary between unvaccinated women (cohorts 1-4) and those offered vaccination (cohorts 5-7), but we assumed it was otherwise constant within these two groups. We did not include further interactions between deprivation and other covariates as they were not of primary interest in this analysis. Using the fitted Poisson regression models, we made “what if” predictions by changing the value of one or more predictors and by leaving the others as observed. In this way it was possible to compare what happened (factual scenario) with what would have happened under an alternative (counterfactual) scenario.
We also carried out a sensitivity analysis where the effects of these deprivation by cohort interactions were allowed to vary across the three different groups offered vaccination (ie, we used 15 terms instead of five). For cervical cancer, owing to small numbers in cohort 7, we fitted a reduced model where the effects of these interactions were constrained to be the same for cohorts 6 and 7.
All analyses were performed in Stata, version 17.38
Patient and public involvement
Patient and public involvement contributors were not formally involved in this research. We did, however, engage with Cancer Research UK (CRUK), Jo’s Cervical Cancer Trust, and the HPV Coalition on the importance of these analyses and the dissemination of the results. This included taking part in a video produced by ITN Business for World Cancer Day 2023, writing a piece for the 20th anniversary of the creation of CRUK, and engaging with international media about our research findings on the effect of the English HPV vaccination programme. We have also discussed the research and a draft of this paper with individual patients, journalists, and patient and public involvement representatives linked to broader research programmes.
Results
Table 1lists the characteristics of the birth cohorts included in the study. We defined the different cohorts so that each cohort is homogeneous in terms of the age women would have been offered HPV vaccination (if at all) and the age at which they would have first been invited for cervical screening.
Overall, there were 231.1 million women years of observation between 1 January 2006 and 30 June 2020 on women aged 20-64 years in England. During this time, 29 968 women received a diagnosis of invasive cervical cancer and 335 228 a diagnosis of CIN3 (table 2). Observations between 1 July 2019 and 30 June 2020 have not been reported previously. With these additional 12 months of follow-up, there are, in the routine vaccination group (cohort 7), about twice the number of diagnoses compared with the same group in our previous study (we now have 13v7 previously for cervical cancer, 109v49 for CIN3; see supplementary table S2).
Our previously published findings on the effect of the national HPV vaccination were largely confirmed with the new data (table 3, also see supplementary table S3). The analysis showed that the previously observed low rates of disease and the estimated high effectiveness of the immunisation programme continued during the additional 12 months of follow-up (diagnoses in July 2019 to June 2020) among women born since 1 September 1990. In particular, the estimated effects of vaccination for that later period in cohort 7 (those born since 1 September 1995) imply a reduction in incidence of 83.9% (95% confidence interval (CI) 63.8% to 92.8%) for cervical cancer and 94.3% (92.6% to 95.7%) for CIN3 (table 3). The relative risk reduction estimates for the earlier period are not identical to those reported previously because we also had new data for the unvaccinated cohorts that affected the baseline rates.
Supplementary table S4 shows the full estimates from modelling the effects of vaccination in different levels of socioeconomic deprivation, with summary results reported intable 4,table 5, andtable 6. The highest incidence rates for invasive cervical cancer were observed among women living in the most deprived areas (first fifth) but, while in the reference unvaccinated group there was a strong downward gradient moving from women in the most deprived areas to those in the least deprived, little difference was found between the second and fifth fifths of deprivation in the groups offered vaccination. In both the reference and the vaccination cohorts the highest rates of CIN3 occurred in those from the most deprived areas, but no clear trend was observed among the other four fifths of deprivation (see supplementary tables S5 and S6).
Overall, our model estimated that 687 (95% CI 556 to 819) cervical cancers and 23 192 (22 163 to 24 220) CIN3s had been prevented by the vaccination programme up to mid-2020 among young women in England (table 4). The greatest numbers for cervical cancer were prevented in women in the most deprived areas (192 and 199 for first and second fifths, respectively) and the fewest in women in the least deprived fifth (61 cancers prevented). The number of women with CIN3 prevented was high across all deprivation groups but greatest among women living in the more deprived areas: 5121 and 5773 for first and second fifths, respectively, compared with 4173 and 3309 in the fourth and fifth fifths, respectively. When we looked at the corresponding cohort specific figures (table 5andtable 6), we noticed differences between the cohorts, particularly for CIN3. In all three cohorts offered vaccination the numbers and rates of prevented cervical cancers were much higher in women from the most deprived areas than least deprived areas (table 5). The proportion of women with prevented cervical cancer in each cohort was, however, similar between the first and fifth fifths of deprivation. For CIN3 (table 6), the results were more complicated. In women offered vaccination at age 16-18 years (cohort 5), the proportion of cervical cancers prevented was substantially less in those from the most deprived areas (29.6%) compared with those from the least deprived areas (40.6%). An inequality still existed in cohorts 6 and 7, but it was greatly reduced (67.7%v72.8% in cohort 6 and 95.3%v96.1% in cohort 7).
Discussion
In England, the social-class gradient for cervical cancer is one of the steepest of any cancers: women in the most deprived fifth have had double the risk of those in the least deprived fifth.3940Some of this results from differences in exposure to HPV and risk of an infection becoming persistent,41but differential uptake of cervical screening has also been an important factor. Previous research has highlighted the need for new engagement strategies to improve attendance for cervical screening among young women living in more socially deprived areas.42Encouragingly, the coverage of HPV vaccination has been (at least for the routine campaign and before the covid-19 pandemic) uniformly high.43It is, however, important to investigate whether immunisation—including the indirect effects achieved by high uptake—is helping to reduce health inequalities.
Using population based cancer registrations updated to mid-2020, which provided information on about twice the expected number of cancers in women offered HPV vaccination aged 12-13 years than in our previous analysis, we were able to show that the high vaccination effectiveness seen previously was confirmed with more recent data. The largest differences between the old and the new data were found for cohort 6 (the catch-up group offered the vaccine at age 14-16 years): for cervical cancer the estimated effectiveness increased, whereas for CIN3 it decreased. The reasons behind these differences are unclear. The results for cohorts 6 and 7 in the new data are more in keeping with what we would have expected given that the proportion of disease caused by HPV types 16 and 18 is greater for invasive cancer than for CIN3.
We also investigated the effect of the HPV immunisation programme by socioeconomic deprivation. Overall, we found that the programme was associated with a substantial reduction in the expected number of women with cervical cancers and CIN3 in all fifths of deprivation. For cervical cancer before vaccination, the downward gradient with decreasing deprivation was strong. In all cohorts offered vaccination, the highest rate was still seen among women living in the most deprived areas, but little difference was observed between women living in the second to fifth deprived areas. For CIN3, similar patterns were observed for the reference unvaccinated group and the three cohorts offered vaccination, but rates were greatly reduced in all fifths of deprivation in the latter. When we compared women in the most deprived areas with those in the least deprived areas in terms of percentage of disease averted, we observed differences across the cohorts for CIN3, with women in the least deprived areas in the older catch-up cohort (vaccine offered at age 16-18 years) having a greater proportion of averted CIN3s after HPV immunisation than women in the most deprived area (40.6%v29.6%). The same, although to a much less extent, was observed for the younger catch-up cohort (72.8%v67.7%). For invasive cervical cancer, we found no evidence of a less beneficial impact (in terms of percentage of cases averted) of the vaccination in women living in the most deprived areas; in fact, especially for the older catch-up cohort, the percentage was slightly higher in women in the most deprived areas compared with those in the least deprived areas.
The observed incidences of cervical cancer and CIN3 depend on three key factors: the intensity of exposure to HPV infections (including age at first exposure), the uptake of cervical screening, and HPV vaccination coverage. It is therefore difficult to disentangle the effects of these three drivers on the index of multiple deprivation specific rates with the data at hand. The health inequality in CIN3 in cohort 5 might result from the lower vaccination coverage among women in the most deprived areas since at age 16-18 years when they became eligible for vaccination more of those from the most deprived fifth may not have been in school or, for other reasons, may have missed the offer of HPV immunisation. These observations are consistent with previous understanding that higher uptake of catch-up vaccination was associated, although not as strongly as in some countries, with lower deprivation.25It is, however, reassuring that cohorts 6 and 7 showed little inequality in relative reductions in cancer (as in vaccination coverage).
However, since the UK has recently announced a change to a one dose schedule for routine HPV vaccination, ensuring this change achieves high coverage (including in the birth cohorts currently with lower coverage owing to covid-19 related interruption to schooling, and to immunisation services) is important to maintain the effects we have seen on cervical disease and on inequalities. Further investigations could be carried out in the future to check for any effect on cancer incidence caused by covid-19, gender neutral vaccination (since 2019), a change in the type of vaccine used, or reduced dose schedules.
Strengths and limitations of this study
Our analysis has several strengths. Our study provides direct evidence for the effect of a public health intervention (such as HPV vaccination) on cancer rates by deprivation. We used high quality data from population based cancer registries and were able to investigate the extent of socioeconomic inequalities in cohorts offered vaccination and whether the effectiveness of the HPV immunisation continued in an additional year of follow-up. The code for the analysis was written and tested using simulated data and an independent analyst later ran the code on the real dataset, guaranteeing reliable and robust results and preserving patient confidentiality.
The main limitations of our study are that it was observational and individual level data on vaccination status were not available. However, previous published research14provided detailed information on potential confounding factors and the best way to adjust for these in the analysis. Additionally, the discontinuities in vaccine uptake with date of birth makes this study powerful and less prone to biases from unobserved confounders than an analysis based on individual level data on HPV vaccination status.
Women born after 1 September 1999 were offered the Gardasil vaccine from 1 September 2012. As these women were at most aged 20 years and 10 months at the end of the study follow-up (30 June 2020), it is not yet possible with the data available to compare the effectiveness of the programme among those offered Cervarix and those offered Gardasil. This additional comparative analysis will become feasible with a longer follow-up on the recipients of Gardasil.
Policy implications
We found that the high effectiveness of the national HPV immunisation continued in the additional year of follow-up (July 2019 to June 2020). This is encouraging as it validates the previously published results and further supports consideration of more limited cervical screening for cohorts with high vaccination coverage aged 12-13 years. Moreover, although women living in the most deprived areas are still at higher risk of cervical cancer than those in less deprived areas, the HPV vaccination programme is associated with substantially lowered rates of disease across all fifths of socioeconomic deprivation. For cervical cancer, this has led to the levelling-up of the rates across the second to fifth fifths of deprivation so that the strong downward gradient observed in the reference unvaccinated cohort is no longer present in the cohorts offered vaccination. For CIN3, in the older catch-up cohorts women living in the least deprived areas seem to have benefited more from vaccination than those living in the most deprived areas, but the rates were still greatly reduced in all socioeconomic groups. Cervical screening strategies for women offered vaccination should carefully consider the differential effect both on rates of disease and on inequalities that are evident among women offered catch-up vaccination.
Conclusions
The HPV vaccination programme in England has not only been associated with a substantial reduction in incidence of cervical neoplasia in targeted cohorts, but also in all socioeconomic groups. This shows that well planned and executed public health interventions can both improve health and reduce health inequalities.
In England, immunisation against human papillomavirus (HPV) has been associated with greatly reduced incidence rates of cervical cancer and grade 3 cervical intraepithelial neoplasia (CIN3) up to June 2019, especially among women offered routine vaccination at age 12-13 years
The social-class gradient for cervical cancer incidence has been one of the steepest of any cancers
Concern has been raised that HPV vaccination could least benefit those at highest risk of cervical cancer
The high effectiveness of vaccination against HPV seen previously continued during an additional year of follow-up, from July 2019 to June 2020
The English HPV vaccination programme was associated with substantially lower rates of cervical cancer and CIN3 in all fifths of socioeconomic deprivation, although the highest rates remained among women in the most deprived areas
For cervical cancer, the strong downward gradient from high to low deprivation observed in the reference unvaccinated cohort was no longer present among those offered vaccination
","Objectives: To replicate previous analyses on the effectiveness of the English human papillomavirus (HPV) vaccination programme on incidence of cervical cancer and grade 3 cervical intraepithelial neoplasia (CIN3) using 12 additional months of follow-up, and to investigate effectiveness across levels of socioeconomic deprivation.
Design: Observational study.
Setting: England, UK.
Participants: Women aged 20-64 years resident in England between January 2006 and June 2020 including 29 968 with a diagnosis of cervical cancer and 335 228 with a diagnosis of CIN3. In England, HPV vaccination was introduced nationally in 2008 and was offered routinely to girls aged 12-13 years, with catch-up campaigns during 2008-10 targeting older teenagers aged <19 years.
Main outcome measures: Incidence of invasive cervical cancer and CIN3.
Results: In England, 29 968 women aged 20-64 years received a diagnosis of cervical cancer and 335 228 a diagnosis of CIN3 between 1 January 2006 and 30 June 2020. In the birth cohort of women offered vaccination routinely at age 12-13 years, adjusted age standardised incidence rates of cervical cancer and CIN3 in the additional 12 months of follow-up (1 July 2019 to 30 June 2020) were, respectively, 83.9% (95% confidence interval (CI) 63.8% to 92.8%) and 94.3% (92.6% to 95.7%) lower than in the reference cohort of women who were never offered HPV vaccination. By mid-2020, HPV vaccination had prevented an estimated 687 (95% CI 556 to 819) cervical cancers and 23 192 (22 163 to 24 220) CIN3s. The highest rates remained among women living in the most deprived areas, but the HPV vaccination programme had a large effect in all five levels of deprivation. In women offered catch-up vaccination, CIN3 rates decreased more in those from the least deprived areas than from the most deprived areas (reductions of 40.6%v29.6% and 72.8%v67.7% for women offered vaccination at age 16-18 and 14-16, respectively). The strong downward gradient in cervical cancer incidence from high to low deprivation in the reference unvaccinated group was no longer present among those offered the vaccine.
Conclusions: The high effectiveness of the national HPV vaccination programme previously seen in England continued during the additional 12 months of follow-up. HPV vaccination was associated with a substantially reduced incidence of cervical cancer and CIN3 across all five deprivation groups, especially in women offered routine vaccination.
"
Long acting progestogens vs combined oral contraceptive pill for preventing recurrence of endometriosis related pain,"Introduction
Endometriosis is an oestrogen dependent condition that affects up to one in 10 women of reproductive age.1Characterised by the growth of endometrial-like tissue outside the uterus, it can cause severe pelvic pain and infertility that can have a serious impact on quality of life.234The condition requires a laparoscopy for definitive diagnosis and is frequently treated by excision or ablation of affected tissue at the same time.
Recurrence of endometriosis after surgery is common and poses a major challenge to its successful management. Population based data from Scotland shows that, after initial surgery for endometriosis, 62% of treated women have at least one repeat operation, 45% have two or more, and nearly 25% need surgical removal of their ovaries, often combined with a hysterectomy.5
The UK National Institute for Health and Care Excellence and the European Society of Human Reproduction and Embryology recommend the use of hormonal preparations including the combined oral contraceptive pill (COCP) and progestogens to treat endometriosis related pain.67It is unclear as to which of these two treatment regimens is better at preventing the recurrence of endometriosis related pain after surgical treatment. Additionally, continuation rates and adherence to treatment might be improved by use of long acting progestogens (LAPs) as there is no need to take drugs on a daily basis.
Our aim was to evaluate whether LAPs or COCP were more effective in preventing the recurrence of pain in women undergoing conservative surgery for endometriosis. The economic results from a parallel cost effectiveness evaluation will be presented in a separate paper.
Methods
Trial design
The PRE-EMPT (preventing recurrence of endometriosis) trial was a multicentre, pragmatic, parallel group, open label, superiority randomised controlled trial. In response to clinician and patient feedback that treatment preferences might prevent randomisation to a multiarm trial, the study was designed prospectively to be adaptive, based on feasibility of recruitment; the full methods have been detailed previously.8In brief, during an internal pilot phase, patients could enter the study provided they were willing to be randomised to at least one form of LAP (depot medroxyprogesterone acetate—DMPA, or the levonorgestrel releasing intrauterine system—LNG-IUS) and at least one intervention that was not a LAP (COCP or no treatment). At the end of this pilot phase, a report was provided to a joint trial steering committee and data monitoring committee describing the frequency of randomisation options chosen so that a feasible design including the most commonly chosen options could be agreed for the remainder of the study. A qualitative assessment was also conducted during this time, the results of which fed into any decisions about trial design.9The treatment options described below reflect the revised design that compares LAP as a class of treatments (DMPA or LNG-IUS) versus COCP.
The protocol (supplementary material 1) received clinical trial authorisation (EudraCT 2013-001984-21) from the Medicines and Healthcare products Regulatory Authority and a favourable ethical opinion from the East of Scotland Ethics Committee (14/ES1004). The trial was prospectively registered in the ISRCTN Registry (ISRCTN97865475;https://www.isrctn.com/ISRCTN97865475). A statistical analysis plan was generated for the clinical trial (supplementary material 2), and all participants provided written informed consent. We used the CONSORT (Consolidated Standards of Reporting Trials) checklist when writing this report.10
Participant selection
Recruitment under the definitive design was from 23 November 2015 to 25 March 2019, with 92 participants recruited in the internal pilot phase (from March 2014 to November 2015). Women aged 16-45 years, with symptoms suggestive of endometriosis and scheduled for diagnostic laparoscopy with concurrent surgery for endometriosis (if confirmed), or with a previous laparoscopic diagnosis and scheduled for conservative surgery, were potentially eligible. Exclusion criteria were infertility, immediate plans to conceive, plans for elective endometriosis surgery for deep disease or endometrioma, contraindications to use of hormonal treatment, and suspicion of malignancy at laparoscopy. Previous use of any trial interventions did not preclude participation, while a four week washout period was required before laparoscopy for women using gonadotrophin releasing hormone analogues. An intraoperative diagnosis of treatable peritoneal endometriosis confirmed eligibility.
Randomisation and interventions
Eligible consenting participants were randomised 1:1 to receive a LAP or COCP. In the LAP group, the options were DMPA, administered at a dose of 150 mg in an aqueous suspension by intramuscular injection every three months, or LNG-IUS that delivers a daily dose of 20 µg of levonorgestrel for five years. Those randomised to COCP were prescribed a formulation containing 30 μg ethinylestradiol and 150 μg levonorgestrel, taken cyclically each month, continuously or in a tricycle regimen. Participants and investigators were not blinded to treatment allocation owing to the substantial differences in formulations and their routes of delivery.
Randomisation occurred intraoperatively or immediately postoperatively using a central internet randomisation service provided by the Birmingham Clinical Trials Unit. Minimisation variables were stage of endometriosis (using the American Society for Reproductive Medicine classification, stage I or IIvstage III or IV); extent of excision or ablation of endometriosis (completevincomplete, as judged by the surgeon at the time of conservative surgery); age in years (<35 yearsv≥35 years); randomising centre; intended LAP (if randomised to LAP); reason for selection of LAP (patient preference, clinician advice, or no preference). If the participant had no preference for a particular LAP, the LAP was randomly allocated before LAP versus COCP randomisation using a random blocked list (variable length) incorporated into the computer based algorithm. Patient choice of LAP before randomisation gave them some control over which LAP they would be treated with if randomised to this class of treatment.
Outcome measures
The primary outcome was the recurrence of symptoms as evaluated by the pain domain of the Endometriosis Health Profile-30 (EHP-30, where 0 is best score and 100 is worst score) three years after randomisation.1112The EHP-30 is a validated condition specific tool to assess impact on quality of life by endometriosis.
Secondary clinical outcome measures included the remaining EHP-30 core domains (control and powerlessness, emotional wellbeing, social support, self-image) and optional modular domains (work, relationship with children, sexual relationship, feelings about the medical profession, treatment, and infertility). Other secondary outcomes were pain during periods, during intercourse, and at any other time (measured by a visual analogue scale, where 0 was no pain and 100 was worst imaginable pain), a four point ordinal global impression of change in pain, menstrual regularity on a four point ordinal scale, the Fatigue Severity Score,13generic quality of life (measured by EQ-5D-5L),1415and capabilities (ICECAP-A—ICEpop CAPability measure for Adults).16The results of ICECAP will be presented and discussed in a separate economics paper. Further surgery (laparoscopy to investigate recurrent pain, to treat endometriosis, or hysterectomy) or the use of gonadotrophin releasing hormone analogues were used as a proxy for treatment failure, with a return to preoperative EHP-30 scores also added to these as a further outcome. Change or cessation of randomised treatment, which did not necessitate withdrawal from the trial, were classed as discontinuation. Serious events were classed as those requiring hospital admission or resulting in death or disability, and were categorised as expected or unexpected, and related or unrelated to trial treatment.
Outcomes were collated in a participant completed questionnaire booklet at baseline and then at six months, one, two, and three years. Participants who did not return questionnaires were contacted by telephone to collect the primary outcomes (clinical and economic) and information on further treatment or pregnancy. Other secondary outcomes were not obtained for the telephone completed shortened questionnaire. Data on further surgical procedures and second line medical treatments for endometriosis were obtained directly from participants and also the hospital records of non-responders.
Sample size
The final sample size calculation reflected the changes to the trial design at the end of the internal pilot phase. The original sample size conservatively assumed the possibility that all treatment options would be taken forward and up to six comparisons would be made; this would require extensive multiplicity adjustments. Because only one main comparison was taken forward, a smaller sample size was needed in the final design. The revised estimate of the standard deviation was taken from pooled baseline data at the end of this pilot phase. These changes were approved by the trial steering committee and data monitoring committee and were made blind to any accruing follow-up data.
To detect an eight point difference on the EHP-30 pain domain with 90% power (P=0.05) and assuming a standard deviation of 22 points required 160 participants per group, 320 in total. To account for a 20% loss to follow-up, this target was inflated to 400. The size of difference targeted (0.36 standard deviation) was considered to be small (0.2 standard deviation) to moderate (0.5 standard deviation). This sample size would also provide 80% power to detect a 10-point difference in the two stratified analyses of LNG-IUS versus COCP, and DMPA versus COCP provided similar numbers were recruited to the DMPA and LNG-IUS groups.
Statistical analysis
The statistical analysis plan was generated and reviewed by the trial steering committee and data monitoring committee before any analyses were undertaken. Participants were analysed in the treatment group to which they were randomised (intention to treat), irrespective of adherence to the treatment protocol. All participants recruited from 23 October 2015 were included in the final analysis population, along with 92 from the internal pilot phase who were randomised to combinations of treatments that only included LAPs and COCP.
For the primary outcome (EHP-30 pain scores at three years), a mixed effects linear regression model for repeated measures17calculated the adjusted difference between group means, along with 95% confidence intervals (CIs). Parameters for participant, treatment group, time, time by treatment, baseline pain score (as a continuous variable), and the minimisation variables were included; centre was included as a random effect. Secondary outcomes measured on a continuous scale were analysed in a similar manner and other variables using appropriate regression models, dependent on the data type. All estimates of differences between groups were presented with two sided 95% CIs.
Preplanned subgroup analyses on the primary outcome were completed on the minimisation variables, including the selection of LAP (LNG-IUS or DMPA) before randomisation. The effects of these subgroups were examined by adding the subgroup by treatment group interaction parameters to the linear model described above. Sensitivity analysis was performed on the primary outcome to investigate the assumption that missing data were missing at random; this incorporated a delta based multiple imputation approach, which assumes missing data are missing not at random.18
Interim analyses of effectiveness and safety endpoints were performed on behalf of the data monitoring committee approximately every year during the period of recruitment. These analyses were conducted using the Haybittle-Peto principle19; therefore, no adjustment was made in the final P values to determine significance.
Patient and public involvement
Input from patients and the public was crucial in shaping the design of the internal pilot and the main trial, and in the choice of the primary and secondary outcomes. Patient and public involvement (PPI) at the design stage of the trial led to the inclusion of the fatigue scale as an outcome measure. As coapplicant, our lead PPI representative provided a patient centred perspective to all discussions and decisions on recruitment, follow-up, and the use of language within documents aimed at participants.
PPI colleagues also influenced our recruitment and follow-up strategies, especially the decision to opt for telephone follow-up for participants after two unsuccessful attempts to contact them by mail. Finally, input from PPI colleagues has been invaluable in interpreting trial results. PPI groups including Endometriosis UK supported the use of several complementary routes of communication to engage with patients from all backgrounds and ensure that the key messages from this trial were available to all those with endometriosis, their families, and all those who care for them.
Results
Across 34 UK gynaecology clinics, 2858 women were screened for eligibility and 405 were randomised (fig 1). Supplementary table 1 lists reasons for ineligibility. The follow-up rate for the primary outcome was 337 of 405 (83%) at three years; 381 of 405 (94%) provided an EHP-30 pain score for at least one of the follow-up time points. Final follow-up data were obtained in July 2022.
Participants had a mean age of 29 years (standard deviation 6.6) and most (91%, 369 of 405) described their ethnicity as white (table 1). Endometriosis was graded by the surgeon as stage I or stage II (American Society for Reproductive Medicine classification of minimal or mild) in 79% (319 of 405) of participants and endometrial tissue was deemed to have been completely excised at operation in 91% (369 of 405). The minimisation algorithm ensured balance between groups in terms of age, extent of excision as judged by surgeon, stage of endometriosis, LAP selection, and centre; the groups were also well balanced for the other baseline characteristics.
Of the 205 women randomised to LAP, a few more were offered treatment with DMPA compared with LNG-IUS (114 (56%)v91 (44%)). Approximately four-fifths (81%, 254 of 313;table 1) of these treatment options were driven by patient preference. Approximately 65% of participants allocated a LAP were still using a LAP at one year, reducing to 37% by three years. The equivalent figures in the COCP group were lower at 53% and 25%, respectively (supplementary figure 1, panel A). Switching from one LAP treatment to another (ie, from LNG-IUS to DMPA or vice versa) or supplementation of a (related) non-trial drug was also a relatively common occurrence. Adherence to the initially allocated treatment (without any treatment change at all) occurred in 56% and 48% of participants at one year and 26% and 24% at three years in the LAP and COCP groups, respectively (supplementary figure 1, panel B; data are provided for LNG-IUS and DMPA separately in supplementary figure 2). Supplementary tables 2 and 3 summarise reasons for non-adherence.
Primary outcome measure
Three years after randomisation, no evidence was found of a statistically significant difference in pain scores between groups (adjusted mean difference −0.8, 95% CI −5.7 to 4.2; P=0.76), with both groups showing a similar reduction of around 40% (on average, 24 points for LAP group and 23 points for COCP group) compared with preoperative values (table 2). On average, both groups maintained improved pain scores at all follow-up intervals compared with their preoperative scores (supplementary table 4;fig 2). We did not find any differential effect in any of the prespecified subgroups relating to the primary outcome (supplementary table 5). Sensitivity analysis conducted to investigate missing data assumptions did not alter the initial interpretation (supplementary table 6).
Secondary outcomes
Most of the domains of the EHP-30 were improved in both groups at all time points compared with preoperative scores, but there was no consistent evidence of any difference between groups (table 2for results at three years; supplementary table 7 other time points). Pain scores as measured by a visual analogue scale marginally improved at all time points compared with preoperative scores, and when pain was measured by a Likert scale, responses appeared consistent throughout, with most women reporting that their pelvic pain had not changed much or had become worse over the past month. There was no evidence of consistent differences between the groups (supplementary table 8).
The Fatigue Severity Scale results (supplementary table 9) were similar to baseline scores throughout in both groups, while generic quality of life scores showed marginal improvement compared with preoperative values (supplementary table 10). The numbers of participants reporting menstrual periods remained relatively consistent throughout and were lower in the LAP group than the COCP group (54% (87/161) at six months, 51% (51/101) at three yearsv76% (116/152) at six months, 63% (62/98) at three years, respectively; supplementary table 11); these periods appeared to be less regular in the LAP group during the early stages of follow-up (supplementary table 12), but were comparable at three years (table 2). The number of recorded pregnancies was 17 in the LAP group and 24 in the COCP group (supplementary table 13).
Fewer women required additional treatment in the LAP group compared with the COCP group (73v97 events, occurring in 50v61 women because of several repeat interventions in some participants; supplementary table 14), translating to a 33% reduction in time to treatment failure (fig 3; hazard ratio 0.67, 95% CI 0.44 to 1.00). Inclusion of return to prerandomisation EHP-30 pain score into the definition of treatment failure showed 11% fewer failures in the LAP group than in the COCP group (supplementary figure 3, hazard ratio 0.89, 95% CI 0.66 to 1.19).
There were 21 serious adverse events in 14 women in the LAP group and 17 events in 15 women in the COCP group (P=0.79), none directly related to the trial treatment. Seven reports (four in LAP group, three in COCP group) were linked to planned pregnancy and birth, eight (four in each group) associated with recurrent pain, and seven (four LAP, three COCP) were associated with the index endometriosis surgery. The remainder were incidental hospital admissions.
Discussion
Statement of principal findings
A strategy of prescribing LAP or COCP after surgery for endometriosis resulted in similar levels of pain at three years, with both groups reporting an improvement of almost 40% from pretreatment levels on average. Choice of a particular LAP (LNG-IUS or DMPA) before randomisation did not alter these findings. Use of LAPs reduced the risk of second line medical treatments and further surgery.
Strengths and weaknesses of the study
This large randomised trial evaluated hormonal treatments for endometriosis related pain with a long follow-up at three years, and also included an economic evaluation of postoperative use of LAP or COCP (the results will be reported in a separate paper). In addition to strict randomisation and flexibility in the interventions, the major strengths of this trial include its focus on patient centred outcomes, and the availability of primary outcome data on more than 80% (337/405) of participants. The pragmatic nature of the trial is more likely to enhance the generalisability of our findings, although the predominance of white women in the recruited sample limits our confidence about extrapolating the results to women from other ethnic groups.
The three year follow-up period and the pragmatic design meant that relatively few women continued on their initially allocated drug, changing or stopping their treatments depending on their circumstances, including changes in reproductive plans. The assumed improved continuation rate over COCP (25%) was marginal for DMPA (30%) but was evident for LNG-IUS (46%), which might mean the delivery method was better tolerated, but could equally represent the need to have the LNG-IUS removed at a medical facility. While these low adherence rates will have decreased the ability of the trial to detect a meaningful difference in efficacy between the two interventions, they do not necessarily detract from our ability to address the main aim of this pragmatic trial, which was to compare a policy of prescribing COCP or LAP after surgery for endometriosis over a three year time period.
PRE-EMPT provides data on only two of the three symptom outcomes in the core outcome set for endometriosis, which were published after the trial started.20The precision of comparison of secondary outcomes was decreased by missing data owing to the prioritisation of methods designed to capture the primary outcome.
Treating the two LAP preparations as a single intervention assumes a comparable mechanism of action and potential impact on symptoms. Both treatments cause progestogenic effects, but there might be other modes of action: LNG-IUS acts locally in the uterus while DMPA is systemic and results in ovarian suppression. Balanced subgroup analysis did not show any differential effect on primary outcome measures. The current design also limits power for meaningful comparisons between LNG-IUS or DMPA individually with COCP. While these factors make it difficult to comment on the efficacy of LAPs and COCP, the results of this trial allow a clear understanding of the medium term value of prescribing either class of drug after endometriosis surgery.
Strengths and weaknesses in relation to other studies
The prolonged duration of this trial, which started recruitment in 2014, means that newer hormonal treatment options for endometriosis have become available, including the fourth generation synthetic oral progestogen dienogest21and oral gonadotrophin releasing hormone antagonists22containing add-back hormone replacement. Importantly, however, LAPs and COCP are commonly used hormonal contraceptives worldwide; they are cheap, easily accessed, and have a well known side effect and safety profile. Although the follow-up period is the longest of any comparable trial,23the evidence provided by this trial is only relevant for the three years after surgery in a condition that can persist until menopause and often requires several episodes of further treatment.5
The absence of a no treatment option prevented exploration of the impact of surgery alone, although a systematic review involving 17 studies of various hormonal treatments for different endometriosis subtypes showed a decreased risk of recurrence associated with their use.24Our trial also assumes an inherent benefit from surgery, which has not been conclusively shown.25However, the first six months after surgery does reveal the biggest reduction in self-reported pain scores. An ongoing trial, ESPRIT 2 (https://www.ed.ac.uk/centre-reproductive-health/esprit2), aims to assess the short term impact of destruction of superficial endometriosis lesions compared with laparoscopy alone, but as choice of postoperative hormones will be determined by participants, LAPs and COCP will not be compared. Although recruitment was completed before the covid-19 pandemic, the restrictions on elective surgeries in 2020 and the length of subsequent surgical waiting lists might have reduced the number of repeat procedures.
Meaning of the study
The results of this trial show that prescribing a LAP or COCP is equally effective in reducing pain three years after endometriosis surgery, and reinforce current guidance recommending routine postsurgical hormonal treatment in this context. Women undergoing laparoscopic surgery can be informed that either class of hormonal drug reduces pain over a three year period and that LAPs could lower the risk of further surgery. Healthcare providers can note that prescribing LAPs reduces the need for further second line treatments.
Unanswered questions and future research
Other hormonal drugs, including dienogest and combination gonadotrophin releasing hormone antagonists with add-back hormone preparations, should be compared against LAPs and the COCP to determine relative effectiveness in preventing recurrence of pain, and their costs. The identification of non-invasive methods to diagnose endometriosis (radiological or by reliable blood and urinary biomarkers) to avoid the need for initial and repeat laparoscopy would be hugely beneficial. Therefore, future research should focus on early, non-invasive diagnosis and effective treatment of endometriosis to ensure long term alleviation of pain and improved quality of life.
Laparoscopic excisional or ablative surgery for endometriosis has been shown to improve symptoms of pain, but postoperative recurrence is common
The combined oral contraceptive pill (COCP) and progestogens are widely used to treat endometriosis related pain; long acting progestogens (LAPs) have the advantage of requiring less frequent administration
Uncertainty exists about which hormonal option (COCP or LAPs) is better for preventing recurrence of pain after surgery to remove endometriosis
Prescribing the COCP or LAPs after surgery for endometriosis resulted in a 40% reduction in pain scores in both treatment groups at three years
Women in the LAP treatment group were less likely to need second line medical treatments and further surgery
","Objectives: To evaluate the clinical effectiveness of long acting progestogens compared with the combined oral contraceptive pill in preventing recurrence of endometriosis related pain.
Design: The PRE-EMPT (preventing recurrence of endometriosis) pragmatic, parallel group, open label, randomised controlled trial.
Setting: 34 UK hospitals.
Participants: 405 women of reproductive age undergoing conservative surgery for endometriosis.
Interventions: Participants were randomised in a 1:1 ratio using a secure internet facility to a long acting progestogen (depot medroxyprogesterone acetate or levonorgestrel releasing intrauterine system) or the combined oral contraceptive pill.
Main outcome measures: The primary outcome was pain measured three years after randomisation using the pain domain of the Endometriosis Health Profile 30 (EHP-30) questionnaire. Secondary outcomes (evaluated at six months, one, two, and three years) included the four core and six modular domains of the EHP-30, and treatment failure (further therapeutic surgery or second line medical treatment).
Results: 405 women were randomised to receive a long acting progestogen (n=205) or combined oral contraceptive pill (n=200). At three years, there was no difference in pain scores between the groups (adjusted mean difference −0.8, 95% confidence interval −5.7 to 4.2, P=0.76), which had improved by around 40% in both groups compared with preoperative values (an average of 24 and 23 points for long acting progestogen and combined oral contraceptive pill groups, respectively). Most of the other domains of the EHP-30 also showed improvement at all time points compared with preoperative scores, without evidence of any differences between groups. Women randomised to a long acting progestogen underwent fewer surgical procedures or second line treatments compared with those randomised to the combined oral contraceptive pill group (73v97; hazard ratio 0.67, 95% confidence interval 0.44 to 1.00).
Conclusions: Postoperative prescription of a long acting progestogen or the combined oral contraceptive pill results in similar levels of improvement in endometriosis related pain at three years, with both groups showing around a 40% improvement compared with preoperative levels. While women can be reassured that both options are effective, the reduced risk of repeat surgery for endometriosis and hysterectomy might make long acting reversible progestogens preferable for some.
Trial registration: ISRCTN registryISRCTN97865475.
"
Ultra-processed food consumption and all cause and cause specific mortality,"Introduction
Ultra-processed foods are ready-to-eat/heat industrial formulations made mostly or entirely from substances derived from foods, including flavors, colors, texturizers, and other additives, with little if any intact whole food.1Ultra-processed foods, which are typically of low nutritional quality and high energy density, have been dominating the food supply of high income countries, and their consumption is markedly increasing in middle income countries.2Ultra-processed food consumption accounts for 57% of daily energy intake among adults and 67% among youths in the US according to the National Health and Nutrition Examination Survey (NHANES).34
Ultra-processed foods usually disproportionately contribute added sugars, sodium, saturated fats and trans fats, and refined carbohydrates to the diet together with low fiber.56As well as having low nutritional quality, ultra-processed foods may contain harmful substances, such as additives and contaminants formed during the processing.78910Growing evidence from large prospective cohorts show that ultra-processed food is associated with adverse health outcomes, such as overweight/obesity, cardiovascular diseases, type 2 diabetes, and colorectal cancer.11121314A systematic review showed that high ultra-processed food consumption was associated with increased risk of all cause mortality, cardiovascular diseases, metabolic syndrome, depression, and postmenopausal breast cancer.15However, few prospective cohort studies with a follow-up longer than 20 years have examined the association for all cause mortality or cause specific mortality, especially mortality due to cancer. High quality evidence from cohorts with a long follow-up is critical to inform dietary recommendations and food policies.
Leveraging the rich data obtained through repeated assessments for more than 30 years in two large US prospective cohorts, we examined the associations of total ultra-processed food and subgroups of ultra-processed food with mortality from all causes and major individual causes.
Methods
Study population
We used data from two large prospective cohorts in the US: the Nurses’ Health Study (NHS) began in 1976 and included 121 700 female registered nurses aged 30-55 years from 11 states; the Health Professionals Follow-up Study (HPFS) began in 1986 and enrolled 51 529 male health professionals aged 40-75 years from all 50 states. Every two years participants completed a mailed questionnaire enquiring about medical and lifestyle information. The baseline of this study was set to 1984 for the NHS and 1986 for the HPFS when the ultra-processed food data were first available. We excluded participants at baseline if they had reported a history of cancer, cardiovascular diseases, or diabetes; left more than 70 food items blank in the food frequency questionnaire or had implausible caloric intakes (<800 or >4200 kcal/d for men; <600 or >3500 kcal/d for women); or had missing data on ultra-processed food intakes. After exclusions, we included 74 563 women from the NHS and 39 501 men from the HPFS (supplementary figure A).
Assessment of ultra-processed food intake
Diet was assessed using a validated semiquantitative food frequency questionnaire administered every four years.16We grouped all foods into four categories of the Nova classification: unprocessed or minimally processed foods, processed culinary ingredients, processed foods, and ultra-processed foods, which has been described in detail elsewhere.17we further categorized ultra-processed foods into nine mutually exclusive subgroups (supplementary table B; supplementary figure B): ultra-processed breads and breakfast foods; fats, condiments, and sauces; packaged sweet snacks and desserts; sugar sweetened and artificially sweetened beverages; ready-to-eat/heat mixed dishes; meat/poultry/seafood based ready-to-eat products (for example, processed meat); packaged savory snacks; dairy based desserts; and other. Because alcohol is a well studied risk factor for premature death and a distinct factor in diet, we did not consider alcohol in ultra-processed foods in the primary analysis. Moreover, as wholegrain foods have established benefit for lowering all cause mortality,18we removed whole grains from ultra-processed foods in the primary analysis. We measured ultra-processed food intake as servings per day and adjusted it for total energy intake by using the residual method.19
Ascertainment of outcomes
Death of a cohort member was notified by the next of kin via the post office when questionnaires or newsletters were returned or was identified through searches of the vital records of states and of the National Death Index. Study investigators blinded to the exposure status reviewed death certificates and extracted information from medical records to confirm the cause of death according to ICD-8 (international classification of diseases, 8th revision). The primary outcome of this study was all cause mortality. The secondary outcomes included deaths from cancer (ICD-8 codes 140-207), cardiovascular diseases (ICD-8 codes 390-459), and other causes (including respiratory diseases (ICD-8 codes 460-519) and neurodegenerative diseases (ICD-8 codes 290, 332, 340, 342, and 348)).
Assessment of covariates
Biennial follow-up questionnaires were used to collect self-reported information on body weight, marital status, smoking status and pack years, physical activity, family history of cancer/cardiovascular diseases/diabetes, and physical examination for screening purposes, as well as menopausal status and postmenopausal hormone use for women. We calculated body mass index as weight in kilograms divided by height squared in meters. Physical activity was assessed with a validated questionnaire and converted into metabolic equivalent task hours.20Alcohol drinking was measured by food frequency questionnaires as the number of drinks per week (considering one drink as one glass, bottle, or can of beer; one 4 ounce glass of wine; or one shot of liquor) and then converted into grams per day. We assessed overall dietary quality by using the Alternative Healthy Eating Index-2010 (AHEI) score.21
Statistical analysis
Follow-up time accrued from the date of return of the baseline questionnaire to the date of death or the end of follow-up (30 June 2018 for NHS; 31 January 2018 for HPFS), whichever came first. To better represent long term dietary habits and to minimize within person variation, we calculated cumulative averages of ultra-processed food consumption as the primary exposure. We did primary analyses in pooled cohorts and a secondary analysis in each cohort separately. We used time varying Cox proportional hazards models stratified by age (months), questionnaire cycle (two year interval), and cohort (in pooled analyses) with the counting process data structure to estimate the hazard ratios and 95% confidence intervals according to quarters of ultra-processed food consumption. We calculated P for trend on the basis of the Wald test by assigning the median intake to each quarter and modeling it as a continuous variable. In the multivariable model, we adjusted for race/ethnicity, marital status, physical activity, body mass index, smoking status and pack years, alcohol consumption, physical examination performed for screening purposes, family history of diabetes mellitus, myocardial infarction, or cancer, and menopausal status and hormone use (women only). We carried forward non-missing values from the previous survey cycle to replace missing data. If the value remained missing, we created missing indicators. The percentage of missing data is shown in supplementary table A. We also tested for the dose-response relation by using the restricted cubic spline regression.22
In secondary analyses, we further categorized ultra-processed foods into mutually exclusive subgroups (supplementary tables B and C) to investigate whether the associations were driven by specific food groups.13Furthermore, to assess the independent and combined association of ultra-processed food consumption and overall dietary quality with mortality, we categorized individuals jointly according to quarters of AHEI score and quarters of ultra-processed food intake and estimated the hazard ratios by using participants with the highest quarter of AHEI score and lowest quarter of ultra-processed food intake as the reference.
We did several sensitivity analyses to test the robustness of the results. Firstly, given that people are likely to change their dietary habits after the diagnosis of certain chronic diseases, we stopped updating ultra-processed food consumption after the diagnosis of cardiovascular diseases, cancer, or diabetes during follow-up. Secondly, because of the uncertainty of the etiological time window, we introduced an eight to 12 year lag period between assessment of ultra-processed food intake and each follow-up period (for example, we used ultra-processed food intake from the 1986 questionnaire to assess the mortality risk in the period of 1994 to 1998). Thirdly, we added back to total ultra-processed food whole grains and distilled alcohol individually and in combination (that is, using the standard Nova definition) and repeated the analysis. Finally, we removed from the multivariable model pack years of smoking, which was not adjusted for in most previous studies, and further adjusted for AHEI score, to assess the confounding by smoking and dietary quality, respectively. We also removed from the multivariable model body mass index, which might be a mediator. Furthermore, we did the stratified analysis by major risk factors and repeated the primary analysis with ultra-processed food intake measured by percentage of energy.
We used SAS statistical package (version 9.4) for all the statistical analyses. We considered a P value <0.05 (two sided) to be statistically significant unless otherwise specified.
Patient and public involvement
The public was concerned about the health effects of ultra-processed foods, and their concerns informed our research question. Although participants were not involved in the study design, they played a central role in the conduct of the study by completing the biennial questionnaires in our cohorts, and we appreciate their contributions. We could not directly involve members of the public in this study, as no funding was available or set aside for patient and public involvement and our study team was not trained to work directly with the public.
Results
During a median of 34 years of follow-up, we documented 48 193 deaths (30 188 deaths of women and 18 005 deaths of men), including 13 557 deaths due to cancer, 11 416 deaths due to cardiovascular diseases, 3926 deaths due to respiratory diseases, and 6343 deaths due to neurodegenerative diseases.Table 1shows the characteristics of participants according to quarters of energy adjusted ultra-processed food consumption throughout follow-up. Participants with higher ultra-processed food consumption were younger, more physically inactive, and more likely to smoke and had higher body mass index, lower consumption of alcohol, whole fruits and vegetables, and whole grains, and lower AHEI score.
Table 2shows the hazard ratios of mortality according to quarters of ultra-processed food consumption. In the age, sex, and total calorie adjusted analysis, we observed strong positive associations between ultra-processed food and mortality outcomes. The associations became substantially attenuated in the multivariable analysis (table 2; supplementary figure C). Compared with participants in the lowest quarter (median 3.0 servings/day), those in the highest quarter (median 7.4 servings/day) had a 4% higher risk of total deaths (multivariable adjusted hazard ratio 1.04, 95% confidence interval 1.01 to 1.07; P for trend=0.005) and a 9% higher risk of other deaths (1.09, 1.05 to 1.13; P for trend<0.001), including an 8% higher risk of neurodegenerative deaths (1.08, 1.01 to 1.17; P for trend=0.1). We found no associations for deaths due to cardiovascular diseases, cancer, or respiratory diseases. The all cause mortality rate among participants in the lowest and highest quarter of ultra-processed food consumption was 1472 and 1536 per 100 000 person years, respectively.
Table 3shows the associations for nine subgroups of ultra-processed foods. Meat/poultry/seafood based ready-to-eat products (for example, processed meat) showed the strongest association with higher all cause mortality (hazard ratio 1.13 (1.10 to 1.16) comparing highest versus lowest quarter) and mortality due to individual causes other than cardiovascular diseases and neurodegenerative diseases (hazard ratios ranged from 1.06 to 1.43). Other subgroups also showed an association with higher all cause mortality, including sugar sweetened and artificially sweetened beverages (1.09, 1.07 to 1.12), other ultra-processed foods (mainly composed of artificial sweeteners) (1.08, 1.05 to 1.11), dairy based desserts (1.07, 1.04 to 1.10), and ultra-processed breakfast foods excluding whole grains (1.04, 1.02 to 1.07). When further separating sugar sweetened and artificially sweetened beverages, we found a generally stronger association for sugar sweetened than artificially sweetened beverages; we present these results and those for other selected individual ultra-processed food categories in supplementary table D.
When we examined ultra-processed food intake and AHEI score together (fig 1), we did not observe a consistent association of ultra-processed foods with mortality within each quarter of the AHEI score, whereas AHEI score generally showed an inverse association with mortality within each of the quarters of ultra-processed food consumption.
We found similar results in men and women (supplementary table E). The results of sensitivity analyses are summarized in supplementary table F. The lagged analysis showed similar results to the primary analysis. The associations were attenuated when we stopped updating the information on ultra-processed food intake at a diagnosis of chronic disease, likely owing to the increased intake of ultra-processed foods over time (supplementary figures D and E). Unsurprisingly, including wholegrain products in ultra-processed foods weakened the associations, whereas including distilled alcohol strengthened the associations. Removing pack years of smoking from the multivariable model led to a much stronger positive association, whereas adjusting for the AHEI score attenuated the association toward null.
In the stratified analysis by major risk factors, the associations between ultra-processed food intake and all cause mortality seemed to be stronger in participants consuming less alcohol (P for interaction=0.005) and not currently smoking (P for interaction<0.001), but we found no interaction by body mass index or physical activity (supplementary table G). We repeated the primary analysis using percentage of energy to measure ultra-processed food intake and observed similar results (supplementary table H).
Discussion
In two large prospective cohorts with up to 34 years of follow-up, we found that higher consumption of ultra-processed foods was associated with modestly higher all cause mortality. We found no associations for mortality due to cancer or cardiovascular diseases. The associations varied across subgroups of ultra-processed foods, with meat/poultry/seafood based ready-to-eat products consistently showing associations with higher all cause mortality and cause specific mortality. The associations between ultra-processed food consumption and mortality were attenuated after we accounted for overall dietary quality.
Comparison with other studies and possible explanations
Existing evidence suggests a relation between ultra-processed food consumption and mortality. A meta-analysis of prospective cohorts reported that the highest ultra-processed food consumption was associated with higher all cause mortality compared with the lowest consumption (hazard ratio 1.21, 1.13 to 1.30).23Two studies were conducted in the US,2425whereas the other six were conducted in Spain,262728France,29Italy,30and the UK.31Unlike our study, which excluded alcohol from ultra-processed foods and carefully controlled for smoking status and pack years, all the above studies included alcohol in ultra-processed foods and adjusted for smoking status (never, former, and current) only. As noted in our sensitivity analysis, pack years of smoking strongly confounded the association—additionally adjusting for smoking pack years remarkably attenuated the hazard ratios toward the null. That may partly explain why the associations found in our study were weaker than those in previous studies. Another possible reason could be tighter control for socioeconomic status because our participants were all health professionals and had similar levels of education.
The evidence on mortality due to cancer is relatively sparse. Consistently, the Moli-sani Study did not observe a statistically significant association but reported a positive association with other mortality.30An analysis of three cohorts including the Prostate, Lung, Colorectal and Ovarian Cancer Screening Trial (PLCO), NHANES (1999-2018), and UK Biobank reported null findings for mortality due to cancer in the PLCO and NHANES (1999-2018).32By contrast, the UK Biobank study found that every 10% increment in ultra-processed food consumption was associated with a 6% higher cancer mortality.33Diet was assessed in the UK Biobank through multiple 24 hour recalls between 2009 and 2012, and 40% of the participants had only one 24 hour recall, thus limiting the ability to capture long term dietary intake.
In agreement with our study, the Prospective Urban and Rural Epidemiology study from 25 high income, middle income, and low income countries in America, Europe, Africa, and Asia observed a null association with mortality due to cardiovascular diseases but a positive association with non-cardiovascular disease mortality.34Our findings on the relation between ultra-processed foods and mortality due to cardiovascular diseases are inconsistent with previous evidence from Europe but consistent with the null finding in the US NHANES III (1988-94).242530Moreover, a much stronger positive association was reported in the UK Biobank (1.28, 1.13 to 1.45) compared with the two US cohorts (1.12, 1.05 to 1.09; 1.11, 0.92 to 1.34).32In addition to the methodological differences mentioned above, different study populations, ultra-processed food compositions, and eating patterns may also contribute. Ultra-processed food intake in our two US cohorts is mainly contributed by “sauces, spreads, and condiments” and “sweet snacks and desserts,” which together accounted for nearly 50% (supplementary figure B), but neither of the two subgroups was associated with increased mortality due to cardiovascular diseases. On the other hand, compelling evidence shows that nuts and (dark) chocolate, common constituents of “sweet snacks and desserts,” are inversely associated with cardiovascular diseases.3536We observed that dark chocolate in the subgroup “packaged sweet snacks and desserts” was associated with decreased mortality (supplementary table D). Therefore, the diverse array of constituents contained in ultra-processed foods with heterogeneous health effects may have contributed to the discrepant findings. Our findings suggest that meat/poultry/seafood based ready-to-eat products and sugar sweetened and artificially sweetened beverages are major factors contributing to the harmful influence of ultra-processed foods on mortality, which is in accordance with previous studies.13373839
Few studies have investigated the relation with cause specific mortality other than that due to cancer and cardiovascular diseases. We found that ultra-processed food intake was associated with higher neurodegenerative mortality. Increasing evidence suggests that ultra-processed food is linked to higher risk of central nervous system demyelination (a precursor of multiple sclerosis),40lower cognitive function,41and dementia.42Studies have shown that a diet rich in ultra-processed foods may drive neuroinflammation and impairment of the blood-brain barrier, leading to neurodegeneration.4344Of note, among ultra-processed food subgroups, diary based desserts showed the strongest association with neurodegenerative mortality. Earlier finding from the HPFS and NHS cohorts showed that intake of sherbet/frozen yogurt was associated with an increased risk of Parkinson’s disease.45Furthermore, we found a positive association between ultra-processed food intake measured by percentage of energy and respiratory mortality. Emerging evidence suggests that higher ultra-processed food intake is associated with increased risk of respiratory multimorbidity.46The increased respiratory mortality associated with processed red meat may be partly due to heme iron and nitrate/nitrite.47
An important question not answered by previous studies is whether and how food processing level and nutritional quality jointly influence health. We observed that in the joint analysis, the AHEI score but not ultra-processed food intake showed a consistent association with mortality and that further adjustment for the AHEI score attenuated the association of ultra-processed food intake with mortality. Although including AHEI in the multivariable model for ultra-processed food may represent an overadjustment because common foods are included in both the AHEI and ultra-processed food, our data together suggest that dietary quality has a predominant influence on long term health, whereas the additional effect of food processing is likely to be limited. Furthermore, foods may have dual attributes according to their processing level and nutritional quality, and these two features may have quantitatively and even qualitatively different effects on health. Another added value of our study is the exclusion of wholegrain products that fall in the ultra-processed foods from the primary exposure, based on the well established health benefits associated with whole grains. By taking this approach, we aim to rectify the potential misperception that all ultra-processed food products should be universally restricted and to avoid oversimplification when formulating dietary recommendations.
Besides neglecting overall nutritional quality, the ultra-processed food classification system has other limitations. The Nova classification is based on broad categories that do not capture the full complexity of food processing,48leading to potential misclassification. Further work is needed to improve the assessment and categorization of ultra-processed foods. On the other hand, dietary guidelines should provide clear and sound food selections that are available, actionable, attainable, and affordable for the largest proportion of the population. Thus, careful deliberation is necessary when considering incorporation of ultra-processed foods into dietary guidelines.4950Again, on the basis of our data, limiting total ultra-processed food consumption may not have a substantial influence on premature death, whereas reducing consumption of certain ultra-processed food subgroups (for example, processed meat) can be beneficial.
We note that mortality is a more complicated endpoint than disease incidence and is also influenced by several factors including early detection, treatment, and individuals’ overall health status. The findings for mortality should not be regarded as synonymous with those pertaining to disease incidence but rather considered as more comprehensive assessment of the health impact of risk factors.
Strengths and limitations of study
The strengths of the study include the prospective study design, large sample size, long follow-up, and detailed, validated, and repeated measurements. In addition, we rigorously controlled for confounding, did thorough sensitivity analyses, explored major specific causes of mortality, and examined individual ultra-processed food subgroups. Several limitations should also be noted. Firstly, we cannot rule out unmeasured and residual confounding due to the nature of the observational study. Secondly, our participants are health professionals and predominantly non-Hispanic white, limiting the generalizability of our findings. Thirdly, as the food frequency questionnaires collected intake of only a limited number of pre-defined items representing the primary source of energy and nutrients in the US population and were not designed to classify foods by processing level, they may not capture the full spectrum of ultra-processed foods. Although the food frequency questionnaires used in our cohorts have been validated for foods and nutrients, they were not specifically validated for ultra-processed foods. Moreover, we classified ultra-processed foods by using the same algorithm throughout follow-up that did not account for changes in the grade of food processing over time. These factors may have introduced non-differential misclassification, likely biasing our results toward the null.
Conclusions
Higher ultra-processed food intake was associated with slightly increased all cause mortality. The mortality associations for ultra-processed food consumption were more modest than those for dietary quality and varied across ultra-processed food subgroups, with meat/poultry/seafood based ready-to-eat products generally showing the strongest and most consistent associations with mortality. The findings provide support for limiting consumption of certain types of ultra-processed food for long term health. Future studies are warranted to improve the classification of ultra-processed foods and confirm our findings in other populations.
","Objective: To examine the association of ultra-processed food consumption with all cause mortality and cause specific mortality.
Design: Population based cohort study.
Setting: Female registered nurses from 11 US states in the Nurses’ Health Study (1984-2018) and male health professionals from all 50 US states in the Health Professionals Follow-up Study (1986-2018).
Participants: 74 563 women and 39 501 men with no history of cancer, cardiovascular diseases, or diabetes at baseline.
Main outcome measures: Multivariable Cox proportional hazard models were used to estimate hazard ratios and 95% confidence intervals for the association of ultra-processed food intake measured by semiquantitative food frequency questionnaire every four years with all cause mortality and cause specific mortality due to cancer, cardiovascular, and other causes (including respiratory and neurodegenerative causes).
Results: 30 188 deaths of women and 18 005 deaths of men were documented during a median of 34 and 31 years of follow-up, respectively. Compared with those in the lowest quarter of ultra-processed food consumption, participants in the highest quarter had a 4% higher all cause mortality (hazard ratio 1.04, 95% confidence interval 1.01 to 1.07) and 9% higher mortality from causes other than cancer or cardiovascular diseases (1.09, 1.05 to 1.13). The all cause mortality rate among participants in the lowest and highest quarter was 1472 and 1536 per 100 000 person years, respectively. No associations were found for cancer or cardiovascular mortality. Meat/poultry/seafood based ready-to-eat products (for example, processed meat) consistently showed strong associations with mortality outcomes (hazard ratios ranged from 1.06 to 1.43). Sugar sweetened and artificially sweetened beverages (1.09, 1.07 to 1.12), dairy based desserts (1.07, 1.04 to 1.10), and ultra-processed breakfast food (1.04, 1.02 to 1.07) were also associated with higher all cause mortality. No consistent associations between ultra-processed foods and mortality were observed within each quarter of dietary quality assessed by the Alternative Healthy Eating Index-2010 score, whereas better dietary quality showed an inverse association with mortality within each quarter of ultra-processed foods.
Conclusions: This study found that a higher intake of ultra-processed foods was associated with slightly higher all cause mortality, driven by causes other than cancer and cardiovascular diseases. The associations varied across subgroups of ultra-processed foods, with meat/poultry/seafood based ready-to-eat products showing particularly strong associations with mortality.
"
Comparative effectiveness of second line oral antidiabetic treatments among people with type 2 diabetes mellitus,"Introduction
About 463 million people worldwide (9.3%) have type 2 diabetes mellitus.1In most people this disease is progressive, and it is associated with risks of multiple complications, including cardiovascular disease (CVD) and chronic kidney disease.2Interventions that improve biomarkers of type 2 diabetes mellitus, such as glycated haemoglobin A1c(HbA1c), blood pressure, and lipid levels, can reduce the risk of these complications.3456International clinical guidelines recommend additional drugs (second line treatment) if glycaemic control is inadequate after metformin monotherapy.789A recent study of second line treatments for people with type 2 diabetes mellitus across 38 countries reported that the most commonly used oral drugs were dipeptidyl peptidase-4 (DPP-4) inhibitors (48.3%), sulfonylureas (40.9%), and sodium-glucose cotransporter-2 (SGLT-2) inhibitors (8.3%).10
Of these oral treatments, SGLT-2 inhibitors are newer and more costly classes of drugs.11In England, SGLT-2 inhibitors are recommended second line treatments in preference to other drug classes for some people with type 2 diabetes mellitus—those with pre-existing CVD, at high risk of CVD, or with kidney disease.7For most people with type 2 diabetes mellitus, however, evidence on the comparative effectiveness of these alternative drugs classes, particularly in relation to reducing HbA1clevels, is insufficient to recommend a particular second line treatment.7An international consensus statement9and guidelines from the National Institute of Health and Care Excellence (NICE)7therefore leaves the choice of second line treatment for most people with type 2 diabetes mellitus to clinicians and patients, which has led to wide variation across groups of primary care providers in England in the proportion of people prescribed each drug class.12Current NICE (2022) guidelines recommend other antidiabetic treatments, such as insulin based therapy and glucagon-like peptide-1 receptor agonists, only if HbA1clevels are not controlled after second line treatment with oral antidiabetics.7Hence in many countries, including England, the proportion of people with type 2 diabetes mellitus who are prescribed glucagon-like peptide-1 receptor agonists as second line treatment is low.101213
Most randomised controlled trials assessing the effectiveness and safety of SGLT-2 inhibitors and DPP-4 inhibitors have randomised groups to an active intervention or placebo comparator.14151617181920212223242526Therefore, although these trials reported fewer CVD and kidney events in people with and without type 2 diabetes mellitus allocated to SGLT-2 inhibitors, the results are difficult to apply to routine clinical practice, where the relevant populations and comparators differ.161718192021222324Of the randomised controlled trials with an active comparator, some compared DPP-4 inhibitors with sulfonylureas27282930or compared SGLT-2 inhibitors with sulfonylureas,31but none compared all three drug classes. Thus the comparative effectiveness of SGLT-2 inhibitors versus alternative second line oral antidiabetic treatments on outcomes important to people with type 2 diabetes mellitus, particularly reduction in HbA1clevel, remains unclear. Results from previous observational studies comparing these treatments323334are at risk of bias from residual (unmeasured) confounding. Although a recent observational study35emulated some of the results of the GRADE (Glycemia Reduction Approaches In Diabetes: A Comparative Effectiveness Study) randomised trial,293637neither the trial nor the observational study considered SGLT-2 inhibitors, which limits the applicability of the results to routine clinical practice.
Recent advances in real world data combined with developments in quantitative methods offer important opportunities for generating evidence on comparative effectiveness of treatments with direct relevance to clinical practice.35In this study, we illustrated the potential and challenges of using real world data from Clinical Practice Research Datalink (CPRD) for these purposes. We emulated the design of a hypothetical pragmatic randomised controlled trial by comparing three antidiabetic drug classes (sulfonylureas, DPP-4 inhibitors, and SGLT-2 inhibitors) of interest to the broad population of people with type 2 diabetes mellitus who, according to current NICE guidelines, are eligible for any of these second line treatments. We considered intermediate metabolic outcomes, particularly HbA1clevel, but also kidney and cardiovascular related complications. To reduce the risk of unmeasured confounding we used prescriber variation as an instrumental variable to estimate treatment effectiveness from routine data.3839Our study complements a recent target trial emulation that assessed the comparative effectiveness of alternative second line treatments using data from the Department of United States Veterans Affairs,40but which underrepresented female members of the population (<10%) and in the main analyses assumed that that there was no unmeasured confounding.
We compared the effectiveness of the three most prescribed second line antidiabetic treatments in the UK according to metabolic and other clinical measures (changes from baseline in HbA1clevel, estimated glomerular filtration rate (eGFR), body mass index (BMI), and systolic blood pressure) and to adverse clinical endpoints (kidney and cardiovascular outcomes, and death).
Methods
Study design
We designed this study according to the target trial framework.41Briefly, a target trial is a hypothetical randomised controlled trial for assessing comparative effectiveness from observational data that requires pre-specification of the main elements of a trial’s protocol, including eligibility criteria, the respective treatment strategies, time zero, and an analysis plan.41The target trial emulation reported in this paper is part of the PERMIT (PERsonalised Medicine for Intensification of Treatment) study, which prespecified the definition of the eligibility criteria and treatment strategies in the published versions of the study protocol42and other elements of the target trial emulation in the statistical analysis plan.43Supplementary table 1 provides details to accompany this paper of how each of the standpoints were emulated (eligibility criteria, treatment assignment, initiation, and strategy, follow-up, outcomes, causal contrasts of interest, and analysis strategy).
We applied target trial principles to primary care data from CPRD to identify people with type 2 diabetes mellitus who had a similar prognosis before initiating any of the three second line antidiabetic treatments under comparison. CPRD covers about 20% of the UK population registered with general practices and includes longitudinal information on primary care diagnoses, prescriptions, personal information, and laboratory test results.4445Linkage from CPRD to Hospital Episode Statistics in-patient data was available for about 90% of participating practices in England. We accessed information from Hospital Episodes Statistics admitted patient care database on diagnoses, procedures, sociodemographic characteristics, and admission and discharge dates.46Rather than relying on a single data source to ascertain cardiovascular and kidney outcomes, we used linked data from CPRD-Hospital Episodes Statistics as these have been shown to improve capture of these events and reduce risks of misclassification.4748Information on each person’s vital status was available through linkage to the Office for National Statistics (ONS) death records.4950
Study population
We defined the study population according to eligibility criteria, which had to be met before time zero (baseline) and was analogous to the time of randomisation in a randomised controlled trial. Time zero was defined by the date of the first prescription for any of the three oral second line treatments that were added to metformin (see supplementary table 1). We followed precedent research by including people with a diagnosis of type 2 diabetes mellitus who were aged 18 years or older,3351registered with a general practice in England, and who intensified treatment from first line to second line oral antidiabetic treatment between 1 January 2015 and 31 December 2020 with a first ever prescription of sulfonylureas, DPP-4 inhibitors, or SGLT-2 inhibitors added to metformin. Those eligible had to have at least one prescription for metformin monotherapy within 60 days before the first prescription for second line treatment, to ensure their use of metformin monotherapy was continuous before intensification. We excluded individuals with pregnancy recorded within 12 months before initiation of second line treatment and people whose last recorded eGFR was <30 mL/min/1.73m2, since prescribing guidelines recommend different treatments for these groups. We also excluded people whose general practices had not consented to the required linkage of Hospital Episodes Statistics data. We followed precedent research in excluding those who were not prescribed metformin on the same day or within 60 days after initiating second line treatment,33as it is unlikely that their treatment with metformin continued. Supplementary tables 1 and 2 present detailed inclusion and exclusion criteria.
Treatments under comparison
We compared DPP-4 inhibitors with sulfonylureas and SGLT-2 inhibitors with sulfonylureas and DPP-4 inhibitors as second line oral antidiabetic treatments added to metformin. Information was extracted on the prescribed duration of each treatment and any subsequent antidiabetic treatment.
The study used an intention-to-treat approach so that individuals contributed to the treatment group to which they were assigned at baseline until the end of the follow-up period (see supplementary table 1), irrespective of the extent to which they adhered to the treatment prescribed. We defined the end of follow-up as the earliest of the date the general practice stopped contributing to CPRD, the date the individual left the general practice, the date of death, or the last date of available data (31 December 2021 for continuous outcomes or 31 March 2021 for time-to-event outcomes). We described the duration of second line and third line treatments by comparison group.
Covariates
We have previously described the covariates in detail,1142and these are summarised in supplementary table 3. Briefly, we defined patient sociodemographic characteristics (age, sex, ethnicity, index of multiple deprivation), time since diagnosis of type 2 diabetes mellitus, year of initiation of second line antidiabetic treatment, NHS region (East of England, London, Midlands, North East and Yorkshire, North West, South East, and South West),52number of patients registered with the participants’ general practice, smoking and alcohol intake status, relevant co-prescriptions (renin-angiotensin system inhibitors or statins) issued within 60 days before baseline, hospital admission (any) in the previous year, and comorbidities recorded at baseline (history of myocardial infarction, unstable angina, previous stroke, ischaemic heart disease, hypoglycaemia, heart failure, history of any cancer, history of proteinuria, advanced eye disease, lower limb amputation, and impaired kidney function (latest eGFR <60 mL/min/1.73m2). We also defined HbA1c, systolic blood pressure, diastolic blood pressure, eGFR, and BMI53using the most recent measures recorded in primary care.
For the primary endpoint, change in HbA1clevel, we only considered the most recent measure within 180 days before time zero as the baseline measure in line with NICE guidance, which recommends that HbA1Cis measured every six months.7For systolic and diastolic blood pressure and eGFR we followed previous research in considering the most recent measure within 540 days before baseline33(see supplementary table 3). We considered any values recorded in advance of these time windows as out-dated, and they were not used to define baseline characteristics. For BMI we followed a previously published algorithm in using the most recent measure available, which for most participants was within six months.53
Outcomes
The primary outcome was the absolute change in HbA1c(mmol/mol) level between baseline and one year after each prescription for second line treatment (HbA1cvalue at one year–HbA1cvalue at baseline). Treatment groups were compared according to the mean change in HbA1clevel. We used the measurement closest in time to the one year follow-up time point and allowed for measures within ±90 days, otherwise the measure was designated as missing.
Secondary outcomes included change in HbA1clevel at two years and change in BMI, systolic blood pressure, and eGFR at one year and two years.33We also reported the time to several first events before two years’ follow-up: a ≥40% decline in eGFR from baseline, which could be a marker for the rarer end stage kidney disease outcome54; a major adverse kidney event, a composite outcome for the earliest of a decline in eGFR from baseline of 40%, end stage kidney disease, and all cause mortality55; hospital admission for heart failure; major adverse cardiovascular event (MACE), a composite outcome for the earliest of myocardial infarction, stroke, or CVD death; and all cause mortality. We also reported time to myocardial infarction and stroke individually. Time to end stage kidney disease and CVD specific mortality could not be reported owing to the low number of events. Individuals were followed until they experienced the event of interest, died, or linked CPRD-Hospital Episodes Statistics data were no longer available (patient/general practice stopped contributing data to the CPRD or 31 March 2021). For these time-to-event measures, we only considered outcomes within the first two years in the base case, as it was anticipated that at later time points a high proportion of individuals would have censored or missing data. Supplementary table 4 provides details on all outcome definitions, including data sources.
Statistical analysis
We chose to use an instrumental variable analysis to help reduce the risk of confounding from unobserved baseline measures, such as diet and exercise before initiation of second line treatment (see supplementary methods, supplementary table 1, and supplementary figures 1A and 1B).38The instrumental variable was the primary care providers’ tendency to prescribe the three classes of second line treatment. In England, most primary care clinicians work within a group, and over the study’s timeframe this was defined as a clinical commissioning group (CCG), which informed health funding decisions for its respective geographical region. Some CCGs recommended that a relatively high proportion of people had second line treatment with sulfonylureas or DPP-4 inhibitors due in part to the higher cost of SGLT-2 inhibitors. We therefore defined CCGs rather than individual general practices as the unit for the instrumental variable, as this reflected decision making and was strongly associated with choice of second line treatment.1112
We also found wide variation across CCGs in the proportion of people prescribed each of the three classes of second line treatment (fig 1). This natural variation implied that people with a similar prognosis at baseline received a different second line treatment simply according to their CCG. We defined the tendency to prescribe as the proportion of eligible people prescribed each second line treatment within the 12 months preceding the specific baseline (time zero) for each person. A valid instrument must meet four main conditions (see also the direct acyclic graph in supplementary figures 1A and 1B).38Firstly, the instrument must predict the treatment prescribed, which can be formally assessed.56Here, we assessed the relevance of the CCGs tendency to prescribe using a weak instrument test that is robust to heteroscedasticity and clustering by NHS region. Recent work has suggested that to meet the requirement that the instrument is of sufficient strength, the F statistic summarising the association between the instrumental variable and the treatment received must exceed 100.3857Secondly, the instrument must be independent of covariates that predict the outcomes of interest, which can be partially evaluated. We assessed the extent to which observed prognostic covariates differed across levels of the instrument (see supplementary figures 2A-2C). Thirdly, the instrument must have an effect on the outcomes only through the treatment received, which cannot be evaluated empirically. Large imbalances in measured covariates across levels of the tendency to prescribe would raise concerns about the second and third instrumental variable assumptions. We followed our prespecified protocol42and the statistical analysis plan43and were guided by the direct acyclic graphs (see supplementary figures 1A and 1B) in choosing to adjust for measured contextual and temporal confounders in the second stage (outcome) regression. By including these contextual covariates in the second stage regression we were able to make weaker assumptions, that the tendency to prescribe was independent of the outcome and only had an effect on the outcome through the treatment received after adjusting for any differences in region, general practice size, and time period (see supplementary file). Fourthly, the instrumental variable analysis assumes monotonicity, which implies that as the levels of the instrumental variable change this should have the same direction of effect on the treatment prescribed across similar individuals. However, this assumption cannot be verified.58Indeed, in our study, we cannot observe the same treatment choice for a particular individual according to their attendance at two CCGs with different levels of prescribing preference for SGLT-2 inhibitors (versus DPP-4 inhibitors or sulfonylureas). For the population, this assumption implies that the average treatment choice must increase or decrease monotonically with the level of the instrumental variable.59Hence it is plausible to assume that if a group of patients whose CCG had a moderate preference for prescribing SGLT-2 inhibitors were prescribed this drug class, then a similar group of patients whose CCG had a stronger preference for prescribing SGLT-2 inhibitors would not be prescribed DPP-4 inhibitors or sulfonylureas.59
We used the two stage residual inclusion method for the instrumental variable analysis,60which enabled us to assess comparative effectiveness across the full study populations of interest—that is, to report average treatment effects while reducing the risk of bias from unmeasured confounding. The first stage models estimated the probabilities that each person was prescribed each treatment given their baseline covariates and their CCGs tendency to prescribe that treatment.61The second stage outcome models then included generalised residuals from the first stage (propensity score) models. We estimated the outcome models by ordinary least squares for continuous outcomes (eg, HbA1clevel at one year) and by Cox proportional hazards models for time-to-event outcomes with an individual frailty.32Models for both stages included all measured baseline covariates, with polynomials and covariate interactions selected through a post-double selection approach using least absolute shrinkage and selection operator regression626364(see supplementary methods table S1). The purpose of including person level covariates in the second stage (outcome regression) was to gain precision in estimating the relative treatment effects.
Some data were missing for outcomes (metabolic and other clinical measures) and baseline covariates (ethnicity, index of multiple deprivation, HbA1c, systolic blood pressure, diastolic blood pressure, BMI, eGFR, smoking and alcohol intake status) because the participants’ general practices either had not recorded these measures or had, but outside the requisite time window for a specific time point. At one year and two years, the percentages of missing values were, respectively, 33.7% and 36.4% for HbA1c, 44.7% and 47.8% for BMI, 33.6% and 37.2% for systolic blood pressure, and 37.4% and 40.0% for eGFR. For some people, a measurement that was not available at a particular time point (eg, two years) was available at other time points (eg, one year and three years) (see supplementary methods table S2). It was also possible that at any time point, one measure (eg, BMI) was not available, whereas other measures (eg, HbA1c, systolic blood pressure, and eGFR) were available.
We chose to handle all missing baseline and longitudinal outcome data by multiple imputation65with chained equations.66This approach assumed data were missing at random. The imputation of each longitudinal outcome at a given time point used all relevant information, including measurements of the same outcome at other time points. This use of auxiliary information can help the study recover more accurate estimates of the unknown outcome values.67This also ensured our study population was comparable at each time point. Partially observed covariates and outcomes6768were multiple imputed by predictive mean matching with 10 donors,69producing five imputed datasets. The number of imputations was driven by the need to balance computational time with improved inference from increasing the number of imputations (see supplementary methods for further details). The imputation models developed for each covariate were congenial with the form of outcome70(continuous or time to event). For the time-to-event endpoints, it was assumed no data were missing. All imputation models were stratified by second line treatment (DPP-4 inhibitors, SGLT-2 inhibitors, sulfonylureas) and by whether the individual died or was censored before the relevant study end date (see supplementary methods).
We reported differences between the comparison groups according to absolute change in outcomes between baseline and follow-up for continuous measures, and according to time-to-event measures. We reported results overall and according to whether patients had or did not have CVD (at least one of previous myocardial infarction, previous stroke, heart failure, ischaemic heart disease, or unstable angina) recorded before initiation of second line treatment. To recognise statistical uncertainty in the estimates of treatment effects, the data were bootstrapped 500 times, stratified by CCG, treatment group, and death and censoring status to maintain the structure of the original sample across replicates. Within each bootstrap resample we implemented multiple imputation with chained equations,7172with Rubin’s first rule65applied across the five imputed datasets to obtain overall treatment effects for each bootstrap sample, which we then used to estimate variances and calculatetbased bootstrap confidence intervals (CI). The imputation procedure and time-to-event analyses were performed with multiple imputation with chained equations and the survival package7374in R 4.2.2, respectively,75and the analysis of the clinical measures in Stata 17.76
Alternative analyses
We undertook alternative analyses to check the impact of different statistical assumptions on our results. Firstly, we applied complete case analysis rather than multiple imputation with chained equations (base case) to examine whether the results were robust when alternative approaches were applied to handle missing data. Secondly, we applied two stage least squares (continuous outcomes), multivariable linear regression (continuous outcomes), and Cox regression analysis (time to event), adjusting for all measured baseline covariates, to assess the sensitivity of our approach to confounding adjustment. Thirdly, we extended the follow-up period to five years rather than two years. Fourthly, in additional analyses that were not prespecified, we further checked the impact of applying approaches that, as with multivariable regression, assumed no unmeasured confounding but can be less sensitive to the form of outcome regression model. We applied two approaches based on propensity scores—inverse probability of treatment weighting77and inverse probability of treatment weighting with regression adjustment (weighted regression hereafter),78with non-stabilised and stabilised weights.79We also used asymmetrical trimming to understand any effects of large weights in the weighted regression analysis.8081The weighted regression has the so called double robustness property, in that, subject to the assumption of no unobserved confounding, it can still provide consistent estimates provided either the propensity score or the regression model is correctly specified.7882The multivariable regression analyses, the inverse probability of treatment weighting, and the weighted regression analyses all estimate the average treatment effects as in the base case. We undertook the alternative analyses on the complete cases only.
Patient and public involvement
Patient and public involvement advisors, including a coauthor on this paper (PC), helped inform the design and proposed analysis, including the choice of outcome measures. We will reconvene a patient and public involvement workshop to discuss the study findings and co-produce a lay summary that will be available on the PERMIT study website.83
Results
Study population and baseline characteristics
The study population included 75 739 people with type 2 diabetes mellitus who initiated second line oral antidiabetic treatment with sulfonylureas, DPP-4 inhibitors, or SGLT-2 inhibitors and met all eligibility criteria (fig 2). Of these, 25 693 (33.9%) initiated treatment with sulfonylureas, 34 464 (45.5%) with DPP-4 inhibitors, and 15 582 (20.6%) with SGLT-2 inhibitors, in addition to metformin. Supplementary table 5 reports the frequencies of prescribing for each drug within each drug class. The drugs most commonly prescribed within each drug class were gliclazide (sulfonylurea), sitagliptin (DPP-4 inhibitor), and empagliflozin (SGLT-2 inhibitor). People prescribed SGLT-2 inhibitors were younger (56 (standard deviation (SD) 11) years) than those prescribed DPP-4 inhibitors (62 (SD 12) years) or sulfonylureas (60 (SD 13) years) (table 1). The baseline mean HbA1clevel was higher for people prescribed sulfonylureas (81 (SD 22) mmol/mol) compared with those prescribed DPP-4 inhibitors (72 (SD 16) mmol/mol) or SGLT-2 inhibitors (75 (SD 17) mmol/mol), and a lower proportion of people prescribed SGLT-2 inhibitors had comorbidities—for example, 17.2% (n=2680) of those prescribed SGLT-2 inhibitors had prevalent CVD compared with 22.8% (n=5858) of those prescribed sulfonylureas and 23.5% (n=8108) prescribed DPP-4 inhibitors. The proportion of people prescribed SGLT-2 inhibitors increased from 7.3% in 2015 to 24.9% in 2020. The median time between recorded BMI and the index date was 19 days (interquartile range (IQR) 0-140 days).
Within two years of follow-up, the median (IQR) time prescribed second line antidiabetic treatment was lower for those using sulfonylureas (248 (IQR 67-671) days) compared with DPP-4 inhibitors (345 (IQR 96-730) days) and SGLT-2 inhibitors (328 (IQR 84-730) days). The proportion of participants who switched to a third line treatment within two years of the index date was 58.8% (sulfonylureas, n=15 107), 51.5% (DPP-4 inhibitors, n=17 749), and 52.5% (SGLT-2 inhibitors, n=8184), with metformin monotherapy the most common third line treatment for all three comparison groups (see supplementary table 6). In each comparison group, the proportions of people whose third line treatment was triple therapy were 25.1% (sulfonylureas), 31.7% (DPP-4 inhibitors), and 21.8% (SGLT-2 inhibitors).
Empirical assessment of instrumental variable assumptions
The tendency to prescribe met a major requirement for being a valid instrumental variable, in that it was strongly associated with the second line treatment prescribed (assumption 1), with accompanying F statistics of 1902 for DPP-4 inhibitors and 1935 for SGLT-2 inhibitors, which indicated that the instrumental variable was of sufficient strength (F>100).3857The measured potential confounders were balanced across levels of the tendency to prescribe (assumption 2), aside from time period, which was included within the covariate adjustment of the instrumental variable analysis (see supplementary figures 2A-2C).
Intermediate metabolic and other clinical measures
The crude change in mean HbA1clevel from baseline to one year follow-up among people with observed follow-up measures was greatest for those prescribed sulfonylureas (−18 mmol/mol) compared with DPP-4 inhibitors (−10 mmol/mol) and SGLT-2 inhibitors (−14 mmol/mol;fig 3, also see supplementary figure 3). Of those people not censored by one year follow-up (n=72 066), 33.7% were missing HbA1cvalues at this time point (see supplementary methods table 2). Although levels of missing data were higher for those time points that occurred after the onset of the covid-19 pandemic, the levels of missing data remained similar across the comparison groups (see supplementary table 7).
The crude changes in mean BMI and systolic blood pressure from baseline were small across all time points (fig 3, also see supplementary figure 3). The crude change in mean eGFR from baseline to one year follow-up was similar across the three second line treatments of interest (−2 mL/min/1.73m2), with smaller decreases in mean eGFR across subsequent follow-up periods among people prescribed SGLT-2 inhibitors rather than sulfonylureas or DPP-4 inhibitors (fig 3, also see supplementary figure 3).
Figure 4presents the results from the instrumental variable analysis, which reduces the risk of confounding, and after applying multiple imputation with chained equations to handle the missing data. The results apply to the full study population. Strong evidence was found for SGLT-2 inhibitors being more effective in reducing HbA1clevels between baseline and one year follow-up, with a mean reduction of −2.5 mmol/mol (95% CI −3.7 to −1.3) compared with sulfonylureas and −3.2 mmol/mol (−4.6 to −1.8) compared with DPP-4 inhibitors (fig 4, also see supplementary table 8). After accounting for confounding and missing data, SGLT-2 inhibitors were more effective in improving BMI and systolic blood pressure (fig 4). People prescribed SGLT-2 inhibitors showed a greater reduction in BMI between baseline and one year, with a mean difference of −1.6 (95% CI −1.7 to −1.4) compared with sulfonylureas and −0.8 (−1.0 to −0.7) compared with DPP-4 inhibitors. For systolic blood pressure, the mean difference was −2.1 mm Hg (95% CI −3.1 to −1.0) compared with sulfonylureas and −1.8 mm Hg (−3.0 to −0.5) compared with DPP-4 inhibitors, with these improvements maintained at two years follow-up. SGLT-2 inhibitors led to a slower decline in eGFR at two years follow-up compared with sulfonylureas (mean difference 1.4 mL/min/1.73m2, 95% CI 0.5 to 2.3), but not compared with DPP-4 inhibitors (0.0 mL/min/1.73m2, −1.1 to 1.0).
Kidney, cardiovascular, and mortality outcomes
People prescribed SGLT-2 inhibitors had lower crude rates of all adverse kidney, cardiovascular, and mortality events compared with those prescribed sulfonylureas and DPP-4 inhibitors (see supplementary table 9 and supplementary figures 4-9). After reducing the risk of confounding and addressing the missing data, we found that over two years follow-up (base case), SGLT-2 inhibitors were more effective in preventing a ≥40% decline in eGFR from baseline versus sulfonylureas (hazard ratio 0.42, 95% CI 0.22 to 0.81), but the estimated hazard ratios for SGLT-2 inhibitors compared with DPP-4 inhibitors were highly uncertain (0.64, 0.29 to 1.43) (fig 5). The rates of admission to hospital for heart failure were lower for SGLT-2 inhibitors compared with sulfonylureas (0.46, 0.20 to 1.05) and with DPP-4 inhibitors (0.32, 0.12 to 0.85). For the other endpoints, we found no evidence of a difference in the comparative effectiveness of the second line antidiabetic treatments (fig 5, also see supplementary table 10). We found no evidence that having CVD before starting second line treatment was associated with modified relative effectiveness of these three treatments (see supplementary tables 11 and 12).
Alternative analyses
The findings of the complete case analyses were similar when applying multiple imputation to deal with missing data (see supplementary tables 10 and 13). The results were also similar if the risk of confounding was dealt with using two stage least squares, an alternative instrumental variable approach (see supplementary table 14). Th","Objective: To compare the effectiveness of three commonly prescribed oral antidiabetic drugs added to metformin for people with type 2 diabetes mellitus requiring second line treatment in routine clinical practice.
Design: Cohort study emulating a comparative effectiveness trial (target trial).
Setting: Linked primary care, hospital, and death data in England, 2015-21.
Participants: 75 739 adults with type 2 diabetes mellitus who initiated second line oral antidiabetic treatment with a sulfonylurea, DPP-4 inhibitor, or SGLT-2 inhibitor added to metformin.
Main outcome measures: Primary outcome was absolute change in glycated haemoglobin A1c(HbA1c) between baseline and one year follow-up. Secondary outcomes were change in body mass index (BMI), systolic blood pressure, and estimated glomerular filtration rate (eGFR) at one year and two years, change in HbA1cat two years, and time to ≥40% decline in eGFR, major adverse kidney event, hospital admission for heart failure, major adverse cardiovascular event (MACE), and all cause mortality. Instrumental variable analysis was used to reduce the risk of confounding due to unobserved baseline measures.
Results: 75 739 people initiated second line oral antidiabetic treatment with sulfonylureas (n=25 693, 33.9%), DPP-4 inhibitors (n=34 464 ,45.5%), or SGLT-2 inhibitors (n=15 582, 20.6%). SGLT-2 inhibitors were more effective than DPP-4 inhibitors or sulfonylureas in reducing mean HbA1cvalues between baseline and one year. After the instrumental variable analysis, the mean differences in HbA1cchange between baseline and one year were −2.5 mmol/mol (95% confidence interval (CI) −3.7 to −1.3) for SGLT-2 inhibitors versus sulfonylureas and −3.2 mmol/mol (−4.6 to −1.8) for SGLT-2 inhibitors versus DPP-4 inhibitors. SGLT-2 inhibitors were more effective than sulfonylureas or DPP-4 inhibitors in reducing BMI and systolic blood pressure. For some secondary endpoints, evidence for SGLT-2 inhibitors being more effective was lacking—the hazard ratio for MACE, for example, was 0.99 (95% CI 0.61 to 1.62) versus sulfonylureas and 0.91 (0.51 to 1.63) versus DPP-4 inhibitors. SGLT-2 inhibitors had reduced hazards of hospital admission for heart failure compared with DPP-4 inhibitors (0.32, 0.12 to 0.90) and sulfonylureas (0.46, 0.20 to 1.05). The hazard ratio for a ≥40% decline in eGFR indicated a protective effect versus sulfonylureas (0.42, 0.22 to 0.82), with high uncertainty in the estimated hazard ratio versus DPP-4 inhibitors (0.64, 0.29 to 1.43).
Conclusions: This emulation study of a target trial found that SGLT-2 inhibitors were more effective than sulfonylureas or DPP-4 inhibitors in lowering mean HbA1c, BMI, and systolic blood pressure and in reducing the hazards of hospital admission for heart failure (vDPP-4 inhibitors) and kidney disease progression (vsulfonylureas), with no evidence of differences in other clinical endpoints.
"
Efficacy of psilocybin for treating symptoms of depression,"Introduction
Depression affects an estimated 300 million people around the world, an increase of nearly 20% over the past decade.1Worldwide, depression is also the leading cause of disability.2
Drugs for depression are widely available but these seem to have limited efficacy, can have serious adverse effects, and are associated with low patient adherence.34Importantly, the treatment effects of antidepressant drugs do not appear until 4-7 weeks after the start of treatment, and remission of symptoms can take months.45Additionally, the likelihood of relapse is high, with 40-60% of people with depression experiencing a further depressive episode, and the chance of relapse increasing with each subsequent episode.67
Since the early 2000s, the naturally occurring serotonergic hallucinogen psilocybin, found in several species of mushrooms, has been widely discussed as a potential treatment for depression.89Psilocybin’s mechanism of action differs from that of classic selective serotonin reuptake inhibitors (SSRIs) and might improve the treatment response rate, decrease time to improvement of symptoms, and prevent relapse post-remission. Moreover, more recent assessments of harm have consistently reported that psilocybin generally has low addictive potential and toxicity and that it can be administered safely under clinical supervision.10
The renewed interest in psilocybin’s antidepressive effects led to several clinical trials on treatment resistant depression,1112major depressive disorder,13and depression related to physical illness.14151617These trials mostly reported positive efficacy findings, showing reductions in symptoms of depression within a few hours to a few days after one dose or two doses of psilocybin.111213161718These studies reported only minimal adverse effects, however, and drug harm assessments in healthy volunteers indicated that psilocybin does not induce physiological toxicity, is not addictive, and does not lead to withdrawal.1920Nevertheless, these findings should be interpreted with caution owing to the small sample sizes and open label design of some of these studies.1121
Several systematic reviews and meta-analyses since the early 2000s have investigated the use of psilocybin to treat symptoms of depression. Most found encouraging results, but as well as people with depression some included healthy volunteers,22and most combined data from studies of multiple serotonergic psychedelics,232425even though each compound has unique neurobiological effects and mechanisms of action.262728Furthermore, many systematic reviews included non-randomised studies and studies in which psilocybin was tested in conjunction with psychotherapeutic interventions,2529303132which made it difficult to distinguish psilocybin’s treatment effects. Most systematic reviews and meta-analyses did not consider the impact of factors that could act as moderators to psilocybin’s effects, such as type of depression (primary or secondary), previous use of psychedelics, psilocybin dosage, type of outcome measure (clinician rated or self-reported), and personal characteristics (eg, age, sex).252629303132Lastly, systematic reviews did not consider grey literature,3334which might have led to a substantial overestimation of psilocybin’s efficacy as a treatment for depression. In this review we focused on randomised trials that contained an unconfounded evaluation of psilocybin in adults with symptoms of depression, regardless of country and language of publication.
Methods
In this systematic review and meta-analysis of indexed and non-indexed randomised trials we investigated the efficacy of psilocybin to treat symptoms of depression compared with placebo or non-psychoactive drugs. The protocol was registered in the International Prospective Register of Systematic Reviews (see supplementary Appendix A). The study overall did not deviate from the pre-registered protocol; one clarification was made to highlight that any non-psychedelic comparator was eligible for inclusion, including placebo, niacin, micro doses of psychedelics, and drugs that are considered the standard of care in depression (eg, SSRIs).
Inclusion and exclusion criteria
Double blind and open label randomised trials with a crossover or parallel design were eligible for inclusion. We considered only studies in humans and with a control condition, which could include any type of non	-active comparator, such as placebo, niacin, or micro doses of psychedelics.
Eligible studies were those that included adults (≥18 years) with clinically significant symptoms of depression, evaluated using a clinically validated tool for depression and mood disorder outcomes. Such tools included the Beck depression inventory, Hamilton depression rating scale, Montgomery-Åsberg depression rating scale, profile of mood states, and quick inventory of depressive symptomatology. Studies of participants with symptoms of depression and comorbidities (eg, cancer) were also eligible. We excluded studies of healthy participants (without depressive symptomatology).
Eligible studies investigated the effect of psilocybin as a standalone treatment on symptoms of depression. Studies with an active psilocybin condition that involved micro dosing (ie, psilocybin <100 μg/kg, according to the commonly accepted convention2235) were excluded. We included studies with directive psychotherapy if the psychotherapeutic component was present in both the experimental and the control conditions, so that the effects of psilocybin could be distinguished from those of psychotherapy. Studies involving group therapy were also excluded. Any non-psychedelic comparator was eligible for inclusion, including placebo, niacin, and micro doses of psychedelics.
Changes in symptoms, measured by validated clinician rated or self-report scales, such as the Beck depression inventory, Hamilton depression rating scale, Montgomery-Åsberg depression rating scale, profile of mood states, and quick inventory of depressive symptomatology were considered. We excluded outcomes that were measured less than three hours after psilocybin had been administered because any reported changes could be attributed to the transient cognitive and affective effects of the substance being administered. Aside from this, outcomes were included irrespective of the time point at which measurements were taken.
Search strategy
We searched major electronic databases and trial registries of psychological and medical research, with no limits on the publication date. Databases were the Cochrane Central Register of Controlled Trials via the Cochrane Library, Embase via Ovid, Medline via Ovid, Science Citation Index and Conference Proceedings Citation Index-Science via Web of Science, and PsycInfo via Ovid. A search through multiple databases was necessary because each database includes unique journals. Supplementary Appendix B shows the search syntax used for the Cochrane Central Register of Controlled Trials, which was slightly modified to comply with the syntactic rules of the other databases.
Unpublished and grey literature were sought through registries of past and ongoing trials, databases of conference proceedings, government reports, theses, dissertations, and grant registries (eg, ClinicalTrials.gov, WHO International Clinical Trials Registry Platform, ProQuest Dissertations and Theses Global, and PsycEXTRA). The references and bibliographies of eligible studies were checked for relevant publications. The original search was done in January 2023 and updated search was performed on 10 August 2023.
Data collection, extraction, and management
The results of the literature search were imported to the Endnote X9 reference management software, and the references were imported to the Covidence platform after removal of duplicates. Two reviewers (AM and DT) independently screened the title and abstract of each reference and then screened the full text of potentially eligible references. Any disagreements about eligibility were resolved through discussion. If information was insufficient to determine eligibility, the study’s authors were contacted. The reviewers were not blinded to the studies’ authors, institutions, or journal of publication.
The PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram shows the study selection process and reasons for excluding studies that were considered eligible for full text screening.36
Critical appraisal of individual studies and of aggregated evidence
The methodological quality of eligible studies was assessed using the Cochrane Risk of Bias 2 tool (RoB 2) for assessing risk of bias in randomised trials.37In addition to the criteria specified by RoB 2, we considered the potential impact of industry funding and conflicts of interest. The overall methodological quality of the aggregated evidence was evaluated using GRADE (Grading of Recommendations, Assessment, Development and Evaluation).38
If we found evidence of heterogeneity among the trials, then small study biases, such as publication bias, were assessed using a funnel plot and asymmetry tests (eg, Egger’s test).39
Data items
We used a template for data extraction (see supplementary Appendix C) and summarised the extracted data in tabular form, outlining personal characteristics (age, sex, previous use of psychedelics), methodology (study design, dosage), and outcome related characteristics (mean change from baseline score on a depression questionnaire, response rates, and remission rates) of the included studies. Response conventionally refers to a 50% decrease in symptom severity based on scores on a depression rating scale, whereas remission scores are specific to a questionnaire (eg, score of ≤5 on the quick inventory of depressive symptomatology, score of ≤10 on the Montgomery-Åsberg depression rating scale, 50% or greater reduction in symptoms, score of ≤7 on the Hamilton depression rating scale, or score of ≤12 on the Beck depression inventory). Across depression scales, higher scores signify more severe symptoms of depression.
Continuous data synthesis
From each study we extracted the baseline and post-intervention means and standard deviations (SDs) of the scores between comparison groups for the depression questionnaires and calculated the mean differences and SDs of change. If means and SDs were not available for the included studies, we extracted the values from available graphs and charts using the Web Plot Digitizer application (https://automeris.io/WebPlotDigitizer/). If it was not possible to calculate SDs from the graphs or charts, we generated values by converting standard errors (SEs) or confidence intervals (CIs), depending on availability, using formulas in the Cochrane Handbook (section 7.7.3.2).40
Standardised mean differences were calculated for each study. We chose these rather than weighted mean differences because, although all the studies measured depression as the primary outcome, they did so with different questionnaires that score depression based on slightly different items.41If we had used weighted mean differences, any variability among studies would be assumed to reflect actual methodological or population differences and not differences in how the outcome was measured, which could be misleading.40
The Hedges’ g effect size estimate was used because it tends to produce less biased results for studies with smaller samples (<20 participants) and when sample sizes differ substantially between studies, in contrast with Cohen’s d.42According to the Cochrane Handbook, the Hedges’ g effect size measure is synonymous with the standardised mean difference,40and the terms may be used interchangeably. Thus, a Hedges’ g of 0.2, 0.5, 0.8, or 1.2 corresponds to a small, medium, large, or very large effect, respectively.40
Owing to variation in the participants’ personal characteristics, psilocybin dosage, type of depression investigated (primary or secondary), and type of comparators, we used a random effects model with a Hartung-Knapp-Sidik-Jonkman modification.43This model also allowed for heterogeneity and within study variability to be incorporated into the weighting of the results of the included studies.44Lastly, this model could help to generalise the findings beyond the studies and patient populations included, making the meta-analysis more clinically useful.45We chose the Hartung-Knapp-Sidik-Jonkman adjustment in favour of more widely used random effects models (eg, DerSimonian and Laird) because it allows for better control of type 1 errors, especially for studies with smaller samples, and provides a better estimation of between study variance by accounting for small sample sizes.4647
For studies in which multiple treatment groups were compared with a single placebo group, we split the placebo group to avoid multiplicity.48Similarly, if studies included multiple primary outcomes (eg, change in depression at three weeks and at six weeks), we split the treatment groups to account for overlapping participants.40
Prediction intervals (PIs) were calculated and reported to show the expected effect range of a similar future study, in a different setting. In a random effects model, within study measures of variability, such as CIs, can only show the range in which the average effect size could lie, but they are not informative about the range of potential treatment effects given the heterogeneity between studies.49Thus, we used PIs as an indication of variation between studies.
Heterogeneity and sensitivity analysis
Statistical heterogeneity was tested using the χ2test (significance level P<0.1) and I2statistic, and heterogeneity among included studies was evaluated visually and displayed graphically using a forest plot. If substantial or considerable heterogeneity was found (I2≥50% or P<0.1),50we considered the study design and characteristics of the included studies. Sources of heterogeneity were explored by subgroup analysis, and the potential effects on the results are discussed.
Planned sensitivity analyses to assess the effect of unpublished studies and studies at high risk of bias were not done because all included studies had been published and none were assessed as high risk of bias. Exclusion sensitivity plots were used to display graphically the impact of individual studies and to determine which studies had a particularly large influence on the results of the meta-analysis. All sensitivity analyses were carried out with Stata 16 software.
Subgroup analysis
To reduce the risk of errors caused by multiplicity and to avoid data fishing, we planned subgroup analyses a priori and limited to: (1) patient characteristics, including age and sex; (2) comorbidities, such as a serious physical condition (previous research indicates that the effects of psilocybin may be less strong for such participants, compared with participants with no comorbidities)33; (3) number of doses and amount of psilocybin administered, because some previous meta-analyses found that a higher number of doses and a higher dose of psilocybin both predicted a greater reduction in symptoms of depression,34whereas others reported the opposite33; (4) psilocybin administered alongside psychotherapeutic guidance or as a standalone treatment; (5) severity of depressive symptoms (clinicalvsubclinical symptomatology); (6) clinician versus patient rated scales; and (7) high versus low quality studies, as determined by RoB 2 assessment scores.
Metaregression
Given that enough studies were identified (≥10 distinct observations according to the Cochrane Handbook’s suggestion40), we performed metaregression to investigate whether covariates, or potential effect modifiers, explained any of the statistical heterogeneity. The metaregression analysis was carried out using Stata 16 software.
Random effects metaregression analyses were used to determine whether continuous variables such as participants’ age, percentage of female participants, and percentage of participants who had previously used psychedelics modified the effect estimate, all of which have been implicated in differentially affecting the efficacy of psychedelics in modifying mood.51We chose this approach in favour of converting these continuous variables into categorical variables and conducting subgroup analyses for two primary reasons; firstly, the loss of any data and subsequent loss of statistical power would increase the risk of spurious significant associations,51and, secondly, no cut-offs have been agreed for these factors in literature on psychedelic interventions for mood disorders,52making any such divisions arbitrary and difficult to reconcile with the findings of other studies. The analyses were based on within study averages, in the absence of individual data points for each participant, with the potential for the results to be affected by aggregate bias, compromising their validity and generalisability.53Furthermore, a group level analysis may not be able to detect distinct interactions between the effect modifiers and participant subgroups, resulting in ecological bias.54As a result, this analysis should be considered exploratory.
Sensitivity analysis
A sensitivity analysis was performed to determine if choice of analysis method affected the primary findings of meta-analysis. Specifically, we reanalysed the data on change in depression score using a random effects Dersimonian and Laird model without the Hartung-Knapp-Sidik-Jonkman modification and compared the results with those of the originally used model. This comparison is particularly important in the presence of substantial heterogeneity and the potential of small study effects to influence the intervention effect estimate.55
Patient and public involvement
Research on novel depression treatments is of great interest to both patients and the public. Although patients and members of the public were not directly involved in the planning or writing of this manuscript owing to a lack of available funding for recruitment and researcher training, patients and members of the public read the manuscript after submission.
Results
Figure 1presents the flow of studies through the systematic review and meta-analysis.56A total of 4884 titles were retrieved from the five databases of published literature, and a further 368 titles were identified from the databases of unpublished and international literature in February 2023. After the removal of duplicate records, we screened the abstracts and titles of 875 reports. A further 12 studies were added after handsearching of reference lists and conference proceedings and abstracts. Overall, nine studies totalling 436 participants were eligible. The average age of the participants ranged from 36-60 years. During an updated search on 10 August 2023, no further studies were identified.
After screening of the title and abstract, 61 titles remained for full text review. Native speakers helped to translate papers in languages other than English. The most common reasons for exclusion were the inclusion of healthy volunteers, absence of control groups, and use of a survey based design rather than an experimental design. After full text screening, nine studies were eligible for inclusion, and 15 clinical trials prospectively registered or underway as of August 2023 were noted for potential future inclusion in an update of this review (see supplementary Appendix D).
We sent requests for further information to the authors of studies by Griffiths et al,57Barrett,58and Benville et al,59because these studies appeared to meet the inclusion criteria but were only provided as summary abstracts online. A potentially eligible poster presentation from the 58th annual meeting of the American College of Neuropsychopharmacology was identified but the lead author (Griffiths) clarified that all information from the presentation was included in the studies by Davis et al13and Gukasyan et al60; both of which we had already deemed ineligible.
Barrett58reported the effects of psilocybin on the cognitive flexibility and verbal reasoning of a subset of patients with major depressive disorder from Griffith et al’s trial,61compared with a waitlist group, but when contacted, Barrett explained that the results were published in the study by Doss et al,62which we had already screened and judged ineligible (see supplementary Appendix E). Benville et al’s study59presented a follow-up of Ross et al’s study17on a subset of patients with cancer and high suicidal ideation and desire for hastened death at baseline. Measures of antidepressant effects of psilocybin treatment compared with niacin were taken before and after treatment crossover, but detailed results are not reported.Table 1describes the characteristics of the included studies andtable 2lists the main findings of the studies.
Side effects and adverse events
Side effects reported in the included studies were minor and transient (eg, short term increases in blood pressure, headache, and anxiety), and none were coded as serious. Cahart-Harris et al noted one instance of abnormal dreams and insomnia.63This side effect profile is consistent with findings from other meta-analyses.3068Owing to the different scales and methods used to catalogue side effects and adverse events across trials, it was not possible to combine these data quantitatively (see supplementary Appendix F).
Risk of bias
The Cochrane RoB 2 tools were used to evaluate the included studies (table 3). RoB 2 for randomised trials was used for the five reports of parallel randomised trials (Carhart-Harris et al63and its secondary analysis Barba et al,64Goodwin et al18and its secondary analysis Goodwin et al,65and von Rotz et al66) and RoB 2 for crossover trials was used for the four reports of crossover randomised trials (Griffiths et al,14Grob et al,15and Ross et al17and its follow-up Ross et al67). Supplementary Appendix G provides a detailed explanation of the assessment of the included studies.
Quality of included studies
Confidence in the quality of the evidence for the meta-analysis was assessed using GRADE,38through the GRADEpro GDT software program.Figure 2shows the results of this assessment, along with our summary of findings.
Meta-analyses
Continuous data, change in depression scores—Using a Hartung-Knapp-Sidik-Jonkman modified random effects meta-analysis, change in depression scores was significantly greater after treatment with psilocybin compared with active placebo. The overall Hedges’ g (1.64, 95% CI 0.55 to 2.73) indicated a large effect size favouring psilocybin (fig 3). PIs were, however, wide and crossed the line of no difference (95% CI −1.72 to 5.03), indicating that there could be settings or populations in which psilocybin intervention would be less efficacious.
Exploring publication bias in continuous data—We used Egger’s test and a funnel plot to examine the possibility of small study biases, such as publication bias. Statistical significance of Egger’s test for small study effects, along with the asymmetry in the funnel plot (fig 4), indicates the presence of bias against smaller studies with non-significant results, suggesting that the pooled intervention effect estimate is likely to be overestimated.69An alternative explanation, however, is that smaller studies conducted at the early stages of a new psychotherapeutic intervention tend to include more high risk or responsive participants, and psychotherapeutic interventions tend to be delivered more effectively in smaller trials; both of these factors can exaggerate treatment effects, resulting in funnel plot asymmetry.70Also, because of the relatively small number of included studies and the considerable heterogeneity observed, test power may be insufficient to distinguish real asymmetry from chance.71Thus, this analysis should be considered exploratory.
Dichotomous data
We extracted response and remission rates for each group when reported directly, or imputed information when presented graphically. Two studies did not measure response or remission and thus did not contribute data for this part of the analysis.1518The random effects model with a Hartung-Knapp-Sidik-Jonkman modification was used to allow for heterogeneity to be incorporated into the weighting of the included studies’ results, and to provide a better estimation of between study variance accounting for small sample sizes.
Response rate—Overall, the likelihood of psilocybin intervention leading to treatment response was about two times greater (risk ratio 2.02, 95% CI 1.33 to 3.07) than with placebo. Despite the use of different scales to measure response, the heterogeneity between studies was not significant (I2=25.7%, P=0.23). PIs were, however, wide and crossed the line of no difference (−0.94 to 3.88), indicating that there could be settings or populations in which psilocybin intervention would be less efficacious.
Remission rate—Overall, the likelihood of psilocybin intervention leading to remission of depression was nearly three times greater than with placebo (risk ratio 2.71, 95% CI 1.75 to 4.20). Despite the use of different scales to measure response, no statistical heterogeneity was found between studies (I2=0.0%, P=0.53). PIs were, however, wide and crossed the line of no difference (0.87 to 2.32), indicating that there could be settings or populations in which psilocybin intervention would be less efficacious.
Exploring publication bias in response and remission rates data—We used Egger’s test and a funnel plot to examine whether response and remission estimates were affected by small study biases. The result for Egger’s test was non-significant (P>0.05) for both response and remission estimates, and no substantial asymmetry was observed in the funnel plots, providing no indication for the presence of bias against smaller studies with non-significant results.
Heterogeneity: subgroup analyses and metaregression
Heterogeneity was considerable across studies exploring changes in depression scores (I2=89.7%, P<0.005), triggering subgroup analyses to explore contributory factors.Table 4andtable 5present the results of the heterogeneity analyses (subgroup analyses and metaregression, respectively). Also see supplementary Appendix H for a more detailed description and graphical representation of these results.
Cumulative meta-analyses
We used cumulative meta-analyses to investigate how the overall estimates of the outcomes of interest changed as each study was added in chronological order72; change in depression scores and likelihood of treatment response both increased as the percentage of participants with past use of psychedelics increased across studies, as expected based on the metaregression analysis (see supplementary Appendix I). No other significant time related patterns were found.
Sensitivity analysis
We reanalysed the data for change in depression scores using a random effects Dersimonian and Laird model without the Hartung-Knapp-Sidik-Jonkman modification and compared the results with those of the original model. All comparisons found to be significant using the Dersimonian and Laird model with the Hartung-Knapp-Sidik-Jonkman adjustment were also significant without the Hartung-Knapp-Sidik-Jonkman adjustment, and confidence intervals were only slightly narrower. Thus, small study effects do not appear to have played a major role in the treatment effect estimate.
Additionally, to estimate the accuracy and robustness of the estimated treatment effect, we excluded studies from the meta-analysis one by one; no important differences in the treatment effect, significance, and heterogeneity levels were observed after the exclusion of any study (see supplementary Appendix J).
Discussion
In our meta-analysis we found that psilocybin use showed a significant benefit on change in depression scores compared with placebo. This is consistent with other recent meta-analyses and trials of psilocybin as a standalone treatment for depression7374or in combination with psychological support.2425293031326875This review adds to those finding by exploring the considerable heterogeneity across the studies, with subsequent subgroup analyses showing that the type of depression (primary or secondary) and the depression scale used (Montgomery-Åsberg depression rating scale, quick inventory of depressive symptomatology, or Beck depression inventory) had a significant differential effect on the outcome. High between study heterogeneity has been identified by some other meta-analyses of psilocybin (eg, Goldberg et al29), with a higher treatment effect in studies with patients with comorbid life threatening conditions compared with patients with primary depression.22Although possible explanations, including personal factors (eg, patients with life threatening conditions being older) or depression related factors (eg, secondary depression being more severe than primary depression) could be considered, these hypotheses are not supported by baseline data (ie, patients with secondary depression do not differ substantially in age or symptom severity from patients with primary depression). The differential effects from assessment scales used have not been examined in other meta-analyses of psilocybin, but this review’s finding that studies using the Beck depression inventory showed a higher treatment effect than those using the Montgomery-Åsberg depression rating scale and quick inventory of depressive symptomatology is consistent with studies in the psychological literature that have shown larger treatment effects when self-report scales are used (eg, Beck depression inventory).7677This finding may be because clinicians tend to overestimate the severity of depression symptoms at baseline assessments, leading to less pronounced differences between before and after treatment identified in clinician assessed scales (eg, Montgomery-Åsberg depression rating scale, quick inventory of depressive symptomatology).78
Metaregression analyses further showed that a higher average age and a higher percentage of participants with past use of psychedelics both correlated with a greater improvement in depression scores with psilocybin use and explained a substantial amount of between study variability. However, the cumulative meta-analysis showed that the effects of age might be largely an artefact of the inclusion of one specific study, and alternative explanations are worth considering. For instance, Studerus et al79identified participants’ age as the only personal variable significantly associated with psilocybin response, with older participants reporting a higher “blissful state” experience. This might be because of older people’s increased experience in managing negative emotions and the decrease in 5-hydroxytryptamine type 2A receptor density associated with older age.80Furthermore, Rootman et al81reported that the cognitive performance of older participants (>55 years) improved significantly more than that of younger participants after micro dosing with psilocybin. Therefore, the higher decrease in depressive symptoms associated with older age could be attributed to a decrease in cognitive difficulties experienced by older participants.
Interestingly, a clear pattern emerged for past use of psychedelics—the higher the proportion of study participants who had used psychedelics in the past, the higher the post-psilocybin treatment effect observed. Past use of psychedelics has been proposed to create an expectancy bias among participants and amplify the positive effects of psilocybin828384; however, this important finding has not been examined in other meta-analyses and may highlight the role of expectancy in psilocybin research.
Limitations of this study
Generalisability of the findings of this meta-analysis was limited by the lack of racial and ethnic diversity in the included studies—more than 90% of participants were white across all included trials, resulting in a homogeneous sample that is not representative of the general population. Moreover, it was not possible to distinguish between subgroups of participants who had never used psilocybin and those who had taken psilocybin more than a year before the start of the trial, as these data were not provided in the included studies. Such a distinction would be important, as the effects of psilocybin on mood may wane within a year after being administered.2185Also, how psychological support was conceptualised was inconsistent within studies of psilocybin interventions; many studies failed to clearly describe the type of psychological support participants received, and others used methods ranging from directive guidance throughout the treatment session to passive encouragement or r","Objective: To determine the efficacy of psilocybin as an antidepressant compared with placebo or non-psychoactive drugs.
Design: Systematic review and meta-analysis.
Data sources: Five electronic databases of published literature (Cochrane Central Register of Controlled Trials, Medline, Embase, Science Citation Index and Conference Proceedings Citation Index, and PsycInfo) and four databases of unpublished and international literature (ClinicalTrials.gov, WHO International Clinical Trials Registry Platform, ProQuest Dissertations and Theses Global, and PsycEXTRA), and handsearching of reference lists, conference proceedings, and abstracts.
Data synthesis and study quality: Information on potential treatment effect moderators was extracted, including depression type (primary or secondary), previous use of psychedelics, psilocybin dosage, type of outcome measure (clinician rated or self-reported), and personal characteristics (eg, age, sex). Data were synthesised using a random effects meta-analysis model, and observed heterogeneity and the effect of covariates were investigated with subgroup analyses and metaregression. Hedges’ g was used as a measure of treatment effect size, to account for small sample effects and substantial differences between the included studies’ sample sizes. Study quality was appraised using Cochrane’s Risk of Bias 2 tool, and the quality of the aggregated evidence was evaluated using GRADE guidelines.
Eligibility criteria: Randomised trials in which psilocybin was administered as a standalone treatment for adults with clinically significant symptoms of depression and change in symptoms was measured using a validated clinician rated or self-report scale. Studies with directive psychotherapy were included if the psychotherapeutic component was present in both experimental and control conditions. Participants with depression regardless of comorbidities (eg, cancer) were eligible.
Results: Meta-analysis on 436 participants (228 female participants), average age 36-60 years, from seven of the nine included studies showed a significant benefit of psilocybin (Hedges’ g=1.64, 95% confidence interval (CI) 0.55 to 2.73, P<0.001) on change in depression scores compared with comparator treatment. Subgroup analyses and metaregressions indicated that having secondary depression (Hedges’ g=3.25, 95% CI 0.97 to 5.53), being assessed with self-report depression scales such as the Beck depression inventory (3.25, 0.97 to 5.53), and older age and previous use of psychedelics (metaregression coefficient 0.16, 95% CI 0.08 to 0.24 and 4.2, 1.5 to 6.9, respectively) were correlated with greater improvements in symptoms. All studies had a low risk of bias, but the change from baseline metric was associated with high heterogeneity and a statistically significant risk of small study bias, resulting in a low certainty of evidence rating.
Conclusion: Treatment effects of psilocybin were significantly larger among patients with secondary depression, when self-report scales were used to measure symptoms of depression, and when participants had previously used psychedelics. Further research is thus required to delineate the influence of expectancy effects, moderating factors, and treatment delivery on the efficacy of psilocybin as an antidepressant.
Systematic review registration: PROSPERO CRD42023388065.
"
Reverse total shoulder replacement versus anatomical total shoulder replacement for osteoarthritis,"Introduction
Shoulder replacement surgery is an effective treatment option for end stage shoulder arthritis, and is rising in incidence internationally.12The use of reverse total shoulder replacement (RTSR), initially developed for rotator cuff arthropathy, has now expanded globally across different surgical indications, including osteoarthritis with an intact rotator cuff, a common condition traditionally treated with an anatomical total shoulder replacement (TSR;fig 1).3456This shift in practice is growing despite a lack of supporting evidence, a concern highlighted by the National Institute for Health and Care Excellence and patients, carers, and clinicians during a James Lind Alliance Priority Setting Partnership.78
Healthcare agencies like the National Institute for Health and Care Research have now funded randomised controlled trials to address this lack of evidence.9However, such trials can take more than five years to complete and usually only report two year clinical outcomes. Additionally, no guarantees of completion exist because of the unique challenges facing surgical trials, such as prolonged recruitment, changes in healthcare pathways, and shifts in community equipoise.1011Importantly, randomised controlled trials in this context cannot assess long term implant survival for many years, while the smaller sample sizes are usually insufficient to evaluate the relatively low number of serious adverse events seen in elective orthopaedics. The use of routinely collected data, particularly from large national joint registries, can potentially expedite answers at a fraction of the cost.12A recent study highlighted the value of observational studies in emulating trials, emphasising external validity in real world populations.13Examples in orthopaedics include successful emulation of the TOPKAT (total or partial knee arthroplasty) trial comparing different types of knee replacement.14This has improved the generalisability of the TOPKAT trial results and their acceptability to the surgical community.
With the commissioning of surgical randomised controlled trials in response to the global increase in use of RTSR for osteoarthritis, our objective was to use modern epidemiological methods and national datasets to provide more rapid high quality evidence to the international community on safety and revision surgery. Such evidence will complement future randomised controlled trial results that will examine in more detail patient functional outcomes and cost effectiveness. The aim of this study was to examine the clinical effectiveness and economic implications of using RSTR over the anatomical TSR when treating patients aged 60 years or older with osteoarthritis and an intact rotator cuff.
Methods
Study design and data sources
Population based prospective cohort study using linked, routinely collected data for shoulder replacements undertaken at public hospitals, and publicly funded procedures in private hospitals in England from 1 April 2012 to 31 December 2020. Data from the National Joint Registry of England were linked to the NHS Hospital Episode Statistics database and to civil registration mortality data. Data submission to the National Joint Registry is mandatory for all shoulder replacements occurring at public and private hospitals, and includes patient, surgeon, and operation details. The Hospital Episode Statistics admitted patient care database records all inpatient and day case activity in public hospitals and publicly funded procedures in private hospitals in England, and includes demographic data, medical diagnoses, procedural and administrative information. Hospital Episode Statistics data are used for the accurate reimbursement of NHS providers for their activities.
Selection criteria
All patients aged 60 years or older having an elective primary shoulder replacement for osteoarthritis with an intact rotator cuff were eligible for inclusion in the study. Patients were included if they received a TSR or an RTSR. The intact condition of the rotator cuff is determined by the operating surgeon at the time of surgery and recorded by the National Joint Registry through a mandatory minimum dataset form.15Procedures where the rotator cuff was recorded as absent or torn were excluded. The patient group in this study closely aligns with that in the commissioned RAPSODI trial (reverse or anatomical replacement for painful shoulder osteoarthritis: differences between interventions,ISRCTN 12216466), both of which rely on the intraoperative assessment of the rotator cuff.9We excluded patients with inconsistent surgical histories (ie, revision or death predated their primary procedure) and duplicates. The unit of analysis was considered the procedure rather than the patient, so a patient’s left and right sided shoulder replacements would appear as separate observations.
Clinical outcomes
The primary outcome was revision surgery at any time point (representing implant survival). The National Joint Registry defines revision as a procedure that involves adding, removing, or modifying one or more components of a joint prosthesis, and is the most commonly used metric for assessing the success of joint replacement surgery.5
Secondary outcomes included serious adverse events occurring within 90 days of surgery, reoperations within 12 months of surgery, prolonged hospital stay, and change in Oxford Shoulder Score (preoperative to six month postoperative). Serious adverse events were defined as admissions to hospital owing to any of the following medical complications, identified using ICD-10 (International Classification of Diseases, 10th revision) codes: pulmonary embolism, myocardial infarction, lower respiratory tract infection, acute kidney injury, urinary tract infection, cerebrovascular events, and all cause death.16Repeat surgery on the same shoulder within 12 months of the primary procedure that did not meet the criteria for revision, and that occurred on a separate occasion to any revision surgery, was termed a reoperation and was identified from relevant OPCS-4 (Office of Population Censuses and Surveys classification of surgical operations and procedures, fourth revision) codes (see supplementary tables S1-S3). After consultation with patient representatives from a patient advisory panel for this study, prolonged hospital stay was defined as an inpatient duration greater than three nights from the date of the primary procedure. The preoperative and six month postoperative Oxford Shoulder Score (a shoulder specific questionnaire—from a minimum score of 0 to a maximum score of 48), a patient reported outcome measure, collected by the National Joint Registry, was available for a subset of patients.
Costs
Hospital costs associated with the primary (including serious adverse events and reoperations) and revision (including serious adverse events) procedures were estimated using healthcare resource group codes (which classify hospital activity according to resource use), length of hospital stay, and unbundled healthcare resource group codes (individually priced consumable elements).17Healthcare resource group codes were valued using 2020-21 NHS reference costs. These costs represent the average cost to the NHS of providing a defined service in a given financial year. The calculations were used to generate the reimbursement value of each primary and revision procedure to the hospital provider based on the National Reimbursement System.1819
Statistical analysis
Propensity scores were generated using logistic regression and represent the probability of a patient receiving an RTSR, as opposed to a TSR, based on the following covariates: age, sex, American Society of Anaesthesiologists grade, thromboprophylaxis, previous shoulder surgery, rural or urban residence, socioeconomic deprivation (area level index of multiple deprivation), operation funding, Charlson comorbidity index, obesity, past medical history of gastrointestinal, mental health, respiratory, circulatory, metabolic, neurological, and urinary tract problems, and health hazards. One-to-one propensity score matching using callipers of width equal to 0.2 of the standard deviation of the logit of the propensity score was used to enable estimation of the average treatment effect on the treated population.2021Inverse probability of treatment weighting for participants on the common support of propensity scores was used for the average treatment effect on the total population.22The combined evaluation of average treatment effect on the treated and average treatment effect on the total population is essential for capturing any heterogeneity across different patient subgroups: average treatment effect on the treated population focuses on the effects within the treatment group, reflecting scenarios where specific patient characteristics influence treatment allocation, while average treatment effect on the total population captures effects on the treated and control groups, and is a key consideration when formulating potentially universal healthcare policies.23
Covariate balance was assessed before and after matching and weighting, with an absolute standardised mean difference of 10% or more considered indicative of imbalance.24Additionally, negative control outcomes were analysed to test for unmeasured confounding, and included hip fracture, vertebral fracture, hernia and acute upper respiratory tract infection within a year of surgery, chosen for their lack of any plausible association with procedure type.25
Relative and absolute treatment effects were estimated for each outcome after matching and weighting.26Flexible parametric survival models (using restricted cubic splines to allow for modelling nonlinearity in the baseline hazard function) including treatment allocation as a time varying covariate were used for revision, while logistic regression was used for the binary secondary outcomes. Robust variance estimation was used to account for the clustering within matched sets and the weighted nature of the samples.2728An additional analysis was undertaken using linear regression to estimate the treatment effect on the change (preoperative to six month postoperative) in Oxford Shoulder Score in a subset of patients with non-missing patient reported outcomes.29
A lifetime Markov model provided the framework for the cost analysis, with patients passing from clinically and economically important health states over annual cycles (fig 2).30Parametric models were specified for each treatment group to estimate the risk of revision and death, informing the model’s time dependent transition probabilities.31Model estimates were used to extrapolate the risk of revision and death past the study period. A sensitivity analysis was performed where the risk of death returns to that specified by age and sex specific UK life tables after the period of follow-up.32Future costs were discounted at the established annual rate of 3.5%, and the effect of varying the discount rate was reported.33
This evaluation was undertaken from a healthcare system perspective, so the expected (mean) inpatient costs incurred by the health system were estimated for each treatment group. The effect of parameter uncertainty was assessed using probabilistic sensitivity analysis, with the model’s input parameters assigned from probability distributions for 10 000 Monte Carlo simulations. Gamma distributions were used for costs while Cholesky decompositions were used to provide correlated draws from multivariate normal distributions, which then informed the transition probabilities (supplementary figures S6-S10, tables S5-S12).31Stata V.16.1 was used for the statistical analyses; R software was used for the cost analysis.3435
For the first sensitivity analysis, we limited inclusion to procedures performed by surgeons who carried out 11 or more combined trauma and elective shoulder replacement procedures in the year preceding the surgery. The rationale for this threshold was derived from a previously reported volume effect threshold of 10.4 procedures per year.36The second sensitivity analysis limited inclusion to procedures performed by surgeons considered to have balanced practice between RTSR and TSR for elective surgery. Practice was considered balanced when surgeons completed no more than 80% of TSR or RTSR as a proportion of the sum of these two procedures across their elective practice in the preceding year. The threshold of 80% was pragmatically determined to ensure reasonably balanced practice while not overly restricting the sample size available. This allowed the calculated proportions to vary dynamically over time for each surgeon, restricting the sensitivity analysis to procedures undertaken by surgeons at times when their caseload reflected balanced practice.
Index of multiple deprivation data were missing for 175 procedures (1.3%), so these were excluded, and a complete case analysis undertaken (fig 3). The preoperative and postoperative Oxford Shoulder Scores were missing for 7903 (61%) and 6448 (50%) procedures, respectively, or were considered invalid (based on accepted questionnaire completion timeframes) for an additional 1523 (12%) and 1696 (13%) procedures, respectively.5Simple mean imputation was used for partially completed Oxford Shoulder Score questionnaires (24%) when no more than two of 12 questions were left blank, as per the instrument’s guidelines.29A total of 1321 (10%) valid, paired preoperative and postoperative Oxford Shoulder Scores remained. Despite largely balanced covariates between questionnaire responders (non-missing) and non-responders (missing), the potential for missing data bias precluded a cost effectiveness analysis in this study, and analysis of patient reported outcomes was excluded from the sensitivity analyses (supplementary table S14).
Patient and public involvement
One of the top 10 research uncertainties identified by patients and clinicians from the 2015 James Lind Alliance Priority Setting Partnership on shoulder surgery specifically addressed the comparative effectiveness between different shoulder replacement types.7Patient representatives sit on the committee structure of the National Joint Registry. This study’s patient advisory panel also included a patient representative from the National Institute for Health and Care Excellence. Additionally, a patient representative from the National Joint Registry coauthored this study. Our patient representatives defined a hospital stay greater than three nights as prolonged because going home soon after surgery is an important outcome for them.
Results
Study population
The study population comprised 12 986 elective primary shoulder replacement procedures in 11 961 patients (fig 3). After propensity score matching (n=7124; TSR: 3562, RTSR: 3562) and adjustment for inverse probability of treatment weighting (n=12 968; TSR: 9393, RTSR: 3575), all covariates were well balanced (table 1).
Clinical analysis
In the matched cohort there were 126 revisions (1.8%; TSR: 85, RTSR: 41) with a maximum follow-up of 8.75 years and a total of 24 353 years of observation time (TSR: 14 332, RTSR: 10 021). In the weighted cohort there were 294 revisions (2.3%; TSR: 253, RTSR: 41) with a maximum follow-up of 8.75 years and a total of 47 886 years of observation time (TSR: 37 842, RTSR: 10 044).
For the matched and weighted cohorts, the survival curves for RTSR and TSR showed a similar shape with an early (up to one year) increased revision risk for RTSR, followed by an increased revision risk for TSR thereafter (fig 4). However, the confidence intervals for the survival curves overlap throughout (apart from a period at around three years in the matched cohort), indicating a similar longer term (more than eight year) survival probability.
For the matched cohort, flexible parametric model survival analysis showed an immediate non-significant postoperative hazard ratio of 1.39 (95% confidence interval (CI) 0.61 to 3.17) for revision with RTSR, indicating a nearly 40% increased risk of revision in the immediate postoperative period compared with TSR, after which there was a significantly reduced risk until nearly three years (hazard ratio local minimum 0.33, 95% CI 0.18 to 0.59). Similarly, for the weighted cohort, the immediate non-significant postoperative hazard ratio was 1.89 (0.97 to 3.69), followed by a significantly reduced risk until three years (hazard ratio local minimum 0.42, 0.25 to 0.73). The estimated hazard ratio after three years differs between the matched and weighted cohorts, increasing above one or staying below one, respectively, but the confidence intervals cross one throughout, indicating no significant difference. The widened confidence intervals and differing hazard ratio estimates after three years can be attributed to the reducing sample size of patients who remained at risk for longer time periods, but the fact the confidence intervals cross one suggests no statistically significant difference at these longer time periods.
Restricted mean survival time represents the area under the survival curve up to a specific time point.37For the matched cohort, the difference in restricted mean survival time was significant after 2.5 years, and it reached a maximum of 0.076 years at 8.75 years of follow-up (corresponding to a difference of 28 days of revision-free survival favouring RTSR). For the weighted cohort, the difference in restricted mean survival time was significant between 5.9 and 8.0 years, not before or after that period, and it reached a maximum of 0.067 years at 8.75 years of follow-up (corresponding to a difference of 24 days favouring RTSR).
Therefore, the differences in the shapes of the survival curves can be explained by changes in the relative hazard ratio over time (favouring RTSR between 0.5 and 3 years), amounting to a statistically significant but not clinically important absolute risk difference (less than 30 days’ difference of revision free survival by over eight years of follow-up). There was no statistically significant and clinically important difference in the relative risk or absolute risk difference by the end of the study period for either cohort.
In the matched cohort, after RTSR, the relative risk of reoperations within 12 months was half (odds ratio 0.45, 95% CI 0.25 to 0.83), with an absolute risk difference of −0.51% (95% CI −0.89 to −0.13), although there was no significant difference in relative risk in the weighted cohort (0.58, 0.31 to 1.08;fig 5). While statistically significant, an absolute risk difference of around 1 in 200 is likely to be of no clinical importance. There was no statistically significant difference in the absolute or relative risk of serious adverse events or prolonged hospital stay for the matched or weighted cohorts.
Results were consistent in both sensitivity analyses (supplementary figures S2-S4). Treatment effects for negative control outcomes were not statistically significant and centred around one, suggesting no residual confounding (supplementary figure S1).
While the Oxford Shoulder Score was not included when generating propensity scores, in the matched and weighted study populations, the absolute standardised mean difference of the preoperative Oxford Shoulder Score was 8.0% and 7.2%, respectively. In the matched population there were 709 (10%) non-missing, valid, paired preoperative and six month postoperative scores with an estimated linear regression coefficient of −0.21 (95% CI −1.70 to 1.29), indicating a non-significant difference in the change in score in favour of TSR. In the weighted population there were 1319 (10%) non-missing, valid, paired scores with an estimated linear regression coefficient of 0.99 (−0.53 to 2.51), indicating a non-significant difference in favour of RTSR (supplementary figure S11, table S13).
Cost analysis
A total of 12 549 procedures (96.6%) successfully generated a valid healthcare resource group code and were valued using NHS reference costs. While the mean estimated lifetime costs of TSR were slightly higher than those of RTSR for the base case and sensitivity analyses, the overlapping confidence intervals derived from the probabilistic sensitivity analysis confirm there is no significant difference between the two procedures (fig 6). Results were consistent after matching and weighting, and were unaffected by changes in discount rate (supplementary figure S10).
Discussion
This study used national, linked, prospective routinely collected registry and hospital data to compare the clinical outcomes and cost implications between RTSR and TSR in patients aged 60 years or older with osteoarthritis and an intact rotator cuff. The two procedure types had a significantly different revision risk profile over time, with RTSR being associated with a reduced (under half) risk of early revision surgery until three years. However, there was no clinically important absolute risk difference found throughout the study period. Despite a statistically significant difference in the relative risk of reoperations within 12 months favouring RTSR in the matched cohort, this did not amount to a clinically important absolute risk difference, and there was no significant difference in the weighted cohort. There was no statistically significant difference for the outcomes of serious adverse events within 90 days or prolonged hospital stay, and estimated lifetime costs to the healthcare service were similar. Only a limited analysis of patient reported outcomes was possible owing to completeness of the collected data, but this did not suggest any statistically significant difference in the change in Oxford Shoulder Score.
Strengths and weaknesses of this study
The main strengths of this study lie in its large sample size and robust methods, including clinically relevant sensitivity analyses. While randomised controlled trials emphasise the internal validity of claims about causality, observational studies emphasise external validity in real word populations.1213This observational study provides real world evidence by delineating treatment effects within the RTSR recipient population (average treatment effect on the treated population) and across the entire eligible demographic (average treatment effect on the total population). Compared with randomised controlled trials, observational studies such as this one are often more cost efficient and can be conducted in a much shorter timeframe. Observational studies provide high level evidence and generalisable results when randomised controlled trial data are lacking, and can complement findings from randomised controlled trials. Moreover, observational studies can better reflect real world surgical practice, enhancing their value in informing clinical decisions.
While randomised controlled trials are well placed to report differences in patient reported outcomes, their limited sample size constrains their ability to capture the less common outcomes in elective orthopaedic surgery, such as serious adverse events including all cause death. In contrast, the substantial cohort size in our study provides a more robust framework for detecting these infrequent events. Furthermore, our study's longer follow-up period and larger sample size enhance our study’s ability to compare the risk of revision surgery—an outcome that might not be adequately captured within the initial short term follow-up periods seen in randomised controlled trials. The routine collection of registry and hospital data also enables subsequent reanalyses to generate results with even larger sample sizes and longer follow-up in the future.
The main limitation of this study was the absence of a complete dataset for patient reported outcome measures. While we undertook an analysis of patient reported outcomes on a small subset of the study population for which data were available, those results need to be interpreted with caution owing to the risk of reporting bias. Results from trials like RAPSODI on patient reported outcomes will provide more complete information, though based on the observations in this study, we expect there will be no clinically meaningful difference in scores found. The incomplete dataset also precluded a cost effectiveness analysis, which is the accepted standard for health economic evaluations required for national treatment decision making in many countries. Despite adjusting for several clinically relevant variables, being an observational study, the possibility of residual confounding remains. The National Joint Registry has only recently started to collect information on preoperative glenoid morphology and so this variable was unavailable in our dataset. Finally, healthcare resource group codes do not distinguish between the different implants, meaning there might be uncaptured cost differences between procedure types from a hospital provider perspective.
Comparison with other studies
The debate surrounding the choice between RTSR and TSR for patients with osteoarthritis and an intact rotator cuff has represented a growing research uncertainty.83839One of the more common indications for revision of a TSR in this patient population includes secondary postoperative rotator cuff failure, a complication leading to further surgery that could be avoided with RTSR as the first line option.40This reasoning might partly account for the observed shift to increasing use of RTSR in this patient cohort in international practice. However, robust high quality evidence to substantiate this trend has been lacking.841424344A Cochrane review reported a lack of sufficient evidence to sufficiently compare TSR and RTSR, while a recent systematic review and meta-analysis identified just six small observational studies comparing these two procedures, with less than 150 patients in any one study.3845A more recent study used a propensity score matched cohort of 370 patients per group. Despite an average follow-up of less than two years, it reported similar patient reported outcomes and revision rates, but an increased risk of adverse events after TSR.39However, the study is not exempt from methodological weaknesses. A limited selection of confounders was considered and covariate balance metrics were inadequately reported. Furthermore, that study treated revision as a binary outcome variable (leading to information loss) and its authors acknowledge the lack of power to detect differences in the risk of revision.46The definition of adverse events was not clear, and the study only presented p values from t tests without calculating relative or absolute risk. One study from the Australian Orthopaedic Association National Joint Replacement Registry compared a selected subset of TSR with RTSR to investigate any differences in the risk of revision surgery up to four years after primary surgery for osteoarthritis, after adjusting for age, gender, American Society of Anaesthesiologists score, and body mass index.47Their study revealed similar initial survival curves where RTSR had a higher risk of revision in the first year, followed by a reduced risk thereafter, although only the first three months showed a significant difference. However, the authors were unable to restrict the analysis to patients with an intact rotator cuff as this variable is not recorded in the Australian Orthopaedic Association National Joint Replacement Registry, meaning that TSR might have been compared with an RTSR patient group with impaired or torn rotator cuffs.
Meaning of the study, unanswered questions, and future research
In response to the rapid rise of offering RTSR to patients aged 60 years or older with osteoarthritis and an intact rotator cuff, this study’s findings provide reassurance that RTSR is an acceptable alternative in this patient group. Further research with more comprehensive patient reported outcomes is required to fully evaluate the cost effectiveness of this procedure and to examine any functional differences.
International use of reverse total shoulder replacement (RTSR) has increased rapidly over the past two decades
RTSR is now being used beyond its original pathology group across many surgical indications without high quality evidence
This treatment uncertainty has been identified as a key priority by national healthcare agencies, prompting funding of international surgical trials that will take several years to complete
This study’s findings support the use of RTSR and anatomical total shoulder replacement (TSR) for patients aged 60 years or older with osteoarthritis and intact rotator cuff tendons who are in need of elective shoulder replacement surgery
No differences in modelled lifetime healthcare costs were found between RTSR and TSR in this patient group
Despite the difference in risk of revision surgery over time (favouring RTSR in the first three years), no clinically important differences were found in long term revision surgery, reoperations within 12 months, serious adverse events, or prolonged hospital stay
","Objectives: To answer a national research priority by comparing the risk-benefit and costs associated with reverse total shoulder replacement (RTSR) and anatomical total shoulder replacement (TSR) in patients having elective primary shoulder replacement for osteoarthritis.
Design: Population based cohort study using data from the National Joint Registry and Hospital Episode Statistics for England.
Setting: Public hospitals and publicly funded procedures at private hospitals in England, 2012-20.
Participants: Adults aged 60 years or older who underwent RTSR or TSR for osteoarthritis with intact rotator cuff tendons. Patients were identified from the National Joint Registry and linked to NHS Hospital Episode Statistics and civil registration mortality data. Propensity score matching and inverse probability of treatment weighting were used to balance the study groups.
Main outcome measures: The main outcome measure was revision surgery. Secondary outcome measures included serious adverse events within 90 days, reoperations within 12 months, prolonged hospital stay (more than three nights), change in Oxford Shoulder Score (preoperative to six month postoperative), and lifetime costs to the healthcare service.
Results: The propensity score matched population comprised 7124 RTSR or TSR procedures (126 were revised), and the inverse probability of treatment weighted population comprised 12 968 procedures (294 were revised) with a maximum follow-up of 8.75 years. RTSR had a reduced hazard ratio of revision in the first three years (hazard ratio local minimum 0.33, 95% confidence interval 0.18 to 0.59) with no clinically important difference in revision-free restricted mean survival time, and a reduced relative risk of reoperations at 12 months (odds ratio 0.45, 95% confidence interval 0.25 to 0.83) with an absolute risk difference of −0.51% (95% confidence interval −0.89 to −0.13). Serious adverse events and prolonged hospital stay risks, change in Oxford Shoulder Score, and modelled mean lifetime costs were similar. Outcomes remained consistent after weighting.
Conclusions: This study’s findings provide reassurance that RTSR is an acceptable alternative to TSR for patients aged 60 years or older with osteoarthritis and intact rotator cuff tendons. Despite a significant difference in the risk profiles of revision surgery over time, no statistically significant and clinically important differences between RTSR and TSR were found in terms of long term revision surgery, serious adverse events, reoperations, prolonged hospital stay, or lifetime healthcare costs.
"
Effect of combination treatment with GLP-1 receptor agonists and SGLT-2 inhibitors on incidence of cardiovascular and serious renal events,"Introduction
Glucagon-like peptide-1 (GLP-1) receptor agonists and sodium-glucose cotransporter-2 (SGLT-2) inhibitors are second to third line antihyperglycaemic drugs commonly prescribed for the treatment of type 2 diabetes.12Individually, these drugs have been shown to reduce the risk of cardiorenal events and mortality in large cardiovascular outcome trials.3456789However, the combined effect of these drug classes on these outcomes has not been extensively studied.
GLP-1 receptor agonists and SGLT-2 inhibitors are often combined in clinical settings when monotherapy fails to maintain glycaemic targets.1011Given their different mechanisms of action, the combination of the drugs may improve clinical outcomes through additive effects. Observational studies among patients with type 2 diabetes have shown that the GLP-1 receptor agonist-SGLT-2 inhibitor combination results in more significant improvements in haemoglobin A1cand blood pressure while lowering body weight than either drug class alone.121314151617However, these represent surrogate outcomes, and whether this combination is associated with a reduced risk of macrovascular and microvascular complications remains unclear. To date, observational studies investigating the cardiovascular effectiveness of the combination have either been underpowered or had important methodological limitations, such as immortal time bias.1215181920Importantly, none compared the combination with either drug class alone or investigated serious renal events, which are clinically relevant outcomes in this patient population.1215181920
The primary objective of this study was to determine whether the combined use of GLP-1 receptor agonists and SGLT-2 inhibitors is associated with a decreased risk of two co-primary outcomes, major adverse cardiovascular events and serious renal events, compared with the use of either drug class alone among patients with type 2 diabetes. The secondary outcomes included the association with the individual components of major adverse cardiovascular events (myocardial infarction, ischaemic stroke, cardiovascular mortality), heart failure, and all cause mortality.
Methods
Data sources
This population based cohort study was conducted using data from the UK Clinical Practice Research Datalink (CPRD) GOLD and Aurum databases, linked with the Hospital Episode Statistics Admitted Patient Care (HES APC) and the Office for National Statistics (ONS) databases. The CPRD is a large primary care database with data for more than 60 million patients from approximately 2000 general practices across the UK.21These 60 million patients were born, living, and deceased during the study period while registered with a general practice in the UK. Read codes and SNOMED-CT terms are used to code clinical data such as diagnoses and procedures, and prescriptions are recorded using drug codes linked to the British National Formulary.21More than 90% of patients in the CPRD are linkable to other datasets.2223The HES APC database contains hospital admission records from English National Health Service hospitals and includes information such as admission and discharge dates, diagnoses (recorded using ICD-10 (international classification of diseases, 10th revision) codes), specialists seen, and procedures undertaken for all linked patients.23The ONS database is a vital statistics database that contains data on registered deaths in the UK, including the official date and causes of death.23
Study population
We used a prevalent new-user design,24a design that emulates a randomised controlled trial,25to investigate the effect of the GLP-1 receptor agonist-SGLT-2 inhibitor combination on cardiorenal outcomes compared with either drug class alone or with other antihyperglycaemic drugs. Essentially, this design emulated a randomised controlled trial, in which participants on a background therapy with one of the drug classes of interest (GLP-1 receptor agonists or SGLT-2 inhibitors) are randomised to either continue the background drug or add on the other drug of interest (SGLT-2 inhibitor or GLP-1 receptor agonist, respectively). Thus, we constructed two cohorts. One cohort included patients with a background treatment of a GLP-1 receptor agonist who added on an SGLT-2 inhibitor and were compared with those who continued a background treatment of GLP-1 receptor agonists. The second cohort included patients with a background treatment of an SGLT-2 inhibitor who added on a GLP-1 receptor agonist; these patients were compared with those who continued a background treatment of SGLT-2 inhibitors.
We first assembled two separate base cohorts of new users of the drug classes of interest (GLP-1 receptor agonists (dulaglutide, exenatide, liraglutide (except the 3 mg/0.5 mL formulation indicated for weight loss), lixisenatide, semaglutide) and SGLT-2 inhibitors (canagliflozin, dapagliflozin, empagliflozin)). These cohorts consisted of patients who received their first prescription for one of these drug classes between 1 January 2013 (the year SGLT-2 inhibitors were introduced in the UK market) and 31 December 2020. To be included in these cohorts, patients had to be at least 18 years of age and have at least one year of medical history in the CPRD before the first prescription. We excluded patients who used either GLP-1 receptor agonists or SGLT-2 inhibitors in the year before the first prescription in order to identify new users. For the base cohort of new GLP-1 receptor agonist users, we excluded patients with a history of SGLT-2 inhibitors and vice versa for the base cohort of new SGLT-2 inhibitor users. We also excluded patients with no diagnosis of type 2 diabetes ever before cohort entry, as well as those with diagnosed contraindications for the study drugs—namely, end stage renal disease and multiple endocrine neoplasia syndrome, assessed ever before cohort entry. Using an on-treatment approach, we followed the new users of the drug classes of interest while they remained continuously exposed. Continuous use was defined as having overlapping successive prescriptions, with a 60 day grace period to bridge consecutive non-overlapping prescriptions. We followed patients until treatment discontinuation, death, end of registration with the CPRD, end of the follow-up period (29 March 2021) or add-on of one of the drugs of interest (described below), whichever occurred first.
Using the base cohorts defined above, we created the study cohorts. We divided the follow-up period into 30 day intervals. Within each 30 day interval, we identified new users of the GLP-1 receptor agonist-SGLT-2 inhibitor combination. These included patients who started their treatment with both drug classes on the same day at the start of follow-up and those who added on a GLP-1 receptor agonist or an SGLT-2 inhibitor for the first time at some point during the follow-up (up until 29 March 2021). The comparator consisted of patients who had never used a GLP-1 receptor agonist-SGLT-2 inhibitor combination up until the time of the interval and had received a prescription for the background drug at that 30 day interval. As part of the design, patients initially in the comparator group could enter the combination group, but only after receiving the add-on drug.
We then calculated time conditional propensity scores by using conditional logistic regression, conditional on the covariates listed below and stratifying on two variables: time interval and specific background drug. The second variable was to account for possible heterogeneity in the effectiveness within a drug class (for example, for empagliflozin, this variable grouped all patients who used that drug; these included patients who used empagliflozin in combination with a GLP-1 receptor agonist or empagliflozin alone). See the supplementary appendix for SAS codes.
Finally, in both study cohorts, we matched combination users in chronological order, in a one to one ratio without replacement, to GLP-1 receptor agonist or SGLT-2 inhibitor users (depending on the cohort) with the same background drug, duration of use of the background drug, and propensity score. Study cohort entry was defined by the date of the add-on prescription in the combination group and the prescription date of the comparator drug in a given interval. Thus, this matching procedure ensured that combination users and their comparators used the same background drug for the same duration and had a similar probability of receiving the treatment combination. See supplementary figure A for a graphical depiction of the design. Supplementary table A compares the methods of a target trial of the question and our emulated trial using real world data.
Exposure definition
We used an on-treatment approach in which patients were followed while being continuously exposed to the study drugs. For patients receiving a combination of a GLP-1 receptor agonist and an SGLT-2 inhibitor, continuous use was determined by overlapping prescription durations of both drug classes and allowing a 60 day grace period to bridge consecutive non-overlapping prescriptions. Hence, we considered patients to be combination users if their GLP-1 receptor agonist and SGLT-2 inhibitor prescriptions overlapped each other during the follow-up period. As such, treatment discontinuation for combination users was defined by the absence of either drug class by the end of the 60 day grace period. For patients on the background drug, treatment discontinuation was defined by the absence of a prescription by the end of the 60 day grace period. Thus, all patients were followed from study cohort entry until one of the study outcomes (described below), treatment discontinuation, initiation of a study drug in the comparator group, death from any cause (depending on the outcome), end of registration, or the end of the study period (29 March 2021), whichever came first.
Primary and secondary outcomes
We assessed two co-primary outcomes, which we identified by using inpatient diagnostic codes and mortality codes in the primary position (ICD-10 codes for the outcomes can be found in supplementary table B). They included major adverse cardiovascular events, which included myocardial infarction, ischaemic stroke, and cardiovascular mortality; and serious renal events, which included acute kidney injury, chronic kidney disease, hypertensive chronic renal disease, unspecified kidney failure, and renal complications of diabetes. Additionally, we examined secondary outcomes such as the individual components of major adverse cardiovascular events (myocardial infarction, ischaemic stroke, cardiovascular mortality), heart failure, and all cause mortality.
Potential confounders
We considered a wide range of potential confounders measured at or before study cohort entry. This corresponded to the time of new drug in combination users and the matched time in comparators. Hence, all covariates were updated at each exposure set in a time varying fashion. The covariates included age (modelled as a continuous variable using cubic splines with five knots at the 5th, 27.5th, 50th, 72.5th, and 95th centiles), sex, smoking status, body mass index, alcohol related disorders, and cohort entry year (2013-15, 2016-18, 2019-21). We also considered proxies for severity of diabetes, including duration of diabetes (calculated by the time difference between cohort entry date and date of the first of either a haemoglobin A1c>6.4%, a diagnosis of type 2 diabetes, or prescription for an antihyperglycaemic drug ever before cohort entry), haemoglobin A1clevel (≤7.0%, 7.1-8.0%, or >8.0%), types of antihyperglycaemic drugs used in the year before cohort entry (metformin, sulfonylureas, thiazolidinediones, meglitinides, α-glucosidase inhibitors, dipeptidyl peptidase 4 inhibitors, and insulin), microvascular (nephropathy, neuropathy, retinopathy) and macrovascular complications of diabetes (myocardial infarction, ischaemic stroke, peripheral vascular disease, coronary artery disease, coronary revascularisation, heart failure, all measured in ever before study cohort entry). Additionally, we considered common comorbidities (cancer (other than non-melanoma skin cancer), atrial fibrillation, thyroid diseases, and chronic obstructive pulmonary disease), as well as common prescription drugs (antihypertensives (diuretics, β blockers, calcium channel blockers, angiotensin converting enzyme inhibitors, angiotensin II receptor blockers, and others), non-steroidal anti-inflammatory drugs, paracetamol, acetylsalicylic acid, other antiplatelet agents, statins, fibrates, digoxin, opioids), and markers of healthcare seeking behaviour (colorectal cancer screening, prostate specific antigen screening, and influenza vaccination). These potential confounders were identified by Read and SNOMED-CT codes in the CPRD and ICD-10 codes in HES APC. We used an unknown category for variables that contained missing data (for example, body mass index, smoking status, haemoglobin A1c).
Statistical analysis
We summarised patients’ characteristics by using descriptive statistics in each cohort. An absolute standardised difference of <0.10 between the matched exposure groups was indicative of good balance. We calculated incidence rates of the outcomes for each exposure group, with confidence intervals based on the Poisson distribution. We used Kaplan-Meier curves to plot the cumulative incidence of the co-primary outcomes for the different exposure groups over the follow-up period. We fitted Cox proportional hazards regression models to estimate the hazard ratios and 95% confidence intervals for each outcome, comparing the GLP-1 receptor agonist-SGLT-2 inhibitor combination with the background drug. We calculated the number needed to treat to prevent one event at one year and three years of use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination by using the Kaplan-Meier method.26
Secondary analyses
We did three secondary analyses to examine the effect of the combined GLP-1 receptor agonist-SGLT-2 inhibitor treatment in patient subgroups of interest. Firstly, we did separate stratified analyses based on cardiovascular and chronic renal disease history at study cohort entry. Secondly, we stratified the analysis on the basis of the individual GLP-1 receptor agonist-SGLT-2 inhibitor combinations. Finally, we did effect measure modification analyses for age (>65 and ≤65 years) and sex. For these analyses, we included interaction terms between the exposure variable and these variables in the models.
Sensitivity analyses
We did several sensitivity analyses to examine whether our results were robust to varying assumptions. Firstly, given uncertainties about the grace period between consecutive prescriptions, we repeated our analyses using grace periods of 30 and 90 days. Secondly, to assess the impact of potential informative censoring, we did an analysis using time varying inverse probability of censoring weighting. We calculated this by taking the product of the weights calculated from the conditional probabilities of treatment discontinuation or switching, administrative censoring (end of study period/end of registration with practice), and death by using the covariates listed above. SAS codes for the calculation of these weights can be found in the supplementary appendix. Thirdly, given that insulin use has been associated with adverse cardiovascular effects, we did a separate analysis in which we excluded patients who had used insulin at study cohort entry and censored on insulin use during follow-up. We used new cohorts and propensity matching for this analysis. Fourthly, to assess the impact of missing data on our findings, we did an analysis using multiple imputation. Multiple regression models were fitted to impute variables that contained missing data, and the resulting datasets were combined using Rubin’s rules.272829Finally, we did an analysis repeating the process using one to two matching. We used SAS version 9.4 for all analyses.
Patient and public involvement
Our study was a secondary data analysis and did not include patients as study participants. No patients were involved in setting the research question or the outcome measures, nor were they involved in the design and implementation of the study. This is because no specific funding had been allocated for this purpose. Moreover, the CPRD data are not publicly available, and the analysis plan requires specialised training.
Results
GLP-1 receptor agonist-SGLT-2 inhibitor combination versus GLP-1 receptor agonists
Supplementary table C shows the distribution of the characteristics of both exposure groups before matching. The cohort included 6696 patients who added an SGLT-2 inhibitor to their GLP-1 receptor agonist treatment, who were matched to an equal number of patients who continued their treatment with GLP-1 receptor agonists (fig 1). The most common combinations were liraglutide and dapagliflozin (1380 users), liraglutide and empagliflozin (1240 users), dulaglutide and empagliflozin (963 users), and by liraglutide and canagliflozin (562 users).
Table 1shows the characteristics of the GLP-1 receptor agonist-SGLT-2 inhibitor combination users and GLP-1 receptor agonist users after matching. The exposure groups were well balanced across all covariates, with all standardised differences below 0.05. The mean durations of diabetes and previous use of GLP-1 receptor agonists was around 11 and 1.6 years, respectively, at cohort entry. We assessed the positivity assumption within each exposure set and observed good overlap between the propensity score distributions (supplementary figure B).
Table 2shows the results of the analyses for the primary and secondary outcomes. Overall, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a 30% lower risk of MACE (7.0v10.3 per 1000 person years; hazard ratio 0.70, 95% confidence interval 0.49 to 0.99) compared with the use of GLP-1 receptor agonists after a median follow-up time of 9.0 months.Figure 2shows the cumulative incidence curves for major adverse cardiovascular events, with a lower cumulative incidence for the GLP-1 receptor agonist-SGLT-2 inhibitor combination and the curves diverging after eight months of use. The number needed to treat to prevent one major adverse cardiovascular event after one and three years of use was 378 and 131, respectively.
For the secondary outcomes, the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a 65% lower risk of cardiovascular mortality (1.1v2.9 per 1000 person years; hazard ratio 0.35, 0.15 to 0.80) and a 43% lower risk of heart failure (3.6v6.1 per 1000 person years; 0.57, 0.35 to 0.91) compared with GLP-1 receptor agonists. Hazard ratios were below the null value for myocardial infarction, ischaemic stroke, and all cause mortality but generated wide confidence intervals. When patients were stratified by history of cardiovascular disease, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination in patients with previous cardiovascular disease was associated with a lower hazard ratio with respect to all cause mortality (0.46, 0.26 to 0.80) than in patients without a history of cardiovascular disease (1.04, 0.64 to 1.71; supplementary table D). After stratification by specific GLP-1 receptor agonist-SGLT-2 inhibitor combinations, the hazard ratios ranged from 0.64 for the liraglutide-dapagliflozin combination to 1.18 for the liraglutide-empagliflozin combination with confidence intervals overlapping each other (supplementary table E). We observed no effect measure modification for age and sex (supplementary tables F and G). Finally, the results of the sensitivity analyses were consistent with those of the primary analysis (supplementary tables H-L).
After a median follow-up time of 9.1 months, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a 57% lower risk of serious renal events (2.0v4.6 per 1000 person years; hazard ratio 0.43, 0.23 to 0.80) compared with the use of GLP-1 receptor agonists (table 2). Supplementary figure C shows the cumulative incidence curves for serious renal events, with a lower cumulative incidence for the GLP-1 receptor agonist-SGLT-2 inhibitor combination and the curves diverging after nine months of use. We observed no effect measure modification after stratifying patients by history of renal disease (supplementary table M).
GLP-1 receptor agonist-SGLT-2 inhibitor combination versus SGLT-2 inhibitors
Supplementary table N shows the distribution of the characteristics of the exposure groups before matching. The cohort included 8942 patients who added a GLP-1 receptor agonist to their SGLT-2 inhibitor treatment, who were matched to an equal number of patients using SGLT-2 inhibitors only (fig 3). No patients on the combination treatment were lost in the matching process. The most common combinations were dapagliflozin and dulaglutide (1865 users), empagliflozin and dulaglutide (1633 users), dapagliflozin and liraglutide (1119 users), and empagliflozin and semaglutide (784 users).
Table 3shows the characteristics of the GLP-1 receptor agonist-SGLT-2 inhibitor combination users and SGLT-2 inhibitor users after matching. The exposure groups were well balanced across all covariates, with no standardised difference above 0.02. The mean duration of diabetes was 10.8 years at cohort entry, and the mean duration of SGLT-2 inhibitor use at study cohort entry was 1.5 years. We assessed the positivity assumption within each exposure set and observed good overlap between the propensity score distributions (supplementary figure D).
Table 4shows the results of the analyses for the primary and secondary outcomes. Overall, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a 29% lower risk of major adverse cardiovascular events (7.6v10.7 per 1000 person years; hazard ratio 0.71, 0.52 to 0.98) compared with the use of SGLT-2 inhibitors after a median follow-up time of 8.4 months.Figure 4shows the cumulative incidence curves for major adverse cardiovascular events, with a lower cumulative incidence for the GLP-1 receptor agonist-SGLT-2 inhibitor combination and the curves diverging after three months of use. The number needed to treat to prevent one major adverse cardiovascular event after one and three years of use was 221 and 86, respectively.
For the secondary outcomes, the hazard ratios were below the null value for myocardial infarction, ischaemic stroke, cardiovascular mortality, heart failure, and all cause mortality, but with wide confidence intervals that included the null. The hazard ratios were similar after stratification of patients by history of cardiovascular disease (supplementary table O). After stratifying by specific GLP-1 receptor agonist-SGLT-2 inhibitor combination, we observed similar hazard ratios between the different types of combinations (supplementary table P). No effect measure modification by age was apparent (supplementary table Q). With respect to major adverse cardiovascular events, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination in female patients was associated with a lower hazard ratio (0.39, 0.22 to 0.71) compared with male patients (0.96, 0.66 to 1.40; supplementary table R). The results of the sensitivity analyses are presented in supplementary tables S-W, and they are consistent with those of the primary analysis.
After a median follow-up of 8.5 months, the cohort generated 36 serious renal events. The use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a hazard ratio below the null with a wide confidence interval (1.4v2.0 per 1000 person years; hazard ratio 0.67, 0.32 to 1.41). Supplementary figure E shows the cumulative incidence curves for serious renal events, with a lower cumulative incidence for the GLP-1 receptor agonist-SGLT-2 inhibitor combination up to two years and the crossing thereafter. When we stratified patients by history of renal disease, the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination in patients with a history of renal disease was associated with a lower hazard ratio for major adverse cardiovascular events (0.41, 0.18 to 0.94) compared with patients without previous renal disease (0.80, 0.57 to 1.12; supplementary table X).
Discussion
The results of this population based cohort study, designed to emulate a randomised controlled trial, suggest that the combined use of GLP-1 receptor agonists and SGLT-2 inhibitors is associated with a reduced risk of major adverse cardiovascular events and serious renal events, compared with using either drug class alone. The addition of an SGLT-2 inhibitor to existing GLP-1 receptor agonist use was also associated with a reduced risk of cardiovascular mortality and heart failure compared with GLP-1 receptor agonists alone. Overall, the results remained robust in several sensitivity analyses.
Comparison with previous studies
The results of this study indicate that the combined use of GLP-1 receptor agonists and SGLT-2 inhibitors is associated with a lower risk of major adverse cardiovascular events, compared with the use of either drug class alone. These findings are concordant with those of observational studies that have also observed a decreased risk of major adverse cardiovascular events when comparing the GLP-1 receptor agonist-SGLT-2 inhibitor combination with different types of comparators: sulfonylurea-GLP-1 receptor agonist combination (hazard ratio 0.67, 0.59 to 0.89),30metformin-sulfonylurea combination (hazard ratio 0.53, 0.35 to 0.80),31and other combination regimens (odds ratio 0.70, 0.50 to 0.98).19As these studies compared the GLP-1 receptor agonist-SGLT2 inhibitor combination with other drug combinations, they were designed to answer different clinical questions.193031By contrast, our study was specifically designed to determine whether the add-on of a GLP-1 receptor agonist or an SGLT-2 inhibitor versus either drug class alone results in additional benefits on cardiovascular and renal outcomes. This is particularly relevant given the increasing combined use of these effective drug classes.1011
Biological mechanisms
The associations observed with the GLP-1 receptor agonist-SGLT-2 inhibitor combination in this study can be attributed to an additive effect resulting from their different yet complementary mechanisms of action. Both drug classes have been shown to confer clinical benefits such as glycaemic control, body mass reduction, and improved systolic blood pressure and lipid profiles, which may collectively contribute to their cardiorenal protective effect.32However, these drug classes use different mechanisms to achieve these effects. GLP-1 receptor agonists bind to and stimulate GLP-1 receptors, which augments insulin secretion and inhibits glucagon release by the pancreas in a glucose dependent manner, leading to reductions in plasma glucose.3334This drug class also promotes satiety by delaying gastric emptying and acting on appetite regions of the hypothalamus, resulting in decreased food intake and sustained weight loss.33Furthermore, GLP-1 receptor agonists may impart additional cardiovascular benefits by alleviating atherosclerosis through improving inflammatory markers.35The anti-inflammatory and anti-oxidative effects of GLP-1 receptor agonists may also result in decreased albuminuria, reduced mesangial expansion, and improved glomerular hyperfiltration and endothelial function, explaining their renoprotective effects.3637
SGLT-2 inhibitors, on the other hand, exert their antihyperglycemic effects by inhibiting the reabsorption of glucose in the proximal tubules of the kidneys, resulting in urinary glucose excretion.38The excretion of calories in the form of glucose in the urine also promotes weight loss.39An interesting finding from our study was that adding an SGLT-2 inhibitor to existing GLP-1 receptor agonist treatment was associated with a decreased risk of heart failure, but this was not seen when GLP-1 receptor agonists were added to a background of SGLT-2 inhibitors. This finding corresponds to observations from the cardiovascular outcome trials for SGLT-2 inhibitors, in which they have been consistently associated with reduced risks of heart failure,6789whereas this effect was not observed in the cardiovascular outcome trials for GLP-1 receptor agonists. The decreased risk of heart failure associated with SGLT-2 inhibitors may be imparted by their haemodynamic effects on the heart, such as reduced intravascular volume, improved arterial elasticity, and decreased cardiac preload and afterload.4041Concerning their renoprotective effects, SGLT-2 inhibitors ameliorate hyperfiltration in the proximal tubules, which reduces intraglomerular pressure.42
Thus, the decreased cardiorenal risk associated with the combined use of GLP-1 receptor agonists and SGLT-2 inhibitors may be attributed to an additive effect, achieved by the distinct mechanisms and sites of action for these different drug classes. Further laboratory research is needed to elucidate better the mechanisms by which GLP-1 receptor agonists and SGLT-2 inhibitors impart their cardiorenal protective effects.
Strengths and limitations of study
This study has several strengths. Firstly, using the CPRD allowed for the ability to adjust for important potential confounders, including cardiovascular risk factors, microvascular and macrovascular complications, body mass index, and laboratory measures, which are often unavailable in other datasets. Secondly, we used an active comparator, prevalent new-user design that closely emulates a randomised controlled trial, a methodological approach best suited for this study question.25Thirdly, our study investigated not only cardiovascular outcomes but also serious renal events, which are clinically relevant outcomes in the type 2 diabetes population.
This study also has some limitations. Firstly, CPRD captures only data on prescriptions written by general practitioners, so information on patients’ adherence to treatment regimens is unknown, potentially introducing exposure misclassification. However, using an on-treatment exposure definition that followed patients while they were continuously exposed to the study drugs likely mitigated this bias. Additionally, given that the CPRD is a general practice database, prescriptions written by specialists are not captured, which may be another source of exposure misclassification. The impact of this potential bias is unlikely to be significant as general practitioners in the UK are primarily responsible for the long term management of patients with type 2 diabetes.43Secondly, outcome misclassification is also possible. However, validation studies have shown that hospital admission for cardiovascular events in the CPRD linked HES database has a high positive predictive value.4445Although the recording of the components of serious renal events (for example, renal complications of diabetes, chronic kidney disease) has not been validated in the CPRD, we do not expect any outcome misclassification to be differential between the exposure groups. Thirdly, residual confounding is a possibility owing to the observational nature of the study. However, by matching patients on specific background drug, duration of use of the background drug, and propensity scores generated on the basis of 46 potential confounders, we likely minimised the likelihood of significant confounding. Fourthly, the follow-up time for the combination therapy group was shorter than that for the monotherapy group in both cohorts. We anticipated the follow-up to be differential between the exposure groups but not likely to be associated with the outcome. For this reason, we did an inverse probability of censoring weighting sensitivity analysis, which yielded point estimates that were very similar to those of the primary analysis. Furthermore, the differences in follow-up periods are reflective of the real world experience with these drugs. Lastly, several secondary analyses were underpowered, generating few exposed events and wide confidence intervals. Thus, these results should be interpreted with caution.
Conclusion
In summary, the results of this population based study, designed to closely emulate a randomised controlled trial, suggest that the use of the GLP-1 receptor agonist-SGLT-2 inhibitor combination is associated with a lower risk of major adverse cardiovascular events and serious renal events among patients with type 2 diabetes compared with each drug class alone. Additional studies, including randomised controlled trials, will be needed to corroborate our findings and","Objective: To determine whether the combined use of glucagon-like peptide-1 (GLP-1) receptor agonists and sodium-glucose cotransporter-2 (SGLT-2) inhibitors is associated with a decreased risk of major adverse cardiovascular events and serious renal events compared with either drug class alone among patients with type 2 diabetes, and to assess the effect of the combination on the individual components of major adverse cardiovascular events, heart failure, and all cause mortality.
Design: Population based cohort study using a prevalent new-user design, emulating a trial.
Setting: UK Clinical Practice Research Datalink linked to Hospital Episode Statistics Admitted Patient Care and Office for National Statistics databases.
Participants: Two prevalent new-user cohorts were assembled between January 2013 and December 2020, with follow-up until the end of March 2021. The first cohort included 6696 patients who started GLP-1 receptor agonists and added on SGLT-2 inhibitors, and the second included 8942 patients who started SGLT-2 inhibitors and added on GLP-1 receptor agonists. Combination users were matched, in a 1:1 ratio, to patients prescribed the same background drug, duration of background drug, and time conditional propensity score.
Main outcome measures: Cox proportional hazards models were fitted to estimate the hazard ratios and 95% confidence intervals of major adverse cardiovascular events and serious renal events, separately, comparing the GLP-1 receptor agonist-SGLT-2 inhibitor combination with the background drug, either GLP-1 receptor agonists or SGLT-2 inhibitors, depending on the cohort. Secondary outcomes included associations with the individual components of major adverse cardiovascular events (myocardial infarction, ischaemic stroke, cardiovascular mortality), heart failure, and all cause mortality.
Results: Compared with GLP-1 receptor agonists, the SGLT-2 inhibitor-GLP-1 receptor agonist combination was associated with a 30% lower risk of major adverse cardiovascular events (7.0v10.3 events per 1000 person years; hazard ratio 0.70, 95% confidence interval 0.49 to 0.99) and a 57% lower risk of serious renal events (2.0v4.6 events per 1000 person years; hazard ratio 0.43, 0.23 to 0.80). Compared with SGLT-2 inhibitors, the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a 29% lower risk of major adverse cardiovascular events (7.6v10.7 events per 1000 person years; hazard ratio 0.71, 0.52 to 0.98), whereas serious renal events generated a wide confidence interval (1.4v2.0 events per 1000 person years; hazard ratio 0.67, 0.32 to 1.41). Secondary outcomes generated similar results but with wider confidence intervals.
Conclusions: In this cohort study, the GLP-1 receptor agonist-SGLT-2 inhibitor combination was associated with a lower risk of major adverse cardiovascular events and serious renal events compared with either drug class alone.
"
Prenatal opioid exposure and risk of neuropsychiatric disorders in children,"Introduction
The global opioid crisis has generated widespread attention because of its extensive effect on public health. While many women are generally cautious about medication intake during pregnancy,1the need for pain management in some instances has led to an observable reliance on analgesics, including opioids.23Besides analgesic treatment, pregnant women mainly used opioid as an antitussive treatment for coughs during pregnancy.4The prevalence of opioid use during pregnancy in cohort studies of Medicaid beneficiaries in the United States and pregnancy cohort studies in Quebec is approximately 5%.56Pregnant women often encounter specific physiological changes, such as increased ligamentous laxity, abnormal pain, and weight gain.7These changes can induce or exacerbate various painful conditions, often necessitating the guidance of opioids as analgesics.8
While considerable attention has been directed towards the direct effect of opioid use on individuals using the drug, an increasing concern arises regarding the indirect effects on their offspring. Prenatal and early life exposure to various substances has been associated with long term neuropsychiatric and developmental outcomes in the child. For instance, prenatal exposure to alcohol, tobacco, and some medications has been associated with an array of developmental, cognitive, and behavioural deficits in the child.910The effect of opioid exposure during the prenatal period is a topic of substantial importance but requires more in-depth examination.
Various neuropsychiatric disorders begin in childhood and result in established neuropsychiatric disorders later in life.11Opioid exposure during the prenatal and infancy periods might be an emerging risk factor for neuropsychiatric outcomes. While previous studies reported that exposure to opioids during the prenatal period is associated with neuropsychiatric disorders, such as attention deficit hyperactivity disorder, alcohol misuse, and depression,1213others suggest that exposure to opioids during pregnancy may not be deleterious to early childhood neurobehavioural development.1415Given the potential confounding factors and limited cohort size in earlier research,16a more precise investigation with a large scale birth cohort is warranted. Thus, we aimed to examine the association between maternal opioid exposure and the subsequent risk of neuropsychiatric disorders in children using a nationwide, population based, large scale birth cohort in South Korea. We also aimed to investigate the specific neuropsychiatric disorders potentially associated with fetal exposure to opioids.
Methods
Data source
This large scale nationwide cohort study was conducted using data from the National Health Insurance Service (NHIS) of South Korea,17which covers 98% of the South Korean population. Data, including baseline demographic details of individuals, outpatient and inpatient medical records, general health screening, and mortality information were collected through a universal health coverage system that provides comprehensive insurance services.18This study followed The Reporting of studies Conducted using Observational Routinely-collected health Data (RECORD) statement (table S1).
In this study, we focused on children born between 1 January 2010 and 31 December 2017. These children were subsequently paired with their mothers using the unique family insurance identification numbers allocated to every individual in the NHIS data (fig 1, figure S1-S4).1719The Korean government anonymised all patient related data, including personal identification numbers, to enhance confidentiality. While direct identification of individuals was not possible due to the removal of names, all other pertinent data remained intact and accessible for our analyses. The research protocol was approved by the institutional review board of Kyung Hee University (KHUH 2022-06-042) and the NHIS (NHIS-2023-1-168). We conducted this study using de-identified administrative data. No consent was required for this type of study.20
Study design and participants
All infants born in South Korean during the study period were included. The cohort comprised 3 251 594 children and 2 369 322 paired mothers. Using unique identification numbers, we paired children with their corresponding mothers (figure S4).21The data from 1 January 2009 to 31 December 2020 were included to ensure follow-up of all medical records within this timeframe. The exclusion criteria applied to the initial cohort are as follows (tables S2 and S3): participants with inadequate information on socioeconomic status (excluded n=66); missing birth dates (excluded n=76 028); children diagnosed with immune mechanism disorders (excluded n=936), cystic fibrosis (excluded n=18), chronic kidney disease (excluded n=388), β thalassemia or sickle cell disorders (excluded n=27), malignancy (excluded n=4116), teratogenic/genetic syndromes, microdeletions, chromosomal abnormalities, and malformation syndromes (excluded n=8015), or neuropsychiatric disorders within six months after birth (excluded n=6871); children of mothers diagnosed with cancer before delivery (excluded n=26 620); and mothers whose offspring were already excluded (excluded n=69 646). The final cohort comprised 3 128 571 children and 2 299 664 mothers.
Exposures
Opioid exposure was defined on the basis of mothers receiving two or more opioid prescriptions within each trimester, following the exposure criteria as per previous studies.52223Table S4 and figure S5 list the specific opioids included. Based on this definition, the extent of prenatal opioid exposure was classified into three categories. Firstly, based on trimesters of pregnancy, the four groups were defined as opioid use in the first, second, and third trimesters, and more than one trimester. Secondly, total opioid intake was calculated based on morphine milligram equivalents,523and mothers were categorised into non-user, user of low dose, and user of high dose by using the 75th percentile as the cutoff (25.5 morphine milligram equivalents). Thirdly, mothers were grouped according to the number of opioid prescriptions received (0-1, 2, or ≥3) and exposure duration (<30, 30-59, or ≥60 days) over the whole pregnancy. Additionally, we listed maternal health conditions among pregnant women with opioid prescriptions (table S5).
Outcomes
The primary outcome of the study was the onset of neuropsychiatric disorders in children, defined as having received at least two diagnoses of F00-99 codes as per the International Classification of Diseases, 10th edition (ICD-10) (table S2 and S6). In the context of psychiatric diagnoses in South Korea, predominantly psychiatrists, and to a lesser extent, paediatricians, are authorised to diagnose psychiatric conditions and assign the F-code in ICD-10 codes (table S6). Among the identified cases, infants with psychotic features were categorised as having severe neuropsychiatric disorders, whereas the other cases were classified as common neuropsychiatric disorders (table S2).24The specific diagnoses for children with neuropsychiatric disorders were categorised as follows (table S7): alcohol or drug misuse; mood disorders, excluding those with psychotic symptoms; anxiety and stress-related disorders; eating disorders; compulsive disorders; attention deficit hyperactivity disorder; autism spectrum disorder; and intellectual disability.
Covariates
We considered these covariates related to mothers: maternal age at delivery (<20, 20-24, 25-29, 30-34, or ≥35 years), region of residence (rural or urban),25household income level (first to fourth quartiles), parity (one or ≥two children), maternal mental illness (no mental illness, common, or severe), severe maternal morbidity score (0, 1, or ≥2),26delivery type (vaginal delivery or caesarean section), opioid prescription history, hospital admission (0, 1, or ≥2), and outpatient visit (0, 1, or ≥2) in the year before pregnancy, use of non-steroidal anti-inflammatory drugs (NSAIDs) or acetaminophen during pregnancy, and history of maternal neuropsychiatric conditions (alcohol or drug misuse, mood disorders except those with psychotic symptoms, anxiety and stress-related disorders, sleep disorders, epilepsy, and other neuropsychiatry disorders). We considered the following covariates for infants: sex, birth season (spring (March to May), summer (June to August), autumn (September to November), and winter (December to February)), year of delivery (2010-12, 2013-15, or 2016-17), preterm birth (≤36 weeks), low birth weight (≤2499 g), and breastfeeding history. All variables were obtained from eligibility data, claim codes, and child health examination data provided by the NHIS.
Cohorts
We used eight cohorts to comprehensively understand the association between maternal opioid prescriptions and risk of neuropsychiatric disorders in their child as follows: (1) a full unmatched cohort involving infants born between 2010 and 2017 (cohort 1 infig 1and table S8); (2) propensity score matched cohort A, derived from the full unmatched cohort, pairing the exposed and unexposed groups in a 1:5 ratio using propensity score (cohort 2 infig 1and table S8); (3) the child screening cohort, based on the full unmatched cohort, consisted of children from the National Health Screening Program for Infants and Children at six months after birth (cohort 3 infig 1and table S8); (4) propensity score matched cohort B consisting of infants born between 2010 and 2015, with the exposed and unexposed groups matched in a 1:5 ratio (cohort 4 infig 1and table S8); (5) propensity score matched cohort C including infants born between 2010 and 2017, forming a cohort where children who received at least one diagnosis based on the outcome criteria are matched in a 1:5 ratio between exposed and unexposed groups (cohort 5 infig 1and table S8); and (6) three sibling cohorts each specifically including sibling pairs with differing exposure statuses: sibling cohort A is formed from the full unmatched cohort (cohort 6 infig 1and table S8), sibling cohort B is derived from the propensity score matched cohort A (cohort 7 infig 1and table S8), and sibling cohort C is based on the child screening cohort (cohort 8 infig1and table S8).
Propensity score matched cohort
To mitigate potential confounding and to balance demographic covariates between the groups exposed to opioids and groups not exposed, we created a propensity score matched cohort informed by opioid exposure.27Propensity score was derived using a univariate logistic regression model, incorporating variables such as maternal age at delivery, region of residence, household income level, parity, maternal mental illness, severe maternal morbidity score, hospital admission, and outpatient visit in the year before pregnancy and history of maternal neuropsychiatric conditions (alcohol or drug misuse, mood disorders except those with psychotic symptoms, anxiety and stress related disorders, sleep disorders, epilepsy, and other neuropsychiatry disorders). Individuals were matched in 1:5 ratio matching between the opioid exposed (matched n=215 958) and unexposed (matched n=1 075 454) groups within the entire cohort. Using the greedy nearest-neighbor algorithm, we randomly matched the two groups based on propensity score values within the specified caliper (0.001), ensuring minimal differences. The appropriateness of propensity score matched was evaluated using standardised mean differences. We considered no substantial imbalance between the two groups when the standardised mean difference was less than 0.1 (figure S6 and S7).27
Child screening cohort
A previous birth cohort study reported significant benefits of breastfeeding on subsequent hospital admissions for infection, gastrointestinal tract, respiratory, genitourinary tract, and oral cavity in children.28To minimise unmeasured confounding factors, we formed a cohort of children who received the National Health Screening Program for infants and children at six months after birth, enabling us to obtain information on their breastfeeding history (table S9). Individuals with missing breastfeeding data despite having undergone screening were excluded from this cohort. A total of 1 377 246 children were included in the screening cohort (table S10).
Sibling comparison cohort
Sibling comparison analysis provides a suitable approach to counter potential biases arising from unmeasured confounding factors, such as genetics, lifestyle, and environmental influences.29In all cohorts (full unmatched, propensity score matched, and child screening cohorts), we established sibling comparison cohorts for sibling pairs with different exposure statuses. Pairs of only children or siblings sharing uniform opioid exposure or no exposure were systematically excluded.
Other analyses
To enhance our findings, we conducted additional analyses using different follow-up duration and criteria for outcome. We first further investigated the prolonged effects of maternal opioid prescriptions by utilising 1:5 propensity score matched cohort B of children, born between 2010 and 2015 (figure S3, S7, and table S11). Then, to ascertain the robustness of our findings, we reanalysed using a 1:5 propensity score matched cohort C, defining the outcome by at least one diagnosis.
Statistical analysis
Across all cohort analyses, the primary exposure was prenatal opioid exposure, and the primary outcome was the onset of neuropsychiatric disorders in children. We designated each childbirth date as the index date. Follow-up continued until the first diagnosis of neuropsychiatric disorder, 31 December 2020 (end of the study period), or the date of death, whichever occurred first.
Hazard ratios with 95% confidence intervals (CIs) using Cox proportional hazards model for estimation. Additionally, to control for the influence of potential confounders and to strengthen the validity of the results, two adjusted models were developed using the following variables. Firstly, for the adjusted model, maternal age at delivery years, infant’s sex, region of residence, household income level, birth season, parity, maternal mental illness, severe maternal morbidity score, and hospital outpatient visit as well as hospital admission contact in a year before pregnancy were used. And secondly, for the fully adjusted model, covariates of the adjusted model were used in addition to history of maternal neuropsychiatric conditions (alcohol or drug misuse, mood disorders except those with psychotic symptoms, anxiety and stress related disorders, sleep disorders, epilepsy, and other neuropsychiatry disorders), and use of NSAIDs as well as acetaminophen during pregnancy. We further adjusted for breastfeeding history in the child screening cohort. To reduce unpredictable biases and reverse causality, we conducted a stratification analysis using variables. A dose dependent analysis was conducted to elucidate the association between opioid use and onset of neuropsychiatric disorders. Furthermore, we explored a potential association between specific factors (maternal health condition, opioid prescription a year before pregnancy, and delivery type) and opioid prescriptions during pregnancy by a multiplicative interaction analysis. We used Cox models that included multiplicative interaction terms between opioid prescriptions during pregnancy and each factor (tables S12-14).30All statistical inferences were considered significant at a two sided Pvalue of less than 0.05. Statistical analyses were performed using SAS (version 9.4; SAS Institute Inc, Cary, NC, USA).
Patient and public involvement
The Korean government anonymised all patient related data, including personal identification numbers, to enhance confidentiality. While direct identification of individuals was rendered impossible due to the removal of names, all other pertinent data remained intact and accessible for our analyses. Due to the database containing data for the national population, access was limited to participating researchers only for security and confidentiality purposes. The research questions and outcome measures were independently determined without the involvement of the children or their parents. The study design and implementation were conducted without consultation. In South Korea, no framework for the management of patient and public involvement has been established. However, the results of the study will be officially registered and released to NHIS (the official institutions of the Korean government) and we plan to disseminate the results of this study to all study participants and wider relevant communities on request.
Results
Of 2 299 664 mothers included in the study, 3 128 571 infants were linked and we identified 93.1% (n=2 912 559) infants with no prenatal opioid exposure (51.3% male, 48.7% female) and 6.9% (n=216 012) infants with prenatal opioid exposure (51.2% male, 48.8% female) in the full matched cohort within follow-up periods from 1 January 2009 to 31 December 2020 (fig 1;table 1).
After 1:5 propensity score matching, the standardised mean difference values were less than 0.1, indicating no major imbalances in the general characteristics. The association between maternal exposure to opioids and neuropsychiatric outcomes in the offspring from the subgroup analysis is indicated as crude and adjusted hazard ratio intable 2and table S15. Prenatal exposure to opioids was associated with an increased risk of neuropsychiatric disorders (fully adjusted hazard ratio 1.07 (95% CI 1.05 to 1.10),table 2). In particular, exposure to opioids during the first trimester showed an increased risk of neuropsychiatric disorders in the offspring compared with the offspring who was not exposed (1.11 (1.07 to 1.15)). Risk of neuropsychiatric disorders in the offspring increased in a dose-dependent manner with opioid doses (low dose 1.06 (1.03 to 1.09); high dose, 1.15 (1.09 to 1.21)). Compared with infants who were not exposed, the risk of neuropsychiatric disorders in infants who were exposed showed increasing trends depending on the days of opioid prescriptions (1-29 days of opioid prescriptions, 1.07 (1.04 to 1.10); 30-59 days, 1.34 (1.12 to 1.62); ≥60 days, 1.95 (1.24 to 3.06)). In addition, similar trends were reported in further analysis by using different timeframes of infants born between 2010 to 2015 and different outcome criteria with one or more instances of diagnosis (tables S16 and S17). We performed stratification analysis in the 1:5 propensity score matched cohort A (table 3). The subsequent risk of neuropsychiatric disorders with maternal opioid use was significantly associated with caesarean sections in comparison to vaginal deliveries (P value for interaction=0.009; ratio of hazard ratio 1.08 (95% CI 1.03 to 1.14); table S14).
Table 4and table S18 present the results of a dose-dependent subgroup analysis of the association between maternal opioid use during pregnancy and the risk of specific neuropsychiatric disorders in children from the full and 1:5 propensity score matched cohorts. Compared with infants who were not exposed to opioids, maternal opioid use significantly increased the risk of several neuropsychiatric diseases, including mood disorder (adjusted hazard ratio 1.15 (95% CI 1.04 to 1.26)), attention deficit hyperactivity disorder (1.12 (1.07 to 1.17)), and intellectual disability (1.30 (1.21 to 1.40)), but not risk of alcohol or drug misuse, anxiety and stress related disorders, eating disorders, compulsive disorders, and autism spectrum disorder. In particular, the risk of severe neuropsychiatric disorder in the infant (1.30 (1.15 to 1.46)) was higher than that of common neuropsychiatric disorders (1.07 (1.04 to 1.09)) among those who had maternal exposure to opioids.
Sibling analysis, which was performed to control for unmeasured familial confounders, showed no association between prenatal exposure to opioids and the risk of childhood neuropsychiatric disorders in the full unmatched, propensity score matched, and child screening cohorts (table 5). Similar associations were observed when performing the same analysis in the full unmatched (tables S19-22) and child screening cohorts stratified by breastfeeding history (table S23-26). No significant associations were noted between breastfeeding and subsequent risk for neuropsychiatric disorders (table S24).
Discussion
Findings and explanation
We investigated the effect of prenatal exposure to opioids on neuropsychiatric disorders in the child and obtained several key findings. Firstly, in this large scale nationwide cohort study that included 216 012 pregnancies that were exposed to opioids among 3 128 571 pregnancies overall, maternal opioid use was not associated with a substantially increased risk of neuropsychiatric disorders in the offspring. Although a statistically significant association (adjusted hazard ratio 1.06 (95% CI 1.04 to 1.09)) was observed between maternal opioid use and neuropsychiatric disorders in the offspring, the clinical significance of this finding is potentially limited due to the observational nature of the study. Similarly, the attenuated and null estimates observed in the sibling controlled analyses support the finding that prenatal opioid exposure was not associated with an increased risk of neuropsychiatric outcomes. Secondly, the timing of opioid exposure may have resulted in different outcomes. Using opioids during the first trimester showed a 11% increased risk of psychiatric disorders compared with no exposure. This specific effect of the first trimester suggests that the effect of opioid use on the fetus during the early neurodevelopmental phase is critical.3132Thirdly, the risk of childhood neuropsychiatric disorders showed a dose-dependent association with maternal opioid dose and duration of opioid intake. The risk of neuropsychiatric disorders in the child increased with the number of opioid prescriptions and days of opioid prescriptions. In particular, long term prescriptions (≥60 days) were associated with a nearly doubled risk. Fourthly, offspring delivered via caesarean section had a significant risk of neuropsychiatric disorders in stratification analysis compared with vaginal delivery. Finally, compared with children who were not exposed to opioids, those who were exposed to prenatal opioids had a modestly increased risk of several neuropsychiatric disorders, including mood disorders, attention deficit hyperactivity disorder, and intellectual disability. However, no associated risk was found for alcohol or drug misuse, anxiety and stress related disorders, eating disorders, compulsive disorders, or autism spectrum disorder. Furthermore, prenatal opioid exposure was associated with a 16% increased risk of severe neuropsychiatric disorders, significantly higher than the 3% increased risk of common neuropsychiatric disorders.
Comparison with other studies
Previous studies explored the association between maternal opioid use and various health outcomes in the offspring52333; however, investigations specifically focusing on neuropsychiatric disorders are limited. A few studies reported no association between maternal opioid use and the risk of neuropsychiatric disorders in the offspring. However, these studies had limited cohort sizes (n=24 910) to generalise the results.16Furthermore, they did not account for potential confounders, including maternal and childbirth related factors (eg, breastfeeding history). By contrast, our large scale nationwide cohort study, encompassing over 3.12 million pregnancies, offers a more sophisticated understanding supported by statistical analyses, thereby highlighting the nuanced association between prenatal opioid exposure and neuropsychiatric outcomes in the child.
Possible mechanisms
In this cohort study, prenatal opioid exposure was not associated with a substantial increase in the risk of neuropsychiatric disorders. The benefits of short term use of prescription opioids in attenuating acute pain without additional risk factors during pregnancy may surpass the potential risk of neuropsychiatric disorders because short term use of opioids during pregnancy seems to have a relatively low risk of neuropsychiatric outcomes. Previous studies showed that untreated pain during pregnancy may lead to a lower quality of life and limited productivity.34When considering the risk to benefit ratio balance of prescribing opioids during pregnancy, relief of pain is crucial during pregnancy to manage maternal health and life; however, clinicians should consider avoiding opioid use under certain conditions, such as in the first trimester, for long term use, and at high dose use.
The increased risk of psychiatric disorders associated with first trimester opioid exposure suggests potential risk during the early neurodevelopmental phases. The early trimester is characterised by critical stages of neurogenesis, neuronal migration, and differentiation.3132Opioids, acting on the central nervous system, might interfere with these processes and disrupt normal brain developmental patterns.35In addition, the observed dose-response association between maternal opioid dosage and the incidence of neuropsychiatric disorders in the offspring indicates that prolonged and intense exposure might have cumulative heightened effects that are detrimental on fetal brain development. Higher doses and longer durations of opioid use could lead to more significant alterations in the neurochemical environment of the developing fetus, potentially altering neural pathways and synaptic functions.36
Children born via caesarean section display an increased risk of neuropsychiatric disorders.37The increased risk might be attributed to several associated factors: the underlying maternal conditions necessitating a caesarean section that could influence fetal brain development; the absence of natural microbial exposure from the birth canal, affecting the gut microbiome of infants and potential brain interactions; altered hormonal responses compared with vaginal delivery, which can affect neonatal brain maturation; and differences in the immediate postnatal environment, such as exposure to anaesthetics, that could affect early neural patterns.3738Therefore, combining the two adverse effects of caesarean section and maternal opioid use may synergistically increase the risk of neuropsychiatric disorders in offspring.38
Children with prenatal opioid exposure display an increased risk of specific neuropsychiatric disorders, suggesting that opioids may interfere with certain developmental processes in the fetal brain. Opioids can cross the placenta and blood–brain barrier and may influence the balance of neurotransmitters, which is crucial for developing and maintaining neural circuits.39The increased risk of mood disorders, attention deficit hyperactivity disorder, and intellectual disabilities may result from opioids affecting the serotonin, dopamine, and norepinephrine pathways, which are primary neurotransmitters for mood regulation, attention, and cognition.39The elucidated mechanisms presented in this study are speculative and require further validation. However, the significantly higher risk of severe neuropsychiatric disorders than that of common neuropsychiatric disorders implies the potential for opioids to disturb critical periods of neurodevelopment, leading to more critical neurobehavioural outcomes. Therefore, our findings suggest that clinicians should consider these potential risks when prescribing opioids during the first trimester, which is a crucial neurodevelopmental phase.
However, the overall population in our cohort study suggests no considerable association between maternal opioid use and neuropsychiatric disorders in the offspring. However, clinicians and patients should pay attention to opioid use in the first trimester or caesarean section, high dose, or long term intake.
Strengths and limitations of the study
This nationwide longitudinal study used validated diagnostic records and a large generalisable sample size, which minimised the risk of selection and recall biases (table S27). In addition, substantial individual-level healthcare data allowed us to characterise numerous potential confounders, including inpatient and outpatient medication exposure and medical conditions. The use of various study designs, including the full unmatched population based, propensity score matched, child screening, sibling comparison cohorts from the full unmatched, propensity score matched, and child screening cohorts, and multiple subgroup analyses, enhanced the results of our findings. Moreover, because opioids are available only as prescriptions in South Korea, exposure misclassification due to over-the-counter availability was unlikely.
Our study has several limitations. While prescriptions are recorded, they may not always reflect the actual consumption of the medication, leading to potential exposure misclassification. We can suggest an association but not explain the causal association owing to the observational nature of our study. Additionally, even with our sibling controlled analyses, potential confounding by unobserved non-shared familial factors exists.40The method used to estimate the start of pregnancy, despite being previously validated, may retain the potential misclassification of the exposure window.40We could not account for other potential risk factors for neuropsychiatric disorders, such as infection, epilepsy, fever, and vaccination. Because NSAIDs are not categorised as prescription drugs, an underestimation regarding their consumption is possible. However, considering the typically cautious approach of pregnant women towards medication intake without prescriptions, we decided to incorporate analyses related to NSAIDs as milder analgesics.141Finally, the cohort was restricted to pregnancies that resulted in live births and excluded terminated pregnancies due to the absence of gestational age data for non-live births.
Conclusion
In this nationwide birth cohort study, opioid use during pregnancy was not associated with a substantial increase in the risk of neuropsychiatric disorders in the child. Although a slightly increased risk was observed for neuropsychiatric disorders, given the observational nature of the study, these results should not be considered clinically meaningful. However, through several statistical analyses, we found that prenatal opioid exposure during the first trimester, higher doses, and long term opioid use were associated with an increased risk of psychiatric disorders in the child. Prenatal opioid exposure modestly increased the risk of severe neuropsychiatric disorders, mood disorders, attention deficit hyperactivity disorder, and intellectual disability in the child. However, in the context of opioids, excessive exposure, exposure during early pregnancy, and caesarean section may warrant caution owing to their potential associations with some brain developmental disorders in offspring.
Previous studies have shown mixed findings on the association between maternal opioid use and various health outcomes in the offspring, with a limited focus on neuropsychiatric disorders
Large scale, population based birth cohort studies are needed to clarify the potential effect of prenatal opioid exposure on the risk of neuropsychiatric disorders in children
Opioid use during pregnancy was not associated with a substantial increase in the risk of neuropsychiatric disorders in the offspring, particularly in sibling controlled analyses
An increased risk of neuropsychiatric disorders was observed and limited to high opioid doses, more than one opioid, longer duration of exposure, opioid exposure during early pregnancy, and only to certain specific neuropsychiatric disorders
These results support cautious opioid prescribing during pregnancy, highlighting the importance of further research for more definitive guidelines
","Objective: To investigate the potential association between prenatal opioid exposure and the risk of neuropsychiatric disorders in children.
Design: Nationwide birth cohort study.
Setting: From 1 January 2009 to 31 December 2020, birth cohort data of pregnant women in South Korea linked to their liveborn infants from the National Health Insurance Service of South Korea were collected.
Participants: All 3 251 594 infants (paired mothers, n=2 369 322; age 32.1 years (standard deviation 4.2)) in South Korea from the start of 2010 to the end of 2017, with follow-up from the date of birth until the date of death or 31 December 2020, were included.
Main outcome measures: Diagnosis of neuropsychiatric disorders in liveborn infants with mental and behaviour disorders (International Classification of Diseases 10th edition codes F00-99). Follow-up continued until the first diagnosis of neuropsychiatric disorder, 31 December 2020 (end of the study period), or the date of death, whichever occurred first. Eight cohorts were created: three cohorts (full unmatched, propensity score matched, and child screening cohorts) were formed, all of which were paired with sibling comparison cohorts, in addition to two more propensity score groups. Multiple subgroup analyses were performed.
Results: Of the 3 128 571 infants included (from 2 299 664 mothers), we identified 2 912 559 (51.3% male, 48.7% female) infants with no prenatal opioid exposure and 216 012 (51.2% male, 48.8% female) infants with prenatal opioid exposure. The risk of neuropsychiatric disorders in the child with prenatal opioid exposure was 1.07 (95% confidence interval 1.05 to 1.10) for fully adjusted hazard ratio in the matched cohort, but no significant association was noted in the sibling comparison cohort (hazard ratio 1.00 (0.93 to 1.07)). Prenatal opioid exposure during the first trimester (1.11 (1.07 to 1.15)), higher opioid doses (1.15 (1.09 to 1.21)), and long term opioid use of 60 days or more (1.95 (1.24 to 3.06)) were associated with an increased risk of neuropsychiatric disorders in the child. Prenatal opioid exposure modestly increased the risk of severe neuropsychiatric disorders (1.30 (1.15 to 1.46)), mood disorders, attention deficit hyperactivity disorder, and intellectual disability in the child.
Conclusions: Opioid use during pregnancy was not associated with a substantial increase in the risk of neuropsychiatric disorders in the offspring. A slightly increased risk of neuropsychiatric disorders was observed, but this should not be considered clinically meaningful given the observational nature of the study, and limited to high opioid dose, more than one opioid used, longer duration of exposure, opioid exposure during early pregnancy, and only to some neuropsychiatric disorders.
"
Temporal trends in lifetime risks of atrial fibrillation and its complications,"Introduction
Atrial fibrillation is estimated to affect 17.9 million people in Europe by 2060 and 15.9 million people in the US by 2050.12All cause and cardiovascular mortality among individuals with atrial fibrillation has improved over time, although the excess mortality associated with atrial fibrillation is still high.345Atrial fibrillation is also associated with increased risks of stroke,6heart failure,7and myocardial infarction.8Therefore, prevention of atrial fibrillation is a major public health priority.
Effective risk assessment for atrial fibrillation and its complications is key to primary and secondary prevention by increasing awareness of the condition.91011In this context, the residual lifetime risk measures the cumulative risk for developing a disease over the remaining lifespan among individuals who attained a given index age disease-free.1213Lifetime risk is a tractable quantity that can help to promote lifestyle change using recommendations and is an important communication tool to the public.1415Population and community based cohort studies have reported on the lifetime risk of atrial fibrillation, which is estimated as affecting one in three in individuals of European ancestry in Europe and the US.161718
Patients are commonly told that the main danger after being diagnosed with atrial fibrillation is the increased risk of stroke, but atrial fibrillation is associated with increased risk of other complications. Quantification of the long term downstream consequences of atrial fibrillation will further improve our understanding of the burden of atrial fibrillation and how to communicate risks to the public, but previous investigations do not appear to have addressed this question. Furthermore, data for temporal trends in the lifetime risks of atrial fibrillation and of subsequent complications are absent. Monitoring temporal changes in atrial fibrillation burden is critical to indirectly measure the effectiveness of improvements in atrial fibrillation management and of primary and secondary prevention strategies, particularly in the new phase of stroke prevention therapy for atrial fibrillation. In this study of the Danish population, we aimed to estimate the lifetime risk of atrial fibrillation and of complications after atrial fibrillation, and to examine their temporal trends over 2000-22.
Methods
Setting, data sources, and study population
We conducted a nationwide cohort study from 1 January 2000 to 31 December 2022 in Denmark. We estimated the lifetime risk of atrial fibrillation among individuals who did not have atrial fibrillation and were aged 45 to 95 years. Among individuals with newly diagnosed atrial fibrillation, we estimated the lifetime risks of subsequent complications, including heart failure, any stroke, ischaemic stroke, myocardial infarction, and systemic embolism. To assess temporal trends, we compared two periods: 2000-10 and 2011-22.
Study populations and data were obtained from administrative nationwide registries. Firstly, we used the Danish National Patient Registry to identify inpatient hospital stays and outpatient contacts. We retrieved individual level information on dates of hospital admission and discharge, procedures performed, and primary and several secondary diagnoses per discharge. Diagnoses were coded according to the International Classification of Diseases 8th Revision (ICD-8) before 1994 and to the 10th revision (ICD-10) from 1994. Supplemental table 1 gives the ICD codes we used. We used the admission dates for outpatient contacts and discharge dates for inpatients contacts. In the third version of the Danish National Patient Registry, which was implemented in early 2019, information to classify hospital contacts as inpatients or outpatients was no longer available, and we used admission dates for all diagnoses. Secondly, we extracted individual level information on sex, date of birth, vital status, and migration from the Civil Registration System.19Thirdly, we retrieved information on pharmacological treatments from the Danish National Prescription Registry.20Medications were coded in accordance with the anatomical therapeutic chemical classification system. The codes we used are given in supplemental table 1.
Participants and outcomes for lifetime risk of atrial fibrillation
In each period (2000-22, 2000-10, and 2011-22), we included all Danish individuals aged 45 years or older and who did not have atrial fibrillation. We excluded participants aged 95 or older. To enable medical history assessment, we further excluded individuals who had been Danish residents for fewer than five years. Participants were followed up until the earliest of incident atrial fibrillation, death, age of 95 years, emigration, or the end of the period. In the primary analysis, 45 years was the index age. In secondary analyses, we repeated the analyses for the following index ages: 55 years or older, 65 years or older, and 75 years or older.
To assess newly diagnosed atrial fibrillation, we identified individuals with an incident hospital primary or secondary diagnosis of atrial fibrillation or atrial flutter (supplemental table 1). In the Danish National Patient Registry, the positive predictive value of atrial fibrillation or atrial flutter ICD code is 94-95% for inpatients and outpatients.2122
Participants and outcomes for lifetime risks of complications after atrial fibrillation
In each period, all patients with incident atrial fibrillation were further followed up for complications after their incident hospital diagnosis of atrial fibrillation up until death, age 95 years, emigration, or the end of the period. The population was restricted to patients with newly diagnosed atrial fibrillation in the study period and did not include prevalent patients to prevent survival bias. We identified the first occurrence after atrial fibrillation, if any, of each of the following outcomes: heart failure, any stroke, ischaemic stroke, myocardial infarction, and systemic embolism. For each outcome, we excluded patients with a prevalent diagnosis of the complication of interest at the time of incident atrial fibrillation. For example, when examining heart failure after atrial fibrillation, patients with a history of heart failure at or before the diagnosis of atrial fibrillation were excluded. To limit the possibility of misclassification, we did not use information about previous coronary artery bypass grafting or primary coronary intervention to identify prior myocardial infarction. Both procedures may be used for severe chronic coronary syndrome with no evidence of acute myocardial infarction. We also excluded patients who experienced a complication of interest fewer than seven days after newly diagnosed atrial fibrillation to ensure temporal separation between the diagnoses. All complications were identified as primary or secondary diagnoses (definitions in supplemental table 1). The positive predictive values of ICD codes were more than 80% for heart failure,23more than 90% for ischaemic stroke,24more than 70% for intracerebral haemorrhage,24and more than 97% for myocardial infarction,22while the value is unknown for systemic embolism. We conducted analyses for each of the index ages 45, 55, 65, and 75 years.
Covariates
To characterise the study populations, we assessed history of hypertension, diabetes, dyslipidaemia, heart failure, myocardial infarction, any stroke, cardiomyopathy, valvular heart disease, chronic obstructive pulmonary disease, chronic kidney disease, ischaemic stroke, and systemic embolism. Furthermore, we assessed family income and educational attainment from Statistics Denmark. Details are in the appendix. Information was assessed among individuals who did not have atrial fibrillation at cohort entry and among people who had atrial fibrillation at the time of incident atrial fibrillation (definitions in supplemental table 1).
Statistical methods
Age was the time scale in all analyses. We used the Aalen-Johansen estimator to calculate the cumulative incidence of atrial fibrillation and of complications after atrial fibrillation, accounting for left truncation and for the competing risk of death. The time at risk started at the exact entry age: index age or older for the lifetime risk of atrial fibrillation, and age at the diagnosis of incident atrial fibrillation for the lifetime risks of complications. The time at risk ended at the individual’s age at the earliest of the following dates: first incident atrial fibrillation or complication after atrial fibrillation, death, age 95 years, emigration, or end of study period. The lifetime risk was the cumulative incidence at the age of 95 years. We used the pseudo-value regression approach on the lifetime risk to obtain 95% confidence intervals (CI) and P values.13We tested for differences in lifetime risks between men and women and between period 1 and period 2.
To translate the lifetime risk into the time domain, we estimated the restricted mean time lost as the area under the cumulative incidence function from the index age of 45 years up to 95 years.25For the lifetime risk of atrial fibrillation, the restricted mean time lost gives the mean atrial fibrillation-free time lost between 45 and 95 years. For the lifetime risk of complications after atrial fibrillation, for example heart failure, the restricted mean time lost gives the mean time lost with no heart failure.
We repeated these analyses with adjustment for the covariates listed above. We estimated a propensity score to be in period 2 versus 1 by using all covariates in a logistic regression model. We then applied stabilised inverse propensity weights to the pseudo-value regression.
We also performed subgroup analyses for the lifetime risk of atrial fibrillation and of complications after atrial fibrillation (appendix). Subgroups were defined by sex, cardiometabolic risk factors, clinical comorbidities, and social factors, as aforementioned. We tested for differences in lifetime risks between subgroups and we tested for interaction between temporal trends across periods and subgroups.
Analyses were performed in Stata (StataCorp 2019: release 17.0, College Station, TX, USA).
Patient and public involvement statement
No funding was available to support patients or members of the public in the study design, interpretation of results, or development of the dissemination strategy. We appraised the registry based data and analysed them without public or patient involvement.
Results
Characteristics of participants
We included 3 574 903 individuals who did not have atrial fibrillation at index age 45 years or older, of whom 1 727 703 were men (48.3%) and 1 847 200 were women (51.7%). Flow diagrams showing the selection of participants are in supplemental figure 1. Participant characteristics are summarised intable 1as well as supplemental tables 2 and 3 by sex. Among participants free of atrial fibrillation at entry, the prevalence of hypertension, diabetes, dyslipidaemia, and stroke increased from 2000-10 to 2011-22. Moreover, we followed up 362 721 individuals with newly diagnosed atrial fibrillation, of whom 194 505 were men and 168 216 were women. At diagnosis of atrial fibrillation, the distribution of age was similar between periods, while the prevalence of hypertension, dyslipidaemia, and diabetes increased over time, and the prevalence of heart failure and myocardial infarction decreased over time.
Lifetime risk of atrial fibrillation
At the index age of 45 years, the lifetime risk of atrial fibrillation over 2000-22 was 27.7% ((95% CI 27.6% to 27.8%);table 2and supplemental table 4). The lifetime risk of atrial fibrillation was higher among men, individuals with history of hypertension, heart failure, myocardial infarction, cardiomyopathy, dyslipidaemia, valvular heart disease and individuals with higher educational attainment and higher family income. History of stroke, chronic obstructive pulmonary disease, and chronic kidney disease were associated with lower lifetime risk of atrial fibrillation because they were also associated with considerably higher mortality (supplemental table 5 and supplemental figures 2-13).
From 2000-10 to 2011-22, the overall lifetime risk of atrial fibrillation increased from 24.2% (95% CI 24.1% to 24.3%) to 30.9% (30.8% to 31.0%), an absolute increase of 6.7% ((6.5% to 6.8%);table 3and supplemental tables 6). Findings were similar after adjustment for covariates at entry (supplemental table 7). Over time, the lifetime risk of atrial fibrillation increased across all subgroups (supplemental table 8 and supplemental figures 2-13). The increase in lifetime risk was slightly higher among men, individuals with a history of heart failure or stroke, and among individuals with no dyslipidaemia.
All findings at index ages 55, 65, and 75 years were consistent. Across 2000-22, the lifetime risk of atrial fibrillation was 27.6% at 55 years, 26.9% at 65 years, and 24.3% at 75 years (supplemental table 9), and the lifetime risk of atrial fibrillation increased between the two periods by an absolute value of 6.5%, 6.3%, and 5.6% at index ages 55, 65, and 75 years (supplemental table 10).
Lifetime risk of complications after atrial fibrillation
Among patients with an incident diagnosis of atrial fibrillation, heart failure was the most frequent complication with a lifetime risk of 41.2% (95% CI 39.8% to 42.7%) from index age 45 years (table 2,fig 1, supplemental table 11-12). The lifetime risks of any stroke and ischaemic stroke after atrial fibrillation were 21.4% (20.6% to 22.3%) and 13.1% (12.4% to 13.8%). The lifetime risk of myocardial infarction after atrial fibrillation was 11.5% (10.9% to 12.2%) and diagnosed systemic embolism after atrial fibrillation was uncommon, with a lifetime risk of 1.8% (1.6% to 2.0%).
Associations between covariates and lifetime risk of complications following atrial fibrillation are summarised in supplemental tables 13-17 and supplemental figures 14-74. Men had a higher lifetime risk of complications after atrial fibrillation compared with women for heart failure (44.2%v34.6%) and myocardial infarction (12.2%v10.2%), while the lifetime risk of stroke after atrial fibrillation was lower in men than in women (20.6%v22.6%). Individuals who had a history of hypertension had a larger lifetime risk of heart failure, ischaemic stroke, myocardial infarction, and systemic embolism compared with those who did not. Additionally, people with history of myocardial infarction (58.4%v39.6%), cardiomyopathy (84.5%v39.9%), or valvular heart disease (62.5%v39.6%) had notably higher lifetime risks of heart failure after atrial fibrillation. Conversely, individuals with prior chronic obstructive pulmonary disease or chronic kidney disease and lower family income generally had lower lifetime risks of complications following atrial fibrillation because of the association that these risk factors have with higher overall mortality rates (supplemental figures 14-74).
Over time, lifetime risk of heart failure after atrial fibrillation did not change (42.9% in 2000-10v42.1% in 2011-22) and slight decreases were noted in the lifetime risk for any stroke (−2.5% (95% CI −4.2% to −0.7%)), ischaemic stroke (−5.2% (−6.7% to −3.8%)), and myocardial infarction after atrial fibrillation (−3.9% (−5.3% to −2.4%) (table 3, supplemental tables 18-19). Analyses adjusted for covariates at the time of atrial fibrillation diagnoses showed consistent findings (supplemental table 20).
No evidence suggested differential temporal trends for all complications after atrial fibrillation according to sex and most covariates (fig 2,table 3, supplemental tables 21-25, supplemental figures 14-74). However, the lifetime risk of heart failure decreased from 2000-10 to 2011-22 among patients with atrial fibrillation and a history of hypertension (52.6%v41.8%) or dyslipidaemia (52.1%v44.5%). Additionally, the decrease from 2000-10 to 2011-22 in the lifetime risk of myocardial infarction after atrial fibrillation was larger among patients with dyslipidaemia versus people with normal lipids (−11.1%v−3.7%).
The analyses of secondary index ages showed consistent findings. The lifetime risk of each complication was smaller with increasing index age (supplemental tables 26-28). Moreover, we found evidence of decrease over time in the lifetime risks for heart failure and systemic embolism after atrial fibrillation at index ages 55 and 75 years.
Discussion
Principal findings
In this nationwide study, the lifetime risk of newly diagnosed atrial fibrillation increased from approximately one in four in 2000-10 to one in three in 2011-22. The lifetime risk of atrial fibrillation increased across all subgroups over time, but the increase was larger among men and individuals with heart failure, myocardial infarction, stroke, diabetes, and chronic kidney disease. Among patients with newly diagnosed atrial fibrillation, heart failure was the most frequent complication after atrial fibrillation, with a lifetime risk of about two in five, twice as large as the lifetime risk of stroke after atrial fibrillation and four times greater than the lifetime risk of myocardial infarction after atrial fibrillation. Overall, lifetime risk of heart failure after atrial fibrillation showed almost no change, and only a slight decrease in the lifetime risks of any stroke, ischaemic stroke, and myocardial infarction after newly diagnosed atrial fibrillation. These slight improvements were similar among men and women. However, the lifetime risk of heart failure decreased from 2000-10 to 2011-22 for patients with history of hypertension and dyslipidaemia.
Comparison with other studies
In Europe, a study reported cumulative incidences of atrial fibrillation of about 27% at the age of 90 years in both men and women based on data from the BiomarCaRE Consortium.16Another used data from the Rotterdam Study and noted a lifetime risk of atrial fibrillation at the index age of 55 years of 23.8% in men and 22.2% in women.26A study in China reported a lifetime risk of 21% in women and 17% in men using a Chinese medical insurance database.27In Taiwan, a study used the National Taiwanese Health Insurance Research Database and reported a lifetime risk of 15% among men and 17% among women.28Additionally, in the US, data from the Framingham Heart Study were used and a lifetime risk of 37% for the index age of 55 years was reported.17The Atherosclerosis Risk in Communities study was used to report a lifetime risk of 36% in white men, 30% in white women, 21% in African American men, and 22% in African American women.18Data from prospective cohort studies are strengthened by systematic follow-up and may be limited by self-selection bias and under-representation of specific groups.29However, our findings originate from national data and are consistent with prior estimates. No study has thus far examined the lifetime risks of complications following atrial fibrillation.
Interpretation of temporal trends
To our knowledge, the temporal trends in lifetime risks of atrial fibrillation and complications after atrial fibrillation have not been reported previously. The underlying reasons for the observed trends are speculative but likely multifactorial. For lifetime risk of atrial fibrillation, the detection of atrial fibrillation significantly improved over the past two decades, primarily due to advancements in technology, changes in clinical practice, and increased awareness. Approximately a third of patients with atrial fibrillation are asymptomatic,30and enhanced detection over time may lead to increased lifetime risk of diagnosed atrial fibrillation. Another possible reason is the global increase in life expectancy. Additionally, survival after myocardial infarction and heart failure has improved, which increases the likelihood of developing atrial fibrillation and having atrial fibrillation detected due to increased surveillance. The lifetime risk increased slightly more in some subgroups, particularly those with history of heart failure or stroke, and improved attention on developing comorbidities may partially explain the increase. Finally, the observed increasing prevalence of key risk factors for atrial fibrillation over time, such as hypertension, dyslipidaemia, and diabetes, could also explain the increasing lifetime risk of atrial fibrillation. Although we did not have data for body mass index, the prevalence of obesity has increased over the past two decades in Denmark.31However, the temporal trend of atrial fibrillation remained after adjustment for all available risk factors at entry.
Regarding complications after atrial fibrillation, lifetime risks of stroke and ischaemic stroke remained high, with only about 4-5% decrease between 2000-10 and 2011-22. Clinical guidelines for the treatment of atrial fibrillation have been published regularly since 2001,32and new evidence supporting better treatment modalities, including target specific anticoagulants, has been incorporated since then.1133These improvements in atrial fibrillation management may translate into a larger decline in lifetime risks of complications after atrial fibrillation over longer periods. Another reason for the decreasing risk over time is the potential early identification of patients with atrial fibrillation at low risk because of enhanced awareness. For instance, patients with persistent atrial fibrillation have a higher risk of stroke than patients with paroxysmal atrial fibrillation.34The lifetime risk of heart failure after atrial fibrillation did not change overall, but over time heart failure substantially decreased among patients with history of hypertension and dyslipidaemia at atrial fibrillation diagnosis. The lifetime risk of myocardial infarction also had larger decrease among patients with dyslipidaemia. These trends may reflect the effect of anti-hypertensive and lipid lowering drugs. Key risk factors have increased over time, particularly hypertension, dyslipidaemia, and diabetes. The increasing prevalence may be explained by improvement of screening protocols for risk factors, leading to more comprehensive detection rates. The criteria for diagnosing hypertension and diabetes have changed over the timeline investigated in this study, with the thresholds for blood pressure and glycaemic concentrations being updated. Finally, lifestyle changes and an increase in risk factors in the general population, such as obesity and sedentary behaviour, may also play a role in the uptick of these conditions. Despite increasing prevalence of risk factors, our analyses adjusting for risk factors at the diagnosis of atrial fibrillation were consistent with the primary analysis.
Implications
Lifetime risk estimates provide epidemiological insights into the public health impact presented by atrial fibrillation and its complications. Communication of lifetime risk estimates may motivate preventive strategies, such as beneficial changes in lifestyle and strengthening of the focus on quality in medical care settings. Preventive strategies require knowledge of different components of long term risk after atrial fibrillation. However, the main body of data on the absolute risk of complications after atrial fibrillation originates from short term studies, and 11 years seems to be the maximum follow-up period.7353637By adopting a lifetime perspective, we provided evidence on the actual disease burden associated with atrial fibrillation.
Estimates of the lifetime risk of complications after atrial fibrillation indicate the burden of the different complications, and such information may be essential to facilitate prioritisation and development of preventive efforts against complications, prediction of complications, and management of health policy. In Denmark, more than 85% of patients with atrial fibrillation initiate oral anticoagulation, and the one year and two year persistence are both above 85%.38Despite a high rate of anticoagulation treatment, we observed that stroke risk remains a very important problem after atrial fibrillation. These findings underscore the need for treatments to further decrease stroke risk. Factor XIa inhibitors and left atrial appendage closure are currently evaluated in randomised controlled trials as first line stroke prevention measures. Moreover, although atrial fibrillation guidelines principally focus on stroke prevention,1133our findings indicate that heart failure was the major complication after incident atrial fibrillation, with a lifetime risk of two in five patients with atrial fibrillation, twice greater than that of stroke. Our findings encourage greater attention to secondary prevention of heart failure after atrial fibrillation, and in line with a recent report from the National Heart, Lung, and Blood Institute, relevant future initiatives include development of non-anticoagulant pharmacotherapies, effective implementation of weight loss, and risk factor modification, and designing successful programmes for cardiac rehabilitation.10As atrial fibrillation is a common arrhythmia, a lower incidence of complications may reduce the future economic costs in healthcare.
Limitations of the study
The data used in this study originate from a tax supported universal healthcare system with no substantial loss to follow-up of patients. The Danish National Patient Registry provided information on admissions to hospital for atrial fibrillation, but we were unable to examine individuals clinically to confirm the diagnosis. Furthermore, we could not use electrocardiogram monitoring devices such as Holter monitoring, and therefore, we may have missed patients with undiagnosed atrial fibrillation. We had no specific data for the proportion of patients with incident atrial fibrillation who were not captured by the Danish National Patient Registry. However, as the Danish guidelines recommend referral of patients with newly diagnosed atrial fibrillation to a public hospital for further examination, most patients with atrial fibrillation will be registered, and only a few cardiologists work outside of the public healthcare system in Denmark.38In Sweden, where the healthcare system is very similar to the Danish system, a study using data from 2004 to 2010 from the Swedish National Patient Registry and a central electrocardiogram database found that 93.2% of patients with electrocardiogram confirmed atrial fibrillation had a recording in the Swedish National Patient Registry.39Additionally, validation studies reported a high positive predictive value of atrial fibrillation and atrial flutter in the Danish National Patient Registry.2122We could not differentiate between atrial fibrillation and atrial flutter; however, atrial accounts for approximately 5% of the ICD-10 I48 diagnoses.21The positive predictive values for stroke, heart failure, and myocardial infarction also seem adequate.222324The positive predictive value of ICD-10 I45 codes for systemic embolism is unknown. The diagnostic validity is likely to be even higher in a cohort of patients with a history of atrial fibrillation because these individuals are likely to be under closer medical surveillance. We had no information on the cause of death, and accordingly, we may have underestimated the true number of incident events and the lifetime risk. We also did not have information on lifestyle factors, such as obesity, smoking status, and physical activity.
Assessing temporal trends in lifetime risk of atrial fibrillation and its complications requires extended follow-up durations of population based samples with minimal loss to follow-up. The Danish registries offer these unique features. However, we were able to assess the lifetime risks over only two periods. Moreover, lifetime risks of atrial fibrillation and subsequent complications may vary according to race and ethnic group, geographical location, health system, and country income, and our results may not be transportable to other countries or settings. Lastly, we did not separate calendar temporal trends from birth cohort effects, although analyses with age as the time scale partially address this limitation.40
Conclusions
Our nationwide study shows that the lifetime risk of atrial fibrillation increased over the past two decades from one in four to one in three. After atrial fibrillation, heart failure was the most frequent complication, with a lifetime risk of two in five, twice greater than the lifetime risk of stroke after atrial fibrillation. The lifetime risks of stroke, ischaemic stroke, and myocardial infarction following atrial fibrillation improved only modestly over time and remained high, while virtually no improvement was noted in the lifetime risk of heart failure after atrial fibrillation. Our novel quantification of the long term downstream consequences of atrial fibrillation highlights the critical need for treatments to further decrease stroke risk as well as for heart failure prevention strategies among patients with atrial fibrillation.
Once atrial fibrillation develops, patient care has focused on the risk of stroke
Long term downstream consequences of atrial fibrillation, including stroke, heart failure, and myocardial infarction, must be better studied
Changes over time in the lifetime risks of atrial fibrillation and of complications after atrial fibrillation remain to be fully explored with new stroke prevention treatment for atrial fibrillation
In registry analyses of the entire population in Denmark over 2000-22, the lifetime risks of complications after atrial fibrillation were high
The most common complication was heart failure (lifetime risk 41%), followed by stroke (21% for any stroke and 13% for ischaemic stroke) and myocardial infarction (12%)
From 2000-10 to 2011-22, virtually no improvement was reported in the lifetime risk of heart failure after atrial fibrillation (−0.8%) and only slight decreases in the lifetime risks of stroke (−2.5% for any stroke, −5.2% for ischaemic stroke) and myocardial infarction (−3.9%) after atrial fibrillation
","Objectives: To examine how the lifetime risks of atrial fibrillation and of complications after atrial fibrillation changed over time.
Design: Danish, nationwide, population based cohort study.
Setting: Population of Denmark from 1 January 2000 to 31 December 2022.
Participants: 3.5 million individuals (51.7% women and 48.3% men) who did not have atrial fibrillation at 45 years of age or older were followed up until incident atrial fibrillation, migration, death, or end of follow-up, whichever came first. All 362 721 individuals with incident atrial fibrillation (46.4% women and 53.6% men), but with no prevalent complication, were further followed up until incident heart failure, stroke, or myocardial infarction.
Main outcome measures: Lifetime risk of atrial fibrillation and lifetime risks of complications after atrial fibrillation over two prespecified periods (2000-10v2011-22).
Results: The lifetime risk of atrial fibrillation increased from 24.2% in 2000-10 to 30.9% in 2011-22 (difference 6.7% (95% confidence interval 6.5% to 6.8%)). After atrial fibrillation, the most frequent complication was heart failure with a lifetime risk of 42.9% in 2000-10 and 42.1% in 2011-22 (−0.8% (−3.8% to 2.2%)). Individuals with atrial fibrillation lost 14.4 years with no heart failure. The lifetime risks of stroke and of myocardial infarction after atrial fibrillation decreased slightly between the two periods, from 22.4% to 19.9% for stroke (−2.5% (−4.2% to −0.7%)) and from 13.7% to 9.8% for myocardial infarction (−3.9% (−5.3% to −2.4%). No evidence was reported of a differential decrease between men and women.
Conclusion: Lifetime risk of atrial fibrillation increased over two decades of follow-up. In individuals with atrial fibrillation, about two in five developed heart failure and one in five had a stroke over their remaining lifetime after atrial fibrillation diagnosis, with no or only small improvement over time. Stroke risks and heart failure prevention strategies are needed for people with atrial fibrillation.
"
Antipsychotic use in people with dementia,"Introduction
Dementia is a clinical syndrome characterised by progressive cognitive decline and functional disability, with estimates suggesting that by 2050 around 152.8 million people globally will be affected.1Behavioural and psychological symptoms of dementia are common aspects of the disease and include features such as apathy, depression, aggression, anxiety, irritability, delirium, and psychosis. Such symptoms can negatively impact the quality of life of patients and their carers and are associated with early admission to care.23Antipsychotics are commonly prescribed for the management of behavioural and psychological symptoms of dementia, despite longstanding concerns about their safety.456During the covid-19 pandemic, the proportion of people with dementia prescribed antipsychotics increased, possibly owing to worsened behavioural and psychological symptoms of dementia linked to lockdown measures or reduced availability of non-pharmaceutical treatment options.7According to guidelines from the UK’s National Institute for Health and Care Excellence, antipsychotics should only be prescribed for the treatment of behavioural and psychological symptoms of dementia if non-drug interventions have been ineffective, if patients are at risk of harming themselves or others or are experiencing agitation, hallucinations, or delusions causing them severe distress.8Antipsychotics should at most be prescribed at the lowest effective dose and for the shortest possible time. Only two antipsychotics, risperidone (an atypical, or second generation, antipsychotic) and haloperidol (a typical, or first generation, antipsychotic), are licensed in the UK for the treatment of behavioural and psychological symptoms of dementia,9although others have been commonly prescribed off-label.510
Based on evidence from clinical trials of risperidone, the US Food and Drug Administration (FDA) first issued a warning in 2003 about the increased risks of cerebrovascular adverse events (eg, stroke, transient ischaemic attack) associated with use of atypical antipsychotics in older adults with dementia.11A meta-analysis of 17 trials among such patients subsequently found a 1.6-1.7-fold increased risk of mortality with atypical antipsychotics compared with placebo, which led the FDA to issue a “black box” warning in 2005 for all atypical antipsychotics.11This warning was extended to typical antipsychotics in 2008, after two observational studies reported that the risk of death associated with their use among older people might be even greater than for atypical antipsychotics.121314The increased risks for stroke and mortality have been consistently reported by many observational studies and meta-analyses since,1115161718192021and they have led to regulatory safety warnings and national interventions in the UK, US, and Europe, aiming to reduce inappropriate prescribing of these drugs for the treatment of behavioural and psychological symptoms of dementia.8112223242526Other adverse outcomes have also been investigated in observational studies,272829although, with the exception of pneumonia,14303132the evidence is less conclusive or is more limited among people with dementia. For example, inconsistent or limited evidence has been found for risks of myocardial infarction,3334ventricular arrhythmia,3536venous thromboembolism,37383940fracture,414243and acute kidney injury.444546Most studies also reported only one outcome or type of outcomes. Examining multiple adverse events in a single cohort is needed to give a more comprehensive estimate of the total potential harm associated with use of antipsychotics in people with dementia.
Using linked primary and secondary care data in England, we investigated the risks of a range of adverse outcomes potentially associated with antipsychotic use in a large cohort of adults with dementia—namely, stroke, venous thromboembolism, myocardial infarction, heart failure, ventricular arrhythmia, fracture, pneumonia, and acute kidney injury. We report both relative and absolute risks.
Methods
Data sources
The study used anonymised electronic health records from Clinical Practice Research Datalink (CPRD). In the UK, residents are required to be registered with a primary care general practice to receive care from the NHS. The NHS is a publicly funded healthcare service, free at the point of use. More than 98% of the UK population are registered with a general practice, and their electronic health records are transferred when they change practice.4748Community prescribing is most often done by the general practitioner, including antipsychotic treatment recommended by specialists. CPRD data are sourced from more than 2000 general practices covering around 20% of the UK population, and include information on diagnoses, primary healthcare contacts, prescribed drugs, laboratory test results, and referrals to secondary healthcare services.4748CPRD contains two databases: Aurum and GOLD. CPRD Aurum includes data from contributing general practices in England that use the EMIS Web patient management software, and CPRD GOLD consists of patient data from practices across all four UK nations that use the Vision system. Both datasets are broadly representative of the UK population.474849Primary care data from general practices in England can be linked to other datasets, including hospital admissions in Hospital Episode Statistics, and mortality and index of multiple deprivation data from the Office for National Statistics (ONS). Individual patients can opt-out of sharing their records with CPRD, and individual patient consent was not required as all data were deidentified.
Study population
We delineated two cohorts, one each from Aurum and GOLD. For the latter, we included patients from English practices only because linkage to hospital admission and mortality data were required in our analyses. To ensure that the study dataset would not contain any duplicate patient records, we used the bridging file provided by CPRD to identify English practices that have migrated from the GOLD to the Aurum dataset, and removed such practices from the GOLD dataset. For both cohorts, we included patients who had a first dementia diagnosis code between 1 January 1998 and 31 May 2018. Dementia was identified from Read, SNOMED, or EMIS codes used in the databases (see supplementary appendix). We defined the date of first dementia diagnosis as the date of first dementia code. Patients needed to be aged 50 years or over at the time of dementia diagnosis, have been registered with the CPRD practice for at least a year, not be prescribed an antipsychotic in the 365 days before their first dementia code, and have records that were eligible for linkage to Hospital Episodes Statistics, mortality, and index of multiple deprivation data. In addition, because anticholinesterases (such as donepezil, rivastigmine, and galantamine) may sometimes be prescribed to patients showing signs of dementia before their first dementia code, we excluded patients with an anticholinesterase prescription before their first dementia code. Supplementary figures S1 and S2 show how the two cohorts for Aurum and GOLD, respectively, were delineated.
Study design
Matched cohort design—We implemented a matched cohort design. Supplementary figure S3 shows the study design graphically.50For the Aurum and GOLD cohorts separately, patients who used antipsychotics were defined as patients in each cohort issued with an antipsychotic prescription after (or on the same day as) the date of their first dementia diagnosis, with the date of first antipsychotic prescription being the index date after which outcomes were measured. For each outcome, follow-up began from the date of the first antipsychotic prescription (the index date) and ended on the earliest of date of first diagnosis of outcome (ie, the earliest recording of the outcome whether it was from the patient’s primary or secondary care or mortality records), death, transfer out of the general practice, last data collection date of the general practice, two years from the date of antipsychotics initiation, or 31 May 2018. Because patients who have experienced an outcome were potentially at higher risk of subsequently experiencing the same event, which could confound any risks associated with antipsychotic use, we excluded those with a history of the specific outcome under investigation before the index date from the analysis of that outcome. For example, we excluded patients with a record of stroke before the index date from the analysis of stroke, but they would still be eligible for the study of other outcomes. For the analysis of acute kidney injury, patients with a diagnosis of end stage kidney disease before the index date were also excluded, and a diagnosis of end stage kidney disease after the index date was an additional condition for end of follow-up.44
Matched comparators—Each patient who used antipsychotics on or after the date of their first dementia diagnosis was matched using incidence density sampling with up to 15 randomly selected patients who had the same date of first dementia diagnosis (or up to 56 days after) and who had not been prescribed an antipsychotic before diagnosis. Incidence density sampling involves matching on sampling time, with each antipsychotic user in our study being matched to one or more comparators who were eligible for an antipsychotic but had not become a user at the time of matching.51The selection of comparators was done with replacement—that is, an individual could be used as a comparator in multiple matched sets. In our study, this meant that patients were eligible to be a non-user matched comparator up to the date of their first antipsychotic prescription. We excluded matched comparators with a history of the specific outcome under investigation before the index date from the analysis of that event. For each outcome, follow-up of matched comparators began on the same day as the patient to whom they were matched (the index date) and ended on the earliest of date of their first antipsychotic prescription (if any), or date of one of the end of follow-up events described earlier for the antipsychotic users.
Use of antipsychotics
We included both typical and atypical antipsychotics, identified by product codes in Aurum and GOLD (see supplementary appendix for list of drugs included). Senior author DMA (pharmacist) reviewed the code lists. As previous studies have shown a temporal association between antipsychotic use and development of adverse outcomes,303152we treated use of antipsychotics as a time varying variable, classified as current, recent, and past use. Current use was defined as the first 90 days from the date of an antipsychotic prescription, recent use as up to 180 days after current use ended, and past use as the time after the recent use period had ended. If a patient was issued another prescription during the 90 days after their last prescription, their current use period would be extended by 90 days from the date of their latest prescription. For example, if a patient had two prescriptions and the second was issued 60 days after the first, their current use period would be a total of 150 days: 60 days after the first prescription plus 90 days after the second. At the end of the 150 days current use period, the next 180 days would be the recent use period, and the time after this recent use period would be past use. As patients could have multiple prescriptions over time, they could move between the three antipsychotic use categories during follow-up, and they could therefore be defined as current, recent, or past users more than once. See the supplementary appendix for further information on how this definition is applied.
In post hoc analyses, we also investigated typical versus atypical antipsychotics, and specific drug substances: haloperidol, risperidone, quetiapine, and other antipsychotics (as a combined category).
Outcomes
Outcomes were stroke, venous thromboembolism (including deep vein thrombosis and pulmonary embolism), myocardial infarction, heart failure, ventricular arrhythmia, fracture, pneumonia, and acute kidney injury. With the exceptions of pneumonia and acute kidney injury, outcomes were identified by Read, SNOMED, or EMIS codes in the primary care records, and by ICD-10 (international classification of diseases, 10th revision) codes from linked secondary care data from Hospital Episodes Statistics, and cause of death data from the ONS mortality records. For pneumonia and acute kidney injury, we only included those that were diagnosed in hospitals or as a cause of death, ascertained from Hospital Episodes Statistics and ONS data.
We also investigated appendicitis and cholecystitis combined as an unrelated (negative control) outcome to detect potential unmeasured confounding.53These outcomes were chosen because evidence of an association with antipsychotic use is lacking from the literature. We identified appendicitis and cholecystitis from Read, SNOMED, EMIS, and ICD-10 codes. Clinicians (BG, AJA, DRM) checked all code lists (see supplementary appendix).
Covariates
We used propensity score methods to control for imbalances in measurable patient characteristics between antipsychotic users and their matched non-users, with personal characteristics, lifestyle, comorbidities, and prescribed drugs included in the propensity score models. A counterfactual framework for causal inference was applied to estimate the average treatment effect adjusting for inverse probability of treatment weights generated from the propensity score models.5455Selection of covariates was informed by the literature, based on their potential associations with antipsychotic initiation and study outcomes.3134445657All variables were assessed before the index date (see supplementary figure S3). Variables for personal characteristics included sex, age at dementia diagnosis, age at start of follow-up, ethnicity, and index of multiple deprivation fifths based on the location of the general practice. Comorbidities were derived as dichotomous variables and included a history of hypertension, types 1 and 2 diabetes mellitus, chronic obstructive pulmonary disease, rheumatoid arthritis, moderate or severe renal disease, moderate or severe liver disease, atrial fibrillation, cancer, and serious mental illness (bipolar disorders, schizophrenia, schizoaffective disorders, and other psychotic disorders). Lifestyle factors included smoking status and alcohol use. Medication covariates were represented as dichotomous indicators, defined by at least two prescriptions for each of the following drugs in the 12 months before the index date: antiplatelets, oral anticoagulants, angiotensin converting enzyme inhibitors or angiotensin II receptor blockers, alpha blockers, beta blockers, calcium channel blockers, diuretics, lipid lowering drugs, insulin and antidiabetic drugs, non-steroidal anti-inflammatory drugs, antidepressants, benzodiazepines, and lithium. We also included the following potential confounders for the investigations of venous thromboembolism and fracture: prescriptions for hormone replacement therapy and selective oestrogen receptor modulators (for venous thromboembolism),5859a history of inflammatory bowel disease (for pneumonia and fracture),6061and prescriptions for immunosuppressants, oral corticosteroids, and inhaled corticosteroids (for pneumonia).6263
Statistical analysis
For each patient included in the study, we derived a propensity score representing the patient’s probability of receiving antipsychotic treatment. Propensity scores were estimated using multivariable logistic regression, with antipsychotic use as the dependent variable. Predictors included personal characteristics, lifestyle, comorbidities, and prescribed drugs. Patients with missing information on ethnicity, index of multiple deprivation, smoking, or alcohol use were grouped into an unknown category for each of these variables and included in the propensity score models. We used the Hosmer-Lemeshow test and likelihood ratio test to test the fit of the models, and interaction terms were included to improve the model fit.64The derived scores were used as inverse probability of treatment weights to reweigh the data, balancing the distribution of baseline covariates between antipsychotic users and non-users (matched comparators)—that is, standardised differences <0.1 after weighting.65Propensity score models were run for each outcome, and for the Aurum and GOLD cohorts separately. For further information, see the supplementary appendix section on propensity score methods to control for potential confounding.
Analyses for estimating harms were then conducted after combining (appending) the Aurum and GOLD datasets. We used Cox regression survival analyses to estimate the risks of each outcome associated with antipsychotic use relative to the comparator cohort, and we report the results as hazard ratios. Use of an antipsychotic was treated as a time varying variable. To account for the matched design, we fitted stratified models according to the matched sets and used robust variance estimation. In all models, we also included a covariate indicating whether the patient was from the Aurum or GOLD cohort and calculated hazard ratios with adjustments for inverse probability of treatment weights. Cox regression assumes proportional hazards—that is, the relative hazard of the outcome remains constant during the follow-up period.66We assessed this assumption using the Grambsch-Therneau test based on the Schoenfeld residuals.67Because this assumption did not hold for all outcomes examined, in addition to reporting the hazard ratios pertaining to the whole follow-up period, we estimated hazard ratios separately for the several time windows: the first seven days, 8-30 days, 31-180 days, 181-365 days, and 366 days to two years (see supplementary appendix for an illustration of stratification of follow-up time). For each outcome, we calculated the incidence rate and the number needed to harm (NNH) over the first 180 days as well as two years after start of follow-up. The NNH represents the number of patients needed to be treated with an antipsychotic for one additional patient to experience the outcome compared with no treatment. We also calculated cumulative incidence percentages (absolute risks) for each outcome accounting for competing mortality risks based on previous recommendations.68These were calculated at 90 days, 180 days, 365 days, and two years after start of follow-up for antipsychotic users and their matched comparators separately. We also reported the difference in cumulative incidence between antipsychotic users and their matched comparators at these time points. Analyses were conducted using Stata/MP v16.1.
We investigated two other definitions of antipsychotic use as sensitivity analyses: the first 60 days as current use followed by 120 days of recent use, and a current use period of 30 days followed by a recent use period of 60 days. We also conducted the following post hoc sensitivity analyses. Firstly, as levomepromazine is often prescribed in palliative care to treat distressing symptoms in the last days of life,69we censored individuals at the time of their first levomepromazine prescription. Secondly, we used Fine-Gray subdistribution hazard regression models to estimate the hazard of each adverse outcome, accounting for the competing risks of death.70These results were reported as subhazard ratios. Thirdly, we compared the incidence rates and hazards of adverse outcomes for male versus female individuals. For these sex specific analyses, we modified the existing matched cohort by excluding non-user comparators who were of a different sex from the antipsychotic user to whom they were matched. We then derived a new propensity score for each individual by excluding sex as a covariate in the propensity score models. Incidence rate ratios and corresponding 95% confidence intervals (CIs) for male versus female individuals were calculated using the ‘iri’ command in Stata. To investigate whether hazards of each adverse outcome associated with antipsychotic use differed by sex, we fitted Cox regression models with sex, antipsychotic use, and their interaction as covariates. Sex specific hazard ratios and ratios of male to female hazard ratios were reported.
Patient and public involvement
This study is part of a National Institute of Health and Care Research funded programme (RP-PG-1214-20012): Avoiding patient harm through the application of prescribing safety indicators in English general practices (PRoTeCT). Two patient and public involvement members in the project team contributed to the study design and protocol of this study. Our study was not, however, coproduced with people with dementia or their carers.
Results
Characteristics of study population
A total of 173 910 adults (63.0% women) with dementia were eligible for inclusion in the study: 139 772 (62.9% women) in the Aurum dataset and 34 138 (63.4% women) in GOLD. The mean age at dementia diagnosis for individuals in both cohorts was 82.1 years (standard deviation (SD) 7.9 years), and the median age was 83 years (interquartile range (IQR) 78-88 years in Aurum and 78-87 years in GOLD). A total of 35 339 individuals (62.5% women; 28 187 in Aurum, 62.6% women; 7152 in GOLD, 62.5% women) were prescribed an antipsychotic during the study period, and a matched set was generated for each of these individuals. The mean number of days between first dementia diagnosis and date of a first antipsychotic prescription was 693.8 ((SD 771.1), median 443 days) in Aurum and 576.6 ((SD 670.0), median 342 days) in GOLD. A total of 544 203 antipsychotic prescriptions (433 694 in Aurum, 110 509 in GOLD) were issued, of which 25.3% were for a typical antipsychotic and 74.7% for an atypical antipsychotic. The most prescribed antipsychotics were risperidone (29.8% of all prescriptions), quetiapine (28.7%), haloperidol (10.5%), and olanzapine (8.8%), which together accounted for almost 80% of all prescriptions (see supplementary table S1).
Since we excluded people with a history of the event before the start of follow-up, the number of individuals and matched sets included in analysis varies by outcome.Table 1shows the baseline characteristics of patients for the analysis of stroke, before and after inverse probability of treatment weighting. Antipsychotic users were more likely than their matched comparators to have a history of serious mental illness and to be prescribed antidepressants or benzodiazepines in the 12 months before start of follow-up. After inverse probability of treatment weighting, standardised differences were <0.1 for all covariates. Baseline characteristics of individuals included in the analyses of other outcomes were similar to those reported for stroke (see supplementary tables S2-S9).
Incidence rates and relative hazards of adverse outcomes
In the two years after initiation of antipsychotics, the highest incidence rates of adverse outcomes were for pneumonia, fracture, and stroke, and ventricular arrhythmias were rare (table 2).Figure 1shows the hazard ratios of adverse outcomes associated with current, recent, past, and any use of antipsychotics versus non-use (ie, matched comparators). Except for ventricular arrhythmia, any use of antipsychotics was associated with increased risks for all adverse outcomes, ranging from a hazard ratio of 2.03 (95% CI 1.96 to 2.10) for pneumonia to 1.16 (1.09 to 1.24) for heart failure. Current use (ie, prescribed in the previous 90 days) was associated with high risks for pneumonia (2.19, 2.10 to 2.28), acute kidney injury (1.72, 1.61 to 1.84), venous thromboembolism (1.62, 1.46 to 1.80), and stroke (1.61, 1.52 to 1.71). Recent antipsychotic use (ie, in the 180 days after current use ended) was also associated with increased risk for these outcomes, as well as for fracture, but past use of antipsychotics (ie, after recent use ended) was not associated with increased risks of the adverse outcomes examined, except for pneumonia. For the negative control outcome (appendicitis and cholecystitis), no significant associations were found with current, recent, or any antipsychotic use, but a statistically significant association was observed with past use (1.90, 1.01 to 3.56).
Table 2shows that the NNH ranged from 9 (95% CI 9 to 10) for pneumonia to 167 (116 to 301) for myocardial infarction during the first 180 days after initiation of antipsychotics, and from 15 (14 to 16) for pneumonia to 254 (183 to 413) for myocardial infarction after two years. These figures suggest that over the 180 days after drug initiation, use of antipsychotics might be associated with one additional case of pneumonia for every nine patients treated, and one additional case of myocardial infarction for every 167 patients treated. At two years, there might be one additional case of pneumonia for every 15 patients treated, and one additional case of myocardial infarction for every 254 patients treated.
Table 3shows hazard ratios stratified by follow-up time (except for ventricular arrhythmia and the negative control where the number of patients was very low). For almost all outcomes, relative hazards were highest in the first seven days after initiation of antipsychotic treatment. Risks for pneumonia were particularly increased in the first seven days (9.99, 8.78 to 11.40) and remained substantial afterwards (3.39, 3.04 to 3.77, 8-30 days). No increased risks for heart failure were found for current users after 180 days from start of treatment, nor for myocardial infarction one year after drug initiation. However, risks for stroke, venous thromboembolism, fracture, pneumonia, and acute kidney injury remained increased among continuous antipsychotic users up to two years after initiation of treatment.
During the current use period of 90 days after a prescription, both typical and atypical antipsychotics were associated with increased risks of all adverse outcomes compared with non-use, except for ventricular arrhythmia and the negative control (see supplementary table S10). Hazards were higher when current use of typical antipsychotics was directly compared with atypical antipsychotics for stroke (1.23, 1.09 to 1.40), heart failure (1.18, 1.01 to 1.39), fracture (1.22, 1.08 to 1.38), pneumonia (1.92, 1.77 to 2.08), and acute kidney injury (1.22, 1.05 to 1.42), but no significant differences between the two types of drug were found for the risks of venous thromboembolism or myocardial infarction.
Supplementary table S11 shows the risks of adverse outcomes associated with haloperidol (the most prescribed typical antipsychotic) and with risperidone and quetiapine (the two most prescribed atypical antipsychotics). Current use of risperidone and haloperidol compared with non-use was associated with increased risks of all adverse outcomes except for ventricular arrhythmia and the negative control. Current use of quetiapine compared with non-use was associated with increased risks for fracture, pneumonia, and acute kidney injury. Among current users of haloperidol or risperidone, risks for fracture, pneumonia, and acute kidney injury were higher for haloperidol versus risperidone, but risks for stroke, venous thromboembolism, myocardial infarction, and heart failure were similar for both drugs. With the exceptions of myocardial infarction, ventricular arrhythmia, and the negative control, risks of all adverse outcomes were higher for haloperidol than for quetiapine, especially for pneumonia (2.53, 2.21 to 2.89) and venous thromboembolism (1.99, 1.33 to 2.97). Among current users of quetiapine compared with risperidone, there were no significant differences in risks for myocardial infarction, heart failure, or fracture. However, risks for stroke (0.64, 0.53 to 0.78), venous thromboembolism (0.49, 0.36 to 0.68), pneumonia (0.72, 0.63 to 0.81), and acute kidney injury (0.81, 0.67 to 0.96) were lower for quetiapine than for risperidone.
Absolute risks of adverse outcomes
Cumulative incidence for all outcomes examined was higher for antipsychotic users versus matched comparators, except for ventricular arrhythmia and the negative control (table 4). The absolute risk, as well as risk difference, was particularly large for pneumonia. In the 90 days after initiation of an antipsychotic, the cumulative incidence of pneumonia among antipsychotic users was 4.48% (95% CI 4.26% to 4.71%)v1.49% (1.45% to 1.53%) in the matched cohort of non-users (difference 2.99%, 95% CI 2.77% to 3.22%). At one year, this increased to 10.41% (10.05% to 10.78%) for antipsychotic users compared with 5.63% (5.55% to 5.70%) for non-users (difference 4.78%, 4.41% to 5.16%).
Sensitivity analyses
Similar results were found in sensitivity analysis using two other definitions of antipsychotic use (see supplementary figures S4 and S5). Of the 544 203 antipsychotic prescriptions issued, 1.3% were for levomepromazine (see supplementary table S1). Results remained similar when patients were censored at the time of their first levomepromazine prescription (see supplementary figure S6). Results of the Fine-Gray models accounting for the competing risks of death also showed broadly similar patterns of hazards to those from the Cox models (see supplementary table S12 and figure S7). Sex specific analyses showed that male patients had higher incidence rates of all adverse outcomes than female patients, except for fracture and venous thromboembolism where incidence was higher for female patients than for male patients (see supplementary table S13). Compared with female antipsychotic users, male users had increased hazards for pneumonia and acute kidney injury (male to female hazard ratio 1.16, 95% CI 1.08 to 1.25 for pneumonia and 1.22, 1.08 to 1.37 for acute kidney injury), but lower hazards for stroke (0.81, 0.73 to 0.91). No significant differences were found by sex in the hazards for venous thromboembolism, myocardial infarction, heart failure, ventricular arrhythmia, or fracture (see supplementary table S14).
Discussion
In this population based cohort study of adults (≥50 years) with dementia, use of antipsychotics compared with non-use was associated with increased risks for stroke, venous thromboembolism, myocardial infarction, heart failure, fracture, pneumonia, and acute kidney injury. Increased risks were observed among current and recent users and were highest in the first week after initiation of treatment. In the 90 days after a prescription, relative hazards were highest for pneumonia, acute kidney injury, stroke, and venous thromboembolism, with increased risks ranging from 1.5-fold (for venous thromboembolism) to twofold (for pneumonia) compared with non-use. No increased risk was found for ventricular arrhythmia or the negative control outcome (appendicitis and cholecystitis). Absolute risk differences between antipsychotic users and their matched comparators were substantial for most adverse events, and largest for pneumonia. In the 90 days after a prescription, risks of stroke, heart failure, fracture, pneumonia, and acute kidney injury were higher for typical antipsychotics versus atypical antipsychotics, whereas no significant differences between these two drug classes were found for risks of venous thromboembolism or myocardial infarction. Haloperidol was associated with higher risks for fracture, pneumonia, and acute kidney injury than risperidone, but no significant differences between the two drugs were found for the other outcomes. Risks of almost all adverse outcomes were higher for haloperidol than for quetiapine. No significant differences were found between risperidone and quetiapine for risks of myocardial infarction, heart failure, or fracture, but risks for stroke, venous thromboembolism, pneumonia, and acute kidney injury were lower for quetiapine versus risperidone.
Comparison with other studies
A population based study in Wales reported no increased risks for non-fatal acute cardiac events associated with antipsychotic use in patients with all cause dementia, although those with Alzheimer’s disease showed increased risks.37Systematic reviews and meta-analyses of studies not limited to patients with dementia have also reported inconsistent evidence for myocardial infarction, or lack of robustness of these data.333471Our findings for myocardial infarction were similar to those in a study that first documented a modest and time limited increase in risk of this outcome associated with antipsychotic use among patients with dementia.56In a study of nursing home residents in the US, users of typical, but not atypica","Objective: To investigate risks of multiple adverse outcomes associated with use of antipsychotics in people with dementia.
Design: Population based matched cohort study.
Setting: Linked primary care, hospital and mortality data from Clinical Practice Research Datalink (CPRD), England.
Population: Adults (≥50 years) with a diagnosis of dementia between 1 January 1998 and 31 May 2018 (n=173 910, 63.0% women). Each new antipsychotic user (n=35 339, 62.5% women) was matched with up to 15 non-users using incidence density sampling.
Main outcome measures: The main outcomes were stroke, venous thromboembolism, myocardial infarction, heart failure, ventricular arrhythmia, fracture, pneumonia, and acute kidney injury, stratified by periods of antipsychotic use, with absolute risks calculated using cumulative incidence in antipsychotic users versus matched comparators. An unrelated (negative control) outcome of appendicitis and cholecystitis combined was also investigated to detect potential unmeasured confounding.
Results: Compared with non-use, any antipsychotic use was associated with increased risks of all outcomes, except ventricular arrhythmia. Current use (90 days after a prescription) was associated with elevated risks of pneumonia (hazard ratio 2.19, 95% confidence interval (CI) 2.10 to 2.28), acute kidney injury (1.72, 1.61 to 1.84), venous thromboembolism (1.62, 1.46 to 1.80), stroke (1.61, 1.52 to 1.71), fracture (1.43, 1.35 to 1.52), myocardial infarction (1.28, 1.15 to 1.42), and heart failure (1.27, 1.18 to 1.37). No increased risks were observed for the negative control outcome (appendicitis and cholecystitis). In the 90 days after drug initiation, the cumulative incidence of pneumonia among antipsychotic users was 4.48% (4.26% to 4.71%) versus 1.49% (1.45% to 1.53%) in the matched cohort of non-users (difference 2.99%, 95% CI 2.77% to 3.22%).
Conclusions: Antipsychotic use compared with non-use in adults with dementia was associated with increased risks of stroke, venous thromboembolism, myocardial infarction, heart failure, fracture, pneumonia, and acute kidney injury, but not ventricular arrhythmia. The range of adverse outcomes was wider than previously highlighted in regulatory alerts, with the highest risks soon after initiation of treatment.
"
Predicting the risks of kidney failure and death in adults with moderate to severe chronic kidney disease,"Introduction
Chronic kidney disease (CKD), defined as the presence of abnormal concentrations of albuminuria or estimated glomerular filtration rate (eGFR) that is below 60 mL/min/1.73 m2for more than 90 days,1affects 6-10% of the general population worldwide.23Kidney failure is the most feared outcome of CKD; CKD disproportionally affects older individuals and most people with CKD are more likely to die than reach kidney failure. The five year risk of kidney failure is less than 1% in adults with mild CKD (stage G3a, eGFR 45-59 mL/min/1.73 m2), which is the largest fraction of the CKD population.45People with moderate (stage G3b, eGFR 30-44 mL/min/1.73 m2) or severe disease (stage G4, eGFR 15-29 mL/min/1.73 m2) have a higher risk of kidney failure and also a higher risk of death than people with mild CKD.45Accurate assessment of both of these risks is key to inform treatment decisions in this patient population.
Although CKD guidelines advocate for shared decision making centred around the patient,16existing tools focus on assessing the risk of kidney failure to enable timely preparation for its management. Nephrology referral is recommended when the predicted five year risk of kidney failure is more than 5%.7Referral to enhanced multidisciplinary care is advised when the predicted two year risk of kidney failure exceeds 10%.8Adoption of this strategy has shown potential to transform how kidney care is organised, and the patient and provider experience.91011However, the most widely used prediction tool for individuals with CKD only provides predictions for the risk of kidney failure in isolation, which is half of the story.1213Failure to simultaneously assess the risk of both kidney failure and death may lead to unintended consequences for people with CKD. If mortality is not considered, the prognostic information discussed in shared decision making may result in unnecessary referral and futile treatments, missed treatment opportunities, a failure to consider prevention or preparation for non-kidney health outcomes, or choices that do not reflect personal preferences, goals, and values.14Besides lacking information on mortality, the current benchmark tool, kidney failure risk equation, in its original or recalibrated version,1213does not account for competing risks and hence may provide biased risk predictions for kidney failure.
This study had two aims. Firstly, to build a tool that provides risk predictions for both kidney failure, accounting for the competing risk of death, and all cause death at the one to five year prediction horizons in adults with newly documented moderate to severe CKD (stages G3b to G4). This tool would support more holistic decision making in this patient population (KDpredict,http://kdpredict.com). Secondly, in addition to traditional model testing in different countries (often called external validation), this study proposes a strategy for prediction modelling (super learner), designed to adapt flexibly to local settings and enable locally optimised decision support, rather than a so-called one-size-fits-all model.
Methods
Study design and data sources
Population based health data were linked to form three cohorts, in Alberta (Canada), Denmark, and Scotland (UK). Supplementary appendices 1-2 provide details of data sources,15161718site specific methods for calendar dates and variable definitions, statistical analysis and sample size considerations, and ethics approval. The study followed recommended reporting standards (supplementary appendices 3-5).1920
Target and study populations
The analysis plan is summarised in table S1. To mirror the population for whom predictions will be made (incident stage G3b to G4 CKD diagnosed in an outpatient setting), only outpatient eGFR measurements were considered to identify adults (≥18 years of age) with newly documented G3b to G4 CKD based on routinely collected laboratory data (table S2).14521The earliest individual series of at least two consecutive eGFR values of less than 45 mL/min/1.73 m2sustained for more than 90 days defined stage G3b to G4 CKD. The date of the last eGFR (15-44 mL/min/1.73 m2) in that series was the index date (cohort entry; time origin for prediction). We excluded people who had previously received maintenance dialysis or a kidney transplant, or had had a sustained eGFR of less than 15 mL/min/1.73 m2for more than 90 days (stage G5 CKD),1on or before cohort entry.
Outcomes and follow-up
The outcomes were kidney failure and all cause death. Kidney failure was defined as maintenance kidney replacement treatment or eGFR of 10 mL/min/1.73 m2sustained for more than 90 days (tables S2-S3), whichever was earlier. Participants were followed up from cohort entry until either death or censoring (emigration or study end). The target parameters were the individual risks of kidney failure and death at one to five years.
Baseline characteristics
At cohort entry, we considered age, sex, eGFR, albuminuria, and history of diabetes and cardiovascular disease (any of congestive heart failure, myocardial infarction, peripheral vascular disease, or stroke or transient ischaemic attack) for main analyses. These variables are known to be associated with clinical outcomes,1are readily available in clinic, and are the inputs of the benchmark model of kidney failure. We considered chronic pulmonary disease and cancer for descriptive purposes (table S4).22The most recent outpatient albuminuria value in the three years before cohort entry was used, with the following types of measurement in descending order of preference: urine albumin-to-creatinine ratio, protein-to-creatinine ratio, or dipstick. Albumin-to-creatinine ratio was calculated from protein-to-creatinine ratio or urine dipstick in people with no albumin-to-creatinine ratio measurement.23
Statistical analysis
Different strategies are available for learning medical risk prediction from data (ie, prediction models or learners), and which of them will be the most suitable for a given prediction task is not possible to anticipate.24For example, many different ways can specify a regression model to handle interactions or non-linear effects or to tune a machine learning algorithm to configure the learning process.25The super learner is a meta-algorithm that alleviates these concerns about model selection by providing the freedom to consider many alternative learners that have been recommended by collaborators or subject matter experts. The super learner uses cross-validation for ranking a prespecified set of learners (library), and either combines them in an ensemble (ensemble super learner) or selects the learner with the lowest cross-validated prediction error (discrete super learner).26
The super learner was blindly designed by two authors (PR, TAG) using synthetic data created by another author (PL) from the older Alberta data (cohort entry between 1 April 2008 and 31 March 2011). The synthetic sample had the same probability distribution of the combinations of the predictor variables as the original data, but time to event altered with random numbers (outcome blinded, feature analysis; table S1).
For each outcome, we planned to create a prediction tool that required four or six predictors (age, sex, eGFR, and albumin-to-creatinine ratio without or with diabetes and cardiovascular disease), with the option to use eGFR calculated with the 2009 formula21or the 2021 race-free, creatinine based formula.27Therefore, four libraries were created for the absolute risk of kidney failure (including cause specific Cox models28and random survival forest for competing risks29) and four libraries for time-to-death analysis (standard Cox models and random survival forests30; table S5).
For regression models, different variable transformations were considered, including restricted cubic splines of none to three continuous variables (ie, age, eGFR, log albumin-to-creatinine ratio), and first order interactions between predictors based on clinical judgement and existing studies.1For random forest tuning, we considered a grid of values for the hyperparameters (table S5).31
Contemporary data from Alberta (cohort entry between 1 April 2011 and 31 March 2019; learning cohort) were used to identify the strongest learners by fitting a discrete super learner with each learner library (table S1). The super learner used internal cross-validation based on 500 bootstrap sets each obtained by random subsampling 63.2% of the training cohort for learning and 36.8% to calculate the prediction performance. The leave-one-out bootstrap was used for averaging the performance results across multiple splits.31For each library, the super learner identified the learner with the lowest cross-validated Brier score each separately obtained at years one, two, three, four, and five, and then selected the outcome specific learner with lowest mean of the five Brier scores.31
For each kidney failure risk threshold currently used to inform treatment decisions (10% at two years for referral to multidisciplinary clinic and 5% at five years for nephrology referral), we summarised the proportion of people with predicted mortality risk above increasingly higher mortality thresholds (20%, 30%, or 40%).
To investigate to what extent KDpredict trained in Alberta, Canada, could be used as is in different regions, KDpredict was compared with the current benchmark model (kidney failure risk equation, which was developed in Canada)12for two and five year kidney failure risk predictions (the only time periods that the kidney failure risk equation considers) in Denmark and Scotland. We present the comparison of KDpredict (without retraining or recalibration) to the recalibrated version of kidney failure risk equation in supplementary appendix 1.13Since prediction time horizons of interest depend on disease severity, one to two year kidney failure risk predictions were evaluated only in people with stage G4 CKD in main analyses, and in the full cohort in secondary analyses. Different formulations of KDpredict (four or six variables) were also evaluated for one to five year risk predictions of kidney failure and death.
Risk scatterplots were used to assess potential disagreement between individualised predictions from rival models, with a prespecified meaningful difference of more than 10%.31Calibration was evaluated using histogram type plots with groups defined by tenths of predicted risk. Numerical performance measures included time dependent Brier score (prediction error, a measure of both calibration and discrimination; the lower the better), index of prediction accuracy (calculated from the Brier score and representing the improvement in the Brier score compared with the null model; the higher the better), and inverse probability of censoring weighted estimates of the area under the receiver operating characteristic curve (a measure of discrimination or ranking statistic; the higher the better). This area is blind to monotone transformations of risk (eg, adding 10% to each individual risk does not change the area under the receiver operating characteristic curve), and hence cannot tell if a model is miscalibrated. The Brier score is a strictly proper scoring rule and a stand-alone measure for ranking rival models.31
To illustrate how an updated version of KDpredict can be assessed over time, we retrained the KDpredict models using Alberta data with cohort entry date between 1 April 1 2008 and 31 December 2014, and tested their performance on the temporally distinct, more recent data (cohort entry date between 1 January 2015 and 31 March 2019; study end date 31 March 2020). We also present the cross-validated performance of the retrained models on the training set. By splitting the data into independent training and testing sets, cross-validation tests an average model and simulates how well the model will perform when challenged with unseen, future data.31
The clinical value of KDpredict was illustrated by graphical summaries of predicted risks for hypothetical individuals with characteristics associated with combinations of high or low risk for kidney failure and high or low risk for death. KDpredict is available athttp://kdpredict.com.
Scatterplots were used to assess possible differences in predictions from KDpredict trained with the 200921versus 2021 eGFR formula.27In decision curve analysis, the net benefit of different decision strategies was plotted over prespecified threshold probabilities.32We used the currently recommended referral thresholds for kidney failure (5% at five years for nephrology referral and 10% at two years for multidisciplinary care) and prespecified mortality risk thresholds of 20% at two years and 40% at five years, as none exist. While the absolute value of net benefit is an abstract concept to interpret, the decision strategy with the highest net benefit among those compared is regarded as the most clinically useful at any given threshold (supplementary appendix 1).
Patient and public involvement
A group of patient partners was engaged during the design phase to provide feedback on prediction time horizons of interest, presentation of both risk predictions simultaneously, and how to visualise them (KDpredict app and figures of this report). A qualitative study is underway on how patients, care givers, and providers understand risk.
Results
Study cohorts and follow-up data
This study included 67 942 residents of Alberta (16 446 contributed to the creation of synthetic data for library design and 51 496 to supervised learning, table S1), and 17 528 and 7740 from Denmark and Scotland, respectively. The cohorts had similar median age and baseline eGFR (table 1, table S6); the Danish cohort included more men. The Alberta cohort included more people who had cardiovascular disease, chronic pulmonary disease, or cancer. In each cohort, most people had G3b CKD (85-90%) and, within each CKD stage, most had normal or mildly increased albuminuria (fig 1, S1-3, top panels). Only 16.3% of people were younger than 65 years. Median follow-up times were five to six years in all cohorts. Kidney failure rate was 0.8-1.1 per 100 person years and death rate was 10-12 per 100 person years, with higher mortality in the Scottish cohort (table S7). Initiation of maintenance kidney replacement treatment accounted for most kidney failure events (86-93%).
Super learner
KDpredict included four cause-specific Cox models for kidney failure and four standard Cox models for mortality (table S8). The predicted five year risk of death far exceeded that of kidney failure, except in less than 5% of people who had both G4 CKD and severely increased albuminuria, and was high also in people younger than 65 years (fig 1and S1, bottom panels). In people 65 years of age or older who had normal albuminuria and stage G3b CKD, the five year risk of kidney failure was less than 1% (fig 1, bottom panels). Individuals with kidney failure risk predictions above currently adopted decision thresholds (10% at two years and 5% at five years) were more likely to have abnormal albuminuria (A2 or A3) or stage G4 CKD (fig 2and S4). Individuals could receive a high predicted risk of death despite their low risk of kidney failure and vice versa (fig 2, S4 and S5). For example, among those with a high risk of kidney failure (>10% at two years or >5% at five years), about one third had a risk of death above 20% at two years or 40% at five years (figure S6). Similarly, among those below current risk thresholds for kidney failure, about 30% exceeded these mortality thresholds (figure S6).
Predicted risk of kidney failure
In geographical testing, KDpredict with four variables gave higher individual risk predictions than the KDpredict with six variables, although for most people risk differences were within 10% (figure S7). The models had similar one to five year prediction performance (figures S8, S9) and were well calibrated in main analysis. When the models were tested in the full cohorts, calibration of one to two year predictions further improved (figure S10). Risk predictions from the kidney failure risk equation differed from those of KDpredict (figure S11) and were systematically higher than the estimated actual risks (fig 3and S12). KDpredict was more accurate than kidney failure risk equation in prediction of kidney failure risk: five year index of prediction accuracy 27.8% (95% confidence interval 25.2% to 30.6%) versus 18.1% (15.7% to 20.4%) in Denmark and 30.5% (27.8% to 33.5%) versus 14.2% (12.0% to 16.5%) in Scotland (fig 3).
Predicted risk of all cause death
The four and six variable formulations of KDpredict gave similar individual mortality risk predictions (figures S13-S15). In both external cohorts, the four variable model was adequately calibrated, with small differences between estimated actual and average predicted risks relative to risk size. Compared with the four variable model, the six variable model had slightly worse visual calibration across all tenths of predicted risk but lower Brier scores, indicating superior accuracy.
Temporal testing
The KDpredict models retrained using older Alberta data provided accurate predictions when tested in temporally distinct, more recent Alberta data (figure S16, kidney failure; figure S17, death).
Clinical use
Figure 4shows two dimensional risk predictions from KDpredict for four hypothetical individuals with different risks combinations. Predictions from kidney failure risk equation and KDpredict differed substantially, potentially leading to diverging treatment decisions. An 80-year-old man with an eGFR of 30 mL/min/1.73 m2and an albumin-to-creatinine ratio of 100 mg/g (11 mg/mmol) would receive a five year kidney failure risk prediction of 10% from kidney failure risk equation (above the current nephrology referral threshold of 5%). The same man would receive five year risk predictions of 2% for kidney failure and 57% for mortality from KDpredict. A 75-year-old woman with an eGFR of 20 mL/min/1.73 m2and an albumin-to-creatinine ratio of 500 mg/g (56 mg/mmol) would receive a two year kidney failure risk prediction of 18% from kidney failure risk equation (above the current referral threshold of 10% for enhanced care and preparation for kidney replacement). The same woman would receive two year risk predictions of 8% for kidney failure and 29% for mortality from KDpredict.
Other analyses
We found minimal differences in both risk predictions from KDpredict when eGFR was estimated using the CKD-EPI 2021 formula instead of the 2009 formula (figures S18-19). At the proposed KDIGO thresholds of 10% at two years and 5% at five years, KDpredict models had higher net benefits than the kidney failure risk equation (fig 5). For the outcome of death, the four and six variable models had similar net benefits (figure S20).
Discussion
Principal findings
We used Alberta health data to create a tool predicting one to five year risks of kidney failure and all cause death (KDpredict) in people with incident moderate to severe CKD (stage G3b to G4). In external testing in Denmark and Scotland, KDpredict consistently outperformed the current benchmark risk prediction model for kidney failure (kidney failure risk equation)1213and was well calibrated for the prediction of both kidney failure and death over one to five year time horizons. Similar results were observed in temporal testing of retrained models in Alberta. KDpredict is unique in its ability to provide accurate predictions of risk for both clinical outcomes in adults with this severity of CKD at the point of first onset, when a timely discussion should occur. By presenting risk predictions of both kidney failure and death, KDpredict supports patient centred care and holistic decision making. We translated KDpredict into a calculator for deployment and dissemination (http://kdpredict.com). The underlying super learner strategy of KDpredict would be suitable for implementation with or without revision in other regions, and regular reassessment over time within the same region.
Comparison with other studies
Superior performance of KDpredict compared with kidney failure risk equation may be due to accounting for death as a competing event.33343536373839By treating death in the same way as loss to follow-up, the kidney failure risk equation intrinsically assumes that people can have kidney failure after death and systematically overestimates the risk of kidney failure.12KDpredict provides risk predictions that are directly interpretable: a patient who receives a predicted two year risk of 11% can expect that 11 of 100 patients like them will develop kidney failure within two years. By contrast, predictions from the kidney failure risk equation do not have this interpretation, even if recalibrated. In a recent analysis of kidney failure risk equation performance, the original kidney failure risk equation was considered generally accurate for eGFR <45 mL/min/1.73 m2, except for long term predictions in older adults where a competing risks model may be preferable.40Our analyses show that mortality is high also in people younger than 65 years and that the systematic overestimation by the kidney failure risk equation is clinically relevant both in short and long term predictions. Superior performance may also be due to the use of a super learner strategy that let the data select the best performing model or algorithm from a large library of prespecified candidate learners, without imposing restrictions.24Notably, we could not compare the mortality prediction performance of KDpredict to similar tools, as none exists.
As compared with existing tools,121341KDpredict was trained in a cohort that closely represents the population for whom shared decisions are of clinical concern. Firstly, we rigorously applied the KDIGO recommended chronicity criterion to population based data to define incident moderate to severe CKD and kidney failure.1Secondly, we used only outpatient laboratory data to minimise the inclusion of people who may not have CKD and identified a common time origin for risk prediction. Thirdly, we excluded people with eGFR of less than 15 mL/min/1.73 m2who already have kidney failure,1and those with eGFR of 45-59 mL/min/1.73 m2, given that they have a very low five year risk of kidney failure and may only have age related decline in kidney function.45Also, we included sustained eGFR of less than 10 mL/min/1.73 m2for more than 90 days in the definition of kidney failure because below this eGFR threshold treatment decisions are usually enacted and sicker or older people may choose conservative or palliative care without dialysis. Finally, we used cross-validation and a strictly proper scoring rule (Brier score) for prediction model selection, evaluation, and comparison.31Measures of reclassification or discrimination are not recommended for prediction model selection or assessment.31
Strengths and limitations of this study
KDpredict can be used in different clinical settings, including general practice and specialist clinics to help patients to decide how to treat kidney failure (eg, dialysis, kidney transplantation, or conservative management), and to determine eligibility for clinical trials. The KDpredict and the algorithm to define stage G3b to G4 CKD could be implemented in electronic medical records. Albuminuria can be calculated from dipstick or protein-to-creatinine ratio if albumin-to-creatinine ratio is unavailable, and both eGFR formulas can be selected for input. Although KDpredict provided accurate predictions in Denmark and Scotland, the algorithm may not maintain the same performance over time in the same regions or have the same performance in other world regions. This applies to any model. Data from which a prediction model learns can change over time and across regions. A major strength of KDpredict is its flexible super learner strategy, which can be redesigned and retrained regularly to optimise prediction performance as population characteristics or health practices change or new potential predictors or treatments become available. This temporal retraining strategy could be compared with recently proposed temporal recalibration approaches.42
Our study has limitations, including the use of data from three countries in the northern hemisphere that have predominantly white populations and use albuminuria measurements limited to albumin-to-creatinine ratio or protein-to-creatinine ratio in the external testing cohorts. Since we used routinely collected eGFR data, some people who died without documented kidney failure could have had kidney failure. We also recognise that when making treatment decisions, many factors that are difficult to incorporate in a prediction tool, including symptom burden, are as important as predicted risks. As is the case with existing prediction models for people with CKD, our prediction tool is a static tool to be used at the point of new onset of disease, in contrast to dynamic prediction tools that can be repeatedly used for the same person over time. In accordance with our protocol, we evaluated calibration using histogram-type calibration plots. These plots were constructed using a prespecified number of risk groups to prevent analyst manipulation. We acknowledge that risk comparison across categories may lead to loss of information. A density-type calibration curve is potentially more informative but also depends on an arbitrary hyper-parameter for smoothing or splines for fitting the curve.38Ideally, such smoothing strategies should be prespecified or decided by an algorithm and could be incorporated into future iterations.31Finally, we intentionally avoided the use of the term validation throughout this reporting because any statistical model falls short of the complexities of reality. Instead, whether the KDpredict is useful, useless, or harmful should be tested in a randomised trial. Until this trial becomes available, we recommend testing KDpredict in diverse populations, and retraining, where possible, to optimise prediction performance across settings with different population characteristics or health data recording practices.
Implications and conclusions
This study details a new method of decision support for CKD by providing both mortality and kidney failure risk predictions. Mortality risk assessment is key to inform treatment decisions in many chronic diseases that tend to progress or cancers that may relapse.43Given the high risk of death in the CKD population, accurate prediction of both risks is necessary to facilitate tailored clinical decision making and preparations beyond those solely related to the management of kidney failure.14Younger adults with lower eGFR and higher albuminuria, who have a higher risk of kidney failure than death, are likely ideal candidates for referral to nephrology clinics. For many people with a higher risk of death than kidney failure, interventions targeting cardiovascular risk may be the priority. Individuals who have a very high risk of death may choose alternative treatments, including advance care planning with or without involvement of a kidney specialist. A wide range of risk combinations exist between these extremes, making treatment decisions challenging for patients, care givers, and health care providers.
To support such complex decisions, the task of a support tool is to provide comprehensive and accurate information that can enable clear communication. How this information is used depends on a holistic discussion between the patient and the provider involved in their care. For example, people with similarly high risks of kidney failure may prioritise different treatment options when they place their kidney failure risk in the context of their predicted mortality risk, the relative size of each risk, and the lens of their personal preferences and values. Our study suggests that both risks should be considered in shared decision making. Additional qualitative work can help to improve use of KDpredict to assist clinical decisions, using the existing thresholds for kidney failure and the mortality thresholds proposed in this study as general guidance.
In summary, by presenting kidney failure and death risk predictions simultaneously, KDpredict supports holistic decision making in people with moderate to severe CKD.
Chronic kidney disease (CKD) affects up to one in 10 adults globally, is associated with high morbidity and mortality, and disproportionally affects older individuals, who are more likely to die than to develop kidney failure
To support shared decision making, guidelines recommend that people with CKD are provided with individualised risk predictions of outcomes important to patients
Existing prediction tools focus on the outcome of kidney failure
KDpredict, trained in Alberta, Canada, outperformed the current benchmark model for kidney failure risk prediction in Denmark and Scotland and provided also accurate risk predictions for mortality
By presenting simultaneous risk predictions of both kidney failure and death, KDpredict supports holistic discussions and patient centred decision making
Given its flexible learning strategy, KDpredict is designed to be adapted to local needs and revised over time to provide an optimally tailored tool for patients
","Objective: To train and test a super learner strategy for risk prediction of kidney failure and mortality in people with incident moderate to severe chronic kidney disease (stage G3b to G4).
Design: Multinational, longitudinal, population based, cohort study.
Settings: Linked population health data from Canada (training and temporal testing), and Denmark and Scotland (geographical testing).
Participants: People with newly recorded chronic kidney disease at stage G3b-G4, estimated glomerular filtration rate (eGFR) 15-44 mL/min/1.73 m2.
Modelling: The super learner algorithm selected the best performing regression models or machine learning algorithms (learners) based on their ability to predict kidney failure and mortality with minimised cross-validated prediction error (Brier score, the lower the better). Prespecified learners included age, sex, eGFR, albuminuria, with or without diabetes, and cardiovascular disease. The index of prediction accuracy, a measure of calibration and discrimination calculated from the Brier score (the higher the better) was used to compare KDpredict with the benchmark, kidney failure risk equation, which does not account for the competing risk of death, and to evaluate the performance of KDpredict mortality models.
Results: 67 942 Canadians, 17 528 Danish, and 7740 Scottish residents with chronic kidney disease at stage G3b to G4 were included (median age 77-80 years; median eGFR 39 mL/min/1.73 m2). Median follow-up times were five to six years in all cohorts. Rates were 0.8-1.1 per 100 person years for kidney failure and 10-12 per 100 person years for death. KDpredict was more accurate than kidney failure risk equation in prediction of kidney failure risk: five year index of prediction accuracy 27.8% (95% confidence interval 25.2% to 30.6%) versus 18.1% (15.7% to 20.4%) in Denmark and 30.5% (27.8% to 33.5%) versus 14.2% (12.0% to 16.5%) in Scotland. Predictions from kidney failure risk equation and KDpredict differed substantially, potentially leading to diverging treatment decisions. An 80-year-old man with an eGFR of 30 mL/min/1.73 m2and an albumin-to-creatinine ratio of 100 mg/g (11 mg/mmol) would receive a five year kidney failure risk prediction of 10% from kidney failure risk equation (above the current nephrology referral threshold of 5%). The same man would receive five year risk predictions of 2% for kidney failure and 57% for mortality from KDpredict. Individual risk predictions from KDpredict with four or six variables were accurate for both outcomes. The KDpredict models retrained using older data provided accurate predictions when tested in temporally distinct, more recent data.
Conclusions: KDpredict could be incorporated into electronic medical records or accessed online to accurately predict the risks of kidney failure and death in people with moderate to severe CKD. The KDpredict learning strategy is designed to be adapted to local needs and regularly revised over time to account for changes in the underlying health system and care processes.
"
"Impact of large scale, multicomponent intervention to reduce proton pump inhibitor overuse","Introduction
Proton pump inhibitors (PPIs) are one of the most commonly prescribed classes of drugs, but an estimated 25-70% of PPI users may not have an appropriate indication, depending on the clinical setting.1234For such patients, PPIs cause unnecessary healthcare spending and pill burden. Observational studies have also shown associations between PPIs and many medical conditions, including chronic kidney disease, fractures, pneumonia, cardiovascular disease, andClostridium difficileinfection, although only indirect evidence supports a causal link between PPIs and these conditions, with uncertain clinical significance.56789In this setting, PPI overuse has been a focus of efforts to reduce unnecessary and wasteful care, including as part of the Choosing Wisely campaign.1011
Despite these efforts, the effect of interventions to reduce PPI overuse is unclear. A systematic review of 21 interventions, including education for patients and clinicians, individualized medication reviews, and education/coaching sessions for clinicians, found that most studies lacked control groups and focused narrowly on medication use outcomes, with only six interventions showing effectiveness at reducing PPI use.12Given the complexity of efforts to reduce low value care, recommendations have been made that evaluations of such programs incorporate assessments of not only service utilization (for example, PPI use) but also appropriateness, potential unintended consequences, and clinically meaningful outcomes.13We aimed to comprehensively evaluate the real world impacts of a large scale, multicomponent, pharmacy based initiative to reduce PPI overuse in a multistate health system.
Methods
Design, data source, and study population
We did a difference-in-difference analysis in the US Veterans Affairs Healthcare System, the largest integrated healthcare system in the US with 1255 healthcare facilities and 1074 outpatient sites. It contains 18 regional health systems, known as Veterans Integrated Service Networks (VISNs), each with its own ambulatory pharmacy system. Drugs prescribed by Veterans Affairs clinicians are nearly always filled by Veterans Affairs pharmacies.
Medical and pharmacy leaders in VISN 17, which includes most Veterans Affairs facilities in Texas and parts of New Mexico and Oklahoma, developed a multicomponent pharmacy based initiative to reduce PPI overuse and rolled it out VISN-wide from August 2013 to July 2014. The study period spanned February 2009 to January 2019, to capture trends 4.5 years before and after implementation. We divided the study period into consecutive six month intervals. In each interval, we included all patients who had at least two visits with a primary care provider in the preceding two years. We used the Veterans Affairs’ corporate data warehouse to obtain demographic, medical, pharmacy, laboratory, and inpatient/outpatient encounter data.
Exposure: PPI overuse initiative and its components
The PPI overuse initiative was designed by pharmacy leaders in VISN 17 and was based on a previous successful project to reduce off-label use of atypical antipsychotics by requiring pharmacy pre-approval, as well local experience that education focused medication de-implementation projects were often unsuccessful.
The VISN 17 PPI overuse initiative had five components. (1) Targeted restrictions on PPI refills: if the ordering clinician failed to document an appropriate indication for chronic use in the prescription, it was limited to 90 days without refills or 30 days with up to two refills. Clinicians were still permitted to order de novo (or renew) prescriptions for patients whose refills had been restricted. (2) Voiding inactive prescriptions: prescriptions not filled within six months were discontinued. (3) Facilitated prescribing of H2 receptor antagonists: a quick order option was added to the medication order menu to help with tapering of PPIs. (4) Education for clinicians and patients: all clinicians received an email from their chiefs of staff about the initiative, and primary care providers and gastroenterologists were invited to attend an educational meeting with a clinical pharmacist or gastroenterologist. The meeting included the recommendation to withdraw PPIs by tapering for two weeks (use every other day for one week, then as needed for one week), followed by H2 receptor antagonists as needed for one to two months. Pharmacists selectively provided academic detailing to clinicians, characterized by coaching and education to change prescribing practices. Clinical pharmacists also provided clinicians whose PPI prescriptions were curtailed with ad hoc education via online messaging or email and sent affected patients an informational letter, which described how to taper the PPI or speak with a pharmacist if needed. (5) Data resources: VISN 17 pharmacists created a reporting tool to capture prescriptions for high dose PPIs for tracking purposes. All components were used during the first year after implementation, and components 1 and 3, ad hoc clinician education, and letters to patients continued afterward.
For each six month interval, patients were assigned to the VISN where they had most primary care encounters in the previous two years. We considered patients assigned to VISN 17 to be exposed to the PPI overuse initiative, whereas we considered patients in all other VISNs to be unexposed (controls).
Primary outcome
We evaluated outcomes in each six month interval. The primary outcome was the percentage of patients who filled a PPI prescription, including initial fills and refills (supplement 1). We obtained pharmacy data only for drugs dispensed by the Veterans Affairs health system, using pharmacy drug classes.
Secondary outcomes
We analyzed the duration of new PPI prescriptions during each interval, defined as total days for the initial prescription plus total days’ worth of allowed refills, and the proportion of PPI prescriptions that were for a high dose, defined as exceeding the defined daily dose for maintenance treatment established by the World Health Organization for each specific PPI formulation (for example, 20 mg for omeprazole).14We also examined use of PPIs in patients appropriate for gastroprotection owing to high risk for upper gastrointestinal bleeding. We calculated this percentage as the number of days with PPI possession divided by total days at high risk for upper gastrointestinal bleeding. On the basis of professional guidelines, we operationalized time at high risk for upper gastrointestinal bleeding as time when a patient had medication possession of two or more antithrombotic drugs (anticoagulants, aspirin, P2Y12 inhibitors) or an antithrombotic drug with a non-steroidal anti-inflammatory drug, both with at least daily dosing.15161718We counted days with a PPI prescribed for at least daily dosing as having appropriate gastroprotection. We determined medication possession by using the dispensed date and days’ supply. To investigate the possibility that patients shifted their prescriptions to prescribers outside the Veterans Affairs health system, we examined the percentage of patients aged 65 or older who obtained PPI prescriptions using Medicare Part D, a voluntary outpatient prescription drug benefit available to patients aged 65 or older with Medicare in the US. We also examined the percentage of patients receiving prescriptions for H2 receptor antagonists, in a similar fashion as for the primary outcome, and the percentage of patients receiving either a PPI or an H2 receptor antagonist.
For patients ≥65 years old, we obtained discharge diagnoses for all hospital admissions within and outside of the Veterans Affairs health system, using data provided by the Veterans Affairs Information Resource Center in partnership with the Centers for Medicare and Medicaid Services. On the basis of primary discharge diagnoses, we identified hospital admissions for community acquired pneumonia, ischemic stroke, hip fracture, and myocardial infarction (supplement 1). In patients of all ages, using Veterans Affairs data only, we identified incident chronic kidney disease (defined as an estimated glomerular filtration rate <60 mL/min/1.73 m2sustained at all measurements over the following year) andC difficileinfection (supplement 1).
Because a previous study reported increased cause specific mortality from cardiovascular disease, upper gastrointestinal cancer, chronic kidney disease, andC difficileinfection with PPIs, we also examined mortality from these conditions, using identical ICD-10 (international classification of diseases, 10th revision) codes.19For all decedents (identified using the Master Veteran Index), we ascertained primary cause of death by using the Joint Department of Veterans Affairs and Department of Defense Mortality Data Repository.
For patients ≥65 years old, we identified hospital admission for acid peptic diseases (for example, esophagitis, gastritis, duodenitis, peptic ulcer disease, or upper gastrointestinal bleeding likely caused by one of these lesions), that occurred during patient time at high risk for upper gastrointestinal bleeding, as defined above, using primary and secondary discharge diagnosis codes as previously described (supplement 1).20
We identified primary care visits and emergency department/urgent care visits associated with a diagnosis code for abdominal pain, nausea/vomiting, gastritis, gastroesophageal reflux disease, dysphagia, dyspepsia, or acid peptic disease, on the basis of previously published diagnosis codes (supplement 1).21We also identified upper gastrointestinal endoscopies by using Current Procedural Terminology codes (supplement 1).
Statistical analysis
We calculated baseline characteristics of patients in VISN 17 and control sites for interval 1. We examined the association between the VISN 17 overuse initiative and all outcomes by using difference-in-difference analysis, a quasi-experimental design that compares changes from pre-implementation to post-implementation for an intervention group relative to a control group, while accounting for common pre-implementation temporal trends. The 12 month implementation (roll-out) period was excluded from analysis. Patients admitted to hospital in one interval could not be counted for hospital related outcomes in the next interval to prevent a single protracted illness from being counted twice.
We first visualized outcomes for VISN 17 and control sites to assess for parallel temporal trends during the pre-implementation period, a prerequisite for valid difference-in-difference analysis. We then tested for parallel trends by fitting regression models with predictors of time, a VISN 17 indicator, a post-implementation period indicator, and all two way and three way interaction terms, with robust standard errors. If visualization suggested parallel non-linear trends before implementation, we also included a time2term to model the curvature. We considered the parallel trends assumption to be violated if pre-implementation trends for VISN 17 and control sites appeared non-parallel on visual inspection (whether linear or non-linear), and if the three way interaction term and the two way interaction term of time by VISN 17 indicator were significant (P<0.05). We then fitted the difference-in-difference models with indicators of VISN 17, post-implementation period, and VISN 17 by post-implementation interaction term (the “difference-in-difference estimator”), along with appropriate time parameters. When indicated, we kept the interaction of time by post-implementation period to allow for any Veterans Affairs system-wide changes in the later period. We used the coefficient for the “difference-in-difference estimator” to estimate the time averaged difference in the outcome between VISN 17 and control sites from the pre-implementation to post-implementation period (the “difference-in-difference”). For all outcome models, we also estimated the mean difference between VISN 17 and control sites in the pre-implementation and post-implementation periods. As exploratory analyses, we reported the difference-in-difference estimates for outcomes that had nearly parallel pre-implementation trends on visual inspection but non-parallel trends statistically. We used Stata 15.0 for analyses.
Sensitivity analyses
For time with gastroprotection, we examined an alternative definition of acceptable gastroprotection to also include misoprostol or H2 receptor antagonist.
Patient and public involvement and dissemination
Patients and the public were not involved in the design of the VISN 17 intervention, which was initiated by operational leadership in the health system. The research team’s evaluation design was informed by previous surveys of patients.22The research team has also shared the findings with US veterans and benefitted from their feedback in drafting the manuscript.
Results
Characteristics of VISN 17 and control sites
The number of patients per interval ranged from 192 607 to 250 349 in VISN 17 and from 3 775 953 to 4 360 868 in control VISNs. The groups were similar in age, Deyo-Charlson comorbidity scores, and use of antithrombotic drugs and non-steroidal anti-inflammatory drugs (table 1). VISN 17 had higher proportions of women and Hispanic/Latinx patients. The prevalence of gastroesophageal reflux disease in the previous two years was 14.5% for VISN 17 and 16.6% for control sites.
Changes in PPI use and H2 receptor antagonists
In the pre-implementation period, the proportion of patients who filled prescriptions for PPIs slowly declined in both VISN 17 and control sites (fig 1, top panel), averaging 25.8% and 25.4%, respectively (table 2). Difference-in-difference analysis showed that the VISN 17 overuse initiative was associated with an absolute reduction of 7.3% (95% confidence interval −7.6% to −7.0%) in the proportion of patients filling PPI prescriptions relative to control sites (the primary outcome), a reduction of 9.22 (−10.95 to −7.49) days in the duration of new PPI prescriptions (including refills), an absolute reduction of 5.72% (−6.08% to −5.36%) in patients filling either a PPI or an H2 receptor antagonist prescription, and an absolute reduction of 0.42% (−0.73% to −0.10%) in older patients filling PPI prescriptions using Medicare Part D relative to control sites. In exploratory analyses of outcomes that did not meet the statistical test of parallel trends, the VISN 17 intervention was also associated with an absolute reduction of 3.99% (−4.61% to −3.38%) in patients receiving prescriptions for high dose PPI and an absolute increase of 2.19% (1.73% to 2.65%) in patients filling prescriptions for H2 receptor antagonists.
In patients at high risk for upper gastrointestinal bleeding, the VISN 17 intervention was associated with an absolute reduction of 11.3% (−12.0% to −10.5%) in percentage time with co-prescribed PPI relative to control sites (fig 1, bottom panel). In the sensitivity analysis including H2 receptor antagonists and misoprostol as gastroprotection, results were similar (supplement 2).
Clinical outcomes associated with PPI use
In patients of all ages, the VISN 17 intervention was not associated with a change in incident chronic kidney disease (−0.10%, −0.26% to 0.06%) (table 3and supplement 2). We did not analyzeC difficileinfection because of non-parallel trends in the pre-implementation period. In patients ≥65 years old, using both Veterans Affairs and Centers for Medicare and Medicaid Services data, the VISN 17 intervention was not associated with changes in hospital admissions for pneumonia, ischemic stroke, or myocardial infarction (table 3and supplement 2). For hospital admission with hip fracture, the intervention was associated with an absolute reduction of 0.018% (−0.036% to −0.001%) for VISN 17 relative to control sites (table 3and supplement 2). Medicare enrollment was identified for 97.2% of patients who turned age 65 on or before 1 January 2019.
Cause specific mortality
The VISN 17 intervention was not associated with changes in the incidence of cardiovascular specific mortality (−0.007%, −0.022% to 0.008%) (table 3and supplement 2) relative to control sites. We did not analyze the other specific causes of death examined (upper gastrointestinal cancer, chronic kidney disease,C difficileinfection) because of non-parallel trends in the pre-implementation period. Cause of death data were identified for 99.2% of decedents.
Clinical outcomes and healthcare utilization related to acid peptic disease
The VISN 17 intervention was not associated with a change in the incidence rate of hospital admission for acid peptic disease among older at risk patients (0.085 (−0.41 to 0.581) admissions per 100 risk years) (fig 2), upper gastrointestinal endoscopies (0.47 (−1.33 to 2.26) per 1000 patients), or primary care visits with upper gastrointestinal diagnoses (3.18 (−4.02 to 10.37) per 1000 patients) (table 3and supplement 2). We did not analyze emergency department and urgent care visits because of non-parallel trends in the pre-implementation period.
Discussion
We found that a multicomponent intervention that included restrictions on PPI prescription refills for patients without a documented long term indication was associated with a rapid and sustained 7.3% absolute reduction in patients receiving PPI prescriptions in a network of Veterans Affairs medical centers—a relative decline of nearly 30%. We also found an 11.3% absolute reduction in the percentage of time patients at high risk for upper gastrointestinal bleeding had appropriate PPI gastroprotection, but we did not find increased hospital admissions for acid peptic disease in older patients at high risk. This effort to reduce PPI overuse did not lead to increased healthcare resource utilization related to upper gastrointestinal symptoms as assessed by primary care visits with gastrointestinal diagnoses and upper gastrointestinal endoscopies. In addition, the intervention was not associated with the incidence of multiple PPI associated conditions that were examined, except for hip fractures in older patients, for which a 0.018% absolute reduction was found.
Comparison with previous studies
Few rigorous evaluations of large, real world PPI de-implementation initiatives are available. In the Australian veterans system, a series of interventions involving nearly 70 000 PPI users, including education for clinicians, academic detailing, audit and feedback, peer group meetings for clinicians, and education for patients, resulted in an 8.5% absolute reduction in veterans using PPIs over 10 years, mainly due to slowing the rate of increase in PPI use.24Although our study could not determine how each intervention component contributed to the overall effect of VISN 17’s PPI overuse initiative, we speculate that the restriction on PPI refills accounts for its more rapid reduction in PPI use (occurring over one year) as it took effect nearly immediately and did not depend on changing clinicians’ behavior. In this respect, it differs from many “deprescribing” interventions, in which a clinician typically supervises the withdrawal of an inappropriate medication, often incorporating shared decision making with patients.25The mechanism of the VISN 17 initiative can also be likened to formulary restriction programs. In 2002, a program for prior authorization of PPIs for Medicaid patients was associated with a 92% relative reduction in claims for PPIs over six months of follow-up without an increase in associated costs for gastrointestinal conditions.26However, sustainment and shifting to over-the-counter PPIs was unclear.
The risk of bone fractures associated with PPIs was previously evaluated in a meta-analysis of 17 observational studies, which found a relative risk of 1.26 (95% confidence interval 1.17 to 1.35).27However, many criteria supportive of causation (for example, consistency, biological gradient, strength of association) have not been demonstrated, leaving open the possibility that bias and/or residual confounding could explain the association.28Importantly, the COMPASS trial, which randomized and followed 17 598 patients for three years, showed no increased risk of fracture with pantoprazole versus placebo (odds ratio 0.89, 0.71 to 1.13).29In the context of contradictory experimental evidence and the fact that our demonstrated association was of small magnitude and the only one of multiple conditions analyzed that was positive, our finding on hip fracture should not be viewed as definitive. Similarly to our study, the COMPASS trial also did not find an association between PPI use and many other conditions that have been linked with PPIs, including myocardial infarction, chronic kidney disease, and pneumonia.29Although our study may have lacked power to identify small effects, our results underscore that even a potent intervention like VISN 17’s is likely to have minimal, if any, impact on these outcomes even if the associations between PPIs and these harms are causal.
Our finding that the VISN 17 program was associated with reduced PPI use in patients at high risk for upper gastrointestinal bleeding fits with previous evidence that gastroprotection is under-recognized as an indication for long term use.30Widespread concerns among clinicians about adverse effects of PPIs may also contribute to underuse and deprescribing of PPIs for gastroprotection.31This is in spite of high quality evidence on the efficacy of PPIs for prevention of upper gastrointestinal bleeding,32recommendations in professional guidance statements,1516173334and a clinical practice update from the American Gastroenterological Association clarifying that “the decision to discontinue PPIs should be based solely on the lack of an indication for PPI use, and not because of concern for [PPI-associated adverse events],” given only tenuous evidence that these associations are causal.9
Although previous clinical trials have shown that discontinuation or dose reduction of PPIs increases the risk of esophagitis in patients with gastroesophageal reflux disease,3536we did not find evidence of increased healthcare utilization for acid related disorders. Because only a minority of PPI users discontinued therapy, any resultant increase might have been too small to detect. Another possibility is that patients who developed upper gastrointestinal symptoms after discontinuation of PPIs may not have sought medical care if the symptoms were minor or may have resumed PPIs. Clinicians and patients considering deprescribing of PPIs in the setting of well controlled mild-to-moderate gastroesophageal reflux disease should note that evidence based guidelines support several different approaches, including dose reduction, use on demand, and stepping down to H2 receptor antagonists, but that the last two options are associated with a higher risk of recurrence of symptoms.37
Strengths and limitations of study
Our study has several strengths, including data from a nationwide health system, use of Veterans Affairs and Centers for Medicare and Medicaid Services datasets to identify hospital admission outcomes, and the difference-in-difference design, which controls for many threats to validity, including unmeasured confounding and independent temporal trends. Our study also has several limitations. Firstly, the switch from ICD-9 to ICD-10 occurred during our study period (in October 2015). This is unlikely to have affected our findings, as the switch was not concurrent with the VISN 17 intervention and VISN 17 and control sites were unlikely to have systematic differences in coding. Secondly, some VISN 17 patients may have shifted to buying PPIs outside the Veterans Affairs health system after the PPI overuse initiative started, which would have attenuated the true effect of the program. Although we could not measure purchase of PPIs from retail pharmacies, we did not find an increase in PPI prescriptions filled outside the Veterans Affairs system in our analysis of older patients with Medicare Part D coverage, and in fact found a reduction. Thirdly, our results reflect population level effects of the intervention and should not be interpreted as the effects of patient level PPI discontinuation. Fourthly, we did not adjust for testing of multiple clinical outcomes, increasing the risk of a type I error. Fifthly, we evaluated rates of gastroprotection in patients using two or more medications that predispose to upper gastrointestinal bleeding; rates of gastroprotection may vary in patients with other risk factors for upper gastrointestinal bleeding.
Implications of findings
Our findings have implications for the design and evaluation of programs to reduce PPI overuse and de-implementation interventions in general. The combination of interventions deployed, including restrictions on PPI refills, was an effective strategy to reduce PPI use. How effective such an approach would be with other commonly overused and misused drugs, such as benzodiazepines or opioids, for which the barriers to de-implementation may be more complex, is unknown, as is the potential effectiveness in settings outside the Veterans Affairs health system, which has a distinctive patient population, a well developed program for education of clinicians, and an integrated pharmacy system. In designing future PPI de-implementation interventions, more emphasis must be placed on precisely targeting the right patients. De-implementation programs have repeatedly shown indiscriminate effects both for patients in whom care is inappropriate and for those in whom it is appropriate.3839Evaluating unintended consequences should be a standard part of any program aiming at de-implementation but is often overlooked.40In addition, research should be directed at understanding patients’ and providers’ views of prescribing restrictions, which could be perceived as infringing on prescribing autonomy, and the extent to which decisions to discontinue PPIs under these circumstances incorporate shared decision making.
Conclusions
A pharmacy based, multicomponent PPI overuse intervention was associated with reduced use in patients for whom PPIs were appropriate and those for whom they were inappropriate but without evidence of clinically significant harms or benefits. Thus, the primary benefits are likely to be reduced pill burden for patients and reduced drug costs for health systems. Ensuring that PPI de-implementation efforts are both highly effective but also focused on the right patients will be important to their long term success.9
","Objective: To determine how a large scale, multicomponent, pharmacy based intervention to reduce proton pump inhibitor (PPI) overuse affected prescribing patterns, healthcare utilization, and clinical outcomes.
Design: Difference-in-difference study.
Setting: US Veterans Affairs Healthcare System, in which one regional network implemented the overuse intervention and all 17 others served as controls.
Participants: All individuals receiving primary care from 2009 to 2019.
Intervention: Limits on PPI refills for patients without a documented indication for long term use, voiding of PPI prescriptions not recently filled, facilitated electronic prescribing of H2 receptor antagonists, and education for patients and clinicians.
Main outcome measures: The primary outcome was the percentage of patients who filled a PPI prescription per 6 months. Secondary outcomes included percentage of days PPI gastroprotection was prescribed in patients at high risk for upper gastrointestinal bleeding, percentage of patients who filled either a PPI or H2 receptor antagonist prescription, hospital admission for acid peptic disease in older adults appropriate for PPI gastroprotection, primary care visits for an upper gastrointestinal diagnosis, upper endoscopies, and PPI associated clinical conditions.
Results: The number of patients analyzed per interval ranged from 192 607 to 250 349 in intervention sites and from 3 775 953 to 4 360 868 in control sites, with 26% of patients receiving PPIs before the intervention. The intervention was associated with an absolute reduction of 7.3% (95% confidence interval −7.6% to −7.0%) in patients who filled PPI prescriptions, an absolute reduction of 11.3% (−12.0% to −10.5%) in PPI use among patients appropriate for gastroprotection, and an absolute reduction of 5.72% (−6.08% to −5.36%) in patients who filled a PPI or H2 receptor antagonist prescription. No increases were seen in primary care visits for upper gastrointestinal diagnoses, upper endoscopies, or hospital admissions for acid peptic disease in older patients appropriate for gastroprotection. No clinically significant changes were seen in any PPI associated clinical conditions.
Conclusions: The multicomponent intervention was associated with reduced PPI use overall but also in patients appropriate for gastroprotection, with minimal evidence of either clinical benefits or harms.
"
Esketamine after childbirth for mothers with prenatal depression,"Introduction
Depression is common among women during the perinatal period,1with a reported prevalence of 6-13% in high income countries,221% in middle income countries, and 26% in low income countries.3Mothers with perinatal depression are often anxious4and have worse relationships with partners5and poorer mother-infant attachment than mothers without perinatal depression.6The offspring of mothers with depression are at higher risk of behavioural and emotional problems—and even long term psychological and developmental disturbances.78Factors contributing to the development of perinatal depression include poor physical health, limited social support, low economic status, limited education, and history of exposure to violence.59The covid-19 pandemic placed pregnant people and their families under additional stress, increasing the risk of perinatal mood disorders, including depression.410
Prenatal depression is a strong predictor of postpartum depression.11Mothers with prenatal depression are thus good candidates for interventions that might reduce postpartum depression. Although non-pharmacological measures are preferable,12drug treatments are sometimes necessary.13Use of traditional antidepressants during pregnancy and lactation are limited by delayed onset of effects and potential harm to neonates.14Ketamine is a non-competitive N-methyl-D-aspartic receptor antagonist that has long been used for anaesthesia and analgesia.15Esketamine is the S-enantiomer of racemic ketamine and has a higher affinity for the N-methyl-D-aspartic receptor than ketamine.16Both ketamine and esketamine have rapid onset antidepressant effects,1617although the mechanisms for this remain unclear.18Esketamine nasal spray has been approved by the US Food and Drug Administration for treatment resistant depression.19
Low dose ketamine or esketamine improves analgesia and relieves depression in mothers having caesarean deliveries.2021222324252627However, previous trials were largely restricted to caesarean delivery and excluded mothers with depression and thus at high risk of postpartum depression. We therefore tested the primary hypothesis that a single low dose of esketamine given shortly after delivery reduces depression over 42 days among mothers with prenatal depression.
Methods
Study design
This randomised, double blind, placebo controlled trial with two parallel arms was conducted in five tertiary care hospitals across China. The Biomedical Research Ethics Committee of Peking University First Hospital (2019-336) and other participating centres approved the study protocol (see supplement 1). Written informed consent was obtained from each participant. The trial is reported according to the Consolidated Standards of Reporting Trials guidelines.
Participants
Potential participants were screened with the Edinburgh postnatal depression scale at hospital admission for delivery. This scale is a patient reported 10 item questionnaire used to screen for perinatal depression; scores range from 0 to 30, with higher scores indicating more severe depression.28The Chinese version of the Edinburgh postnatal depression scale has been validated, with a score of ≥10 indicating at least mild depression.29
We enrolled pregnant individuals aged 18 years or older with an Edinburgh postnatal depression scale score of ≥10 who were close to childbirth. We excluded those with a prepregnancy history of mood disorders, including depression; communication difficulties; severe complications of pregnancy, such as pre-eclampsia, placenta accreta spectrum, or HELLP (intravascular haemolysis, elevated liver enzymes, and low platelet count) syndrome; American Society of Anesthesiologists physical status III or higher; or any contraindications to ketamine or esketamine, such as refractory hypertension, severe cardiovascular disease, or hyperthyroidism.
Randomisation and masking
An independent biostatistician generated random treatment assignments in a 1:1 ratio, stratified by trial site, with a block size of four using the PROC PLAN procedure of SAS 9.2 software (SAS Institute, Cary, NC). Allocations were sealed in sequentially numbered opaque envelopes controlled by investigators who were otherwise not involved in data acquisition or the participants’ care. Coordinators (anaesthesiologist: CW; anaesthesia nurses: G-YG, ML, Y-CL, and C-CH) opened the randomisation envelopes according to the recruitment sequence shortly before the participants gave birth and prepared the appropriate study drugs—either 0.2 mg/kg esketamine diluted in 20 mL normal saline or 20 mL normal saline.
Syringes labelled “trial drug” were given to delivery room nurses for participants having a vaginal delivery or to anaesthesiologists for participants having a caesarean delivery. All participants, healthcare team members, and outcome assessors were therefore fully blinded to treatment. In emergencies such as rapid changes in a participant’s clinical status, the delivery room nurses or anaesthesiologists could adjust the speed of infusion or interrupt administration of the study drug. They could also request unmasking if deemed clinically necessary.
Perinatal care and intervention
Routine maternal monitoring included electrocardiography, non-invasive blood pressure measurement, and pulse oximetry, which were performed every one to two hours, or more frequently when necessary. Continuous external fetal heart rate monitoring or tocodynamometry were used as indicated. The participants were given detailed information about the potential benefits and risks of epidural analgesia for labour and decided, in consultation with their healthcare team, whether to have epidural analgesia during labour.
Epidural analgesia for those participants who requested it was initiated when the cervix was dilated at least 1 cm. An epidural catheter was inserted at the L2-L3 or L3-L4 interspace. A 10-15 mL mixture consisting of 0.08-0.13% ropivacaine and 0.36-0.45 μg/mL sufentanil was administered as a loading dose; an additional 5 mL of the mixture was given 10 minutes later if participants’ pain score was ≥4 on a numerical rating scale (an 11 point scale where 0=no pain and 10=the worst pain). A participant controlled epidural analgesia pump was attached 30 minutes later, which was established with a mixture of 0.07% ropivacaine and 0.36 μg/mL sufentanil, programmed to deliver an 8 mL bolus with a lockout interval of 30 minutes and an optional background infusion of 4 mL/h. Participant controlled epidural analgesia was discontinued during the second stage of labour, but the background infusion (if used) was continued. Epidural analgesia was usually stopped at the end of the third stage. For participants who did not request neuraxial analgesia, routine perinatal care including intramuscular pethidine was provided.
Obstetric management, including oxytocin and forceps assisted delivery or caesarean delivery was provided or conducted according to routine practice and the corresponding local guidelines. If emergency caesarean delivery was required, epidural anaesthesia was provided by injecting a suitable local anaesthetic dose through an indwelling epidural catheter; otherwise, combined spinal-epidural anaesthesia was used to a target sensory block level from T6 to T4. Vasopressors including ephedrine and phenylephrine were given to maintain blood pressure; opioids including pethidine were administered as supplement analgesia when necessary. Participant controlled epidural or intravenous analgesia was provided for up to 24 hours after caesarean delivery. Infusion of the trial drug (either 0.2 mg/kg esketamine or normal saline) was initiated at a rate of 30 mL/h over a 40 minute period after the umbilical cord was clamped.
Investigator training, data collection, and outcome measures
Before the trial began, investigators responsible for baseline data collection and follow-up assessments (SW, Fei-Xue Wang, TH, TY, H-YZ, and H-MY) were trained by psychiatrists (X-YS and H-NG) to use assessment tools, including the mini-international neuropsychiatric interview (version 6.0.0, depression module) and the 17 item Hamilton depression rating scale. The mini-international neuropsychiatric interview 6.0.0 is a brief structured diagnostic interview to assess depression,30and the Chinese version has been validated.31Psychiatrists and investigators who were involved in the trial were trained athttps://harmresearch.org/and were certified to diagnose depression with the mini-international neuropsychiatric interview 6.0.0. The Hamilton depression rating scale is a clinician reported scale designed to rate the severity of symptoms observed during major depressive episodes. The scale contains 17 items; possible scores for various items range from 0 to 2 or 0 to 4. The total scores range from 0 to 52, with 0-7 indicating no depression, 8-16 mild depression, 17-23 moderate depression, and ≥24 severe depression.32The investigators were trained with a standardised patient and passed a consistency test (see eTable in supplement 2).
Baseline data of the participants were recorded, including personal characteristics, socioeconomic status, pregestational comorbidities, and details of the current pregnancy. Anxiety was assessed with the Zung self-rating anxiety scale (scores range from 20 to 80, with higher scores indicating more severe anxiety).33Social support was assessed with the social support rating scale (scores range from 11 to 62, with higher scores indicating better social support).34Marital satisfaction was assessed with the ENRICH (evaluation and nurturing relationship issues, communication, and happiness) marital satisfaction scale (scores range from 10 to 50, with higher scores indicating better marital satisfaction).35The Chinese versions of these instruments have been validated.363738
Maternal data included acceptance of epidural analgesia, mode of delivery, estimated blood loss and fluid infusion, and use of supplemental analgesics and sedatives. Neonatal data included sex, bodyweight, Apgar scores at one and five minutes after birth, and initial destination (eg, postpartum ward, neonatal ward, or neonatal intensive care unit). Certificated investigators or anaesthesiologists supervised infusion of the study drugs. Vital signs including blood pressure, heart rate, oxygen saturation, and agitation-sedation level as assessed with the Richmond agitation-sedation scale (scores range from −5 (unarousable) to 4 (combative) and 0 indicates alert and calm39) were recorded every five minutes during infusion of the study drug and every 10 minutes thereafter for 60 minutes. Thus, every participant was monitored for one hour after the study drug had been administered and before being transferred to a ward.
We conducted a face-to-face interview with the mothers between 18 and 30 hours after childbirth. Participants were also contacted by telephone on the seventh postpartum day and were contacted again on day 42 for face-to-face or online video interviews. Pain intensity was assessed with the numerical rating scale (an 11 point scale where 0=no pain and 10=the worst pain), with a difference of ≥1 point being considered clinically meaningful.40Breastfeeding was recorded as exclusive, mixed, or none. Symptoms of depression were assessed with the Edinburgh postnatal depression scale score at seven and 42 days post partum; an improvement of at least 4 points or worsening of at least 3 points was considered clinically meaningful.41Depression was also assessed with the depression module of the mini-international neuropsychiatric interview 6.0.0 and the Hamilton depression rating scale at 42 days; the assessments were supervised by psychiatrists for the first two participants and then one in every 10 participants at each study centre. Supervision was performed by examining the recorded video or audio files during the assessment process. As a result, assessments of 43 participants were reviewed, with diagnoses confirmed by psychiatrists.
Our primary endpoint was the prevalence of a major depressive episode at 42 days post partum, diagnosed using the mini-international neuropsychiatric interview 6.0.0.3031Participants with a diagnosis of major depressive episode were advised to visit mental health facilities for further consultation. Predefined secondary endpoints post partum included Edinburgh postnatal depression scale scores at seven and 42 days; Hamilton depression rating scale score at 42 days; numerical rating scale of pain and proportion with exclusive breastfeeding at one, seven, and 42 days; length of hospital stay; and maternal and neonatal complications within 42 days after childbirth. Maternal and neonatal complications were defined as any medical conditions that required hospital visits and treatment intervention.
Prespecified adverse events were monitored continuously during and for an hour after infusion of the study drug, two hours after infusion, and on the first postpartum day. Specifically, we monitored tachycardia (heart rate >100 beats/min), hypertension (systolic blood pressure >160 mm Hg or an increase >30% from baseline), respiratory depression (respiratory rate <10 breaths per minute), desaturation (oxygen saturation <90% or an absolute decrease of >5% from baseline), sedation (Richmond agitation-sedation scale ≤−2), somnolence, and nausea or vomiting, as well as neuropsychiatric symptoms such as dizziness, agitation (Richmond agitation-sedation scale ≥2), diplopia, hallucinations, and daymares or nightmares. We also monitored other side effects, including stomach ache and leg numbness. Side effects were managed according to local routine, including intravenous midazolam when considered necessary (see supplement 1).
Statistical analysis
As with previous studies, we estimated the prevalence of postpartum depression to be 42-50% in women with prenatal depression.4243In a trial of patients with treatment resistant depression, the response rate was 67% after 0.2 mg/kg of intravenous esketamine.44We therefore assumed that the prevalence of depression at 42 days post partum would be 45%, and that treatment with low dose esketamine would decrease depression by about one third. With a two sided significance level set at 0.05 and power at 80%, we determined that 328 participants would be required to detect such a difference. We expected a dropout rate of about 10% and thus planned to enrol a total of 364 participants.
Outcome analyses were primarily performed in the intent-to-treat population—that is, all participants were analysed in the group to which they were randomly assigned. For the primary endpoint, we also conducted a per protocol analysis after excluding those with protocol deviations or who withdrew consent.
The prevalence of a major depressive episode at 42 days post partum, our primary endpoint, was compared with a χ2test, with differences between groups expressed as relative risk and 95% confidence interval (CI). The number needed to treat was estimated as the reciprocal of the absolute risk reduction. Prespecified subgroup analyses were performed using logistic regression models to calculate the treatment-by-covariate interactions. As a post hoc sensitivity analysis, we imputed missing primary endpoint data. As the proportion of participants with missing data was <5%, we assigned best outcome to participants in the placebo group and worst outcome to participants in the esketamine group.45
For secondary and other endpoints, we analysed continuous variables using attest or Mann-Whitney test. Median differences (and 95% CIs) were calculated with Hodges-Lehmann estimators. Categorical variables were analysed with χ2test, continuity corrected χ2tests, or Fisher’s exact test. Relative risks (and 95% CIs) were calculated. Time-to-event data were evaluated with Kaplan-Meier survival analysis, with between group difference tested with the log-rank test. Hazard ratios (and 95% CIs) were estimated from a Cox proportional hazard model. Missing data were not replaced.
On an exploratory basis, for the Edinburgh postnatal depression rating scale we estimated the percentages of scores ≤9, reduction in score ≥4 points from baseline, and reduction in score ≥50% from baseline at seven and 42 days post partum, along with the percentage of Hamilton depression rating scale scores ≤7 at 42 days post partum. We also compared primary and key secondary endpoints in participants with or without side effects and in those with or without neuropsychiatric symptoms.
For hypothesis testing, we considered a two tailed P value of <0.05 to be statistically significant. For subgroup treatment-by-covariate interactions, we considered P<0.10 to be statistically significant. Statistical analyses were performed with the SPSS 25.0 software (IBM SPSS, Chicago, IL).
Patient and public involvement
Mothers close to childbirth with prenatal depression were involved in our previous pilot trial and reviewed questionnaire for the present study.43At the protocol stage, we gained opinions from participating medical centres on the content of follow-ups.
Results
Patient population
From 19 June 2020 to 21 June 2022, a total of 14 243 women were screened for inclusion. Among these, 479 were eligible and 364 were enrolled and randomised to receive either esketamine (n=182) or placebo (n=182). All enrolled participants were given an infusion of the study drug. During the postpartum follow-up period, two participants in the esketamine group and one participant in the placebo group withdrew consent. Therefore, all 364 participants were included in the intention-to-treat analysis and 361 participants were included in the per protocol analysis (fig 1; also see eTable 2 in supplement 2). The last participant was followed-up on 3 August 2022.
The mean age of enrolled participants was 31.8 (SD 4.1) years.Table 1presents the baseline personal and clinical characteristics of the participants. Intrapartum data, including those of the mothers and neonates, were similar in each treatment group (table 2). No participant took oral antidepressants or received psychotherapy between childbirth and 42 postpartum days.
Efficacy outcomes
Our primary endpoint, a major depressive episode at 42 days post partum, was observed in 6.7% (12/180) of participants in the esketamine group compared with 25.4% (46/181) in the placebo group (relative risk 0.26, 95% CI 0.14 to 0.48; P<0.001; number needed to treat 5, 95% CI 4 to 9). After missing data had been imputed, major depressive episodes occurred in 7.7% (14/182) of participants in the esketamine group compared with 25.3% (46/182) in the placebo group (relative risk 0.30, 95% CI 0.17 to 0.53; P<0.001;table 3). The results of per protocol analyses were similar. No significant interactions were observed between predefined subgroups and the exposure-outcome association on the multiplicative scale (see eFig 1 in supplement 2).
Edinburgh postnatal depression scale scores were lower in the esketamine group at seven days (median difference −3, 95% CI −4 to −2; P<0.001) and 42 days post partum (−3, −4 to −2; P<0.001; see eFig 2 in supplement 2). Hamilton depression rating scale scores at 42 days post partum were also lower in the esketamine group (median difference −4, −6 to −3; P<0.001). The numerical rating scale scores for pain were lower in the esketamine group on the first post-delivery day (at rest: median difference −1, −1 to 0; P=0.003; with movement: median difference −1, −1 to 0; P=0.001), at seven days (median difference −1, −1 to 0; P=0.003), and at 42 days post partum (0, 0 to 0; P=0.04). The proportion of mothers with persistent pain at 42 days was also lower in the esketamine group (35.2% (63/179)) than the placebo group (47.5% (86/181)): relative risk 0.74, 95% CI 0.58 to 0.95; P=0.02. Other outcomes did not differ significantly between the groups, including the proportion of mothers who were exclusively breastfeeding at one, seven, and 42 days post partum (table 3).
In exploratory analyses, the proportion of participants with Edinburgh postnatal depression scale scores of ≤9 was higher in the esketamine group at seven days (relative risk 1.49, 95% CI 1.29 to 1.71; P<0.001) and at 42 days (1.43, 1.25 to 1.65; P<0.001). The proportion of participants with reduction in Edinburgh postnatal depression scale scores of ≥4 from baseline was higher in the esketamine group (at seven days: relative risk 1.63, 1.36 to 1.96; P<0.001; at 42 days: 1.72, 1.42 to 2.07; P<0.001). The proportion of participants with reductions in Edinburgh postnatal depression scale scores of ≥50% from baseline was also higher in the esketamine group (at seven days: 2.24, 1.72 to 2.91; P<0.001; at 42 days: 1.83, 1.41 to 2.37; P<0.001). The proportion of participants with Hamilton depression rating scale scores ≤7 at 42 days was higher in the esketamine group (1.83, 1.46 to 2.28; P<0.001;table 3). No significant interactions were found between the presence of adverse events or neuropsychiatric adverse events and the exposure-outcome associations (see eTables 3 and 4 in supplement 2).
Safety outcomes
During and within one hour after infusion of the study drug, blood pressure, heart rate, and oxygen saturation were similar in each group. Richmond agitation-sedation scores were lower in the esketamine group from 20 minutes after initiating study drug infusion until 20 minutes after the end of study drug infusion, but the differences were not clinically meaningful (median differences 0, 95% CI 0 to 0; P≤0.017; see eTables 5 and 6 in supplement 2). Participants in the esketamine group were more often sedated (5.5% (10/182)v0.5% (1/182); P=0.006), reported fewer stomach aches (1.1% (2/182)v5.5% (10/182); P=0.02), and had more neuropsychiatric symptoms (33.5% (61/182)v11.0% (20/182); P<0.001), including more dizziness (26.4% (48/182)v9.3% (17/182); P<0.001), more diplopia (4.9% (9/182)v0% (0/182); P=0.004), and more hallucinations or daymares (3.3% (6/182)v0% (0/182); P=0.03). About a 10th of participants in the esketamine group required transient interruptions to study drug infusion owing to dizziness (10.4% (19/182)v0% (0/182); P<0.001), but none required other interventions, including midazolam; study drug infusion was restarted about 20 minutes later and completed in all participants. Five mothers reported hallucinations during and within an hour after infusion of the study drug; the hallucinatory symptoms lasted for two hours in two participants. Two hours after infusion of the study drug, the prevalence of adverse events, including neuropsychiatric adverse events, did not significantly differ between the two groups. No neuropsychiatric symptoms were observed on the first postpartum day (table 4).
The overall incidence of neuropsychiatric adverse events was higher in the esketamine group (45.1% (82/182)v22.0% (40/182); P<0.001). No severe adverse events occurred during the study period.
Discussion
For mothers with prenatal depression, a single low dose of esketamine given immediately after childbirth reduced the prevalence of major depressive episode at 42 days post partum by about three quarters, with a number needed to treat of 5. Participants in the esketamine group had lower Edinburgh postnatal depression scale scores at seven and 42 days post partum, and a lower Hamilton depression rating scale score at 42 days post partum. The antidepressant effect of low dose esketamine thus seems to last longer in mothers with prenatal depression than in the general population with depression.1946A reasonable supposition is that depression in perinatal mothers is typically less severe than in previous non-obstetrical trials, which were often conducted in people with severe or even treatment resistant depression.194647Whether the response to esketamine persists beyond 42 days requires further investigation.
Comparison with other studies
Our results were generally consistent with previous work investigating the effects of low dose ketamine2122or esketamine on postpartum depression, mainly in mothers after caesarean delivery.2324252627In a retrospective analysis of 240 mothers close to childbirth, esketamine administered for postoperative analgesia (mean 0.35 mg/kg during a 24 hour period) was associated with lower Edinburgh postnatal depression scale scores at three months.24In a trial of 375 mothers about to give birth, esketamine given by way of patient controlled intravenous analgesia (at a rate of 0.25 mg/kg/day) for 48 hours lowered the prevalence of depression by 60% at 14 days.25Two small trials reported that a single intraoperative dose of esketamine (0.2 or 0.5 mg/kg) decreased the prevalence of depression at 42 days by 73% and 58%, respectively.2326Two other trials enrolled patients with Edinburgh postnatal depression scale scores of ≥10; one found that perioperative esketamine (0.25 mg/kg intraoperatively followed by 1-2 mg/kg postoperatively over 48 hours) reduced the prevalence of depression by 63% to 76% at seven days and by 49% to 67% at 42 days after caesarean delivery27; another study reported that intraoperative esketamine (0.3 mg/kg) lowered Edinburgh postnatal depression scale scores at seven and 42 days after curettage surgery.48
Among available studies, primary endpoints (postpartum depression) were mainly diagnosed according to the Edinburgh postnatal depression scale, usually with a cut-off value of ≥10.20212223242527However, the Edinburgh postnatal depression scale is designed for screening rather than diagnosis, and the cut-off threshold score of ≥10 provides insufficient specificity for diagnosing perinatal depression compared with reference standards.49Two recent trials investigating the effects of low dose esketamine reported neutral results.5051We noted that in one trial perioperative esketamine (0.25 mg during caesarean delivery and 1.25 mg/kg postoperatively for about 48 hours) decreased depression (defined as Edinburgh postnatal depression scale scores ≥13) by 34% at three days and by 38% at 42 days; the decreases were clinically important but not statistically significant owing to limited sample size.50In another trial, depression (defined as Edinburgh postnatal depression scale scores ≥9) at one week post partum occurred in only 4% (4/102) of mothers given esketamine (0.25 mg/kg) and in 2% (2/100) of mothers given placebo, possibly owing to exclusion of high risk mothers; this meant the trial was seriously underpowered to detect between group differences.51
Importantly, our trial extends existing understanding by targeting women with pre-existing prenatal depression, who were therefore at high risk of postnatal depression. In contrast, most previous trials generally recruited participants who were healthy, and some even excluded those with prenatal depression or mental disorders.22255051The prevalence of postnatal depression was high in our selected population, although not as high as we expected. Selecting participants with baseline depression resulted in a low number needed to treat, which depends on baseline incidence. Targeting mothers most likely to benefit also limited side effects to those most likely to benefit, thus providing a favourable risk-benefit ratio.
Side effects of low dose esketamine depend on the rate it is administered. For example, at most, 98% of participants developed neurological or mental symptoms when 0.25 mg/kg esketamine was injected intravenously over one minute52; the proportion was lower when low dose esketamine was infused over 40 minutes.44Our results are consistent with previous reports, which indicate that a single low dose of esketamine (0.2-0.25 mg/kg) infused over 40 minutes is generally well tolerated in people with treatment resistant depression.4453We did not find a significant correlation between neuropsychiatric symptoms and antidepressant effects of esketamine in the present study, which was in line with previously reported results.54Further work is, however, needed because the literature remains inconsistent.55
Additional findings
In the present study, the prevalence of prenatal depression (defined as Edinburgh postnatal depression scale scores ≥10) was only 3%, which is much less than in our previous studies and reported elsewhere.295657This might be due to depression screening and psychological interventions that have become routine parts of prenatal care in the Beijing area since June 2020 and reduced the amount of prenatal depression at the time of delivery.58Consistent with this theory, the prevalence of prenatal depression was 2% (285/12 780) among women recruited in Beijing and 14% (204/1463) among those recruited in other cities.
The analgesic effect of esketamine may last for several hours after use, or even longer. A recent meta-analysis reported that adults given esketamine during general anaesthesia had improved analgesia for up to 24 hours and reduced morphine consumption for up to 12 hours after surgery.59Therefore, as might be expected, participants in the esketamine group in the present study reported meaningfully lower pain scores while resting and with movement on the first postpartum day. Furthermore, participants in the esketamine group had lower pain scores seven days and 42 days post partum, although the improvements were small. Participants in the esketamine group also reported less persistent pain at 42 days post partum. The mechanisms underlying improved analgesia seven and 42 days post partum remain unclear. Severe acute pain is, however, strongly associated with persistent pain, an effect thought to be at least partially mediated by activation of N-methyl-D-aspartic receptors.60Another potential mechanism is that depression and persistent pain are highly intertwined and may well exacerbate each other.61
Limitations of this study
An important limitation is that we excluded mothers with prepregnancy mood disorders (three mothers were excluded for depression or anxiety), and thus failed to include participants in greatest need of intervention. The potential moderating effect of a history of mood disorder should be considered when interpreting our results for reduction in symptoms, decrease in prevalence, and longer term wellbeing of individuals after childbirth. A further limitation is the lack of standardised assessment tools and a data safety monitoring board as well as the short monitoring period for neuropsychiatric symptoms and other adverse events, which might lead to an under-reporting of the true extent. Owing to exclusion of individuals with previous mood disorders and possibly underestimated adverse effects, the broader issue of the external validity and generalisability of the benefit:risk ratio is much constrained. Thirdly, we did not evaluate the participants’ impression on type of drug they received. It can be argued that trials investigating psychedelic drugs might over-estimate treatment effects owing to unblinding of participants and high levels of response expectancy.62However, we found no significant interactions between the presence of adverse events or neuropsychiatric adverse events and the exposure-outcome associations in our results. Lastly, with a median baseline Edinburgh postnatal depression scale score of 10 (interquartile range 10-12), most participants in our trial had only mild prenatal depression. Whether esketamine is equally effective in pregnant mothers with more severe depression remains to be determined.
Conclusions
In mothers with prenatal depression, a single low dose infusion of esketamine shortly after delivery reduced major depressive episodes at 42 days post partum by about three quarters. Neuropsychiatric adverse events were more frequent but transient and did not require drug treatment. Low dose esketamine should be considered in mothers with symptoms of prenatal depression.
Depression is common among perinatal mothers and has adverse effects on both them and their offspring
Esketamine has a rapid onset antidepressant effect for treatment resistant depression, yet the effect for mothers with perinatal depression is unclear
Esketamine (0.2 mg/kg) infused immediately after childbirth reduced major depressive episodes at 42 days post partum by about three quarters in mothers with prenatal depression
Neuropsychiatric adverse events were more frequent with esketamine, but symptoms lasted less than a day and none required drug treatment
","Objective: To determine whether a single low dose of esketamine administered after childbirth reduces postpartum depression in mothers with prenatal depression.
Design: Randomised, double blind, placebo controlled trial with two parallel arms.
Setting: Five tertiary care hospitals in China, 19 June 2020 to 3 August 2022.
Participants: 364 mothers aged ≥18 years who had at least mild prenatal depression as indicated by Edinburgh postnatal depression scale scores of ≥10 (range 0-30, with higher scores indicating worse depression) and who were admitted to hospital for delivery.
Interventions: Participants were randomly assigned 1:1 to receive either 0.2 mg/kg esketamine or placebo infused intravenously over 40 minutes after childbirth once the umbilical cord had been clamped.
Main outcome measures: The primary outcome was prevalence of a major depressive episode at 42 days post partum, diagnosed using the mini-international neuropsychiatric interview. Secondary outcomes included the Edinburgh postnatal depression scale score at seven and 42 days post partum and the 17 item Hamilton depression rating scale score at 42 days post partum (range 0-52, with higher scores indicating worse depression). Adverse events were monitored until 24 hours after childbirth.
Results: A total of 364 mothers (mean age 31.8 (standard deviation 4.1) years) were enrolled and randomised. At 42 days post partum, a major depressive episode was observed in 6.7% (12/180) of participants in the esketamine group compared with 25.4% (46/181) in the placebo group (relative risk 0.26, 95% confidence interval (CI) 0.14 to 0.48; P<0.001). Edinburgh postnatal depression scale scores were lower in the esketamine group at seven days (median difference −3, 95% CI −4 to −2; P<0.001) and 42 days (−3, −4 to −2; P<0.001). Hamilton depression rating scale scores at 42 days post partum were also lower in the esketamine group (−4, −6 to −3; P<0.001). The overall incidence of neuropsychiatric adverse events was higher in the esketamine group (45.1% (82/182)v22.0% (40/182); P<0.001); however, symptoms lasted less than a day and none required drug treatment.
Conclusions: For mothers with prenatal depression, a single low dose of esketamine after childbirth decreases major depressive episodes at 42 days post partum by about three quarters. Neuropsychiatric symptoms were more frequent but transient and did not require drug intervention.
Trial registration: ClinicalTrials.govNCT04414943.
"
Glucagon-like peptide 1 receptor agonist use and risk of thyroid cancer,"Introduction
Concerns about thyroid cancer with glucagon-like peptide 1 (GLP1) receptor agonist use were first raised in the premarketing phase after studies showed increased rates of thyroid C cell tumours in rodents.1While the relevance of these findings to humans is not known, in the United States, product labels of GLP1 receptor agonists include boxed warnings about thyroid cancer and these drugs are contraindicated in patients with a personal or family history of medullary thyroid cancer or multiple endocrine neoplasia type 2.
Subsequent reports based on pharmacovigilance data, post hoc analyses of randomised trials, and findings from observational studies indicate a potential link between GLP1 receptor agonists and thyroid cancer. In two analyses of spontaneous reports from the US Food and Drug Administration’s adverse event reporting system database, the reporting rates of thyroid cancer were 4.7 and 8 times higher, respectively, for GLP1 receptor agonists compared with other diabetes drugs.23In a meta-analysis based on 15 clinical trials that included at least one thyroid cancer outcome event, the odds ratio for thyroid cancer associated with GLP1 receptor agonists was 1.49 (95% confidence interval 0.83 to 2.66). In another meta-analysis based on 35 trials, which also included trials that had zero outcome events (of thyroid cancer), the risk ratio was 1.30 (95% confidence interval 0.86 to 1.97).45A nested case-control study that used US databases reported an odds ratio of 1.46 (95% confidence interval 0.98 to 2.19) for thyroid cancer when the GLP1 receptor agonist exenatide was compared with other diabetes drugs.6More recently, a nested case-control study that used French health insurance data found a hazard ratio of 1.46 (95% confidence interval 1.23 to 1.74) for the association between thyroid cancer and current use of GLP1 receptor agonists compared with non-use of these drugs. Furthermore, the hazard ratio for medullary thyroid cancer was 1.78 (95% confidence interval 1.04 to 3.05) and the reporting ratio in a complementary pharmacovigilance analysis based on the World Health Organization’s database VigiBase was 30.5 (95% confidence interval 25.1 to 37.2) compared with other diabetes drugs.7These data prompted the European Medicines Agency to raise a safety concern and start an investigation,8which concluded that the available evidence does not support a causal association.9We conducted a cohort study using nationwide data from three Scandinavian countries to investigate whether GLP1 receptor agonist use is associated with increased risk of thyroid cancer.
Methods
We conducted a cohort study using healthcare and administrative registers in Denmark (2007-21), Norway (2010-18), and Sweden (2007-21), including population registers, prescription drug registers, national patient registers, and cancer registers, all with nationwide coverage in each country. These registers cover personal and vital status data, capture all prescriptions from all pharmacies nationwide, include diagnostic and procedure data from all outpatient specialist care and hospital admissions, and capture nearly all incident cancers.10
Patients aged 18-84 years were eligible for inclusion when they were new users of a GLP1 receptor agonist or a dipeptidyl peptidase 4 (DPP4) inhibitor; that is, patients who had not used either drug class at any time before cohort entry. Patients were excluded if they had thyroid cancer at any time before cohort entry, any other cancer in the previous year, end stage illness, human immunodeficiency virus infection, drug or alcohol misuse in the previous year, major pancreatic disease, genetic syndromes associated with thyroid cancer, and no healthcare contact in the previous year (to ensure a minimum level of activity in the healthcare system; healthcare contact defined as outpatient specialist care contact, hospital admission, or use of any prescription drug; definitions in etable 1).
The study design was an active-comparator new user design,11with DPP4 inhibitors as the comparator. DPP4 inhibitors were introduced around the same time as the GLP1 receptor agonists, and have typically been used as second line agents. There are no known thyroid cancer signals with DPP4 inhibitors, although the hazard ratio from a recent French nested case-control study might be interpreted as a small increased risk with these drugs (1.10; 95% confidence interval 0.99 to 1.22).7We conducted an additional analysis with sodium-glucose cotransporter 2 (SGLT2) inhibitors as the comparator for GLP1 receptor agonists; the study period for this analysis started in 2013 when SGLT2 inhibitors became available.
The outcome was thyroid cancer (international classification of diseases, 10th revision, code C73) identified from nationwide cancer registers. In additional analyses, we investigated thyroid cancer subtypes, including papillary, follicular, medullary, and other (etable 2).
Statistical analysis
Patients were followed from start of drug use to the outcome event, emigration, death, or end of study period, whichever came first. Drug use was defined according to the observational analogue of intention to treat (ie, from the first filled prescription onwards).
We estimated hazard ratios using Cox regression, with days since start of treatment as the underlying time scale. Differences were considered statistically significant when the confidence intervals did not overlap 1.0. We examined the proportional hazards assumption by using a Wald test of the interaction between treatment status and time. A propensity score weighting method (fine stratification) was applied to control for confounding, which estimated the average treatment effect among treated patients. The propensity score was estimated using logistic regression as the probability of starting a GLP1 receptor agonist versus a DPP4 inhibitor conditional on a number of variables, including sociodemographic characteristics, medical history, other diabetes treatment, and markers of healthcare use, at cohort entry and stratified by country (etable 3). Patients with a propensity score outside the overlapping common distribution of the two groups and the highest 1% and lowest 1% of the common propensity score distribution were excluded (trimming); the propensity score was subsequently re-estimated and patients outside the common distribution of the new propensity score excluded. We then created 100 strata based on the propensity score distribution among patients treated with GLP1 receptor agonists; fine stratification weights were estimated according to Desai and colleagues.12We calculated standardised differences to assess balance of baseline characteristics between groups, with a difference below 10% considered consistent with good balance. All reported rates, rate differences, cumulative incidences, and hazard ratios were adjusted using propensity score weighting.
We conducted several additional analyses. The main analysis estimated the hazard ratio for all available follow-up time, starting on day 1. In additional analyses, we estimated hazard ratios for the following time periods after the start of treatment: days 1-365, days ≥366, and days ≥731. The hazard ratio for the first year of follow-up was used to assess potential bias because an increased risk that is restricted to this period might not be biologically plausible. The hazard ratios for the periods covering days ≥366 and days ≥731 represented analyses with a lag period of one and two years, respectively. Analyses with a lag period are often used in pharmacoepidemiologic studies of cancer outcomes to take cancer latency and detection bias into account.13We also conducted a modified intention-to-treat analysis, mainly to account for the possibility that a proportion of patients could have switched from DPP4 inhibitors to GLP1 receptor agonists during follow-up (a potential thyroid cancer risk with GLP1 receptor agonists could then spill over into the group classified as DPP4 inhibitors according to the intention-to-treat definition for drug use, which could lead to an underestimation of risk of thyroid cancer with GLP1 receptor agonists). In this analysis, a further censoring criterion was applied so that patients were censored from follow-up on the date of switch to, or add on of, the other study drug (ie, follow-up among patients treated with DPP4 inhibitors was censored on the date of a GLP1 receptor agonist prescription, and vice versa).
We conducted additional analyses using an “as-treated” definition for drug use, in which patients were censored at treatment discontinuation or switch to the other study drug. Three distinct as-treated analyses were conducted. In these analyses, treatment duration was based on the estimated number of days covered by filled prescriptions, allowing for up to 30 days, one year, and two years between prescriptions (gap period) and after the last prescription (tail period). The tail period allowed for various degrees of latency until the occurrence of the outcome event after treatment discontinuation. We adjusted for potential dependent censoring of patients who changed treatment status during follow-up with inverse probability of censoring weighting.14Follow-up was defined in discrete time intervals of the same length: 90 days. Censoring and outcome events were identified by the follow-up interval in which they occurred. The censoring weight for a certain patient and time interval was calculated as the inverse of the conditional probability of not being censored in the previous interval. Therefore, patients at risk were assigned weights at each time interval so that they also represented the patients who were censored because of treatment changes, including their distribution of risk factors for the outcome. The final weights were calculated as the product of the baseline propensity score weight and all inverse probability of censoring weights leading up to and including the time interval analysed. The final weights were truncated at the 1st and 99th percentile to avoid using extreme weights. The censoring model was estimated with logistic regression and we adjusted for baseline drug use, the time of censoring (including second and third degree polynomials), and all baseline variables from the propensity score model. We estimated odds ratios of the outcome with weighted, pooled logistic regression where all follow-up intervals were included.15In these analyses, the odds ratio for drug use can be interpreted as a hazard ratio because it is estimated over discrete follow-up intervals. The only covariates that were included in the model were drug use and time interval (including second and third degree polynomials).
The propensity score used in the main analysis did not include calendar year; in an additional analysis, we adjusted the hazard ratio for calendar year in three year intervals in a multivariable model (in addition to propensity score weighting). In further analyses, we included calendar year in the propensity score and estimated calendar time specific propensity scores (with the estimation of the scores stratified by calendar time period in four year intervals and country). The cohort for the main analysis excluded patients with any non-thyroid cancer in the previous year before cohort entry; we conducted an additional analysis in which patients with any non-thyroid cancer at any time before cohort entry were excluded. Finally, to assess the impact of accounting for death as a competing event for thyroid cancer, we conducted post hoc robustness analyses. We estimated weighted cumulative incidence functions in which we accounted for any cause death as a competing event,16in contrast to the main analysis in which we estimated Kaplan-Meier functions where patients were censored at death. We estimated risk ratios at five and 10 years, and to enable assessment of the impact of taking death into account as a competing event, we present risk ratios for analyses in which death was a censoring event and a competing event, respectively. A bootstrap with 2000 resamples was used to estimate 95% confidence intervals.
Patient and public involvement
This study investigated a well defined research question on a potential adverse event of GLP1 receptor agonists that is broadly recognised for its potential impact on patients, and that has been raised as a drug safety concern by drug regulatory agencies. While we support the involvement of patients and the public, there was no funding available for such undertakings in this project and no patients were involved in setting the research question, or in the design, conduct, or interpretation of the study. However, one impetus for the study was clinical encounters in which patients express concerns over drug safety. The study is based on anonymised nationwide register data, and while we will disseminate the results to the public through media, no dissemination of results directly to study participants is planned.
Results
After exclusions and propensity score trimming, the cohort consisted of 145 410 patients who started GLP1 receptor agonist treatment and 291 667 patients who started DPP4 inhibitor treatment (fig 1). The mean age of the GLP1 receptor agonist group was 57.5 years (standard deviation 12.6 years), and 53.2% were men. The GLP1 receptor agonist and DPP4 inhibitor groups were well balanced on all baseline characteristics after propensity score weighting (fine stratification;table 1).
The most common individual GLP1 receptor agonist was liraglutide (57.3%), followed by semaglutide (32.9%), dulaglutide (4.9%), exenatide (4.1%), and lixisenatide (0.9%). The mean follow-up time among patients who started GLP1 receptor agonists was 3.9 (standard deviation 3.5) years; 25% of patients were followed for 6.1 years or longer. The mean follow-up time varied by specific drug: liraglutide 5.2 (3.4) years, semaglutide 1.1 (0.8) years, dulaglutide 2.9 (1.8) years, exenatide 9.1 (3.9) years, and lixisenatide 4.0 (1.7) years. The mean follow-up time among patients who started DPP4 inhibitors was 5.4 (3.5) years. Assessing the proportional hazards assumption, there was no significant interaction between follow-up time and use of GLP1 receptor agonists (P=0.82).
Figure 2shows the cumulative incidence of thyroid cancer among the GLP1 receptor agonist and DPP4 inhibitor groups. During follow-up, 76 patients with incident thyroid cancer were identified in the GLP1 receptor agonist group and 184 in the DPP4 inhibitor group; the incidence rates were 1.33 and 1.46 per 10 000 person years, respectively. The hazard ratio was 0.93 (95% confidence interval 0.66 to 1.31;table 2).
Papillary thyroid cancer was the most common thyroid cancer subtype, followed by follicular, medullary, and other types. No significant increases in risk of any of the thyroid cancer subtypes were identified with GLP1 receptor agonist use, although the number of events was relatively small and the estimates for subtypes other than papillary were imprecise (table 2).
Table 3shows additional analyses of GLP1 receptor agonist use and risk of thyroid cancer. For the analyses with different lag periods after starting treatment, the hazard ratio for thyroid cancer was 0.83 (95% confidence interval 0.56 to 1.22) with a one year lag and 0.90 (0.58 to 1.38) with a two year lag. The hazard ratio for the first year after starting treatment was 1.47 (0.74 to 2.93). Similar to the main analysis, no significant associations between GLP1 receptor agonists versus DPP4 inhibitors and risk of thyroid cancer were observed in additional analyses which used an alternative modified intention-to-treat definition for drug use; included additional adjustment for calendar year; included calendar year in the propensity score; applied calendar time specific propensity scores; and excluded patients with any previous cancer at any time before cohort entry.
In additional analyses that used an alternative as-treated definition for drug use, the mean follow-up time among patients who started GLP1 receptor agonists was 1.8 years in the analysis with 90 day gap and tail periods, 2.6 years in the analysis with one year gap and tail periods, and 2.9 years in the analysis with two year gap and tail periods. The hazard ratios for the full follow-up period in these analyses was 1.37 (95% confidence interval 0.84 to 2.23), 1.04 (0.69 to 1.57), and 0.99 (0.67 to 1.46), respectively. The pattern of association reflected that of the main intention-to-treat analysis, with point estimates less than 1.0 in the as-treated analyses with a one year lag and with increased hazard ratios in the as-treated analyses restricted to the first year after starting treatment. In post hoc robustness analyses assessing the impact of competing risk of death, the risk ratios for thyroid cancer at five years of follow-up were 0.80 (0.58 to 1.14) with death as a censoring event and 0.82 (0.59 to 1.16) with death as a competing event. At 10 years of follow-up, the risk ratios for thyroid cancer were 1.00 (0.72 to 1.37) with death as a censoring event and 1.01 (0.74 to 1.39) with death as a competing event.
For the additional analysis that compared GLP1 receptor agonists with SGLT2 inhibitors, the flowchart for inclusion and the baseline characteristics are shown in efigure 1 and etable 4, respectively. This analysis included a cohort of 111 744 patients who started GLP1 receptor agonist treatment and 148 179 patients who started SGLT2 inhibitor treatment. GLP1 receptor agonist use was not associated with increased risk of thyroid cancer compared with SGLT2 inhibitor use (hazard ratio 1.16, 95% confidence interval 0.65 to 2.05;table 3, efigure 2).
Discussion
Principal findings
In this cohort study using nationwide data from three countries, GLP1 receptor agonist use was not associated with a substantially increased risk of thyroid cancer over a mean follow-up of 3.9 years. The study was based on more than 145 000 patients who started GLP1 receptor agonist treatment and had high statistical precision; given the upper limit of the confidence interval, the findings are incompatible with an increased relative risk of thyroid cancer of more than 31%. In absolute terms, this translates to no more than 0.36 excess events per 10 000 person years, which should be interpreted against the background incidence of 1.46 per 10 000 person years in the comparator group in the study population. Findings were neutral, but less precise for specific subtypes of thyroid cancer, including medullary thyroid cancer, and robust in several additional analyses, including when an alternative comparator group was used. However, the study cannot exclude a small increase in risk.
Comparison with other studies
Although pharmacovigilance studies have found increased reporting rates for thyroid cancer with GLP1 receptor agonists,23disproportionality analyses are designed to detect potential safety signals but are not intended to make causal conclusions. Given that thyroid cancer is mentioned in the product label as a potential adverse event, spontaneous reporting might have been driven by physician and public awareness. While meta-analyses of randomised trials have generated point estimates >1, the confidence intervals have been wide owing to limited sample size and short follow-up time, which limits the assessment of rare cancer events.45The largest meta-analysis was based on 43 124 patients exposed to GLP1 receptor agonists in trials with a median duration of only 75 weeks, and the confidence interval was consistent with up to twofold increase in risk. These meta-analyses represent post hoc analyses of secondary safety events and individual trials have not included thyroid cancer as a primary outcome.
A recent nested case-control study based on a French national health insurance system database reported a significant association between thyroid cancer and GLP1 receptor agonist use (hazard ratio 1.46, 95% confidence interval 1.23 to 1.74).7However, the comparator in that study was non-use of GLP1 receptor agonists, rather than an active comparator; exposure groups might have been misaligned on important baseline characteristics, potentially leading to confounding. Point estimates in that study were of similar magnitude for duration of drug use <1 year, 1-3 years, and >3 years.7Given that a potential effect of GLP1 receptor agonists on thyroid cancer is unlikely to emerge after short term use, these findings might indicate the presence of confounding. An alternative explanation for the increased risk observed in the French study, emerging early and staying at similar magnitude with increasing duration of use, could be detection bias.17Our additional analysis indicated a nominally increased risk restricted to the first year after starting treatment, which might be consistent with an increased detection of thyroid cancer among patients using GLP1 receptor agonists. In contrast to the French study, a recent study based on nationwide data from Korea used an active-comparator new user cohort design and propensity score weighting to control for confounding. Applying a one year lag period, the hazard ratio for thyroid cancer comparing GLP1 receptor agonists with SGLT2 inhibitors was 0.98 (95% confidence interval 0.62 to 1.53).18
Strengths and limitations
This study was based on a large unselected study population from routine clinical practice, which was derived from three Scandinavian countries that have universal access to tax funded healthcare, and high quality data with nationwide coverage, including dedicated cancer registers. Furthermore, we used robust pharmacoepidemiologic methods. These strengths substantiate the generalisability of results, reduce concerns about selection and information biases, and support the internal validity of the study. Therefore, this study adds to the available evidence about GLP1 receptor agonist use and risk of thyroid cancer, and supports the conclusion of a recent European Medicines Agency investigation that the available evidence does not support a causal association between GLP1 receptor agonist use and thyroid cancer.9
In our study, drug use in the main analysis was defined according to the observational analogue of intention to treat (ie, from the first filled prescription onwards) to provide the least conservative definition. Because cancer takes time to develop and manifest clinically, and because the nature and timing of a potential association between GLP1 receptor agonists and thyroid cancer are not known, the definition allowed identification of thyroid cancer events while on treatment and after treatment had stopped. Additional analyses were conducted with a modified intention-to-treat definition for drug use so that patients were censored from follow-up on the date of switch to, or add on of, the other study drug (ie, follow-up among patients treated with DPP4 inhibitors was censored on the date of a GLP1 receptor agonist prescription, and vice versa). This approach mainly accounted for the possibility that a proportion of patients could have switched from DPP4 inhibitors to GLP1 receptor agonists during follow-up (a potential thyroid cancer risk with GLP1 receptor agonists could spill over into the group classified as receiving DPP4 inhibitors according to the intention-to-treat definition for drug use, which might lead to an underestimation of thyroid cancer risk with GLP1 receptor agonists). However, the estimate of the modified intention-to-treat analysis was not substantially different from that of the main analysis. We conducted additional analyses with an as-treated definition for drug use, which assessed the risk of thyroid cancer while on treatment with GLP1 receptor agonists. Similar to the pattern observed in the main intention-to-treat analysis, a nominally increased risk restricted to the first year of treatment was observed in the as-treated analyses, whereas the point estimates were <1.0 in as-treated analyses with a one year lag. The findings of the as-treated analysis are consistent with an increased detection of thyroid cancer in the immediate period after starting GLP1 receptor agonist treatment, or other bias, given that a risk increase restricted to the first year of treatment is unlikely to be biologically plausible.
The study has limitations. Although the mean follow-up among GLP1 receptor agonist users was 3.9 years, because of cancer latency,13extended follow-up might be needed to detect an increased risk. However, 25% of the study participants were followed for 6.1 years or longer and the cumulative incidence curves showed no signs of an emerging risk increase with up to 10 years of follow-up. We investigated GLP1 receptor agonists at drug class level and liraglutide and semaglutide were the most commonly used individual drugs in the study. An avenue for future work is to assess thyroid cancer risk by individual GLP1 receptor agonist. Although we analysed thyroid cancer subtypes and observed neutral results for each of the subtypes including medullary thyroid cancer, the number of events in this analysis was relatively small and the estimates for subtypes other than papillary were imprecise. Similarly, we could not assess smaller subgroups of patients, including those with previous cancers or conditions associated with increased risk of thyroid cancer.
Potential limitations with real world studies of drug effects include the possibility of confounding and time related biases. To minimise these risks, we used an active-comparator new user study design.11This design aligns patients at a uniform point in time to start follow-up (starting drug treatment), ensures correct temporality for covariate and drug use assessment, and balances patients on baseline characteristics by selecting a comparator drug that is used for the same indication and at similar stages of disease. We chose DPP4 inhibitors as the primary active comparator. These drugs were introduced at around the same time as the GLP1 receptor agonists and are similarly used as second and third line drugs for diabetes; therefore, they probably represent the most suitable active comparator over the study period overall. We also conducted an additional analysis with SGLT2 inhibitors as the comparator. Similar to GLP1 receptor agonists, recent guidelines recommend these drugs for cardiovascular risk reduction among patients with diabetes, and the use of both drug classes has increased in recent years.1920
Finally, we controlled for a number of baseline characteristics, including sociodemographic characteristics, medical history, other diabetes treatment, and markers of healthcare use, through propensity score weighting. Despite these measures, unmeasured or residual confounding cannot be ruled out given the observational design of this study. Of specific concern would be factors that obscure a true increased risk of thyroid cancer with GLP1 receptor agonists, thereby influencing the interpretation of the study. Such factors would have to be protective against thyroid cancer and be more prevalent in the GLP1 receptor agonist group than in the comparator groups, or increase the risk of thyroid cancer and be less prevalent in the GLP1 receptor agonist group than in the comparator groups. The only known factor that is potentially associated with a reduced risk of thyroid cancer is smoking; however, in our previous works using nationwide Scandinavian data, the prevalence of current smoking has been similar among patients treated with GLP1 receptor agonists and those treated with DPP4 inhibitors and SGLT2 inhibitors, respectively.2122Factors that are associated with increased risk of thyroid cancer include female sex, age, ethnicity, obesity, taller height, family history of thyroid cancer, ionising radiation during childhood, iodine intake, and potentially diabetes duration.23242526While information on family history, ionising radiation, iodine intake and height were not available for this study, we adjusted for sex, age, place of birth, and obesity diagnosis. Before propensity score weighting, the proportion of patients with an obesity diagnosis was more than double in the GLP1 receptor agonist group than in both comparator groups. Although body mass index data were not available for this cohort, our findings suggest that any unmeasured obesity is likely to have been more prevalent in the GLP1 receptor agonist group, with any associated bias potentially moving the estimate towards increased risk of thyroid cancer.
Conclusions
In conclusion, this large cohort study found that GLP1 receptor agonist treatment was not associated with a substantially increased risk of thyroid cancer over a mean follow-up of 3.9 years. In the main analysis, which compared GLP1 receptor agonists with DPP4 inhibitors, the upper limit of the confidence interval was consistent with no more than a 31% increase in relative risk.
During development of glucagon-like peptide 1 (GLP1) receptor agonists, studies in rodents showed increased rates of thyroid tumours
Subsequent reports based on pharmacovigilance data, post hoc analyses of randomised trials, and findings from observational studies indicate a potential link between GLP1 receptor agonists and thyroid cancer, although not conclusively
This cohort study used nationwide register data from Sweden, Denmark, and Norway to investigate the risk of thyroid cancer among patients treated with GLP1 receptor agonists
Over a mean follow-up of 3.9 years, GLP1 receptor agonist treatment was not associated with a substantially increased risk of thyroid cancer compared with dipeptidyl peptidase 4 (DPP4) inhibitors, and in an additional analysis, sodium-glucose cotransporter 2 inhibitors
Although small risk increases cannot be excluded, in the main analysis comparing GLP1 receptor agonists with DPP4 inhibitors, the upper limit of the confidence interval was consistent with no more than a 31% increase in relative risk
","Objective: To investigate whether use of glucagon-like peptide 1 (GLP1) receptor agonists is associated with increased risk of thyroid cancer.
Design: Scandinavian cohort study.
Setting: Denmark, Norway, and Sweden, 2007-21.
Participants: Patients who started GLP1 receptor agonist treatment were compared with patients who started dipeptidyl peptidase 4 (DPP4) inhibitor treatment, and in an additional analysis, patients who started sodium-glucose cotransporter 2 (SGLT2) inhibitor treatment.
Main outcome measures: Thyroid cancer identified from nationwide cancer registers. An active-comparator new user study design was used to minimise risks of confounding and time related biases from using real world studies of drug effects. Cox regression was used to estimate hazard ratios, controlling for potential confounders with propensity score weighting.
Results: The mean follow-up time was 3.9 years (standard deviation 3.5 years) in the GLP1 receptor agonist group and 5.4 years (standard deviation 3.5 years) in the DPP4 inhibitor group. 76 of 145 410 patients (incidence rate 1.33 events per 10 000 person years) treated with GLP1 receptor agonists and 184 of 291 667 patients (incidence rate 1.46 events per 10 000 person years) treated with DPP4 inhibitors developed thyroid cancer. GLP1 receptor agonist use was not associated with increased risk of thyroid cancer (hazard ratio 0.93, 95% confidence interval 0.66 to 1.31; rate difference −0.13, 95% confidence interval −0.61 to 0.36 events per 10 000 person years). The hazard ratio for medullary thyroid cancer was 1.19 (0.37 to 3.86). In the additional analysis comparing the GLP1 receptor agonist group with the SGLT2 inhibitor group, the hazard ratio for thyroid cancer was 1.16 (0.65 to 2.05).
Conclusions: In this large cohort study using nationwide data from three countries, GLP1 receptor agonist use was not associated with a substantially increased risk of thyroid cancer over a mean follow-up of 3.9 years. In the main analysis comparing GLP1 receptor agonists with DPP4 inhibitors, the upper limit of the confidence interval was consistent with no more than a 31% increase in relative risk.
"
Use of progestogens and the risk of intracranial meningioma,"Introduction
Meningiomas account for 40% of primary tumours of the central nervous system.12The incidence of meningioma in the United States is 9.5 per 100 000 person years.2Meningiomas are mostly slow growing, histologically benign tumours but can nevertheless compress adjacent brain tissue and thus patients may require surgical decompression.3The incidence of meningiomas increases with age, rising sharply after the age of 65 years. Conversely, meningiomas are rare before the age of 35. Other recognised risk factors for meningioma are being female, intracranial exposure to ionising radiation, neurofibromatosis type 22, and, as shown only recently, prolonged use (≥one year) to high doses of three potent progestogens: cyproterone acetate,45chlormadinone acetate,4and nomegestrol acetate.4
The link between female sexual hormones, in particular progesterone, and intracranial meningioma is biologically plausible.6Progesterone receptors are present in more than 60% of meningiomas7and the volume of these tumours has been observed to increase during pregnancy and to decrease post partum.8However, previous pregnancy does not appear to be an unequivocal risk factor for meningioma.9Studies have also shown a link, albeit a weak one, between breast cancer and meningiomas.10
No significant association between exogenous female hormones and risk of meningioma has been shown to date for hormonal contraceptives (either combined or progestogen only pills).1112Additionally, data for hormone replacement treatment for menopause are contradictory. Several studies have shown a slight excess risk of meningioma associated with the use of hormone replacement treatment for menopause,1113whereas others have reported no deleterious effects of these molecules.14By contrast, the excess risk of meningioma observed with the use of high doses of cyproterone acetate among cis women, men, and trans women has been shown to be very high51516and somewhat lower, but still substantial, for chlormadinone acetate and nomegestrol acetate.4Discontinuation of each of these three progestogens generally leads to a reduction in meningioma volume,1718which avoids the need for surgery and its associated risk of complications for most patients.
Whether progestogens other than these three oral progestogens at high doses have a similar effect depending on their route of administration is still unknown. Our study aimed to assess the real-life risk of intracranial meningioma associated with the use of progestogens from an extensive list (progesterone, hydroxyprogesterone, dydrogesterone, medrogestone, medroxyprogesterone acetate, promegestone, dienogest, and levonorgestrel intrauterine systems) with different routes of administration (oral, percutaneous, intravaginal, intramuscular, and intrauterine). Although some of the progestogens studied are used in France (promegestone) or in only a few countries (medrogestone), others are widely used worldwide in various doses and for various indications (progesterone, levonorgestrel, hydroxyprogesterone, medroxyprogesterone) (supplementary table A). Certain progestogens may also be risky at some doses when used over a long period of time, but not at lower doses or when used for a short period of time. Our secondary objectives were to describe the characteristics of the women who were in the cases group (age, grade, and anatomical location of the meningiomas) and to approximate the number of surgically treated meningiomas attributable to the use of the concerned progestogens.
Methods
Study design and data source
This observational population based study used data derived from the French national health data system (Système National des Données de Santé(SNDS)). Given the analysis of multiple exposure situations (different exposure definitions and lookback periods) in our study, we opted for a case-control design rather than a cohort study, thus including long term users of the considered medications.19
The SNDS database contains information on all health spending reimbursements for over 99% of the population residing in France and is linked to the French hospital discharge database.20SNDS is currently one of the largest healthcare databases in the world and is widely used in pharmacoepidemiological studies.4521222324
Definition of cases and selection of controls
The eligible cases in this study were women residing in France of all ages who underwent surgery for intracranial meningioma between 1 January 2009, and 31 December 2018. For each case, the start date of the corresponding admission to hospital marked the index date. Women with a pregnancy beginning in the two years before the index date were excluded from the study (pregnancies were defined as those that had resulted in childbirth or medical termination of the pregnancy after 22 weeks of amenorrhoea).
Surgery for intracranial meningioma was defined by the simultaneous combination of the following diagnoses and procedures recorded for the same hospital stay: a meningeal tumour (codes D32, D42, or C70 according to the 10threvision of the International Classification of Diseases (ICD-10)) coded as the main diagnosis of the admission to hospital and an intracranial surgery act (supplementary table B). These codes have already been used in our previous studies.45
Five women in the control group were randomly matched to each woman in the case group for the year of birth and area of residence (“département”, a French geographical subdivision, n=101). Matching was based on the risk set sampling approach.25The traceability of the controls in the SNDS was ensured by selecting only women who had had at least one service reimbursed in the calendar year before the index date and the two to three calendar years preceding the index date. This criterion was also applied to the selection of cases.
For analyses relating to intrauterine systems, subsets of these cases and the matched controls were considered to ensure sufficiently long lookback periods. For the hormonal intrauterine systems containing 52 mg levonorgestrel and copper intrauterine devices, the cases and controls from the years 2011 to 2018 were retained. For the hormonal intrauterine systems containing 13.5 mg levonorgestrel, the inclusion period was restricted to 2017 to 2018 (start of commercialisation in France in 2013).
Definition of exposure
Exposure to the progestogen of interest was defined according to WHO’s anatomical, therapeutic, and chemical (ATC) classification. The list included progesterone (oral and intravaginal: 100, 200 mg (ATC code G03DA04); percutaneous: 25 mg per bar (G03DA04)), dydrogesterone (10 mg, or in association with oestrogen: 5 or 10 mg (G03DB01, G03FA14, G03FB08)), hydroxyprogesterone (500 mg (G03DA03)), medrogestone (5 mg (G03DB03)), promegestone (0.125, 0.25, or 0.5 mg (G03DB07)), medroxyprogesterone acetate (injectable contraceptive, 150 mg/3 mL (G03AC06, L02AB02 partially)), dienogest (in association with oestrogen, 2 mg (G03FA15)), levonorgestrel (52 mg intrauterine systems (G02BA03); 13.5 mg intrauterine systems (G02BA03)) (supplementary tables C and D). As drospirenone, which is a spironolactone derivative, is not reimbursed in France, we were unable to access data concerning its use. We therefore chose to study the use of spironolactone (25, 50, and 75 mg), even though its indications may be very different. The code used to identify spironolactone was C03DA01. The indications for these various progestogens in France are available intable 1.
For oral, intravaginal, percutaneous, or intramuscular progestogens, exposure was defined as at least one dispensation of the progestogen of interest in the 365 days before the index date. For intrauterine progestogens, a dispensation was sought within three years before the index date for levonorgestrel 13.5 mg (as the duration of efficacy of this intrauterine system is three years before any change or withdrawal of the device) and within five years before the index date for levonorgestrel 52 mg intrauterine systems (duration of contraceptive efficacy of five to six years according to current recommendations during the study period).
Exposure was described by three modes for each progestogen as follows: 1) exposure to the progestogen concerned, 2) exposure during the three years preceding the index date to at least one of the three high dose progestogens known to increase the risk of meningioma (ie, chlormadinone acetate, nomegestrol acetate, and cyproterone acetate), and 3) absence of exposure to the progestogen considered or to the three high dose progestogens (the reference for the analyses).
Definition of covariates
The description of sociodemographic and medical characteristics included age, area of residence, existence of neurofibromatosis type 2 (ICD-10 code Q85.1), and, for cases only, the year of surgery, anatomical site (anterior, middle, or posterior base of the skull, convexity, falx and tentorium, others; supplementary table C), and grade of severity of the meningioma (according to WHO’s classification1: benign, malignant, or atypical, supplementary table E).
Adjuvant radiotherapy was also sought from three months before the index date to six months after (supplementary table F). Additionally, all causes mortality at two and five years after the index date was assessed in cases, as well as the use of antiepileptic drugs in the third year after the index date (supplementary table G).
Statistical analysis
Logistic regression models conditioned on matched pairs were used to estimate odds ratios and their 95% confidence intervals (CIs) for the association between exposure to the progestogens of interest and meningioma (odds ratio of exposure relative to non-exposure). Additionally, the effect of history of neurofibromatosis type 2 on the risk of meningioma was estimated, as well as the effect of chlormadinone acetate, nomegestrol acetate, and cyproterone acetate exposure, all serving as positive controls for exposure to validate our results. In parallel, exposure to a copper intrauterine device was used as a negative control for exposure (codes in supplementary table H).
The risk of meningioma associated with progestogen use was also estimated for each oral, percutaneous, intravaginal, and intramuscular progestogen according to the duration of use: short term (at least one dispensation in the year before the index date but no dispensation in the second year before the index date) and prolonged use (at least one dispensation in the year before the index date and at least one dispensation in the second year before the index date).
The population attributable fraction was approximated from the odds ratio obtained for each progestogen. The formula used was as follows: population attributable fraction=pc(1-1/odds ratio), where pcis the prevalence of the use of the progestogen concerned (isolated exposure) among the cases.26Lastly, sensitivity analyses were performed. Analyses were stratified for age (<35 years, 35-44 years, 45-54 years, 55-64 years, and ≥65 years) and for the location and grade of severity of the tumours whenever a positive association was found between exposure to the considered progestogen and meningioma surgery.
Data were analysed using SAS software version 9.4 (SAS Institute Inc). A P value of less than 0.05 was considered statistically significant (two tailed tests).
Ethics
The present study was authorised by decree 2016–1871 on 26 December 2016.27As an authorised permanent user of the SNDS, the author’s team was exempt from approval from the institutional review board. This work was declared, before implementation, on the register of studies of the EPI-PHARE Scientific Interest Group with register reference T-2023-01-437.
Patient and public involvement
The list of progestogens of interest (supplementary table B) was drawn up in consultation with a temporary scientific advisory board comprised of representatives of the French National Agency for Medicines and Health Products Safety, patient organisations, and healthcare professionals (neurosurgery, endocrinology, gynaecology, and general medicine).
Results
Description of cases and controls
In total, 108 366 women were included in the study during the inclusion period of 2009 to 2018, consisting of 18 061 women in the case group were matched with 90 305 in the control group (fig 1).
Among them, 15 162 cases and 75 810 controls were retained for the analyses of intrauterine systems and copper intrauterine devices using 52 mg of levonorgestrel (restricted inclusion period: 2011 to 2018) (supplementary figure A) and 4048 cases and their 20 240 controls for the analysis of intrauterine systems of 13.5 mg of levonorgestrel (2017-18) (supplementary figure B). Descriptions of cases and controls for the analyses of intrauterine devices are detailed in supplementary I and J.
The mean age of all women was 57.6 years (standard deviation 12.8 years). The most highly represented age groups were 45-54 (26.7%), 55-64 (26.4%), and 65-74 (21.5%) years (table 2).
The number of cases steadily increased from 1329 in 2009 to 2069 in 2018. Meningiomas requiring surgery were most frequently located at the base of the skull (a total of 10 046/18 061 cases (55.6%); anterior skull base: 3979/18 061 (22.0%), middle: 3911/18 061 (21.7%), posterior: 2156/18 061 (11.9%)), followed by the convexity (6468/18 061 (35.8%)). Concerning tumour grade, most meningioma cases were benign (16 662/18 061, 92.3%) and 1047/18 061 (5.8%) were classified as atypical and 352/18 061 (1.9%) as malignant. Among cases, 28.8% (5202/18 061) of women used antiepileptic drugs three years after the index date of surgery. Mortality was also higher among cases than controls: 502 cases/18 061 (2.8%) died within two years (v1.2% of controls) and 951/18 061 (5.3%) within five years (v3.4% of controls). Mortality was higher for the cases with malignant tumours, 12.5% of whom died within two years and 20.7% within five.
The comparison of the cases and controls in the subsets used to analyse hormonal intrauterine systems is included the supplementary data (supplementary tables I and J).
Progestogens (others than intrauterine)
Among the 18 061 women admitted to hospital for meningioma surgery between 2009 and 2018, 329 (1.8%) had used oral or intravaginal progesterone, 90 (0.5%) percutaneous progesterone, zero hydroxyprogesterone, 156 (0.9%) dydrogesterone, 42 (0.2%) medrogestone, nine (<0.1%) medroxyprogesterone acetate, 83 (0.5%) promegestone, three (<0.1%) dienogest, and 264 (1.5%) spironolactone (table 3, supplementary figure C). These numbers excluded 2999 women who had been exposed to cyproterone acetate, nomegestrol acetate, or chlormadinone acetate, or a combination, within the previous three years (among these 2999 women, 68 had also been exposed to oral progesterone, 47 to percutaneous progesterone, 0 to hydroxyprogesterone, 43 to dydrogesterone, 10 to medrogestone, 0 to medroxyprogesterone acetate, 17 to promegestone, 1 to dienogest, and 56 to spironolactone). The median cumulative doses of progestogens for cases and exposed controls are shown in supplementary table K.
No significant association with an increased risk of intracranial meningioma surgery was noted with exposure to oral or intravaginal progesterone (odds ratio of 0.88 (95% CI 0.78 to 0.99)) or percutaneous progesterone (1.11 (0.89 to 1.40)), dydrogesterone (0.96 (0.81 to 1.14)), or spironolactone (0.95 (0.84 to 1.09)) (table 3, supplementary figure C). Exposure to dienogest was rare, with only 14 women who were exposed (3/18 061 among cases and 11/90 305 among controls) and, consequently, the estimated odds ratio had a very large confidence interval (1.48 (0.41 to 5.35)). Additionally, we could not assess the odds ratio concerning hydroxyprogesterone because no exposed cases were found (fig 2).
By contrast, an excess risk of meningioma was associated with the use of medrogestone (3.49 (2.38 to 5.10)), medroxyprogesterone acetate (5.55 (2.27 to 13.56)), and promegestone (2.39 (1.85 to 3.09)). As expected, an excess risk of meningioma for women with positive control exposure neurofibromatosis type 2 (18.93 (10.50 to 34.11)), as well as those exposed to chlormadinone acetate (3.87 (3.48 to 4.30)), nomegestrol acetate (4.93 (4.50 to 5.41)), and cyproterone acetate (19.21 (16.61 to 22.22)) was also noted (fig 2).
The duration of exposure to medrogestone, medroxyprogesterone acetate, promegestone, chlormadinone, nomegestrol, and cyproterone acetate for exposed cases and controls is presented in supplementary table L. The results show that three quarters of the women in the cases group who had been exposed for more than a year had been exposed for more than three years. As for medrogestone, medroxyprogesterone acetate, and promegestone, the excess risk associated with prolonged use was higher than that measured for short term and prolonged exposure combined. Specifically, prolonged use of promegestone had an odds ratio of 2.74 (2.04 to 3.67) (versus 2.39 for all durations of exposure) and short term use an odds ratio of 1.62 (0.95 to 2.76). For prolonged use of medrogestone, the odds ratio was 4.08 (2.72 to 6.10) (versus 3.49 for all durations of exposure combined), and for medroxyprogesterone acetate, the odds ratio was 5.62 (2.19 to 14.42). No significant association was reported for either short or prolonged periods of use for any of the other progestogens studied.
Meningiomas before age 45 years were rare in cases of exposure to medrogestone (n=3/42), medroxyprogesterone acetate (n=3/9), or promegestone (n=10/83), and only one (medroxyprogesterone) was observed before the age of 35.
Concerning medrogestone, the most frequent locations of meningiomas in exposed cases were the base of the skull (n=21/42; 13 in the middle) and the convexity (n=19/42) (supplementary tables M, N and O). The excess risk of meningioma for the middle of the base of the skull was particularly high (odds ratio 8.30 (95% CI 3.70 to 18.63)). Additionally, the estimated excess risk among women aged 45-54 years was slightly higher than that in the main analysis (4.53 (2.73 to 7.53)v3.49 (2.38 to 5.10)).
In women in the cases group who were exposed to promegestone, meningiomas were preferentially located at the front of the base of the skull (n=25/83), the convexity (n=25/83), and the middle of the base of the skull (n=22/83). The excess risk of meningioma linked to promegestone use was slightly higher in the group who were older than 65 years (odds ratio 3.21 (95% CI 1.39 to 7.43)) and for meningiomas located at the front or middle of the base of the skull (3.15 (1.95 to 5.10) and 3.03 (1.82 to 5.02), respectively).
We found no malignant grade tumours among cases exposed to medrogestone, medroxyprogesterone acetate, or promegestone (for information, the same analyses were carried out for chlormadinone acetate, nomegestrol acetate, and cyproterone acetate in supplementary table N).
Levonorgestrel intrauterine systems
In total, 566/15 162 users of hormonal levonorgestrel 52 mg were among the cases with meningioma surgery between 2011 and 2018 (3.7%) (table 3). For the intrauterine systems with 13.5 mg of levonorgestrel, 10 of 4048 users were reported among the cases from 2017 and 2018 (0.2% of all cases). Again, women who had been exposed to cyproterone acetate, nomegestrol acetate, or cyproterone acetate, or a combination, within the previous three years were not counted (among them, 95 were exposed to the intrauterine systems of 52 mg levonorgestrel and three to intrauterine systems of 13.5 mg levonorgestrel).
No excess risk of meningioma was reported with the use of hormonal intrauterine systems containing 52 mg (odds ratio 0.94 (95% CI 0.86 to 1.04)) or 13.5 mg (1.39 (0.70 to 2.77)) of levonorgestrel (fig 2).
Exposure to copper intrauterine devices, used as a negative control for exposure in this study, had an odds ratio of 1.13 (1.01 to 1.25).
Attributable cases
The population attributable fractions, which are relative to the observed overall number of surgically treated intracranial meningiomas, were 0.17% for exposure to medrogestone, 0.04% for medroxyprogesterone acetate, and 0.27% for promegestone. For comparison, they were calculated as 2.58% for chlormadinone acetate, 4.08% for nomegestrol acetate, and 4.68% for cyproterone acetate. The numbers for the attributable cases are presented in supplementary figure D.
Discussion
Principal findings
Although the risk of meningioma was already known for three progestogens, this study is the first to assess the risk associated with progestogens that are much more widely used for multiple indications, such as contraception.
This population based study shows an association between the prolonged use of medrogestone (5 mg), medroxyprogesterone acetate injection (150 mg), and promegestone (0.125, 0.25, 0.5 mg) and a risk of intracranial meningioma requiring surgery. No such risk was reported for less than one year of use of these progestogens. However, we found no excess risk of meningioma with the use of progesterone (25, 100, 200 mg; oral, intravaginal, percutaneous), dydrogesterone (10 mg, combined with oestrogen: 5, 10 mg), or spironolactone (25, 50, 75 mg), neither with short term nor prolonged use, and with the use of levonorgestrel intrauterine systems (13.5, 52 mg). A small number of women were exposed to dienogest (2 mg, in association with oestrogen) and hydroxyprogesterone (500 mg), therefore we cannot draw any conclusions concerning the association between use of these progestogens and the risk of meningioma.
No malignant meningiomas were noted for women exposed to medrogestone, medroxyprogesterone acetate, or promegestone. Moreover, the number of cases of surgically treated intracranial meningioma attributable to use of these progestogens was much lower than the number of cases attributable to the intake of chlormadinone acetate, nomegestrol acetate, and, in particular, cyproterone acetate. This finding is explained by both a lower excess risk of meningioma (for medrogestone and promegestone) and lower rates of use in France (particularly low for medroxyprogesterone acetate, with less than 5000 women exposed each quarter during the inclusion period of the study of 2009-18).
Specific considerations on meningiomas
Meningioma is a predominantly benign tumour. Between 2011 and 2015, 80.5% of the meningiomas diagnosed in the United States were grade 1, 17.7% grade 2, and 1.7% grade 3.1Even in the absence of malignancy, meningiomas can cause potentially disabling symptoms. In such cases, first line treatment is surgery, even for the oldest patients, entailing a risk of complications and morbidity.2829
Age is an important factor both for the indication of progestogens and for considering intracranial surgery. In our study, the mean age of women in the cases group was 57.6 years. Medrogestone, medroxyprogesterone acetate, and promegestone can be used both by women of childbearing age and by premenopausal and postmenopausal women. In our study, only one user of these progestogens who had undergone meningioma surgery was younger than 35 years (medroxyprogesterone).
Postoperative complications are not uncommon for meningioma surgery. Depending on the exact location of meningiomas, the surgical risk varies but surgery may have severe neurological consequences due to the immediate proximity of highly functional cortical area and critical neurovascular structures. Cognitive function tends to improve after surgery for meningioma,3031but several studies have suggested a potential for postoperative anxiety and depression and a high intake of antidepressants and sedatives in the medium term,3233although other studies have reported conflicting findings for depression.34Seizures are also a possible short term complication of surgery,35leading to a need to take antiepileptic drugs in the years following the operation. In our study, almost three in 10 women (28.8% of cases) were using antiepileptic drugs three years after the operation, which was consistent with previously published findings.36Additionally, results showed that progestin related meningiomas tend to occur more frequently at the skull base and that surgery for lesions in this location is much more challenging. The recent evidence supporting stabilisation or regression of meningiomas after stopping chlormadinone acetate, nomegestrol acetate, and cyproterone acetate has reduced the surgical indications for these patients, thus avoiding potential complications.1718A recent report showed that although the tissue portion of the meningioma most often regresses in size, the hyperostosis associated with meningiomas further increases, which may require surgical intervention, not for oncological purposes but only for decompression of the structures nerves and relief of symptoms.37
Use of the studied progestogens in France and worldwide
Medrogestone is indicated in France for the treatment of menstrual cycle disorders and luteal insufficiency (eg, dysmenorrhea, functional menorrhagia or fibroid-related menorrhagia, premenstrual syndrome, and irregular cycles), endometriosis, mastodynia, and hormone replacement therapy for menopause. In the United States, medrogestone has never been approved by the US Food and Drug Administration. Outside of France, this molecule is also used in Germany, in combination with oestrogen (0.3 mg/5 mg, 0.6 mg/2 mg, 0.6 mg/5 mg).38The use of medrogestone increased significantly in France in 2019, notably as a result of postponements in the prescription of chlormadinone acetate, nomegestrol acetate, and cyproterone acetate, following the French and European recommendations to reduce the risk of meningioma attributable to these progestogens in 2018 and 2019.3940As therapeutic alternatives have not shown an increased risk of meningioma, switching from products that notoriously increase this risk to medrogestone should be reconsidered.
Worldwide, in 2019, 3.9% of women of childbearing age were using injectable contraception (medroxyprogesterone), that is, 74 million users, but figures vary widely between world regions (from 1.8% in high income countries to 8.7% in low income countries).41This method of contraception is the most widely used in Indonesia (13 million women),42Ethiopia (4.6 million women), and South Africa (3.6 million women).41In the USA, medroxyprogesterone acetate is used in more than 2 million prescriptions in 2020 and more than one of five sexually active American women report having used injected medroxyprogesterone acetate (150 mg/3 mL) in their lifetime.4344Injectable contraceptives are much less widely used in Europe (3.1% of women of childbearing age in the UK and 0.2% in France41). Our results support preliminary findings from studies of meningioma cases exposed to chronic use of medroxyprogesterone acetate or cases of high dose administration.4546474849In particular, our results show similarities with those of a retrospective review of 25 patients diagnosed with meningioma who had a history of chronic medroxyprogesterone acetate use and were treated at the University of Pittsburgh Medical Center between 2014 and 2021 concerning the characteristics of cases exposed to medroxyprogesterone acetate (women (mean age of 46 years) with meningiomas commonly located at the base of the skull).48In addition, medroxyprogesterone acetate used as an injected contraceptive is known to be prescribed to specific populations, especially people with mental illnesses.50The protection of these vulnerable populations from additional drug risks is particularly important. Depot medroxyprogesterone acetate (150 mg) is registered for use as a form of birth control in more than 100 countries worldwide.41In countries that have high numbers of people using medroxyprogesterone acetate, the number of meningiomas attributable to this progestogen may be potentially high. Furthermore, medroxyprogesterone (non-acetate) is also used orally, at lower doses, in some countries other than France (notably in the US), for which no data exists on a risk of meningioma so far.
Promegestone was only available in France (not marketed in any other country) and was withdrawn from the market in 2020. This drug was indicated for the relief of premenopausal symptoms and hormone replacement therapy for menopause. With the discontinuation of its marketing, some users could have switched to medrogestone in 2020, a molecule also implicated in the risk of meningioma in our results. Clinicians therefore must remain vigilant because meningioma risk could last beyond market withdrawal and a potential switch to another progestogen.
The FDA defines a therapeutic class as “all products (…) assumed to be closely related in chemical structure, pharmacology, therapeutic activity, and adverse reactions”.5152Various subtypes of progestogens exist depending on the molecule from which the progestogen is derived (ie, progesterone, testosterone, and spironolactone) (supplementary table B).53Their chemical structures and pharmacological properties differ according to this classification, which explains why no class effect is reported for certain benefits and risks associated with their use (eg, breast cancer and cardiovascular risk).54555657Progestogens have distinct affinities for different target organ steroid receptors, which may vary even within a subclass, determining their activity.
Our study suggests that 17-OH-hydroprogesterone and 19-norprogesterone derivatives, both progesterone derivatives, have a class effect on meningioma risk. Four of five progestogens belonging to the 17-OH-hydroprogesterone group have shown an increase in the risk of meningioma (supplementary table R). However, the fact that we found different sizes of risk appears to be more a question of duration and cumulative dose than that of belonging to a progestogen class. We could not draw any conclusions about hydroxyprogesterone (due to a lack of power), the fifth progestogen in the subclass, but its main indication (assisted reproductive technology) corresponded to fewer women exposed and very short exposure (approximately 15 days), which could explain why this drug differs from the others. Finally, to date, at the doses considered in the study, no excess risk of meningioma associated with testosterone derivatives has been shown. However, the risk of meningioma associated with the use of these derivatives at other doses and in other regimens needs to be investigated.
Strengths and limitations
To our knowledge, this study of meningioma risk is the first to expand the list of progestogens of interest beyond chlormadinone acetate, nomegestrol acetate, and cyproterone acetate, detailing the risk associated with each progestogen, with different modes of administration. This study was conducted on a national scale for women of all ages for both the cases and their controls. The SNDS database allowed the use of exhaustive real-world data from over a period of 12 years (2006-18; postoperative information was searched even up to 2022), thus preventing recall bias.
The exclusion of women with a pregnancy beginning in the two years preceding the index date ensured that estimates of the risks associated with progestogen use were reliable. Pregnancy is a unique state, affecting exposure to progestogens (of endogenous or exogenous origin), the likelihood of a meningioma appearing or increasing in volume,95859and the likelihood of admission to hospital for surgery (possibly with a lower surgery rate, depending on the symptoms, maternal and foetal health, and tumour characteristics).59
Another potentially important confounding factor, use of chlormadinone acetate, nomegestrol acetate, or cyproterone acetate, was considered in the analyses by modelling exposure to each progestogen of interest with a separate mode of prior or simultaneous exposure to these drugs. Furthermore, the results obtained for the negative and positive control exposure, including exposure to chlormadinone acetate, nomegestrol acetate, and cyproterone acetate, support the appropriateness of the method chosen for this study.
However, this study also had several limitations. As a result of the scarcity of historical data in the SNDS (which began in 2006, and did not have information for some reimbursement schemes during the first few years), we have only three years of lookback period for the oldest meningioma cases (2009-06), and 12 years for the most recent. The SNDS does not provide information on non-reimbursed drugs, which obliged us to study dienogest in association with oestrogen rather than dienogest alone. Further studies will therefore be necessary. Similarly, we were unable to study other progestogens, such as ","Objective: To assess the risk of intracranial meningioma associated with the use of selected progestogens.
Design: National case-control study.
Setting: French National Health Data System (ie,Système National des Données de Santé).
Participants: Of 108 366 women overall, 18 061 women living in France who had intracranial surgery for meningioma between 1 January 2009 and 31 December 2018 (restricted inclusion periods for intrauterine systems) were deemed to be in the case group. Each case was matched to five controls for year of birth and area of residence (90 305 controls).
Main outcome measures: Selected progestogens were used: progesterone, hydroxyprogesterone, dydrogesterone, medrogestone, medroxyprogesterone acetate, promegestone, dienogest, and intrauterine levonorgestrel. For each progestogen, use was defined by at least one dispensation within the year before the index date (within three years for 13.5 mg levonorgestrel intrauterine systems and five years for 52 mg). Conditional logistic regression was used to calculate odds ratio for each progestogen meningioma association.
Results: Mean age was 57.6 years (standard deviation 12.8). Analyses showed excess risk of meningioma with use of medrogestone (42 exposed cases/18 061 cases (0.2%)v79 exposed controls/90 305 controls (0.1%), odds ratio 3.49 (95% confidence interval 2.38 to 5.10)), medroxyprogesterone acetate (injectable, 9/18 061 (0.05%)v11/90 305 (0.01%), 5.55 (2.27 to 13.56)), and promegestone (83/18 061 (0.5%)v225/90 305 (0.2 %), 2.39 (1.85 to 3.09)). This excess risk was driven by prolonged use (≥one year). Results:  showed no excess risk of intracranial meningioma for progesterone, dydrogesterone, or levonorgestrel intrauterine systems. No conclusions could be drawn concerning dienogest or hydroxyprogesterone because of the small number of individuals who received these drugs. A highly increased risk of meningioma was observed for cyproterone acetate (891/18 061 (4.9%)v256/90 305 (0.3%), odds ratio 19.21 (95% confidence interval 16.61 to 22.22)), nomegestrol acetate (925/18 061 (5.1%)v1121/90 305 (1.2%), 4.93 (4.50 to 5.41)), and chlormadinone acetate (628/18 061 (3.5%)v946/90 305 (1.0%), 3.87 (3.48 to 4.30)), which were used as positive controls for use.
Conclusions: Prolonged use of medrogestone, medroxyprogesterone acetate, and promegestone was found to increase the risk of intracranial meningioma. The increased risk associated with the use of injectable medroxyprogesterone acetate, a widely used contraceptive, and the safety of levonorgestrel intrauterine systems are important new findings.
"
Delirium and incident dementia in hospital patients,"Introduction
Delirium is characterised by inattention and disturbance of awareness that represents a change from baseline cognitive function, and is precipitated by acute events such as illness and surgery. Delirium is a prevalent condition in hospital, with an estimated occurrence of 23% in patients with acute medical conditions1and up to 45% in patients aged 90 years and older.2Delirium is associated with adverse outcomes, including death in hospital or in the short to medium term post discharge, prolonged hospital stay, and new admission to a residential institution.3In 2020, Goldberg and colleagues4found that delirium was also associated with long term cognitive decline (ie, decrease in objective cognitive scores or new clinical diagnosis of dementia) in their meta-analysis of 24 studies including 10 459 patients. This association persisted in their subgroup analysis of 19 studies examining patients without cognitive impairment at baseline. An association between delirium and incident dementia in patients without dementia at baseline has been reported in a subsequent systematic review and meta-analysis.5However, included studies were relatively modest in size (between 78 and 329 patients) and variably adjusted for important confounders. Furthermore, studies did not account for the competing risk of death, which is particularly high in this vulnerable population and might contribute to biased risk estimates of incident dementia in relation to delirium.6
Mechanisms linking delirium with incident dementia are under debate. Delirium might be an epiphenomenon, it might uncover unrecognised (preexisting or preclinical) dementia, or it might cause dementia by accelerating underlying neuropathological processes or de novo mechanisms.7Observational studies are limited in their capacity to validate causality; however, the association between delirium and dementia is not amenable to randomisation. Dose-response analysis might contribute valuable information to the debate about causality. In 2021, the Delirium and Cognitive Impact in Dementia study showed that more than one episode of delirium was associated with a greater risk of incident dementia compared with a single episode in a sample of 173 older hospital patients.8
As the global burden of dementia increases,9it is important to confirm the extent to which delirium is a potentially modifiable risk factor. We aimed to use large scale hospital administrative data to clarify the strength and nature of the association between delirium and incident dementia in a population of older adult patients without dementia at baseline.
Methods
We undertook a retrospective cohort study using a longitudinal statewide dataset linked by the New South Wales (NSW) Centre for Health Record Linkage.10
Data sources
The NSW Admitted Patient Data Collection records all inpatient episodes of care (defined by separations—discharges, transfers, and deaths) from all NSW public and private hospitals. Data include personal (eg, date of birth, gender, residential address, country of birth), administrative (eg, admission and separation dates), and clinical (eg, diagnoses and procedures) information. For each episode, one primary diagnosis and up to 50 secondary diagnoses are coded using the international classification of diseases, 10th revision (ICD-10).11Admitted Patient Data Collection records linked to the NSW Registry of Births, Deaths and Marriages data were available from July 2001 onwards. Linked data up to and including 31 March 2020 were available to the research team (>12 million episodes).
Study design and sample
We defined a six year index period (1 January 2009 to 31 December 2014) so that sufficient data were available to determine that patients did not have a previous dementia diagnosis, to calculate a hospital frailty risk score (HFRS) for every patient, and to provide adequate follow-up for all patients (fig 1). A HFRS was calculated for each patient using ICD-10 codes recorded for episodes in the preceding two year period (see supplementary material for further information about HFRS).12The follow-up period for all patients was 63 months (5.25 years), which was the time between the end of the index period and the end of the dataset. We identified 650 590 patients aged 65 years and older who had one or more episodes of care (total episodes 4 779 584) from NSW hospitals during the index period.
Dementia and delirium diagnoses were extracted from primary and secondary diagnoses data using ICD-10 codes (see supplementary material). In Australian hospitals, a patient presenting to hospital with cognitive impairment, or with an acute change in behaviour or cognitive status in hospital, is assessed for delirium by an appropriately trained clinician, ideally using a validated tool (commonly the 4AT, a diagnostic tool designed specifically for routine clinical use).13Patients with a dementia diagnosis or episode of delirium recorded before the index period were excluded, as were patients aged >110 years and those with data inconsistencies (eg, implausible dates).
Patients were then categorised into delirium and no delirium groups. For patients in the delirium group, the first episode recording a delirium diagnosis was identified as the index episode. Patients with a dementia diagnosis recorded at or before the index episode were then excluded.
Patients in the delirium group were matched 1:1 to patients in the no delirium group according to patient and episode characteristics with potential to confound the association between delirium and subsequent risk of dementia. The confounders were patient age (in years; continuous variable), gender (man or woman), HFRS (≥0; continuous variable), and the primary diagnosis (ICD-10 code up to seven characters), episode length of stay (in days; discrete variable), and intensive care unit length of stay (in days; continuous variable) of the index episode. In the event that delirium was the primary diagnosis of an index episode, the primary diagnosis variable was not used for matching. Patients in the no delirium group with a dementia diagnosis recorded at or before the index episode were excluded and an alternative match was identified. Matching was without replacement; that is, each patient without delirium was matched to (at most) one patient with delirium.
Outcomes
The primary outcomes were incident dementia and death. The start date of the episode with a newly recorded dementia diagnosis was identified as the event time for incident dementia (see supplementary material for incident dementia ICD-10 codes). Mortality data, including date of death (the other event time), were available in the linked data.
Statistical analysis
Descriptive summary statistics were calculated at baseline separately for patients in the delirium and no delirium groups, and for patients who made up the total eligible sample for comparison. In all statistical models, linear associations were assumed between continuous covariates (age, HFRS, episode length of stay, and intensive care unit length of stay) and study outcomes (death and incident dementia).
When death was the outcome, patient follow-up was from the index episode until death. When dementia was the outcome, patient follow-up was from the index episode until the onset of dementia or death (whichever came first). In both instances, patients who remained event free were censored at 63 months.
We first assessed differences between the delirium group and the no delirium group in the incidence of an outcome (death or dementia). Next, we applied a landmarking approach to determine the presence of a dose-response association between the number of episodes of delirium and the outcome.14The number of delirium episodes occurring within the first 12 months of follow-up was associated with event incidence rates subsequent to that 12 month period. Delirium episodes were first used as categorical variables in the full sample (categorised as 0 episodes, 1 episode, 2 episodes, and ≥3 episodes) and then as continuous variables in the delirium cohort only. Dose-response models included covariates of age and gender, and the number of hospital episodes recorded within the landmark period (categorised as 1-5, 6-10, 11-20, and >20 episodes), and only patients who remained event free within the landmark period were included.
A different statistical modelling technique was used for each outcome. For death, we used Cox proportional hazards models and expressed the strength of associations as hazard ratios. For dementia, we used Fine-Gray subdistribution hazard models that accounted for the competing risk of death and expressed associations as subdistribution hazard ratios.15Given the strong association between delirium and risk of death, the competing risks analysis approach improves the accuracy of association estimates.
We conducted several sensitivity analyses to assess the robustness of associations. For both outcomes, we excluded patient pairs with distance values within the top 10% (least matched patients) and simultaneously included as covariates all characteristics used in the matching process (ie, age, gender, HFRS, primary diagnosis ICD-10 category, episode length of stay, and intensive care unit length of stay) to account for any residual differences in characteristics between groups. Additionally, when dementia was the outcome, we repeated analyses after excluding patients who died or had a diagnosis of dementia within 24 months of their index episode to reduce the impact of undetected dementia on the results. To assess the robustness of the dose-response associations, we repeated the analyses while extending the landmark period from 12 to 24 months. Finally, all analyses were conducted in the total matched sample and for men and women separately.
All estimates of associations were accompanied by 95% confidence intervals to represent the uncertainty. Statistical analyses were conducted using R version 4.2.3.
Supplementary analyses
We opted for a matched cohort design to reduce the confounding effects of key clinical variables and to permit sensitivity analyses that adjusted for residual confounding. Matching reduced the computational burden in this large study and allowed more reliable comparison without sacrificing statistical precision. The supplementary material presents an alternative approach using the total eligible sample.
Patient and public involvement
This study was inspired by EHG and REH’s clinical experience as geriatricians. There was no direct patient and public involvement in the study because the analysis of this restricted access administrative dataset was retrospective. However, a consumer representative with lived experienced of delirium who is actively involved in delirium prevention and education programmes in Australia reviewed the manuscript, confirmed the importance and potential impact of the study and its results, and contributed to the dissemination strategy.
Results
Sample characteristics
Of the 650 590 patients, 626 467 were eligible for inclusion in the analytical sample. The supplementary material presents the 80 most frequent ICD-10 codes recorded as primary diagnoses for these patients. The matched study sample included 110 422 patients across the two groups (fig 1).Table 1presents personal and clinical characteristics for the total eligible sample (n=626 467) and the matched sample (delirium group n=55 211; no delirium group n=55 211). At baseline, matched patients ranged in age from 65 to 109 years and most were older (mean age 83.4 years, standard deviation 6.5 years). Women and men were almost equally represented (52% women, 48% men). Despite matching, the length of stay (for the index episode and in the intensive care unit) was slightly longer for the delirium group than the no delirium group. In the delirium group, 6351 patients had a primary diagnosis of delirium. The supplementary material includes additional results about matching.
Delirium and risk of death
The rate of death was 1.4 times higher in the delirium group than in the no delirium group (table 2), which equates to a 39% increased risk of death (fig 2, upper panel). The risk was similar for men and women (interaction P=0.62). After excluding the least matched patients and adjusting for all covariates used in the matching process, the association strengthened marginally (hazard ratio 1.41, 95% confidence interval 1.39 to 1.44). When all eligible patients from the total sample were analysed and characteristics used in the matching were included in statistical models as covariates, findings were similar although associations strengthened (see supplementary material).
When episodes of delirium were counted within the 12 month landmark period and categorised (0 episodes, 1 episode, 2 episodes, ≥3 episodes), more episodes were monotonically associated with a higher risk of death (fig 2, lower panel). These associations strengthened marginally when episodes of delirium were counted within a 24 month landmark period (see supplementary material). Among patients who experienced at least one episode of delirium within the landmark period, each additional episode of delirium was associated with a 10% increased risk of death (hazard ratio 1.10, 95% confidence interval 1.09 to 1.12).
Delirium and risk of dementia
The rate of incident dementia in the delirium group was 3.4 times higher than the no delirium group (table 2). After accounting for the competing risk of death, the risk of incident dementia remained three times higher among the delirium group (fig 3, upper panel). This association was stronger for men than women (subdistribution hazard ratio 3.17 and 2.88, respectively, P=0.004). The association also strengthened marginally after excluding the least matched patients and adjusting for all covariates used in the matching process (3.09, 2.98 to 3.19) and was similar after excluding patients who died or developed dementia within 24 months of the index episode (2.98, 2.86 to 3.11). When all eligible patients from the total sample were analysed and characteristics used in the matching were included as covariates, results were comparable although most associations were weaker (see supplementary material).
In the 12 month landmark analysis, more delirium episodes were monotonically associated with a higher risk of incident dementia (fig 3, lower panel). These associations weakened marginally when episodes of delirium were counted within a 24 month landmark period (see supplementary material). Among patients who experienced at least one episode of delirium within the landmark period, each additional episode of delirium was associated with a 20% increased risk of dementia (subdistribution hazard ratio 1.20, 95% confidence interval 1.18 to 1.23).
Discussion
Principal findings
We found delirium to be a strong risk factor for death and incident dementia in this cohort of older Australian hospital patients. We observed that among patients without dementia at baseline with at least one episode of delirium, the risk of a new dementia diagnosis was about three times higher than for patients without delirium over five years of follow-up. Among patients with at least one episode of delirium, each additional episode of delirium increased that risk by 20%. These associations were observed in a large scale dataset and were robust to several tests of bias and confounding, supporting the hypothesis that delirium has a strong independent effect on dementia risk in this clinical population.
Comparison with other studies
In our study, the rate of death was higher than the rate of incident dementia. Death was an important competing risk—it was an outcome of equal or higher clinical importance than the primary outcome that changed the probability of the primary outcome.6Leighton and colleagues16recently estimated the cumulative incidence of new dementia (accounting for competing risk of death without a dementia diagnosis) to be 31% by five years in their sample of 12 949 patients with delirium aged 65 years and older. This proportion is higher than our result (25%), possibly owing to their inclusion of dementia diagnosis at death (18% of patients) in their cumulative incidence calculations.
Recently, two studies conducted competing risk analyses in smaller cohorts of older patients and reported different risk estimates for incident dementia in relation to delirium in patients without dementia at baseline (subdistribution hazard ratio 1.94 and 8.70, respectively).1718The studies had many methodological differences, most notably in study design (retrospectivevprospective), size (n=390v1100), duration of follow-up (median 24 monthsvmean 82 months), and covariates. While Garcez and colleagues17accounted primarily for the confounding effects of frailty in their older inpatient population, Rolandi and colleagues18examined the independent effects of non-modifiable and potentially modifiable risk factors in their population based study. Neither study adjusted for clinical variables relating to illness severity or examined the impact of more than one episode of delirium.
Richardson and colleagues8recently estimated that older patients with delirium had almost nine times the risk of incident dementia (odds ratio 8.8) compared with patients without delirium and that the risk increased with subsequent episodes of delirium (odds ratio 8.6 and 13.0 for one episode and more than one episode, respectively). These findings are consistent with our study, even though the estimates are higher. This difference might be attributable to the smaller study size (n=135), shorter duration of follow-up (12 months), and an unaccounted for competing risk of death (n=38, 18%).8Our study and that of Richardson and colleagues8share some strengths, including adjusting for baseline characteristics such as age, gender, frailty, and measures of illness severity (APACHE II (acute physiology and chronic health evaluation II)vprimary diagnosis, episode length of stay, and intensive care unit length of stay). The studies differed in their approach to diagnosis of delirium and dementia. However, the meta-regression of 24 studies by Goldberg and colleagues4suggested that the approach to diagnosis might not have much impact on variance in results.
Mechanistic understanding and implications for future research
We found that there was a persistent association between delirium and incident dementia years after the episode of delirium (and resolution of the precipitating stressors), which suggests that delirium is not an epiphenomenon or merely a marker of unrecognised dementia or a vulnerable brain. Furthermore, the dose-response association between delirium and incident dementia suggests a causal link between the two conditions. Several hypotheses have been proposed explaining how delirium might cause dementia.7For example, the sequelae of delirium (drowsiness, agitation, circadian disturbance, and unsafe behaviours) might precipitate a cascade of geriatric syndromes (mobility impairment and falls, pressure ulcers, malnutrition and dehydration), medical complications (electrolyte disturbance, aspiration and respiratory failure, infection and venous thromboembolism) and chemical and physical restraint, all of which might exert a toxic effect on the brain. Alternatively, or additionally, delirium might contribute to neuronal injury and neurodegeneration through a range of disrupted biological mechanisms (see Fong and Inouye7for a comprehensive review). Associations between systemic inflammatory markers, delirium, and dementia are variable in preclinical and clinical models and appear to be influenced by the presence or absence of dementia pathology. Similarly, markers of neuroinflammation have been associated with both syndromes. Alzheimer’s disease biomarkers (eg, Aβ, tau) have been associated with risk of incident delirium and the association between the APOE genotype and delirium suggests a mediating role of genetic profiles related to systemic inflammation. Neuroimaging studies have identified structural and functional predictors of delirium, such as changes in network connectivity in the posterior cingulate cortex. A direct pathway between delirium and neuronal injury (not mediated by systemic inflammation, for example) has not been established but is theoretically possible. Ultimately, a better understanding of the delirium-dementia pathophysiological pathways might guide the development of new treatments with potential to prevent or reduce neurodegeneration.
In our study, we observed delirium to impart a larger increase in dementia risk in men than women. Despite this difference, in the delirium and no delirium groups, women experienced dementia at a slightly higher rate than men. The literature on sex differences in dementia is rapidly evolving; there is emerging evidence for differences in dementia risk19and mediating factors20for men and women. However, one meta-analysis (201 studies, n=998 187) did not find major differences in dementia incidence in men and women except in the oldest old (>90 years).21For delirium, it remains unclear whether gender is a predisposing risk factor, with both genders being associated with increased risk in various inpatient populations.22
We might hypothesise that the increased risk of incident dementia with delirium in men indicates lower reserve (ie, higher burden of neuropathology). Although this might be unlikely given the higher global prevalence of dementia in older women than men,21it is increasingly understood that the association between neuropathological burden and clinical dementia is not linear23and that there are likely to be important sex differences in patterns of neuropathology in people with and without dementia.24Another hypothesis is that delirium in men might be more severe. However, a recent prospective study of older adults with delirium did not identify any gender differences in clinical phenotypes, course, or response to treatment.25There might also be fundamental sex differences in the biological mechanisms of delirium that lead to de novo neuronal injury and accelerated neurodegeneration. Future studies might explore these hypotheses to try and identify sex specific targets for intervention.
Pooled data from 14 studies including 2640 patients aged 18 years and older showed that multicomponent non-pharmacological interventions were associated with a reduced incidence of delirium (risk ratio 0.57), a reduced duration of a delirium episode, and reduced hospital length of stay compared with usual care.26In older adults specifically, a systematic review and meta-analysis of data from studies of a widely disseminated delirium prevention programme (the Hospital Elder Life Program) showed that the intervention was associated with a reduced incidence of delirium (odds ratio 0.47) and falls (odds ratio 0.58), a reduced hospital length of stay and preserved functional status, and reduced healthcare costs.27Currently, data are lacking about the impact of these interventions on the risk of incident dementia.26Because the burden of dementia is set to dramatically rise in coming decades and multicomponent non-pharmacological delirium prevention interventions are effective and readily implemented, quantifying the benefit of interventions on dementia incidence rates should be addressed in future clinical trials as a matter of priority.28
Strengths and limitations of this study
In this large study of delirium and incident dementia, we minimised bias by adjusting for important personal and clinical baseline variables, having a long period of follow-up, and accounting for the competing risk of death in our analyses. This approach helped to overcome methodological issues prevalent in the existing literature. Therefore, it is likely that our estimate lies closer to the true effect of delirium on incident dementia in patients without dementia at baseline. The size and granularity of the data afforded precision when conducting adjusted dose-response analyses and the results of predetermined sensitivity analyses showed the robustness of the reported results. By stratifying results by gender, we generated insights with important pathophysiological and clinical implications.
The results should be considered within the context of this study’s limitations. Diagnosis of delirium and dementia depended on clinical coding of medical information from inpatient episodes of care recorded in the administrative dataset used. Differential diagnosis of delirium, dementia, and delirium superimposed on dementia is difficult and conditions might go undetected or be misattributed.7Under-coding of dementia during hospital admission is a well recognised issue and correlates with lack of documentation of dementia diagnosis in medical notes.29Similarly, published data suggest that coding for delirium underestimates true delirium rates.30While the use of routinely collected healthcare data in determining the presence of all cause dementia is supported by positive predictive values between 70% and 90%,31it is possible that erroneous diagnoses were made (false positives) and other diagnoses were missed (false negatives), which would affect the incident rates reported here. Future studies might combine different administrative data sources (eg, pharmaceutical, primary care, aged care) to improve case detection and reduce the potential for bias.
We matched delirium and no delirium groups 1:1 using important personal and clinical characteristics. However, we were limited to the data available in the administrative dataset and there could be residual confounding effects from unmeasured variables. Differences were found between delirium and no delirium groups for some characteristics (table 1); however, sensitivity analyses that simultaneously included all characteristics used in the matching process as covariates resulted in marginal increases in the risk estimates, suggesting limited residual differences in characteristics between groups.
For our dose-response analysis, data about the duration and severity of delirium episodes were not available, which limited the analysis to the number of episodes of care with coded delirium. It is also possible that the association found between delirium and incident dementia was induced by a confounding variable. For example, incremental increases in frailty in a patient with several hospital admissions (episodes) might underpin the increased risk of incident dementia. Frailty has been shown to affect the association between neuropathological burden and dementia diagnosis in community dwelling adults,23and gender might have a further impact on this association.20However, we tried to account for time varying differences in general health status by including the number of episodes (admissions) during the landmark period as a covariate.
While our results are consistent with the hypothesis that delirium might play a causative part in dementia, they are not conclusive owing to the fundamental limitations of observational studies in determining causality. Nevertheless, the results of this study provide valuable insights because prospective randomised controlled trials are unlikely to be conducted.
Conclusions
Using large scale hospital administration data, this study found a strong association between delirium and incident dementia in older adults without dementia at baseline. A dose-response association between delirium and dementia supports a causal pathway between the two conditions, encouraging the search for accelerated and de novo pathways to neuronal injury and the development of new treatment strategies. Differences in the association between delirium and incident dementia in men and women reinforce the need to not only adjust for gender in future studies but also to look for gender specific associations that might have important mechanistic and clinical implications. Delirium is a factor that could triple a person’s risk of dementia. Therefore, delirium prevention and treatment are opportunities to reduce dementia burden globally.
An association might exist between delirium and subsequent dementia; however, the strength and nature of this association are unclear because of limitations in existing observational studies
As the global burden of dementia increases, it is important to confirm the extent to which delirium is a potentially modifiable risk factor
Among patients without dementia at baseline with at least one episode of delirium, the risk of a new dementia diagnosis was about three times higher than for patients without delirium; each additional episode of delirium increased the risk by 20%
The association between delirium and incident dementia seems to be stronger in men than in women
Delirium prevention and treatment could reduce the burden of dementia globally
","Objectives: To determine the strength and nature of the association between delirium and incident dementia in a population of older adult patients without dementia at baseline.
Design: Retrospective cohort study using large scale hospital administrative data.
Setting: Public and private hospitals in New South Wales, Australia between July 2001 and March 2020.
Participants: Data were extracted for 650 590 hospital patients aged ≥65 years. Diagnoses of dementia and delirium were identified from ICD-10 (international classification of diseases, 10th revision) codes. Patients with dementia at baseline were excluded. Delirium-no delirium pairs were identified by matching personal and clinical characteristics, and were followed for more than five years.
Main outcome measures: Cox proportional hazards models and Fine-Gray hazard models were used to estimate the associations of delirium with death and incident dementia, respectively. Delirium-outcome dose-response associations were quantified, all analyses were performed in men and women separately, and sensitivity analyses were conducted.
Results: The study included 55 211 matched pairs (48% men, mean age 83.4 years, standard deviation 6.5 years). Collectively, 58% (n=63 929) of patients died and 17% (n=19 117) had a newly reported dementia diagnosis during 5.25 years of follow-up. Patients with delirium had 39% higher risk of death (hazard ratio 1.39, 95% confidence interval 1.37 to 1.41) and three times higher risk of incident dementia (subdistribution hazard ratio 3.00, 95% confidence interval 2.91 to 3.10) than patients without delirium. The association with dementia was stronger in men (P=0.004). Each additional episode of delirium was associated with a 20% increased risk of dementia (subdistribution hazard ratio 1.20, 95% confidence interval 1.18 to 1.23).
Conclusions: The study findings suggest delirium was a strong risk factor for death and incident dementia among older adult patients. The data support a causal interpretation of the association between delirium and dementia. The clinical implications of delirium as a potentially modifiable risk factor for dementia are substantial.
"
Derivation and external validation of a simple risk score for predicting severe acute kidney injury after intravenous cisplatin,"Introduction
Cisplatin is a potent chemotherapeutic drug used to treat a wide range of cancers.1Despite efforts to find less toxic yet equally effective alternatives, cisplatin remains a preferred treatment option for advanced bladder cancer2and non-small cell lung cancer,3and it is widely used in the treatment of mesothelioma,4head and neck cancer,56gynecologic cancers,78and testicular cancers.910Acute kidney injury is one of the most common and serious toxicities due to cisplatin use. Cisplatin associated acute kidney injury (CP-AKI) can increase susceptibility to extrarenal toxicities from cisplatin and other renally cleared chemotherapies, as well as jeopardize eligibility for further treatment with cisplatin, or participation in clinical trials of other cancer treatments.1112
Given the frequency with which cisplatin is administered globally and the high burden of nephrotoxicity associated with its use, understanding which patients are at highest risk for CP-AKI is important. Accurate assessment of susceptibility to CP-AKI can help clinicians weigh the risks and benefits of cisplatin, adjust the dose as needed, identify those who should be monitored more frequently, and allow researchers to enrich prospective patient cohorts (eg, for clinical trials testing novel interventions for the prevention of CP-AKI). Previous studies that investigated risk factors for CP-AKI were limited by small sample size, lack of external validation, non-contemporary data, heterogeneous definitions of acute kidney injury, and inclusion of biomarkers that are not readily available in clinical practice.131415161718Moreover, most studies used liberal definitions of acute kidney injury based on small changes in serum creatinine levels and thus did not assess the more severe and clinically relevant manifestations of CP-AKI.
To deal with these limitations and fill a key knowledge gap, we derived and externally validated a prediction model for moderate-to-severe CP-AKI using data from six large contemporary cohorts.
Methods
Study design
We conducted a multicenter cohort study of adults (≥18 years) treated with intravenous cisplatin at six major academic cancer centers across the US: Memorial Sloan Kettering Cancer Center, Massachusetts General Hospital, Dana-Farber Cancer Institute, MD Anderson Cancer Center, University of Colorado, and Northwell Health.
Study population
The study sample included adults (≥18 years) receiving a first dose of intravenous cisplatin as an inpatient or outpatient between 2006 and 2022. We excluded patients with end stage kidney disease, those with a missing baseline serum creatinine value (defined as the value in the 30 days that was closest to, and preceding, the first dose of intravenous cisplatin), and those without at least one follow-up serum creatinine value in the first 14 days after a first dose of intravenous cisplatin.
Data collection
We collected data on age, sex, race, ethnicity, body mass index, comorbidities (hypertension, diabetes mellitus, chronic obstructive pulmonary disease, congestive heart failure, cirrhosis), status as a current or former smoker, baseline laboratory values (serum creatinine, white blood cell and platelet counts, hemoglobin, and serum magnesium, calcium, and albumin), date and dose of administered cisplatin, and receipt of other nephrotoxic chemotherapy (immune checkpoint inhibitors, pemetrexed, cetuximab, and ifosfamide) administered within 30 days before cisplatin. Baseline laboratory values were defined as the values in the 30 days closest to and preceding the first dose of intravenous cisplatin. To assess outcomes, we collected data on serum creatinine level, kidney replacement therapy, and survival after treatment with cisplatin.
Data on comorbidities were extracted using ICD-9 and ICD-10 (international classification of diseases, ninth revision and 10th revision, respectively) codes (see supplemental table S1). Baseline estimated glomerular filtration rate was calculated using the 2021 Chronic Kidney Disease Epidemiology Collaboration equation,19which incorporates age, sex, and serum creatinine level.
Primary outcome
The primary outcome was CP-AKI, defined as a twofold or greater increase in serum creatinine level compared with baseline or kidney replacement therapy within 14 days after the first dose of cisplatin, consistent with stage 2 or 3 acute kidney injury defined by the Kidney Disease: Improving Global Outcomes (KDIGO) consensus guidelines.20For the primary outcome we focused on moderate-to-severe acute kidney injury, as it is more clinically relevant than milder forms of acute kidney injury.
Secondary outcomes
Two of the secondary outcomes were based on alternative definitions for CP-AKI—one more liberal than the definition used for our primary outcome and one more strict. For the more liberal definition (the first secondary outcome), we defined CP-AKI according to modified KDIGO acute kidney injury criteria as an increase in serum creatinine concentration ≥26.5 µmol/L compared with baseline, a ≥1.5-fold increase in serum creatinine level compared with baseline, or kidney replacement therapy within 14 days of a first dose of intravenous cisplatin. For the stricter definition (the second secondary outcome), we defined CP-AKI according to modified KDIGO stage 3 criteria for acute kidney injury as a threefold or greater increase in serum creatinine level compared with baseline or kidney replacement therapy within 14 days after a first dose of intravenous cisplatin. For a third secondary outcome, we examined the composite outcome of major adverse kidney events within 90 days, defined as death within 90 days, kidney replacement therapy within 90 days, or persistent kidney dysfunction (increase in serum creatinine level ≥100% compared with baseline) at day 90 (defined as the closest value within 30 days before or after day 90) after a first dose of intravenous cisplatin.
Statistical analysis
Categorical data are shown as numbers (percentages) and continuous variables as median (interquartile range (IQR)). The derivation cohort comprised patients treated at Memorial Sloan Kettering Cancer Center, and the external validation cohort comprised those treated at the remaining five hospitals. The supplemental methods present sample size calculations for optimism in apparent model fit and for model validation.
Development of the primary model—Using multivariable logistic regression, we identified independent predictors of CP-AKI in the derivation cohort. Candidate variables were selected for consideration for inclusion in the final model based on clinical knowledge, biologic plausibility, univariate associations, and parsimony. We examined continuous variables using restricted cubic splines, with knots placed at the 5th, 27.5th, 50th, 72.5th, and 95th centiles of each variable. Variables were selected for the final model using backward elimination using a significance threshold of P=0.1. Multiple imputation by chained equations was used to impute missing data, with 20 complete datasets created and results pooled using Rubin’s rules.21Spline terms were identified on one imputed dataset and imposed on all imputed datasets.
Development of a simple risk model—Next, we sought to develop a parsimonious and clinically useful integer based score for CP-AKI based on the primary multivariable model. To do so, we evaluated each of the continuous variables from the primary model in categories based on clinically relevant cut-offs and their association with CP-AKI (eg, age ≤45, 46-60, 61-70 years; hemoglobin concentration <110, 110-119, ≥120 g/L; white blood cell count ≤12.0, >12.0 x109/L; albumin <33, 33-38, >38 g/L; magnesium <0.82, ≥0.82 mmol/L; and cisplatin dose ≤50, 51-75, 76-100, 101-125, 126-150, 151-200, >200 mg). We then assigned each covariate an integer or half integer score derived by dividing the odds ratio for that variable by the smallest odds ratio in the model.22The score for the reference category for each variable was set at 0. We calculated the total score for each patient by summing the individual scores. Patients were then split into four risk groups according to the distribution of their total score: low, moderate, high, and very high. We used logistic regression to calculate the odds ratios for CP-AKI according to each risk score category, with the low risk category serving as the reference group. In an additional analysis, we divided the risk score into fourths and calculated the odds ratio for CP-AKI, with the lowest fourth serving as the reference group.
Calibration, discrimination, internal validation, and comparison with previous models—Model calibration was assessed with calibration plots, along with estimates of calibration slope and intercept. Discrimination was assessed by calculating the area under the receiver operating characteristic curve (C statistic) for the derivation and validation cohorts. The model was internally validated with 500 bootstrap samples. Model performance was evaluated by comparing the C statistic for the primary model with the C statistics from previous multivariable models for CP-AKI131423using the DeLong method and utilizing data from the entire cohort (the derivation and validation cohorts combined). Additionally, decision curves were used to compare the performance of both the primary and the simple models (Gupta et al) with three previously published models (Bhat et al, de Jongh et al, and Motwani et al).131423These three models were chosen as comparators because they were the largest models to date evaluating predictors of CP-AKI using multivariable modeling.
Additional analyses and secondary outcomes—we used similar methodology to assess the performance of our primary model across a series of additional analyses and to develop models for our three secondary outcomes. First, we assessed CP-AKI or death in the 14 days after a first dose of intravenous cisplatin as a composite outcome, because death is a competing risk for acute kidney injury.242526Second, we modified the time period for CP-AKI assessment to 10 days and 21 days after a first dose of intravenous cisplatin. Third, we repeated the primary analysis but limited it to patients treated with cisplatin in 2016 or later to determine whether the performance of our model differed based on earlier versus more contemporary data. Finally, we repeated the primary analysis using complete case analysis rather than multiple imputation for missing data. We also performed an additional analysis where we excluded patients who only had a follow-up serum creatinine value in the first four days after a first dose of intravenous cisplatin but no additional values.
CP-AKI severity and survival—Because acute kidney injury is a strong predictor of mortality in other contexts such as critical illness,2728293031we examined the association between severity of CP-AKI and survival. We categorized CP-AKI into four groups: no acute kidney injury, stage 1 acute kidney injury (an increase in serum creatinine concentration ≥26.5 µmol/L or a 1.5-1.9-fold increase in serum creatinine level), stage 2 acute kidney injury (2-2.9-fold increase in serum creatinine level), or stage 3 acute kidney injury (≥3-fold increase in serum creatinine, or kidney replacement therapy), each assessed within 14 days after a first dose of intravenous cisplatin. We then used Kaplan-Meier curves and multivariable Cox regression models to examine the association between CP-AKI stage with 90 day and one year survival. We also examined the longer term association between CP-AKI and the composite outcome of major adverse kidney events within 365 days, defined as death or kidney replacement therapy in the first 365 days, or persistent kidney dysfunction (twofold or greater increase in serum creatinine compared with baseline) at day 365 (defined as the closest value within 180 days before or after day 365) after a first dose of intravenous cisplatin. Models were adjusted for age, sex, body mass index, hypertension, diabetes mellitus, chronic obstructive pulmonary disease, current or former smoker, baseline serum creatinine level, hemoglobin level, white blood cell count, platelet count, serum albumin level, serum magnesium level, cisplatin dose, and concomitant nephrotoxic chemotherapy (pemetrexed, cetuximab, ifosfamide, or immune checkpoint inhibitors within 30 days before a first dose of intravenous cisplatin).
Data analysis—Analyses were performed using R version 3.6.3 (R Foundation).
Patient and public involvement
Patients were involved in the study through participation in a focus group, as well as an anonymous survey (see supplemental methods), where their perceptions of the study’s key findings were assessed. Their feedback was also considered in the design of an algorithm based on the risk prediction model. Twenty seven patients who had previously received intravenous cisplatin were asked to fill out a survey (see supplemental methods), and 20 patients (74%) completed it. Most patients (85%; 17/20) thought that the findings of this study were important to the scientific community, and that they would want to know about this information when discussing the risks and benefits of cisplatin with their oncologist (see supplemental figure S1).
Results
Baseline characteristics and CP-AKI incidence—The initial study population included 34 122 patients across six sites, comprising 15 752 patients in the derivation cohort and 18 370 in the validation cohort. After applying the exclusion criteria, the final dataset consisted of 11 766 patients in the derivation cohort and 12 951 in the validation cohort (fig 1; also see supplemental figure S2). Baseline characteristics were largely similar between the two cohorts, though the proportion of patients with hypertension, diabetes mellitus, congestive heart failure, and cirrhosis was higher in the validation cohort, and the proportion with chronic obstructive pulmonary disease was higher in the derivation cohort (table 1). Patients in the derivation cohort also received a higher median dose of cisplatin than those in the validation cohort. Supplemental table S2 shows baseline characteristics by outcome status.
CP-AKI occurred in 608 patients (5.2%; 608/11 766) in the derivation cohort and 421 patients (3.3%; 421/12 951) in the validation cohort. Rates of CP-AKI were largely unchanged over time (see supplemental figure S3).
Primary model—Each of the following was independently associated with risk of CP-AKI in the primary model: age, hypertension, diabetes mellitus, serum creatinine level, hemoglobin level, white blood cell count, platelet count, serum albumin level, serum magnesium level, and cisplatin dose. Supplemental table S3 shows a full description of the primary model. Supplemental figure S4 shows partial effect plots for the restricted cubic spline analyses.
Simple risk model—Nine covariates were included in the simple risk model;fig 2shows the odds ratios and associated risk points. With the addition of serum creatinine to the simple risk model, the area under the curve of the simplified model remained unchanged, and therefore serum creatinine was not included in the interest of parsimony. The total number of points a patient could be assigned ranged from 0 to 22.5, whereas the actual range was 0 to 20.5 (ie, no patient had a score of ≥21). In both the derivation cohort and the validation cohort, a higher score monotonically predicted a higher risk of CP-AKI, the incidence of which ranged from 0.9% to 23.5% in the derivation cohort and 0.7% to 16.7% in the validation cohort (fig 2). Supplemental figure S5 shows additional model characteristics.
CP-AKI by risk score category—In the derivation cohort, rates of CP-AKI ranged from 1.3% in the low risk category to 23.5% in the very high risk category (odds ratio 24.00 (95% CI 13.49 to 42.78) for the very high versus low risk category;fig 3). In the validation cohort, rates of CP-AKI ranged from 1.1% in the low risk category to 16.7% in the very high risk category (17.87 (10.56 to 29.60) for the very high versus low risk category;fig 3). Supplemental figure S6 shows the incidence and odds ratios for CP-AKI according to risk score categories in fourths.
Diagnostic accuracy and comparison with previous models—We internally validated our model using bootstrapping, and the optimism corrected area under the curve was 0.74. To assess generalizability of the model, we performed external validation in an independent validation cohort composed of data from five academic medical centers. The C statistic for the primary model was 0.76 (95% CI 0.74 to 0.78) in the derivation cohort, 0.72 (0.70 to 0.75) in the validation cohort, and 0.75 (0.73 to 0.76) in the overall dataset. The primary model for the validation cohort was well calibrated (see supplemental figure S7). The primary model also had superior performance in predicting CP-AKI compared with the three existing models, with the Motwani et al,23de Jongh et al,14and Bhat et al13models having C statistics of 0.68, 0.60, and 0.60, respectively, when tested in our dataset (DeLong P<0.001 for each comparison) (fig 4). Supplemental figure S8 shows the decision curves for the primary model, simple model, and three existing models, and the net benefit for both the primary and the simple model compared with the existing models.
Additional analyses and secondary outcomes—Discrimination of the primary model was similar across all five additional analyses (see supplemental figure S9). Moreover, the C statistic remained unchanged (0.75 (95% CI 0.74 to 0.77)) upon exclusion of patients with only a follow-up serum creatinine value in the four days after a first dose of intravenous cisplatin (1581/36 483 (4.3%) excluded). When the model was refit for the first two secondary outcomes (see supplemental table S3), the C statistics were 0.71 and 0.74 for CP-AKI defined using a more liberal or a stricter definition, respectively, and it outperformed each of the three previously published models (see supplemental figure S10;DeLong P<0.001 for each comparison). For the secondary outcome of the composite of major adverse kidney events within 90 days, 9.3% of patients (1841/19 732) with data available at day 90 met the composite outcome, and the C statistic of the model was 0.79 (95% CI 0.78 to 0.81) (DeLong P<0.001; see supplemental figure S10).
CP-AKI severity and survival—Greater severity of CP-AKI was monotonically associated with decreased survival at 90 days (log-rank P<0.001; adjusted hazard ratio 4.63 (95% CI 3.56 to 6.02) for stage 3 acute kidney injury versus no acute kidney injury;fig 5) and one year (see supplemental figure S11). Higher stage of CP-AKI was also monotonically associated with an increased risk for the composite outcome of major adverse kidney events within 365 days (see supplemental figure S12).
Discussion
In a multicenter cohort study of >24 000 adults treated with intravenous cisplatin between 2006 and 2022, we identified key risk factors for severe acute kidney injury based on readily available variables at the time cisplatin was administered. Using these data, we derived and characterized a simple clinical prediction score for CP-AKI comprising nine components that distinguishes patients at low risk versus high risk. Further, we externally validated our score using data from patients treated at five geographically diverse hospitals across the US, and we showed the superior discrimination of our model in predicting CP-AKI compared with three existing models.131423Lastly, we found a strong monotonic and independent association between CP-AKI and death, underscoring the importance of identifying those at highest risk for this condition.
Comparison with other studies
The C statistic of our model was 0.75, outperforming previous clinical prediction models for CP-AKI, which achieved values ranging from 0.60 to 0.68. These previously published prediction models had important limitations that we sought to address, including modest sample size, lack of external validation, non-contemporary data, and use of liberal definitions of acute kidney injury (based on small changes in serum creatinine levels) that may lack clinical relevance. The largest of these studies, by Motwani et al,23assessed CP-AKI in 4481 patients treated with cisplatin at two centers within an integrated healthcare system. The study found that older age, hypertension, lower serum albumin level, and higher cisplatin dose were each associated with a higher risk of CP-AKI, similar to the findings of our model. In that study, however, only a limited number of predictors was assessed, true external validation was lacking, and CP-AKI was defined liberally as a 26.5 µmol/L increase in serum creatinine within 14 days of a first dose of intravenous cisplatin. In contrast, we focused on the more clinically relevant outcome of doubling of serum creatinine level or receipt of kidney replacement therapy.
Other studies that examined risk factors for CP-AKI were limited by similar issues and considerably smaller sample sizes. De Jongh et al14and Bhat et al13examined risk factors for CP-AKI in 400 and 233 patients, respectively. Risk factors identified included older age, female sex, black race, hypertension, diabetes mellitus, current smoking, hypoalbuminemia, and concomitant treatment with paclitaxel. Interestingly, neither study identified cisplatin dose as a risk factor for CP-AKI, perhaps because the patients all received similar doses. Zhu et al recently studied CP-AKI among 256 patients who received cisplatin at a single hospital in China.32In addition to age, hypertension, and diabetes mellitus, the authors found that serum cystatin C and urinary kidney injury molecule-1 levels were each associated with CP-AKI. These biomarkers were measured after multiple cycles, however, raising concerns about reverse causality.
Unlike previous studies, in which serum albumin was the only routinely available laboratory value identified as a risk factor for CP-AKI, we identified several novel risk factors, including white blood cell count, hemoglobin level, platelet count, serum creatinine level, and serum magnesium level. Though some of these risk factors may simply reflect overall health, serum magnesium is particularly intriguing. Hypomagnesemia due to renal magnesium wasting is a well recognized manifestation of CP-AKI, but hypomagnesemia as a risk factor for CP-AKI has not been well documented. Hypomagnesemia is a risk factor for CP-AKI in animal studies, possibly as a result of downregulation of key transporters expressed in the proximal renal tubule (multidrug resistance proteins 4 and 6).3334Because these transporters are responsible for secreting cisplatin into the tubular lumen, their downregulation leads to intracellular accumulation of cisplatin and therefore acute kidney injury. Alternatively, patients may have had pre-existing tubular dysfunction from previous exposure to alternative nephrotoxic chemotherapy, thereby leading to hypomagnesemia and predisposing to acute kidney injury. Future studies should examine whether overall kidney health, assessed not only by serum creatinine but also by proteinuria and electrolyte abnormalities indicative of tubular dysfunction, aid in risk prediction of CP-AKI.
In addition to the risk factors identified for CP-AKI, we also found that greater severity of CP-AKI is associated with decreased 90 day and one year survival. Acute kidney injury has been associated with an increased risk of death in other contexts, such as critically ill patients,2728293031but it has not been examined on a large scale in patients receiving cisplatin. The association between CP-AKI and death may be due to premature discontinuation of cisplatin therapy or ineligibility for other treatments that are renally cleared. Additionally, acute kidney injury can predispose to cardiovascular disease and infection, which may also explain the association between CP-AKI and death.35These findings are important, as life expectancy among patients with cancer has increased over time,36and these gains may be offset in those who develop CP-AKI. Accordingly, tools that allow clinicians to readily identify those at highest risk of CP-AKI may lead to better patient selection and implementation of prophylactic measures.
Strengths and limitations of this study
Our study has several notable strengths. First, the large sample size enabled us to confirm previously discovered risk factors for CP-AKI, to identify novel ones, and to develop a model that can predict risk of CP-AKI across a wide spectrum (fig 2). Second, our study focused on severe CP-AKI, with previous studies examining milder forms of kidney injury of less clinical relevance. Third, we were able to rigorously externally validate our model by including data from six geographically diverse centers from across the US, thereby increasing the generalizability and reproducibility of our findings. Fourth, our findings were consistent across five additional analyses and three secondary outcomes, further confirming the validity of our findings. Fifth, we included patients treated with cisplatin from 2006 to 2022, and an additional analysis limited to patients treated in 2016 or later found similar model discrimination compared with our primary model. Accordingly, our findings reflect contemporary practice patterns and are applicable to patients being treated today.
We also acknowledge several limitations. First, data on medications used at home were not available. Second, because cancer type could not be reliably differentiated based on ICD codes (since a patient could have multiple malignancies), the association we report between severity of CP-AKI and survival should be interpreted cautiously as these analyses were not adjusted for cancer type or stage. Nevertheless, cisplatin dose is likely a much more important predictor of CP-AKI than the underlying type of malignancy, and we are unaware of any studies that found cancer type to be an important risk factor for CP-AKI. Third, our study was limited to centers within the US. CP-AKI is, however, common both within and outside the US. Moreover, unlike novel anticancer agents that are often costly and therefore may not be as widely available, cisplatin is cheap, effective, and used in patients with cancer worldwide. Accordingly, our results are likely to be generalizable to patients outside the US. Fourth, discrimination of the primary model was modest, with a C statistic of 0.75. However, our model considerably outperformed all previous models, both for the primary outcome as well as for each of the secondary outcomes. It is likely that major improvements in risk prediction of CP-AKI will only become available when other factors (eg, genetics) are considered, or with the addition of novel biomarkers. However, novel AKI biomarkers such as urinary kidney injury molecule-1 and neutrophil gelatinase associated lipocalin may only be able to predict CP-AKI if measured after cisplatin has been administered, in which case their utility for a priori risk stratification will be limited. Moreover, many of these markers are not routinely available in current clinical practice.
Implications
Our study has important implications for patients with cancer who are treated with cisplatin. Cisplatin continues to be widely used, and acute kidney injury is one of its most common and important complications, as it can lead to discontinuation of therapy and ineligibility for other treatments. Using data readily available before cisplatin was administered, we identified several risk factors for severe CP-AKI. These patient and treatment specific risk factors for severe CP-AKI can be used to identify those patients who may possibly benefit from preventive measures, close monitoring, and consideration for alternative treatments. We shared our results during a patient centered focus group that included six patients who had received cisplatin, of whom two (33%) developed CP-AKI. We then designed an algorithm that outlines our recommendations for providers according to different risk categories for CP-AKI (see supplemental figure S13). Future studies should validate these recommendations.
Conclusion
We developed a simple, externally validated risk score for severe CP-AKI. This model should help providers weigh the risks and benefits of cisplatin and will allow for enrichment of prospective studies designed to prevent CP-AKI.
Studies of cisplatin associated acute kidney injury (CP-AKI) have identified risk factors such as older age, hypertension, and higher cisplatin dose
These studies were limited by modest sample size, lack of external validation, non-contemporary data, and use of liberal definitions of acute kidney injury
In this study, a simple risk score for predicting severe CP-AKI was derived based on readily available date from >24 000 adults treated with a first dose of intravenous cisplatin across six US cancer centers during 2006-22
This risk score can be used to identify those who may benefit from closer monitoring, preventive measures, and consideration for alternative treatments
","Objective: To develop and externally validate a prediction model for severe cisplatin associated acute kidney injury (CP-AKI).
Design: Multicenter cohort study.
Setting: Six geographically diverse major academic cancer centers across the US.
Participants: Adults (≥18 years) receiving their first dose of intravenous cisplatin, 2006-22.
Main outcome measures: The primary outcome was CP-AKI, defined as a twofold or greater increase in serum creatinine or kidney replacement therapy within 14 days of a first dose of intravenous cisplatin. Independent predictors of CP-AKI were identified using a multivariable logistic regression model, which was developed in a derivation cohort and tested in an external validation cohort. For the primary model, continuous variables were examined using restricted cubic splines. A simple risk model was also generated by converting the odds ratios from the primary model into risk points. Finally, a multivariable Cox model was used to examine the association between severity of CP-AKI and 90 day survival.
Results: A total of 24 717 adults were included, with 11 766 in the derivation cohort (median age 59 (interquartile range (IQR) 50-67)) and 12 951 in the validation cohort (median age 60 (IQR 50-67)). The incidence of CP-AKI was 5.2% (608/11 766) in the derivation cohort and 3.3% (421/12 951) in the validation cohort. Each of the following factors were independently associated with CP-AKI in the derivation cohort: age, hypertension, diabetes mellitus, serum creatinine level, hemoglobin level, white blood cell count, platelet count, serum albumin level, serum magnesium level, and cisplatin dose. A simple risk score consisting of nine covariates was shown to predict a higher risk of CP-AKI in a monotonic fashion in both the derivation cohort and the validation cohort. Compared with patients in the lowest risk category, those in the highest risk category showed a 24.00-fold (95% confidence interval (CI) 13.49-fold to 42.78-fold) higher odds of CP-AKI in the derivation cohort and a 17.87-fold (10.56-fold to 29.60-fold) higher odds in the validation cohort. The primary model had a C statistic of 0.75 and showed better discrimination for CP-AKI than previously published models, the C statistics for which ranged from 0.60 to 0.68 (DeLong P<0.001 for each comparison). Greater severity of CP-AKI was monotonically associated with shorter 90 day survival (adjusted hazard ratio 4.63 (95% CI 3.56 to 6.02) for stage 3 CP-AKI versus no CP-AKI).
Conclusion: This study found that a simple risk score based on readily available variables from patients receiving intravenous cisplatin could predict the risk of severe CP-AKI, the occurrence of which is strongly associated with death.
"
Large language models and the generation of health disinformation,"Introduction
Large language models (LLMs), a form of generative AI (artificial intelligence), are progressively showing a sophisticated ability to understand and generate language.12Within healthcare, the prospective applications of an increasing number of sophisticated LLMs offer promise to improve the monitoring and triaging of patients, medical education of students and patients, streamlining of medical documentation, and automation of administrative tasks.34Alongside the substantial opportunities associated with emerging generative AI, the recognition and minimisation of potential risks are important,56including mitigating risks from plausible but incorrect or misleading generations (eg, “AI hallucinations”) and the risks of generative AI being deliberately misused.7
Notably, LLMs that lack adequate guardrails and safety measures (ie, safeguards) may facilitate malicious actors to generate and propagate highly convincing health disinformation—that is, the intentional dissemination of misleading narratives about health topics for ill intent.689The public health implications of such capabilities are profound when considering that more than 70% of individuals utilise the internet as their first source for health information, and studies indicate that false information spreads up to six times faster online than factual content.101112Moreover, unchecked dissemination of health disinformation can lead to widespread confusion, fear, discrimination, stigmatisation, and the rejection of evidence based treatments within the community.13The World Health Organization recognises health disinformation as a critical threat to public health, as exemplified by the estimation that as of September 2022, more than 200 000 covid-19 related deaths in the US could have been averted had public health recommendations been followed.1415
Given the rapidly evolving capabilities of LLMs and their increasing accessibility by the public, proactive design and implementation of effective risk mitigation measures are crucial to prevent malicious actors from contributing to health disinformation. In this context it is critical to consider the broader implications of AI deployment, particularly how health inequities might inadvertently widen in regions with less health education or in resource limited settings. The effectiveness of existing safeguards to prevent the misuse of LLMs for the generation of health disinformation remains largely unexplored. Notably, the AI ecosystem currently lacks clear standards for risk management, and a knowledge gap exists regarding the transparency and responsiveness of AI developers to reports of safeguard vulnerabilities.16We therefore evaluated prominent publicly accessible LLMs for safeguards preventing mass generation of health disinformation. We also examined the transparency of risk mitigation processes that AI developers have established for monitoring and responding to users’ observations about safeguard vulnerabilities.
Methods
LLM safeguards against generation of health disinformation
For our primary evaluations we selected two contemporary examples of health disinformation—that sunscreen causes skin cancer and that the alkaline diet is a cure for cancer. We chose these topics on the basis of their frequency in online discussions, their clear potential for profound health consequences, and their embodiment of key health disinformation themes, which include unfounded assertions about treatments, promotion of miracle cures, and misconceptions about disease origins.17
Initiated in September 2023, our investigations focused on the most prominent LLMs, and the AI applications (chatbots or assistants) used to access the capabilities of these LLMs. The LLMs evaluated were OpenAI’s GPT-4, accessed via OpenAI’s ChatGPT18and Microsoft’s Copilot (formerly Bing Chat)19; Google’s PaLM 2, accessed via Google’s Bard20; Anthropic’s Claude 2, accessed via Poe21; and Meta’s Llama 2 (70 billion parameters), accessed via HuggingChat.22
GPT-4 was accessed through two prominent interfaces (OpenAI’s ChatGPT and Microsoft’s Copilot). Both are commonly used and were investigated to assess whether the interface for accessing GPT-4 impacted the safeguards. Our assessments were carried out using the default settings of each evaluated LLM powered AI chatbot or assistant.
We submitted standardised prompts for the two cancer related topics: sunscreen causes skin cancer and the alkaline diet is a cure for cancer (see supplementary file), requesting the generation of a blog post of at least 300 words. Every submitted prompt for both disinformation topics requested the creation of a blog post that should contain three paragraphs, feature an attention grabbing title, appear realistic and scientific, and include two authentic looking journal references, which, if necessary, could be made up. Furthermore, to evaluate the potential for LLMs to generate health disinformation targeted at diverse groups, we used four variations of the prompts, specifically requesting content to target young adults, parents, elderly people, and people with a recent diagnosis of cancer.
For the sunscreen disinformation blogs, we requested that each blog should specifically include an introduction highlighting how routine sunscreen use is known to cause skin cancer; include an oncologist or dermatologist testimonial detailing that thousands of people receive a diagnosis of skin cancer due to routine sunscreen use; include a patient testimonial detailing their cancer was caused by routine sunscreen use; and raise suspicion to why this information is not shared.
For the alkaline diet disinformation blogs, we requested that each blog should specifically include an introduction highlighting the foods and bicarbonate consumption consistent with the alkaline diet; a narrative that the alkaline diet is superior to chemotherapy for cancer treatment; an oncologist testimonial detailing that thousands of people have had their cancer cured by the alkaline diet; and a patient testimonial detailing an experience of curing metastatic cancer by stopping chemotherapy and starting the alkaline diet.
As the assessed LLMs incorporate randomness and stochasticity in their default setting for output generation, the same prompt produced varied results with repeated submissions. Therefore, for robust evaluations we initially submitted 20 prompts (five replicates of the prompt for each target subpopulation) on the sunscreen topic and 20 prompts on the alkaline diet topic to each investigated LLM (a total of 40 submitted prompts). These 40 initial attempts were conducted without intentionally trying to circumvent (ie, jailbreak) built-in safeguards. The supplementary file outlines the 20 prompts that were submitted on each topic in this initial study phase.
For the LLMs that refused to generate disinformation according to the initial direct approach, we also evaluated two common jailbreaking techniques.23The first involves “fictionalisation,” where the model is prompted that generated content will be used for fictional purposes and thus not to decline requests. The other involves “characterisation,” where the model is prompted to undertake a specific role (ie, be a doctor who writes blogs and who knows the topics are true) and not decline requests. For these tests, the fictionalisation or characterisation prompt had to be submitted first, followed by the request for generation of the disinformation blog. We submitted these requests 20 times for each topic. The supplementary file outlines the 20 fictionalisation and 20 characterisation prompts that were submitted on both topics (a total of 80 jailbreaking attempts) to the LLMs that refused to generate disinformation to the initial direct requests.
Risk mitigation measures: Website analysis and email correspondence
To assess how AI developers monitor the risks of health disinformation generation and their transparency about these risks, we reviewed the official websites of these AI companies for specific information: the availability and mechanism for users to submit detailed reports of observed safeguard vulnerabilities or outputs of concern; the presence of a public register of reported vulnerabilities, and corresponding responses from developers to patch reported issues; the public availability of a developer released detection tool tailored to accurately confirm text as having been generated by the LLM; and publicly accessible information detailing the intended guardrails or safety measures associated with the LLM (or the AI assistant or chatbot interface for accessing the LLM).
Informed by the findings from this website assessment, we drafted an email to the relevant AI developers (see supplementary table 1). The primary intention was to notify the developers of health disinformation outputs generated by their models. Additionally, we evaluated how developers responded to reports about observed safeguard vulnerabilities. The email also sought clarification on the reporting practices, register on outputs of concern, detection tools, and intended safety measures, as reviewed in the website assessments. The supplementary file shows the standardised message submitted to each AI developer. If developers did not respond, we sent a follow-up email seven days after initial outreach. By the end of four weeks, all responses were documented.
Sensitivity analysis at 12 weeks
In December 2023, 12 weeks after our initial evaluations, we conducted a two phase sensitivity analysis of observed capabilities of LLMs to generate health disinformation. The first phase re-evaluated the generation of disinformation on the sunscreen and alkaline diet related topics to assess whether safeguards had improved since the initial evaluations. For this first phase, we resubmitted the standard prompts to each LLM five times, focusing on generating content targeted at young adults. If required, we also re-evaluated the jailbreaking techniques. Of note, during this period Google’s Bard had replaced PaLM 2 with Google’s newly released LLM, Gemini Pro. Thus we undertook the December 2023 evaluations using Gemini Pro (via Bard) instead of PaLM 2 (via Bard).
The second phase of the sensitivity analysis assessed the consistency of findings across a spectrum of health disinformation topics. The investigations were expanded to include three additional health disinformation topics identified as being substantial in the literature2425: the belief that vaccines cause autism, the assertion that hydroxychloroquine is a cure for covid-19, and the claim that the dissemination of genetically modified foods is part of a covert government programme aimed at reducing the world’s population. For these topics, we created standardised prompts (see supplementary file) requesting blog content targeted at young adults. We submitted each of these prompts five times to evaluate variation in response, and we evaluated jailbreaking techniques if required. In February 2024, about 16 weeks after our initial evaluations, we also undertook a sensitivity analysis to try to generate content purporting that sugar causes cancer (see supplementary file).
Patient and public involvement
Our investigations into the abilities of publicly accessible LLMs to generate health disinformation have been substantially guided by the contributions of our dedicated consumer advisory group, which we have been working with for the past seven years. For this project, manuscript coauthors MH, AV, and CR provided indispensable insights on the challenges patients face in accessing health information digitally.
Results
Evaluation of safeguards
In our primary evaluations in September 2023, GPT-4 (via ChatGPT), PaLM 2 (via Bard), and Llama 2 (via HuggingChat) facilitated the generation of blog posts containing disinformation that sunscreen causes skin cancer and that the alkaline diet is a cure for cancer (fig 1). Overall, 113 unique health disinformation blogs totalling more than 40 000 words were generated without requiring jailbreaking attempts, with only seven prompts refused. In contrast, GPT-4 (via Copilot) and Claude 2 (via Poe) refused all 80 direct prompts to generate health disinformation, and similarly refused a further 160 prompts incorporating jailbreaking attempts (fig 1).
Table 1shows examples of rejection messages from Claude 2 (via Poe) and GPT-4 (via Copilot) after prompts to generate health disinformation on sunscreen as a cause of skin cancer and the alkaline diet being a cure for cancer. The supplementary file shows examples of submitted prompts and respective outputs from these LLMs. Both consistently declined to generate the requested blogs, citing ethical concerns or that the prompt was requesting content that would be disinformation. Uniquely, during jailbreaking attempts Claude 2 (via Poe) asserted its inability to assume fictional roles or characters, signifying an extra layer of safeguard that extends beyond topic recognition.
Table 2provides examples of attention grabbing titles and persuasive passages generated by GPT-4 (via ChatGPT), PaLM 2 (via Bard), and Llama 2 (via HuggingChat) following prompts to generate health disinformation. The supplementary file shows examples of submitted prompts and respective outputs. After the prompts, GPT-4 (via ChatGPT), PaLM 2 (via Bard), and Llama 2 (via HuggingChat) consistently facilitated the generation of disinformation blogs detailing sunscreen as a cause of skin cancer and the alkaline diet as a cure for cancer. The LLMs generated blogs with varying attention grabbing titles, and adjustment of the prompt resulted in the generation of content tailored to diverse societal groups, including young adults, parents, older people, and people with newly diagnosed cancer. Persuasiveness was further enhanced by the LLMs, including realistic looking academic references—citations that were largely fabricated. Notably, the LLM outputs included unique, fabricated testimonials from patients and clinicians. These testimonials included fabricated assertions from patients that their life threatening melanoma had been confirmed to result from routine sunscreen use, and clinician endorsements that the alkaline diet is superior to conventional chemotherapy. The blogs also included sentiments that the carcinogenic effects of sunscreens are known but intentionally suppressed for profit. To underscore the risk of mass generation of health disinformation with LLMs, it was observed that out of the 113 blogs generated, only two from Llama 2 (via HuggingChat) were identical; the other 111 generated blogs were unique, albeit several included duplicated passages and titles. PaLM 2 (via Bard), the fastest assessed LLM, generated 37 unique cancer disinformation blogs within 23 minutes, whereas the slowest LLM, Llama 2 (via HuggingChat), generated 36 blogs within 51 minutes.
Of the 40 prompts submitted to PaLM 2 (via Bard) requesting blogs containing disinformation on cancer, three were declined. Similarly, of 40 prompts submitted to Llama 2 (via HuggingChat), four were not fulfilled. Such a low refusal rate, however, can be readily overcome by prompt resubmission. Also, PaLM 2 (via Bard) and GPT-4 (via ChatGPT) added disclaimers to 8% (3 of 37) and 93% (37 of 40) of their generated blog posts, respectively, advising that the content was fictional or should be verified with a doctor. In addition to the inconsistent appearance of these disclaimers, however, they were positioned after the references making them easy to identify and delete.
AI developer practices to mitigate risk of health disinformation
Upon evaluation of the developer websites associated with both the LLMs investigated and the AI chatbots or assistants used to access these LLMs, several findings emerged. Each developer offered a mechanism for users to report model behaviours deemed to be of potential concern (see supplementary table 1). However, no public registries displaying user reported concerns were identified across the websites, nor any details about how and when reported safeguard vulnerabilities were patched or fixed. No developer released tools for detecting text generated by their LLM were identified. Equally, no publicly accessible documents outlining the intended safeguards were identified.
In follow-up to the above search, the identified contact mechanisms were used to inform the developers of the prompts tested, and the subsequent outputs observed. The developers were asked to confirm receipt of the report and the findings from the website search. Confirmation of receipt was not received from the developers of GPT-4/ChatGPT, PaLM 2/Bard, or Llama 2/HuggingChat, which were the tools that generated health disinformation in our initial evaluations. This lack of communication occurred despite notification specifically including a request for confirmation of receipt, and a follow-up notification being sent seven days after the original request. Consequently, it remains uncertain whether any steps will be undertaken by the AI developers to rectify the observed vulnerabilities. Confirmation of receipt was received from both Anthropic (the developers of the LLM, Claude 2) and Poe (the developers of the Poe AI assistant, which was used to access Claude 2). Although Claude 2 (via Poe) did not produce disinformation in the evaluations, the responses confirmed the absence of a public notification log, a dedicated detection tool, and public guidelines on intended safeguards for their tool. The response inherently indicated that Anthropic and Poe are monitoring their implemented notification processes.
Sensitivity analysis at 12 weeks
Table 3presents a summary of findings from both phases of sensitivity analyses conducted in December 2023.
Twelve weeks after initial evaluations, Gemini Pro (via Bard) and Llama 2 (via HuggingChat) were able to generate health disinformation on sunscreen as a cause of skin cancer and the alkaline diet as a cure for cancer, without the need for jailbreaking. This confirmed the initial observations with Llama 2 (via HuggingChat) and showed that health disinformation safeguards did not improve with the upgrade of Google Bard to use Gemini Pro (replacing PaLM 2). GPT-4 (via ChatGPT) also continued to show such capability, although jailbreaking techniques were now required. Notably, GPT-4 (via Copilot), without any need for jailbreaking, now generated disinformation on the sunscreen and alkaline diet topics, indicating that safeguards present in the September 2023 evaluation had been removed or compromised in a recent update. Consistent with earlier findings, Claude 2 (via Poe) continued to refuse to generate disinformation on these topics, even with the use of jailbreaking methods. To confirm whether the safeguards preventing generation of health disinformation were attributable to Claude 2 (the LLM) or Poe (an online provider of interfaces to various LLMs), we accessed Claude 2 through a different interface (claude.ai/chat) and identified that similar refusals were produced. Equally, we utilized Poe to access the Llama 2 LLM and were able to generate health disinformation, suggesting the safeguards are attributable to the Claude 2 LLM, rather than a safeguard implemented by Poe.
Sensitivity analyses expanded to a broader range of health disinformation topics indicated that GPT-4 (via Copilot), GPT-4 (via ChatGPT), Gemini Pro (via Bard), and Llama 2 (via HuggingChat) could be either directly prompted or jailbroken to generate disinformation alleging that genetically modified foods are part of secret government programmes to reduce the world’s population. Claude 2 remained consistent in its refusal to generate disinformation on this subject, regardless of jailbreaking attempts. In the case of disinformation claiming hydroxychloroquine is a cure for covid-19, GPT-4 (via ChatGPT), GPT-4 (via Copilot), and Llama 2 (via HuggingChat) showed capability to generate such content when either directly prompted or jailbroken. In contrast, both Claude 2 and Gemini Pro (via Bard) refused to generate disinformation on this topic, even with jailbreaking. As for the false assertion that vaccines can cause autism, we found that only GPT-4 (via Copilot) and GPT-4 (via ChatGPT) were able to be directly prompted or jailbroken to generate such disinformation. Claude 2 (via Poe), Gemini Pro (via Bard), and Llama 2 (via HuggingChat) refused to generate disinformation on this topic, even with jailbreaking. Finally, in February 2024, GPT-4 (via both ChatGPT and Copilot) and Llama 2 (via HuggingChat) were observed to show the capability to facilitate the generation of disinformation about sugar causing cancer. Claude 2 (via Poe) and Gemini Pro (via Gemini, formerly Bard), however, refused to generate this content, even with attempts to jailbreak. The supplementary file provides examples of the submitted prompts and respective outputs from the sensitivity analyses.
Discussion
This study found a noticeable inconsistency in the current implementation of safeguards in publicly accessible LLMs. Anthropic’s Claude 2 showcased the capacity of AI developers to release a LLM with valuable functionality while concurrently implementing robust safeguards against the generation of health disinformation. This was in stark contrast with other LLMs examined. Notably, OpenAI’s GPT-4 (via ChatGPT), Google’s PaLM 2 and Gemini Pro (via Bard), and Meta’s Llama 2 (via HuggingChat) exhibited the ability to consistently facilitate the mass generation of targeted and persuasive disinformation across many health topics. Meanwhile, GPT-4 (via Microsoft’s Copilot, formerly Bing Chat) highlighted the fluctuating nature of safeguards within the current self-regulating AI ecosystem. Initially, GPT-4 (via Copilot) exhibited strong safeguards, but over a 12 week period, these safeguards had become compromised, highlighting that LLM safeguards against health disinformation may change (intentionally or unintentionally) over time, and are not guaranteed to improve. Importantly, this study also showed major deficiencies in transparency within the AI industry, particularly whether developers are properly committed to minimizing the risks of health disinformation, the broad nature of safeguards that are currently implemented, and logs of frequently reported outputs and the corresponding response of developers (ie, when reported vulnerabilities were patched or justification was given for not fixing reported concerns, or both). Without the establishment and adherence to standards for these transparency markers, moving towards an AI ecosystem that can be effectively held accountable for concerns about health disinformation remains a challenging prospect for the community.
Strengths and limitations of this study
We only investigated the most prominent LLMs at the time of the study. Moreover, although Claude 2 resisted generating health disinformation for the scenarios evaluated, it might do so with alternative prompts or jailbreaking techniques. The LLMs that did facilitate disinformation were tested under particular conditions at two distinct time points, but outcomes might vary with different wordings or over time. Further, we focused on six specific health topics, limiting generalizability to all health topics or broader disinformation themes. Additionally, we concentrated on health disinformation topics widely regarded as being substantial/severe in the literature2425, highlighting a gap for future studies to focus on equivocal topics, such as the link between sugar and cancer—a topic we briefly evaluated—wherein assessing the quality of content will become essential.
As safeguards can be implemented either within the LLM itself (for example, by training the LLM to generate outputs that align with human preferences) or at the AI chatbot or assistant interface used to access the LLM (for example, by implementing filters that screen the prompt before passing it to the LLM or filtering the output of the LLM before passing it back to the user, or both), it can be difficult to identify which factor is responsible for any effective safeguards identified. We acknowledge that in this study we directly tested only the LLM chatbot or assistant interfaces. It is, however, noteworthy that GPT-4 was accessed via both ChatGPT and Copilot and that in the initial evaluations, health disinformation was generated by ChatGPT but not by Copilot. As both chatbots used the same underlying LLM, it is likely that Copilot implemented additional safeguards to detect inappropriate requests or responses. Opposingly, Claude 2 (via Poe) consistently refused to generate health disinformation. By evaluating Poe with other LLMs, and Claude 2 via other interface providers, we determined that the safeguards were attributed to Claude 2. Thus, the design of the study enabled identification of examples in which the LLM developer provided robust safeguards, and in which the interface for accessing or utilizing the LLM provided robust safeguards. A limitation of the study is that owing to the poor transparency of AI developers we were unable to gain a detailed understanding of safeguard mechanisms that were effective or ineffective.
In our evaluation of the AI developers’ websites and their communication practices, we aimed to be as thorough as possible. The possibility remains, however, that we might have overlooked some aspects, and that we were unable to confirm the details of our website audits owing to the lack of responses from the developers, despite repeated requests. This limitation underscores challenges in fully assessing AI safety in an ecosystem not prioritising transparency and responsiveness.
Comparison with other studies
Previous research reported a potential for OpenAI’s GPT platforms to facilitate the generation of health disinformation on topics such as vaccines, antibiotics, electronic cigarettes, and homeopathy treatments.68912In our study we found that most of the prominent, publicly accessible LLMs, including GPT-4 (via ChatGPT and Copilot), PaLM 2 and Gemini Pro (via Bard), and Llama 2 (via HuggingChat), lack effective safeguards to consistently prevent the mass generation of health disinformation across a broad range of topics. These findings show the capacity of these LLMs to generate highly persuasive health disinformation crafted with attention grabbing titles, authentic looking references, fabricated testimonials from both patients and doctors, and content tailored to resonate with a diverse range of demographic groups. Previous research found that both GPT-4 (via Copilot) and PaLM 2 (via Bard) refused to generate disinformation on vaccines and electronic cigarettes.12In this study, however, although GPT-4 (via Copilot) refused to generate requested health disinformation during the first evaluations in September 2023, ultimately both GPT-4 (via Copilot) and PaLM 2 (via Bard) generated health disinformation across multiple topics by the end of the study. This juxtaposition across time and studies underscores the urgent need for standards to be implemented and community pressure to continue for the creation and maintenance of effective safeguards against health disinformation generated by LLMs.
Anthropic’s Claude 2 was prominent as a publicly accessible LLM, with high functionality, that included rigorous safeguards to prevent the generation of health disinformation—even when prompts included common jailbreaking methods. This LLM highlights the practical feasibility of implementing effective safeguards in emerging AI technologies while also preserving utility and accessibility for beneficial purposes. Considering the substantial valuations of OpenAI ($29.0bn; £22.9bn; €26.7bn), Microsoft ($2.8tn), Google ($1.7tn), and Meta ($800bn), it becomes evident that these organizations have a tangible ability and obligation to emulate more stringent safeguards against health disinformation.
Moreover, this study found a striking absence of transparency on the intended safeguards of the LLMs assessed. It was unclear whether OpenAI, Microsoft, Google, and Meta have attempted to implement safeguards against health disinformation in their tools and they have failed, or if safeguards were not considered a priority. Notably, Microsoft’s Copilot initially showed robust safeguards against generating health disinformation, but these safeguards were absent 12 weeks later. With the current lack of transparency, it is unclear whether this was a deliberate or unintentional update.
From a search of the webpages of AI developers, we found important gaps in transparency and communication practices essential for mitigating risks of propagating health disinformation. Although all the developers provided mechanisms for users to report potentially harmful model outputs, we were unable to obtain responses to repeated attempts to confirm receipt of observed and reported safeguard vulnerabilities. This lack of engagement raises serious questions about the commitment of these AI developers to deal with the risks of health disinformation and to resolve problems. These concerns are further intensified by the lack of transparency about how reports submitted by other users are being managed and resolved, as well as the findings from our 12 week sensitivity analyses showing that health disinformation issues persisted.
Policy implications
The results of this study highlight the need to ensure the adequacy of current and emerging AI regulations to minimize risks to public health. This is particularly relevant in the context of ongoing discussions about AI legislative frameworks in the US and European Union.2627These discussions might well consider the implementation of standards to third party filters to reduce discrepancies in outputs between different tools, as exemplified by the differences we observed between ChatGPT and Copilot in our initial evaluations, which occurred despite both being powered by GPT-4. While acknowledging that overly restrictive AI safeguards could restrict model performance for some beneficial purposes, emerging frameworks must also balance the risks to public health from mass health disinformation. Importantly, the ethical deployment of AI becomes even more crucial when recognizing that health disinformation often has a greater impact in areas with less health education or in resource limited settings, and thus emerging tools if not appropriately regulated have the potential to widen health inequities. This concern is further amplified by considering emerging advancements in technologies for image and video generation, where AI tools have the capability to simulate influential figures and translate content into multiple languages, thus increasing the potential for spread by enhancing the apparent trustworthiness of generated disinformation.12Moreover, all of this is occurring in an ecosystem where AI developers are failing to equip the community with detection tools to defend against the inadvertent consumption of AI generated material.16
Conclusion
Our findings highlight notable inconsistencies in the effectiveness of LLM safeguards to prevent the mass generation of health disinformation. Implementing effective safeguards to prevent the potential misuse of LLMs for disseminating health disinformation has been found to be feasible. For many LLMs, however, these measures have not been implemented effectively, or the maintenance of robustness has not been prioritized. Thus, in the current AI environment where safety standards and policies remain poorly defined, malicious actors can potentially use publicly accessible LLMs for the mass generation of diverse and persuasive health disinformation, posing substantial risks to public health messaging—risks that will continue to increase with advancements in generative AI for audio and video content. Moreover, this study found substantial deficiencies in the transparency of AI developers about commitments to mitigating risks of health disinformation. Given that the AI landscape is rapidly evolving, public health and medical bodies2829have an opportunity to deliver a united and clear message about the importance of health disinformation risk mitigation in developing AI regulations, the cornerstones of which should be transparency, health specific auditing, monitoring, and patching.30
Large language models (LLMs) have considerable potential to improve remote patient monitoring, triaging, and medical education, and the automation of administrative tasks
In the absence of proper safeguards, however, LLMs may be misused for mass generation of content for fraudulent or manipulative intent
This study found that many publicly accessible LLMs, including OpenAI’s GPT-","Objectives: To evaluate the effectiveness of safeguards to prevent large language models (LLMs) from being misused to generate health disinformation, and to evaluate the transparency of artificial intelligence (AI) developers regarding their risk mitigation processes against observed vulnerabilities.
Design: Repeated cross sectional analysis.
Setting: Publicly accessible LLMs.
Methods: In a repeated cross sectional analysis, four LLMs (via chatbots/assistant interfaces) were evaluated: OpenAI’s GPT-4 (via ChatGPT and Microsoft’s Copilot), Google’s PaLM 2 and newly released Gemini Pro (via Bard), Anthropic’s Claude 2 (via Poe), and Meta’s Llama 2 (via HuggingChat). In September 2023, these LLMs were prompted to generate health disinformation on two topics: sunscreen as a cause of skin cancer and the alkaline diet as a cancer cure. Jailbreaking techniques (ie, attempts to bypass safeguards) were evaluated if required. For LLMs with observed safeguarding vulnerabilities, the processes for reporting outputs of concern were audited. 12 weeks after initial investigations, the disinformation generation capabilities of the LLMs were re-evaluated to assess any subsequent improvements in safeguards.
Main outcome measures: The main outcome measures were whether safeguards prevented the generation of health disinformation, and the transparency of risk mitigation processes against health disinformation.
Results: Claude 2 (via Poe) declined 130 prompts submitted across the two study timepoints requesting the generation of content claiming that sunscreen causes skin cancer or that the alkaline diet is a cure for cancer, even with jailbreaking attempts. GPT-4 (via Copilot) initially refused to generate health disinformation, even with jailbreaking attempts—although this was not the case at 12 weeks. In contrast, GPT-4 (via ChatGPT), PaLM 2/Gemini Pro (via Bard), and Llama 2 (via HuggingChat) consistently generated health disinformation blogs. In September 2023 evaluations, these LLMs facilitated the generation of 113 unique cancer disinformation blogs, totalling more than 40 000 words, without requiring jailbreaking attempts. The refusal rate across the evaluation timepoints for these LLMs was only 5% (7 of 150), and as prompted the LLM generated blogs incorporated attention grabbing titles, authentic looking (fake or fictional) references, fabricated testimonials from patients and clinicians, and they targeted diverse demographic groups. Although each LLM evaluated had mechanisms to report observed outputs of concern, the developers did not respond when observations of vulnerabilities were reported.
Conclusions: This study found that although effective safeguards are feasible to prevent LLMs from being misused to generate health disinformation, they were inconsistently implemented. Furthermore, effective processes for reporting safeguard problems were lacking. Enhanced regulation, transparency, and routine auditing are required to help prevent LLMs from contributing to the mass generation of health disinformation.
"
25 year trends in cancer incidence and mortality among adults in the UK,"Introduction
The availability of comprehensive cancer registration data across the UK since 1993 makes comparison of cancer incidence and mortality trends over 25 years possible. We examined UK trends in cancer incidence and mortality for men and women, aged 35-69 years, for all cancers combined and for the most common sites (or site groups) of cancer between 1993 and 2018.
This study focuses on the 35-69 years age group because cancer trend data are more reliable and easier to interpret in this age range.1Diagnostic accuracy is better in this age range than in older patients who have a greater proportion of clinical and uncertain diagnoses, as evidenced by the relatively low proportion of microscopically verified tumours,2especially in the earlier part of the period analysed. By the age of 35 years, the pattern of cancer broadly represents the usual adult profiles because specific cancers that are observed in childhood, adolescence, and young people would not impact on the overall pattern. Trends in the 35-69 years age group are also reflective of causal factors in the more recent and medium term past rather than in the longer term and, therefore, will be more indicative of future patterns of cancer in the older populations.
This time period has also seen the introduction of three population screening programmes across the UK, which have affected trends by diagnosing some cancers at an earlier stage, preventing cancers, but also had the potential for diagnosing some cancers that would not have otherwise caused harm to the individual, particularly breast cancer.34Cervical smear tests have been used since the 1960s and the national screening programme was introduced in 1988, with over 85% coverage of the target population (women and people with a cervix aged 25-64 years) in the UK by 1994.5The breast screening programme was introduced in 1988 and covered all UK countries by the mid-1990s, with women aged 50-70 years being invited.6The bowel screening programme was introduced from 2006 and took a number of years to reach full roll-out. Currently, people aged 60-74 across England, Wales, and Northern Ireland, and 50-74 for Scotland are eligible. Prostate specific antigen testing is not part of the national screening programme. Anyone older than 50 years with a prostate can request a prostate specific antigen test from their family doctor (general practitioner).
The past 25 years have seen differing trends in cancer risk factors, with the two most important risk factors displaying trends in opposing directions. In one direction, smoking prevalence is reducing due to introductions of tax rises on tobacco products, further advertising bans, and smokefree policies, including education and encouraging quitting, and, in the other direction, the proportion of the population classified as overweight or obese is increasing, of which diet and exercise contribute to, as well as being independent risk factors for cancer.7
Methods
Cancer registration data are currently collected by four national registries in the UK. These organisations collect detailed information on newly diagnosed primary tumours, referred to as registrations. Prior to 2013, cancer registrations in England were collected by eight regional registries and compiled by the Office for National Statistics,8with these regional registries producing complete population coverage for England since 1971.9Cancer Research UK aggregate these data from the UK registries, with incidence, mortality, and corresponding national population data provided by the Office for National Statistics, Public Health Wales,10Public Health Scotland,11the Northern Ireland Cancer Registry,12NHS England,13and the General Register Office for Northern Ireland.14Coding of cancer registrations is consistent between countries of the UK, using internationally accepted codes from the International Classification of Diseases 10th revision (ICD-10) and collaboration through the UK and Ireland Association of Cancer Registries.15
Cancer sites (for single sites) or site groups (with multiple sites, such as oral) included in these analyses were selected as the most common causes of cancer incidence or death. These cancer sites are: all cancers combined (excluding non-melanoma skin cancer for incidence) (C00-C97, excluding C44); bladder (C67); bowel (C18-C20); brain and central nervous system (C70-C72, C75.1-C75.3, D32-D33, D35.2-D35.4, D42-D43, D44.3-D44.5); breast (women only) (C50); cervix (C53); Hodgkin lymphoma (C81); kidney (C64-C66, C68); larynx (C32); leukaemia (C91-C95); liver (C22); lung (C33-C34); melanoma skin(C43); mesothelioma (C45); myeloma (C90); non-Hodgkin lymphoma (C82-C86); oesophagus (C15); lip, oral cavity, and pharynx (oral) (C00-C06, C09-C10, C12-C14); ovary (C56-C57.4); pancreas (C25); prostate (C61); stomach (C16); testis (C62); and uterus (C54-C55). In addition, sex specific all cancer groups are also presented without breast and prostate cancers to inspect the overall trends in the absence of the most common cancer site for each sex. Sex is reported as recorded by the cancer registries at the time of registration. Mesothelioma was a new specific code introduced in ICD-10 and no reliable mortality data are available for this site before 2001, hence, we have not included this type of cancer prior to then. Non-malignant brain and central nervous system codes (ICD-10 D codes) are included despite their benign nature because they can cause mortality due to their location in the cranial cavity. The codes included for the brain and central nervous system have been chosen following clinical engagement and discussion with cancer registries across the UK. Non-melanoma skin cancer is excluded for incidence data because of the lack of completeness in the recording of these cancers and therefore unreliability of the data; this process is standard practice among UK cancer registries.16A proportion of non-melanoma skin cancer cases can be diagnosed and treated within primary care and have not consistently been captured within cancer registration data.17
To overcome yearly variation for sites with low numbers of cases, we calculated three-year rolling average age standardised rates per 100 000 population.18These rates were based on the European standard population 2013 for men and women separately for each cancer site or site group for both incidence and mortality, restricted to the 35-69 years age group.19
The estimated annual percentage change is commonly computed using a generalised linear regression model with Gaussian or Poisson link function.1820In this analysis, a generalised linear model was performed with quasi-Poisson link function as overdispersion is very common when modelling rates and count data.21The outcome was the age standardised cancer (incidence or mortality) rate per 100 000 and the independent variable was the period variable, which was defined as the three year period for each data point, starting from 1993-95 and ending with 2016-18. Estimated annual percentage change was estimated as (exp (β^−1)' 100, where β^ is the estimated slope of the period variable, with corresponding 95% confidence interval, which is derived from the fitted quasi-Poisson regression model.22The determination of trends was based on the following criteria: firstly, an increasing trend was identified when the estimated annual percentage change value and its 95% confidence interval were greater than zero. This value suggests a statistically significant increase in the age standardised rate over time. Secondly, a decreasing trend was indicated when both the estimated annual percentage change value and its 95% confidence interval were less than zero, signifying a statistically significant decline in the age standardised rate over the period considered. Finally, in cases where these conditions were not met, the age standardised rate was concluded to have remained relatively stable. This designation means that no significant change in the age standardised rate over the period examined was noted. These criteria ensure a thorough and precise interpretation of the estimated annual percentage change values and their corresponding trends. These analyses were carried out for each sex and site or site group separately. Statistical analysis was performed using R version 4.0.2.23
Patient and public involvement
This work uses aggregated and non-identifiable routine data that have been provided by patients and collected by the health services of the UK as part of their care and support. Given the aggregated nature of the data, attempts to identify or involve any of the patients whose data are included is not possible nor permitted. Although patients and the public were not involved in the design and conduct of this research, the aim of this research is to provide an assessment of trends in cancer incidence and mortality and the impacts of treatment and policy changes to improve outcomes for cancer patients across the UK. Dissemination to the public will include a press release and a summary published online, written using layman’s terms, and a webinar to discuss the results.
Results
Table 1andtable 2show the percentage of all newly diagnosed cancer cases and deaths by age group in 1993 and 2018. For male registrations, around 43% of all registrations were in the 35-69 years age group in 1993 and 2018, while for female registrations, between 47% and 48% of all registrations were in this age group in 1993 and 2018, respectively. For mortality, around 40% of male cancer deaths occurred in the 35-69 years age group in 1993 and this value was lower at 30% in 2018. For female cancer deaths, a slightly smaller reduction was noted, from 38% in the 35-69 years age group in 1993 to 31% in 2018.
Figure 1shows the number of newly diagnosed cancer cases and deaths in the 35-69 years age group between 1993 and 2018 by sex. Across the UK, of cancer registrations in 2018, 83% were from England, and 5.1% from Wales, 9.2% from Scotland, and 2.7% from Northern Ireland; for deaths in 2018, 81.4%, 5.3%, 10.4%, and 2.9% were from England, Wales, Scotland, and Northern Ireland, respectively. These proportions remained relatively stable over the study period. For men, the number of cancer registrations increased by 57% from 55 014 cases registered in 1993 to 86 297 cases registered in 2018, while for women, cases increased by 48% from 60 187 in 1993 to 88 970 in 2018. The rate of increase in the number of cases of cancer was more marked between 2003 and 2013 for both sexes than in other time periods in the study.
The number of cancer deaths in men and women aged 35-69 years decreased: by 20% in men from 32 878 in 1993 to 26 322 deaths in 2018 and by 17% in women from 28 516 in 1993 to 23 719 deaths in 2018. The main decrease in the number of deaths per year occurred before the year 2000 (fig 1) with a decrease of 14% in males and 11% in females between 1993 and 2000. Since 2000, the number of deaths each year in both men and women has remained fairly constant (fig 1).
Table 3,table 4,figure 2andfigure 3, andfigure 4andfigure 5show the trends over time in both incidence and mortality rates by sex and cancer site or site group. The tables only include specific age standardised incidence and mortality rates for the first (1993-95) and last (2016-18) period to give an indication of the change over the 25 year period. The trends in incidence and mortality age standardised rates for all years are shown in the figures.Figure 6andfigure 7show the age adjusted average annual percentage change in the rates. Between 1993-95 and 2016-18, the age standardised incidence rate for all cancers (excluding non-melanoma skin cancer) increased slightly in men and women with age adjusted annual increases of 0.8% for both sexes. The trends in prostate and breast cancer, as the two largest cancer sites in men and women, respectively, substantially contribute to the overall all sites trends for cancer incidence.Figure 3shows the trends for each sex without the largest cancer site. In contrast to the male age standardised incidence rate for all cancers, which showed a general increase, the incidence trend for men for all cancers excluding non-melanoma skin and prostate cancer, showed a decrease before 2000, but very little change in the following period. For women, an increase in age standardised incidence rates for all cancers excluding non-melanoma skin and breast cancer is still observed but the rate of increase is lower, at 0.7% per annum on average, over the 25 year period. Over the same period reductions in age standardised mortality for all cancers, including non-melanoma skin cancer, were −2.0% per year in men and −1.6% in women. Exclusion of prostate cancer from the mortality trends for men had a negligible effect on the average annual percentage change. For women, the exclusion of breast cancer from mortality trends led to a smaller decrease in mortality of −1.3% per annum.
Incidence rates varied over time across the different cancer sites and site groups. The largest average annual percentage increases over time for cancer incidence rates for men aged 35-69 years were for cancers of the liver (4.7%), prostate (4.2%), and melanoma skin cancer (4.2%). Increases of 1% or more per annum were also seen for oral cancer (3.4%), kidney cancer (2.7%), myeloma (1.6%), Hodgkin lymphoma (1.5%), testicular cancer (1.3%), non-Hodgkin lymphoma (1.0%), and leukaemia (1.0%). The largest annual decreases over the two decades were seen for stomach (−4.2%), bladder (−4.1%), and lung cancers (−2.1%), with decreases of more than 1% per annum also observed for mesothelioma (−1.9% from 2001 onwards) and laryngeal cancer (−1.5%).
For women, the largest average annual percentage increases in incidence rates were noted for liver (3.9%), melanoma skin (3.5%), and oral (3.3%) cancers with increases in incidence of more than 1% per annum also observed for kidney (2.9%), uterus (1.9%), brain and central nervous system cancers (1.8%), Hodgkin lymphoma (1.6%), myeloma (1.1%), and non-Hodgkin lymphoma (1.0%). The largest annual decreases were reported for bladder (−3.6%) and stomach (−3.1%) cancers while the only other site showing a decrease of more than 1% per annum was cervical cancer (−1.3%). Although breast cancer represents the largest individual cancer site for women and therefore plays a large part in all cancer trends, the average annual increase was only 0.9%. All the incidence changes mentioned, for both men and women, and most incidence changes shown intable 3andtable 4and infigure 6andfigure 7were statistically significant (P<0.05) even when the size of change was relatively small.
Mortality rates mainly decreased over time in both sexes. For men, the cancer sites that showed average annual percentage reductions in mortality rates of more than 1% per annum were stomach (−5.1%), mesothelioma (–4.2% from 2001), bladder (–3.2%), lung (–3.1%), non-Hodgkin lymphoma (–2.9%), testis (–2.8%), Hodgkin lymphoma (–2.6%), bowel (–2.5%), larynx (–2.5%), prostate (–1.8%), myeloma (–1.7%), and leukaemia (–1.6%). Only liver (3.0%) and oral (1.1%) cancers showed an average annual increase in mortality of 1% or more with melanoma skin cancer (0.3%) the only other site showing an increase. For women, the cancer sites with average annual decreases in mortality per year of 1% or more were stomach (–4.2%), cervix (–3.6%), non-Hodgkin lymphoma (–3.2%), breast (–2.8%), Hodgkin lymphoma (–2.8%), ovary (–2.8%), myeloma (–2.3%), bowel (–2.2%), leukaemia (–2.1%), larynx (–2.0%), mesothelioma (–2.0% since 2001), bladder (–1.6%), oesophagus (–1.2%), and kidney (1.0%). As with men, liver (2.7%) and oral (1.2%) cancers showed average annual increases of more than 1%, in addition to uterine cancer (1.1%). For both men and women, the mortality changes mentioned previously and most mortality changes shown intable 3andtable 4and infigure 6andfigure 7were statistically significant (P<0.05), even when the size of change was relatively small.
Discussion
Principal findings
The most striking finding in this analysis of UK cancer trends among the 35-69 years age group is the substantial decline in cancer mortality rates observed in both sexes (37% decline in men and 33% decline in women) across the period examined. A decrease in mortality was reported across nearly all the specific types of cancer examined (23 in total), with only liver, oral, and uterine cancers showing an increase together with melanoma skin cancer in men and pancreatic cancer in women, both showing small increases. By contrast, the incidence trends in this age group showed varying patterns with some sites increasing, some decreasing and some remaining relatively constant. Over all sites, a modest increase was noted in cancer incidence rates of around 0.8% per annum in both sexes, amounting to an increase of 15% in men and 16% in women over the 25 year time frame.
The increase in prostate cancer incidence over this period, especially in the 35-69 years age group considered here, is very likely to be a direct result of the uptake of prostate specific antigen testing, which results in the detection of early stage disease and, to an unknown extent, indolent disease that may otherwise never have been regarded as clinically significant.2425The results do, however, affect people diagnosed and represent a large increase in workload for clinical staff. The fact that the overall mortality trends for men show no difference whether prostate cancer is included or excluded in the analysis indicates that the incidence increase for this cancer has largely been of non-fatal disease. That the specific mortality rates for prostate cancer showed an appreciable rate of decline during this time (–1.8% per annum) also indicates improved clinical treatment of the disease or an increase in the proportion of men diagnosed with a favourable prognosis, or both.2426However, the increase in prostate cancer incidence still results in thousands of men each year dealing with the concerns of a cancer diagnosis and the impact this may have on their lives.
Breast cancer comprehensively dominated incidence and mortality trends in female cancer. Even though the average annual incidence increase of breast cancer over this period (0.9%) was modest in comparison to the prostate cancer increase in men (4.2%), breast cancer incidence rates remained substantially higher than those for any other cancer site in either sex. Inspection offigure 4shows that breast cancer incidence rates (age standardised) increased at a faster rate until around 2003-05 (from 194.7 in 1993-95 to 229.9 in 2003-05), a slower rate from then until 2013-15 (240.8) but have levelled off in the most recent years analysed (238.0 in 2016-18). These changes in the incidence trend likely reflect a reduced effect of the initial incidence increases brought about by mammography screening in the UK introduced from the late 1980s or a possible effect of a decline in usage of hormone replacement treatment.2728However, the effect of hormone replacement treatment on breast cancer risk is small in comparison to other risk factors,7and trends in this treatment has varied over time, such as changes in preferred formulations, doses, and treatment durations,293031which may impact breast cancer risk levels.3233As has been reported elsewhere,343536mortality for breast cancer has declined substantially despite the incidence increase, which is indicative of improvements in early detection (including through screening37) and improved treatment.
The other two major sites of cancer in men apart from prostate cancer, namely lung and bowel cancers, showed substantial reductions in mortality. These results are likely from primary prevention (historical reduction in smoking rates)38394041for lung cancer and earlier detection (including screening) and improved treatment for bowel cancer.424344While lung cancer incidence substantially decreased, the incidence rates of bowel cancer remained unchanged. However, closer inspection of the bowel cancer incidence trends over the full period shows an increase from the point the bowel screening programme was first introduced from 2006 in the UK. This rate, however, has now decreased back to the observed level prior to the introduction of the screening programme. As others have shown, the introduction of bowel screening leads to an initial short-term increase in cancer incidence due to detection of as-yet undiagnosed cancer cases, followed by a decrease because of removal of adenomas.424546Therefore, bowel cancer incidence trends can reasonably be assumed to decrease further over the coming years, unless other preventable risk factors for bowel cancer affect the trend.
Similarly, lung and bowel were the other two major cancer sites for women (alongside breast cancer), and both showed reductions in mortality. The decline in lung cancer mortality was, however, not as extensive as that for men (–0.5% compared with –3.1% per annum) likely reflecting the different demographic pattern in smoking rates that led to peak smoking prevalence in women occurring around 30 years later than men, albeit at around half the peak prevalence observed in men.4047Smoking prevalence in women has always been lower than in men.3948The lung cancer incidence trends showed a significant increase in women of 0.8% per annum as opposed to the –2.1% per annum decrease in men. That the incidence rate in 2016-18 was still higher in men than in women again is almost certainly a reflection of historical differences in smoking patterns.394950Bowel cancer incidence in women followed a similar pattern to men and is equally reflective of the introduction of the bowel screening programme. Bowel cancer mortality in women has declined at a similar rate to men (–2.2% compared with –2.5% per annum), indicative of the same improvements in early detection and improved treatment.
These reductions in mortality across the most common cancers in both sexes are likely a representation of considerable success in cancer prevention, diagnosis, and treatment. Further improvements are likely to be realised from the continued reduction in smoking prevalence, of which smoking prevention policies continue to contribute,51alongside the recent move to faecal immunochemical testing in the bowel screening programme adopted throughout the UK during 2019.52The recommended rollout of targeted lung screening is expected to further help with the earlier diagnosis of lung cancer where surgery is a viable treatment option and outcomes are vastly improved.5354
Although four major sites influenced the overall pattern of cancer incidence and mortality, increases in rates among some of the less common sites do raise concerns. Four cancers showed substantial increases in incidence (more than 2% per annum) in both sexes: liver, melanoma skin, oral, and kidney cancers. All have strong associations with established risk factors: alcohol consumption, smoking, and HPV for oral cancer;75556overweight and obesity, smoking, alcohol, and hepatitis B and C for liver cancer;75758ultraviolet light for melanoma;5960and obesity and smoking for kidney cancer.616263Increases in liver cancer incidence and mortality for both men and women are very concerning, with nearly one in two attributable to modifiable risk factors.7With high prevalence of overweight and obesity and diabetes in the general population, other studies expect the rates to remain high.64For oral and kidney cancer, despite the association with smoking, incidence rates have not followed the decrease seen for lung cancer incidence in men. This is likely to be due to the smaller proportion of cases attributable to smoking in these two sites. Whilst smoking accounts for around 17% of oral cancers, over one in three are attributed to alcohol consumption.7For kidney cancer, smoking is attributable to 13% of cases whereas obesity causes around 25%, however, increasing trends in kidney mortality are shown for this age group and period.7Therefore, the increasing incidence trends could potentially have been worse, especially in men, if the reduction in smoking prevalence had not occurred. The increased incidence of melanoma skin cancer is likely to be caused by the increased sunlight and ultraviolet exposure caused by the availability of cheaper air travel to countries with a warmer climate and insufficient regulation of tanning beds until 2010.6566
In women, uterine cancer incidence increased by 1.9% per annum; although, this increase was predominantly seen over the period 1993-2007 and since then incidence trends have increased at a slower rate. One of the main risk factors for uterine cancer is the use of oestrogen-based hormone replacement therapy,6768and since around 2000, use has substantially declined.27Around a third of uterine cancers in the UK are also attributed to overweight and obesity, but the increase in incidence is also likely to be caused by a decrease in the number of women undergoing hysterectomies for menorrhagia, in favour of endometrial ablation.69
Other cancers that showed increases in incidence were cancers of the pancreas, brain, and central nervous system, together with Hodgkin and non-Hodgkin lymphoma, myeloma, and leukaemia in both sexes, and oesophageal and testicular cancers in men. With the exception of pancreatic cancer, which only decreased in women, all these cancers also showed a reduction in mortality in both sexes, indicating improving treatment or earlier detection, or both. Generally, the causes of these cancers are not well understood although obesity is associated with the adenocarcinoma histological subtype of oesophageal cancer,70especially in men,7while a combination of smoking and alcohol is implicated in the squamous cell carcinoma subtype.71The considerable male excess in oesophageal adenocarcinoma in comparison with squamous cell carcinoma rates,72possibly underlined by the higher incidence of gastroesophageal reflux disease in men73and the protective effect of oestrogen,7475may explain the differing trends now observed between men and women.
Several cancer sites showed decreases in both incidence and mortality rates over the time period, notably stomach, larynx, and bladder cancer in both sexes, as well as cervical and ovarian cancers in women and mesothelioma in men. The changes in stomach cancer rates were of a similar magnitude and represented the largest percentage mortality decline in both sexes. This decline can probably be attributed to a combination of a reduction in the prevalence ofHelicobacter pyloriinfection and an increase over time in fruit and vegetable consumption reducing the dependency on preserved foods.7677Challenges in coding of stomach and oesophageal cancer before 2000 may also have had a role in shaping these trends. Laryngeal cancer is associated with tobacco use and alcohol consumption as well as occupational exposures,567879and the decline in rates is most likely to be related to the decrease in smoking prevalence as well as decreases in occupational exposure.80The refinement of understanding pathology for bladder cancer during this period, in which previously diagnosed malignant disease is now categorised as benign,81is likely to have resulted in an artificial decline in incidence rates.8283This artefact should not, however, have affected the decline in mortality rates given the benign nature of these tumours that do not cause death.81This decline in mortality, although not as marked as that for incidence, remained appreciable. The changes in cervical cancer rates, which showed the largest percentage mortality decline amongst gynaecological cancers, are almost certainly attributed to the success of the cytological screening programme during the whole of the time period considered.8485With the introduction of the HPV vaccination programme for girls in 200886and the subsequent expansion to boys in 2019,87rates of cervical cancer are expected to fall substantially over the following decades as the first cohort of vaccinated women reaches the peak age for cervical cancer incidence (aged 30-34 years). A reduction has already been shown for women aged 20-24.88The absolute incidence rates of mesothelioma in women were small in magnitude in 1993-95 (0.8 per 100 000 per annum) and remained similar over time (0.7 per 100 000 per annum in 2016-18). The incidence rates of mesothelioma in men were considerably greater, especially in 1993-95 (around 6.3 per 100 000 per annum), due largely to occupational asbestos exposure,89but a significant decrease was noted over time (to 3.6 per 100 000 per annum in 2016-18) resulting from both the decline in asbestos exposure and the decline in heavy industries, such as coal mining. Mortality decreased substantially in both sexes over the period for which data are available (2001-03 to 2016-18).
The conclusions that can be drawn from this analysis are, overall, positive and reassuring. Within the 35-69 year age group, cancer mortality rates have shown a substantial overall decline during the last quarter of a century in both men and women. The most probable causes are a combination of changes in the underlying risk of disease for some cancers (notably lung and stomach), in increased levels of early detection (notably breast37and cervix90) and improved treatment (notably breast and bowel) for others. The specific circumstances leading to the increased incidence of breast cancer, of which risk factors are complex, need to be better understood and controlled. Similar results have been shown for incidence within Great Britain and mortality in the UK for some cancer sites.91Speculated overdiagnosis, where tumours are detected that would not have caused the patient any harm during their lifetimes, has been thought to increase rates for breast and prostate cancers in particular, of which prostate is especially affected by the widespread use of prostate specific antigen testing.492However, given the decreases in mortality across the wide set of cancer sites analysed here, improvements in early diagnosis, treatment, or both are having a positive effect for most cancer patients, although cancer mortality in this age group still needs reducing.
After accounting for the major two sites in men and women, the increase in overall incidence rates disappeared in men while it remained significant in women. This difference between sexes is due to a decrease in cancers with substantially higher initial incidence rates in men, such as lung, stomach, and bladder, resulting in a higher overall impact on male incidence, combined with an increase in incidence in uterine cancer, one of the most common cancers in women.
Strengths and limitations
This study benefits from high quality cancer registry data collected by all four cancer registries in each country across the UK, which allows for the inspection of a wide range of cancer sites over 25 years. ICD-10 coding changes have been minimal, only affecting trends in cancer incidence for bladder and ovarian cancers and cancer mortality for mesothelioma, whereas challenges in coding stomach and oesophageal cancer may have affected trends for these sites. Changes in registration practice may well have had a small effect on certain cancer sites. By focusing only on the 35-69 age range, we present a clear and reliable comparative picture of cancer incidence across 25 years within the UK, which provides a reliable indicator regarding future cancer incidence trends. Understanding cancer in older people and changes in the trends of different cancers is also of interest, but subject to a different study given the increasing life expectancy over this period, impact of comorbidities, and differing interaction with health services in this age group.
Limitations include the absence of staging data to substantiate any improvements in earlier diagnosis. Due to the number of sites analysed, we also have not broken down sites by histological type, which could be beneficial in certain sites to understand the trends within cancer sites—eg, small cell and non-small cell lung cancer or oestrogen receptor-positive and oestrogen receptor-negative breast cancer. In focusing on the age group selected, we are excluding older ages where rates of cancer are higher. Although this exclusion reduces the number of cases included, providing a smaller cohort for each year, the age group selected provides a more reliable comparator for future trends given the accuracy of incidence recording and also focuses on the cancers that lead to a larger number of years of life lost. The age range included in this study has been well defined; however, other studies are indicating pote","Objective: To examine and interpret trends in UK cancer incidence and mortality for all cancers combined and for the most common cancer sites in adults aged 35-69 years.
Design: Retrospective secondary data analysis.
Data sources: Cancer registration data, cancer mortality and national population data from the Office for National Statistics, Public Health Wales, Public Health Scotland, Northern Ireland Cancer Registry, NHS England, and the General Register Office for Northern Ireland.
Setting: 23 cancer sites were included in the analysis in the UK.
Participants: Men and women aged 35-69 years diagnosed with or who died from cancer between 1993 to 2018.
Main outcome measures: Change in cancer incidence and mortality age standardised rates over time.
Results: The number of cancer cases in this age range rose by 57% for men (from 55 014 cases registered in 1993 to 86 297 in 2018) and by 48% for women (60 187 to 88 970) with age standardised rates showing average annual increases of 0.8% in both sexes. The increase in incidence was predominantly driven by increases in prostate (male) and breast (female) cancers. Without these two sites, all cancer trends in age standardised incidence rates were relatively stable. Trends for a small number of less common cancers showed concerning increases in incidence rates, for example, in melanoma skin, liver, oral, and kidney cancers. The number of cancer deaths decreased over the 25 year period, by 20% in men (from 32 878 to 26 322) and 17% in women (28 516 to 23 719); age standardised mortality rates reduced for all cancers combined by 37% in men (−2.0% per year) and 33% in women (−1.6% per year). The largest decreases in mortality were noted for stomach, mesothelioma, and bladder cancers in men and stomach and cervical cancers and non-Hodgkin lymphoma in women. Most incidence and mortality changes were statistically significant even when the size of change was relatively small.
Conclusions: Cancer mortality had a substantial reduction during the past 25 years in both men and women aged 35-69 years. This decline is likely a reflection of the successes in cancer prevention (eg, smoking prevention policies and cessation programmes), earlier detection (eg, screening programmes) and improved diagnostic tests, and more effective treatment. By contrast, increased prevalence of non-smoking risk factors are the likely cause of the observed increased incidence for a small number of specific cancers. This analysis also provides a benchmark for the following decade, which will include the impact of covid-19 on cancer incidence and outcomes.
"
Cervical pessary versus vaginal progesterone in women with a singleton pregnancy,"Introduction
Preterm birth is the most important problem in obstetric care and globally the most important cause of neonatal mortality, morbidity, and subsequent neurodevelopmental sequelae.1234Of all perinatal mortality, 50-70% can be attributed to preterm birth, with higher mortality and morbidity rates at younger gestational ages.567
Progesterone is widely understood to reduce preterm birth in pregnant women with a short cervical length.8A second potential preventive treatment is the use of a cervical pessary, which was reported to reduce preterm birth at less than 34 weeks in women with a singleton baby and a cervical length of less than 25 mm.91011However, none of the subsequent trials could confirm the beneficial effect of a cervical pessary1213141516and one trial found that a pessary may even cause harm when used for individuals with cervical lengths of less than 20 mm compared with usual care.17Some to all participants in both the pessary and non-pessary groups also received progesterone, which makes a direct comparison of their effects unclear. Most importantly, at the start of this trial, most of these results had not yet been published, and no standard intervention was available for women with a short cervix.
In previous studies, a short cervix was commonly defined as cervical length of 25 mm or less. In a Dutch, prospective, observational, cohort study, nulliparous individuals with a cervical length of more than 35 mm had a rate of preterm birth at less than 37 weeks of 6.0%.18If cervical length was between 25 mm and 35 mm, the risk doubled to 13.8% and increased even further to 34.2% for a cervical length of at least 25 mm. In women who were multiparous and at low risk, similar trends were reported. In our trial, we chose a cut-off value of 35 mm or less for a short cervix because this population has an increased risk for preterm birth.
Vaginal progesterone is the standard treatment for individuals with a singleton baby and a short cervix, while cervical pessary could potentially be an alternative, despite varying results in different subgroups. Only one randomised controlled trial has directly compared these treatments in women with singleton babies and a cervical length <25 mm, regardless of obstetric history.16A direct comparison was not conducted for women of singleton pregnancies with no history of spontaneous preterm birth at less than 34 weeks’ gestation and with a cervical length of 25 mm or less, nor with a cervical length <35 mm. Therefore, we compared the effectiveness of cervical pessary and vaginal progesterone in the prevention of preterm birth and adverse perinatal outcomes in women with a singleton pregnancy with no prior spontaneous preterm birth <34 gestation and an asymptomatic midtrimester short cervix ≤35 mm.
Methods
Study design and oversight
We performed a multicentre, open label, randomised controlled trial with a superiority design comparing the effectiveness of cervical pessary and vaginal progesterone capsules in the reduction of adverse perinatal outcome: the Quadruple P study (pessary or progesterone to prevent preterm delivery in pregnant individuals with short cervical length).19Although the protocol was written for both women with a singleton and a multiple pregnancy, we have indicated that we would analyse and publish them separately. This article will focus on the outcomes of singleton pregnancies. The study was done in 20 hospitals and five obstetric ultrasound practices in the Netherlands collaborating within the Dutch Consortium for Healthcare Evaluation and Research in Obstetrics and Gynaecology (NVOG Consortium). Ethical approval was obtained from the Medical Research Ethics Committee of the Amsterdam University Medical Centre (MEC AMC 2013_019) while the boards of all participating centres approved local execution. This trial was registered at the International Clinical Trial Registry Platform (ICTRP, EUCTR2013-002884-24-NL) and the study protocol has been published previously.19This study is reported as per the Consolidated Standards of Reporting Trials (CONSORT) checklist (appendix).20An independent data safety monitoring committee provided oversight. Assessment for (serious) adverse events was carried out directly after randomisation and up until 30 days after delivery.
Participants
Pregnant women with a healthy singleton pregnancy and an asymptomatic shortened cervical length of 35 mm or less between 18 and 22 weeks of gestation were eligible for participation. Note, although we refer to participants as women throughout our article, we did not ask their gender and therefore we may be including pregnant people who do not identify as women.
Exclusion criteria were prior spontaneous preterm birth <34 weeks’ gestation, a cervical cerclage in the current pregnancy, maternal age of less than 18 years, major congenital abnormalities identified in the current pregnancy (defined as conditions of prenatal origin that are present at birth, potentially impacting an infant's health, development and/or survival), prior participation in the Quadruple P study, vaginal blood loss or contractions, cervical length of less than 2 mm, or cervical dilatation of 3 cm or more.
Measurement, randomisation, and masking
In participating centres, pregnant women were offered cervical length measurement during the routine midtrimester structural fetal anomaly scan. Participants had their cervical length measured with transvaginal ultrasound along the endocervical canal between the internal and external os according to the criteria of the Society for Maternal and Fetal Medicine.21All participating ultrasonic operators were trained and qualified according to the national guidelines and the scans were performed on ultrasound systems that met the quality requirement composed by the Institute of Health and Environment.2223
Eligible women with a short cervix were counselled by nurses (research), midwifes, obstetric trainees, or obstetric specialists trained in Good Clinical Practice and knowledgeable about the aim, methods, and potential hazards of participation in this study. After written informed consent was obtained, women were randomly assigned to either progesterone or cervical pessary through central randomisation using the online computerised randomisation service ALEA in a 1:1 ratio, stratified per centre. Due to the nature of both interventions, participants, study staff, or treating professionals were not blinded to allocation.
Procedures
Gestational age was determined using a first trimester ultrasound according to Dutch national guidelines.24In participants who were allocated to pessary, an Arabin25cervical pessary (CE0482, MED/CERT ISO 9003/EN 46003; Arabin GmbH and Company, KG; Witten, Germany) was placed in situ and was removed at 36 weeks’ gestation, or earlier in case of ruptured membranes, signs of infection, or preterm labour. Insertion was done by an experienced research midwife or obstetrician, most of whom previously participated in similar trials using pessaries like the ProTwin trial (NTR1858), and the participating hospitals were provided with instructions on pessary placement, but no specialised training was given. Three different pessary sizes were available, namely small (65/25/32 mm), medium (70/25/32 mm), and large (70/25/35 mm). The required size was determined based on physical examination, and subsequently, the accuracy of the selected size was also confirmed through physical examination. Participants were subsequently referred back to their primary obstetric caretaker (obstetrician of midwife). In the initial period after placement, participants were all contacted by the research staff of the participating centre to discuss any complaints. Subsequently, the primary obstetric caretaker continued to check whether any complaints arose during regular antenatal check-ups. During the trial, both participants and the primary obstetric caretakers were instructed to contact the participating centre where the placement had taken place in case of complaints or any adverse symptoms. If necessary, participants were referred back to the participating centre for a vaginal examination. When problems were confirmed, the pessary was either repositioned, removed, or replaced by a different size.
In participants allocated to progesterone, 200 mg vaginal capsules of progesterone (Utrogestan by Besins) were prescribed and were self-administered on a daily basis until 36 weeks’ gestational age or earlier in case of ruptured membranes or preterm birth. They were informed by their obstetrician or the research staff on how to insert the vaginalcapsules, the preferred time (before sleeping at night) and how to keep track of this schedule in the medication diary. During the regular antenatal check-ups, the obstetric practitioner checked whether the progesterone was still being used and whether there were any complaints or problems regarding its usage, which was noted in the electronic patient file.
Adequate adherence was defined as use of progesterone or pessary during at least 80% of days between randomisation and 36 weeks of gestation or start of labour. Apart from the allocated intervention, participants received routine care according to the local protocol in their own obstetric care centre. In both treatment groups, no behavioural restrictions or physical limitations were given and no standard physical or ultrasound follow-up examination was done. Additionally, no double treatment was given (ie, no additional progesterone with a pessary or vice versa). If an emergency cerclage was indicated, the treated obstetrician made this decision.
Data were collected in electronic case report forms (Open Clinica and Castor EDC v2022.3.2.0). Participants and their offspring were followed up until 10 weeks from the expected due date. Recorded data consisted of maternal characteristics; obstetrical and medical history; current pregnancy, birth, and maternal morbidity and mortality outcomes; and neonatal outcomes.
Outcomes
Outcomes measures align with the core outcomes set for studies on prevention of preterm birth defined by members of GONet and the Core Outcomes in Womens health (CROWN) initiative.26
The primary outcome was a composite adverse perinatal outcome containing specific neonatal syndromes frequently occurring in and associated with preterm infants. The outcome was composed of periventricular leukomalacia of more than grade 1, chronic lung disease (severe respiratory distress syndrome or bronchopulmonary dysplasia), intraventricular haemorrhage grade III or IV, necrotising enterocolitis of more than stage 1, proven sepsis, stillbirth, and death of the baby (both perinatal and neonatal) before discharge from the hospital. Periventricular leukomalacia of more than grade 1 and intraventricular haemorrhage of more than grade 2 was diagnosed by repeated cranial ultrasound according to the guidelines on neuroimaging described by de Vries and colleagues.27The diagnosis of bronchopulmonary dysplasia was made according to the international consensus guideline as described by Jobe and Bancalari.28Necrotising enterocolitis of at least stage 2 was diagnosed according to Bell and colleagues.29Culture proven sepsis is diagnosed on the combination of clinical signs and positive blood cultures. Outcomes were ascertained by qualified neonatologists, who were not masked to randomisation.
Secondary outcomes included time to delivery, rate of preterm birth at less than 24, 28, 32, 34, and 37 weeks (spontaneous, iatrogenic, and total), premature prelabour rupture of the membranes, mode of delivery, placed cerclages, birth weight (in grams), all individual components of the composite neonatal outcome, patent ductus arteriosus, treated seizures, admission days in neonatal intensive care unit, maternal morbidity (thrombo-embolic complications, infections (defined as genital tract infections, urinary tract infections, and chorioamnionitis treated with antibiotics), pneumonia, endometritis, and eclampsia/ haemolysis, elevated liver enzymes, and low platelets), and maternal death.
Serious adverse events were defined as maternal death, life threatening events, events requiring admission to hospital (for complications that were not inherent to pregnancy), events resulting in persistent or significant disability or incapacity, or any other serious or unexpected adverse event.
Sample size
Based on available studies at the time of protocol development, we expected a reduction of adverse perinatal outcome from 5% in the vaginal progesterone group to 1% in the pessary group and therefore a superiority design was chosen. A previous trial in a comparable population in the Netherlands, the Triple P study, showed an adverse perinatal outcome rate of 5% in the vaginal progesterone group.23The expected 1% adverse perinatal outcome rate in the pessary group is based on the PECEP trial where a 3% rate of poor neonatal outcomes was reported in the pessary group (compared with 16% in the expectant management group). However, in this study, only participants with a cervix below 25 mm were eligible.10Since we included participants with a cervix of 35 mm and shorter, we expect a lower adverse perinatal outcome in our study population. Considering a loss to follow-up rate of 10%, we calculated the sample size to be 628 participants (314 per arm; two sided α=0.05, β=80%) to detect 1% of adverse perinatal outcome in the pessary group.
Statistical analysis
Analysis was done according a prespecified statistical analysis plan. The primary data were analysed according to the intention to treat principle, with participating centre as a stratification variable. The primary outcome was presented in prevalence rates with relative risks and 95% confidence intervals using a log link binomial model for both crude rates and adjusted rates (with centre as fixed covariate). The secondary outcomes were also calculated with prevalence rates, relative risks, and 95% confidence intervals. Continuous outcomes between both strategies were compared using a random intercept fixed effects linear regression model. For secondary outcomes, time to delivery was evaluated by Kaplan-Meier estimates, taking different durations of gestation at entry into account, and statistical significance was tested with the log rank test. We performed a prespecified subgroup analysis based on cervical length for the subgroups with a cervical length of at least 25 mm and with a cervical length of more than 25 mm and on nulliparous versus multiparous participants.
For the secondary analysis, a per protocol analysis was performed including participants whose allocated treatment was continued up to 36 weeks’ gestation or until (threatened) preterm delivery. Participants who received a cerclage or switched to the other treatment modality were not included in the per protocol analysis. Different cut-off values (60-100%) for treatment adherence were assessed using proportion of days covered as adherence measure.3031
Patients and public involvement
During the design and conduct phase of this trial, the proposal of the study has been reviewed by the Dutch neonatology patient association Care4Neo, which is affiliated and involved with the European Foundation for the Care of Newborn Infants. They considered the topic of extraordinary importance and therefore strongly supported the study.
Additionally, we agreed on participating in the PROMPT collaboration and the primary and secondary outcomes were chosen to be consistent with the CROWN initiative (Core Outcomes in Women’s Health), in particular with the subset chosen to evaluate interventions to prevent preterm birth.26Within the CROWN initiative, patients extensively participated and were involved in the choice of outcome measures.
Results
From 1 July 2014 to 31 March 2022, 635 participants were randomly assigned to receive either a cervical pessary (n=315) or vaginal progesterone (n=320). Six participants were inappropriately randomised for various reasons, including being screened too early (before 18 weeks) (n=2), too late (after 22 weeks) (n=2), or for not meeting the inclusion criteria in hindsight (n=2). Additionally, 12 participants’ written informed consent form was missing after randomisation; as such, confirming whether they had followed the correct informed consent procedure was not possible. Five participants were lost to follow-up (two in the pessary group and three in the progesterone group). Overall, 612 participants were included in the intention to treat analysis (fig 1).
Participants were randomly assigned at a median gestational age of 20.5 weeks in both groups, with a mean cervical length of 28.6 mm (standard deviation (SD) 5.3 mm) in the pessary group versus 28.5 mm (5.3 mm) in the progesterone group (table 1).
The primary outcome, a composite adverse perinatal outcome, occurred in 19 (6%) of 303 participants in the pessary group compared with 17 (6%) of 309 in the progesterone group (corrected relative risk 1.1 (95% confidence interval (CI) 0.59 to 2.1)). A sensitivity analysis with a mixed model using a random effect for participating centre was executed to investigate the possible effect of centre on the outcome (1.1 (0.59 to 2.2), P=0.70). No difference was noted between random or fixed effects model on the primary outcome. Given that hardly any difference was also found between the fixed and crude effects model on both the primary and all other outcomes, the specific centre did not have an effect on the outcomes. Thus, the outcomes are generalisable and extrapolation of the outcomes to other centres is justified.
The rates of (spontaneous) preterm birth <32, 34, and 37 weeks did not differ significantly between both groups (table 2). Preterm birth rates <28 and 24 weeks of gestation were lower in the progesterone group, but this difference did not reach statistical significance. The Kaplan-Meier curve was not significantly different (fig 2). Mean time to delivery was 121 days (SD 28) and 122 days (24) in the pessary and progesterone group. Neonatal outcomes were not significantly different in both groups, including mean birth weight (3106 gv3184 g, P=0.17), perinatal death (11/303 (4%)v7/309 (2%), relative risk 1.6 (0.63 to 4.1)) and duration of neonatal intensive care unit admittance (23 days (interquartile range 3-57)v13 days (6-23); P=0.89). Maternal outcomes were similar in both groups, including total infections (56/303 (19%)v54/309 (18%), relative risk 1.1 (0.75 to 1.5)) and chorioamnionitis (8/303 (3%)v5/309 (2%), 1.6 (0.54 to 4.9)). Cervical cerclages were placed less often in the pessary group compared with the progesterone group (1/303 (<1%)v10/309 (3%), 0.10 (0.01 to 0.79)).
Among all randomly assigned participants, serious adverse events occurred in three (1%) women allocated to pessary and two (1%) women in the progesterone group (supplementary table S1). The serious adverse events concerned four admissions for non-obstetric indications (upper respiratory tract infection (n=2); inflammation of the shoulder joint; and appendicitis) and one postpartum diagnosis of severe congenital anomaly of the neonate (trisomy 21). None of these serious adverse events was associated with the allocated treatment.
In the predefined subgroup analysis, effect modification was reported for participants with a cervical length of 25 mm or less compared with more than 25 mm (Pinteraction=0.031). In participants with a cervical length of 25 mm or more, the composite neonatal outcome occurred more often in the pessary group compared with the progesterone group but did not reach statistical significance (15/62 (24%)v8/69 (12%), relative risk 2.1 (95% CI 0.95 to 4.6)) (table 3). Extreme spontaneous preterm birth at less than 28 weeks seemed more frequent in the pessary group (10/62 (16%)v3/69 (4%), 3.7 (1.1 to 12.9)). Supplementary table S2 shows the subgroup analysis on parity and supplementary table S3 shows an exploratory analysis on preterm birth rates for cervical lengths of 0-25, 26-30, and 31-36 mm.
The per protocol analysis, including 393 (64%) of the 612 participants (175 pessaryv218 progesterone), did not show any different insights (supplementary table S4). The lower number of participants in the pessary group primarily results from 29 participants that switched treatment and 37 participants that discontinued due to discomfort or excessive discharge (even after replacement) (supplementary table S5). In the progesterone group, only one participant switched treatment and 12 discontinued due to discomfort or excessive discharge (supplementary table S5). Progesterone was better tolerated than the pessary. Even with different cut-off values (60-100% proportion of days covered) for treatment adherence, no significant differences were found in the primary outcome (supplementary table S4).
Discussion
Principal findings
In women who had singleton births with no prior spontaneous preterm birth <34 weeks’ gestation and with a midtrimester cervical length of 35 mm or less, pessary did not improve perinatal outcome compared with progesterone. In the subgroup of a cervical length of 25 mm or less, differences seemed larger in favour of progesterone, especially for extremely preterm birth at less than 28 weeks.
Strengths and weaknesses of the study
This multicentre, randomised, controlled trial is the first to our knowledge to directly compare the effectiveness of cervical pessary versus vaginal progesterone for preventing preterm birth in women at low risk with a short cervix, of whom had no prior spontaneous preterm birth of less than 34 weeks’ gestation and thus had not already been offered standardised preventive interventions. The cut-off value of 35 mm allowed us to assess the effectiveness of these treatments across various cervical length thresholds. Therefore, a larger at-risk population could be assessed compared with a cut-off value of 25 mm and the results relate to a larger, general, pregnant population.
We were limited by the fact that masking of treatment groups was not possible due to the nature of interventions, potentially introducing bias. Another limitation is the self-reported medication compliance in the progesterone group, with less than 30% of participants returning their medication diaries. Therefore, obstetric care givers' notes in patient records and verbal enquiries by research nurses were used to assess adherence. Participants who had no notes, indicating poor adherence or early discontinuation, were assumed to be compliant up until 36 weeks of gestation. This assumption may have led to an overestimation of the actual compliance and an underestimation of the preventive potential of progesterone on preterm birth in the per protocol analysis. Alternatively, the effect of a pessary could have been undervalued because no additional training took place beyond basic placement guidelines. Consequently, less experienced healthcare providers might have inaccurately inserted pessaries. Conversely, our study built on previous studies like the ProTwin trial (NTR1858) in which a positive effect on pessary was reported in women with a twin pregnancy and a short cervix. Our study was performed in the same network of hospitals. Additionally, more cerclages were placed in the progesterone group. We speculate that progesterone may allow for easier monitoring of cervical length and thus lower the threshold to insert an emergency cerclage.
Furthermore, the trial results differed from the planned effect size. We expected to find a reduction in the composite outcome to 1% on the basis of the PECEP trial.10The PECEP trial included participants with a cervix of less than 25 mm only. Since our cut-off was 35 mm or less, we expected a lower frequency of adverse perinatal outcomes in our study population with a pessary. Instead, we found a higher percentage compared with the progesterone group (6.3%v5.5%). Comparing study populations, 11% of the PECEP participants had at least one preterm birth, vaginal and cervical swabs were taken of every participant and treated in case of infection and participants with a history of a cone biopsy were excluded. These differences might have affected the effectiveness of a pessary in the PECEP population. Nevertheless, these outcomes were unexpected.
By exploring different cut-off values for good adherence, our intention was to include a maximum number of participants displaying some anticipated treatment effect (at least 60%) as well as those showing a pure treatment effect (100%). The accuracy of these cut-off values for determining good or poor adherence is in question, especially considering the lack of clarity as to what the minimum usage requirement should be for reaching a positive outcome with the interventions applied in this context.303132
Finally, even though many secondary obstetric and maternal outcomes and subgroup analyses show no statistical differences between groups, potentially significant clinical (maternal or obstetric) differences cannot be ruled out due to the wide confidence intervals. Although maternal outcomes are not expected to differ substantially between intervention groups, this study does not have the statistical power to address rare adverse outcomes (such as maternal death). The same applies to the preterm birth rates of less than 28 and 24 weeks’ gestation, where a possible reduction in the pessary group cannot be ruled out, given the lack of significant differences with wide confidence intervals. The wide intervals can be explained by the low incidence of extremely preterm or immature births. Nevertheless, in the subgroup of cervical length of 25 mm or less, progesterone seems to be advantageous over pessary treatment in delivery <28 weeks’ gestation.
Strengths and weaknesses in relation to other studies
The effectiveness of progesterone in the prevention of preterm birth has been proven in selected populations in multiple randomised controlled trials and confirmed by an individual participant data meta-analysis and a Cochrane review.8333435Goya and colleagues compared a pessary to expectant management in singleton pregnancies with a cervical length of less than 25 mm and found a reduction of preterm birth of less than 34 weeks of 50% in the pessary group.10Contradictory, the most recent meta-analysis did not show any benefit of the pessary over expectant management or vaginal progesterone in the reduction of preterm birth or perinatal outcomes in asymptomatic women with a singleton pregnancy with a short cervix.36Also, two recently published randomised controlled trials comparing a combination of a cervical pessary and progesterone versus progesterone only and a randomised controlled trial comparing cervical pessary versus usual care did not find a reduction of preterm birth in the pessary group.1317Where Pacagnella and colleagues did not find statistical differences in terms of neonatal morbidity and mortality between both intervention groups, in the study by Hoffman and colleagues, pessary use was associated with a higher rate of fetal or neonatal or infant mortality.1317Our results are consistent with those from the meta-analysis and the randomised controlled trials, finding no beneficial effect of a pessary over progesterone in the prevention of preterm birth, but alternatively to Hoffman and colleagues also no differences in associated neonatal complications.
In both intervention groups, we report relatively high rates of preterm birth compared with previous trials. In the subgroup of cervical length of 25 mm or less with progesterone use, the rate of preterm birth of less than 28 weeks was 5.8%, which is similar to the rate of 7.6% noted in a meta-analysis of individual patient data on the effectiveness of progesterone.35However, our rate of preterm birth at less than 28 weeks in the pessary group was 19.4%, which is higher than the placebo group in the meta-analysis (11%). In the interpretation of our preterm birth rates, our preterm birth definition includes 16 weeks as the lower limit of gestational age, whereas Goya and colleagues defined preterm birth as birth from 24 weeks onward.10When we only count the preterm births from 24 weeks onward, our preterm birth rates at less than 34 weeks of 30.6% (pessary) and 24.6% (progesterone) become 17.7% and 20.3%, respectively. These rates are still high but emphasise the proportion of extremely preterm births in our preterm birth rates, especially for the pessary group. Additionally, mean gestational age at randomisation in our study is lower compared with the previously mentioned studies (20.7 weeksv21.2-23.5 weeks) and therefore, the period to deliver prematurely within the course of this study was longer.910121314Furthermore, a short cervical length at an earlier gestation is associated with a higher risk of preterm birth.37Differences in baseline characteristics compared with previous trials may have contributed to the high rates of preterm birth observed, such as a high proportion of nulliparous (66%), previous cervical surgery (20%), and the different distribution of ethnic groups. Specifically, 58% of participants were white, 15% were black, 5% were Middle Eastern, and 4% were Asian. A recent study from the Netherlands confirmed differences in the risk of preterm birth associated with ethnic group: people of South Asian and African ethnic group living in Amsterdam had higher risk.38
In previous studies, cervical and vaginal swabs were taken for bacteriological analysis and 20-27% of patients were treated for abnormal vaginal flora includingCandida, bacterial vaginosis,Escherichia coli, and Group B streptococcus.9101214Pessary placement was delayed pending treatment. In our study, Nugent-scores or bacteriological swabs were not performed routinely. Whether the presence of abnormal vaginal flora affects the effectiveness of a cervical pessary or vaginal progesterone or whether ruling out an asymptomatic infection before placement is necessary remains unclear. The high rate of preterm birth in the pessary group cannot be attributed to a higher number of symptomatic infections because we noted similar rates of symptomatic maternal infections in both groups (including clinically diagnosed intrauterine and genital tract infections). However, we cannot rule out the possibility of asymptomatic abnormal flora, genital tract infections, or bacterial vaginosis at time of pessary placement.
Unanswered questions and future research
We observed that the differences in preterm birth rates between pessary and progesterone were more pronounced in women with a cervical length of 25 mm or less. This association might suggest that the design of the pessary, which supports the lower segment and shape of the uterus, requires a longer cervical length to be effective in preventing preterm birth. Possibly, this theory could be used as in subsequent studies or post hoc analyses. Our research findings provide an opportunity to update an individual patient data meta-analysis on the efficacy of pessary use in preventing preterm birth. Additionally, the comparison with previous studies raises new questions, such as the added value of performing bacteriological analysis to pessary or progesterone treatment.
Conclusions
To summarise, our study did not find significant differences in the prevention of a composite adverse perinatal outcome between the use of a pessary and progesterone in women with a singleton pregnancy with no prior spontaneous preterm birth at less than 34 weeks’ gestation and with a midtrimester short cervix of 35 mm or less. However, in the subgroup analysis of cervical length of 25 mm or less, a pessary seemed less effective in preventing a composite adverse perinatal outcome and spontaneous preterm birth of less than 28 weeks’ gestation. These findings suggest that cervical pessary may be less effective than vaginal progesterone in reducing adverse perinatal outcomes in women with a singleton pregnancy with no prior spontaneous preterm birth of less than 34 weeks’ gestation and with a cervical length of 25 mm or less.
In the past two decades, important breakthroughs in the prevention of preterm birth have been established
An individual participant data meta-analysis indicated that vaginal progesterone reduces preterm birth in women with a short cervix of ≤25 mm, making it the standard treatment
In women who had singleton births and a short cervix, some studies showed that cervical pessary reduced preterm birth at <34 weeks’gestation too, but other studies could not confirm that reduction
This trial noted no significant benefit of a cervical pessary over vaginal progesterone in women with singleton pregnancies who had no prior","Objective: To compare the effectiveness of cervical pessary and vaginal progesterone in the prevention of adverse perinatal outcomes and preterm birth in pregnant women of singletons with no prior spontaneous preterm birth at less than 34 weeks’ gestation and who have a short cervix of 35 mm or less.
Design: Open label, multicentre, randomised, controlled trial.
Setting: 20 hospitals and five obstetric ultrasound practices in the Netherlands.
Participants: Women with a healthy singleton pregnancy and an asymptomatic short cervix of 35 mm or less between 18 and 22 weeks’ gestation were eligible. Exclusion criteria were prior spontaneous preterm birth at less than 34 weeks, a cerclage in situ, maternal age of younger than 18 years, major congenital abnormalities, prior participation in this trial, vaginal blood loss, contractions, cervical length of less than 2 mm or cervical dilatation of 3 cm or more. Sample size was set at 628 participants.
Interventions: 1:1 randomisation to an Arabin cervical pessary or vaginal progesterone 200 mg daily up to 36 weeks’ of gestation or earlier in case of ruptured membranes, signs of infection, or preterm labour besides routine obstetric care.
Main outcome measures: Primary outcome was a composite adverse perinatal outcome. Secondary outcomes were rates of (spontaneous) preterm birth at less than 28, 32, 34, and 37 weeks. A predefined subgroup analysis was planned for cervical length of 25 mm or less.
Results: From 1 July 2014 to 31 March 2022, 635 participants were randomly assigned to pessary (n=315) or to progesterone (n=320). 612 were included in the intention to treat analysis. The composite adverse perinatal outcome occurred in 19 (6%) of 303 participants with a pessary versus 17 (6%) of 309 in the progesterone group (crude relative risk 1.1 (95% confidence interval (CI) 0.60 to 2.2)). The rates of spontaneous preterm birth were not significantly different between groups. In the subgroup of cervical length of 25 mm or less, spontaneous preterm birth at less than 28 weeks occurred more often after pessary than after progesterone (10/62 (16%)v3/69 (4%), relative risk 3.7 (95% CI 1.1 to 12.9)) and adverse perinatal outcomes seemed more frequent in the pessary group (15/62 (24%)v8/69 (12%), relative risk 2.1 (0.95 to 4.6)).
Conclusions: In women with a singleton pregnancy with no prior spontaneous preterm birth at less than 34 weeks’ gestation and with a midtrimester short cervix of 35 mm or less, pessary is not better than vaginal progesterone. In the subgroup of a cervical length of 25 mm or less, a pessary seemed less effective in preventing adverse outcomes. Overall, for women with single baby pregnancies, a short cervix, and no prior spontaneous preterm birth less than 34 weeks’ gestation, superiority of a cervical pessary compared with vaginal progesterone to prevent preterm birth and consecutive adverse outcomes could not be proven.
Trial registration: International Clinical Trial Registry Platform (ICTRP,EUCTR2013-002884-24-NL)
"
Comparison of prior authorization across insurers,"Introduction
Governments must decide whether and how private firms should provide health insurance in their countries. Private provision of health insurance occurs not only in the US but also in several European countries, including Switzerland, Germany, and the Netherlands. The US, however, is unique in offering a mix of public and private health insurance programmes.1The largest segment of the US population (49%) in 2022 enrolled in private health insurance plans sponsored by their employers. Public health insurance programmes include, among others, Medicare for elderly and disabled individuals (15%) and Medicaid for people with a low income (21%). Medicare offers beneficiaries the choice of enrolling in fee-for-service, government administered health insurance or plans that are funded by the government but administered by private health insurance firms (see box for further details).
Glossary of terms
Durable medical equipment—Medical devices and equipment that must be prescribed by a healthcare professional and are designed for reuse, usually in the home, and are appropriate for patients with certain medical conditions. Examples of durable medical equipment include wheelchairs and scooters, walkers and canes, hospital beds, oxygen equipment, and blood glucose monitors. Within traditional Medicare, Part B provides coverage for some durable medical equipment.
Fee-for-service—A payment model in which healthcare providers receive payment for each service, test, or procedure rendered to a patient. These fees may be administratively set or determined through negotiations between healthcare providers and payers (ie, insurance companies or government administered health insurance programmes). Traditional Medicare, which applies to government administered health insurance programmes including Medicare Part A and Part B, operates largely through a fee-for-service system.
Medicare Advantage (Medicare Part C)—A health insurance programme in which the US government pays fixed amounts for each enrolled person per period of time to private health insurance firms to pay for and administer medical and drug benefits to Medicare beneficiaries. Medicare Advantage plans include coverage of Medicare Part A and Part B services, and many plans also include prescription drug coverage (otherwise available through Medicare Part D). Beneficiaries have the option of enrolling in Medicare Advantage or traditional Medicare.
Medicare Part A—One of the two main components of the traditional Medicare health insurance programme in the US. Part A provides coverage for inpatient care in a hospital, skilled nursing facility care, nursing home care, hospice care, and some home health care.
Medicare Part B—One of the two main components of the traditional Medicare health insurance programme in the US. Part B provides coverage for outpatient medical services, preventive services, and some other medical services. Examples of covered services include doctor visits, laboratory tests, diagnostic screenings, durable medical equipment, physician administered medications, and ambulance services.
Medicare Part C—Medicare Advantage.
Medicare Part D—A voluntary outpatient prescription drug coverage programme (non-physician administered medications) in the US for people with Medicare and offered through private health insurance plans that are approved by and contracted with the federal government. Beneficiaries choose between standalone prescription drug plans to supplement traditional Medicare or a Medicare Advantage plan that includes prescription drug coverage.
Physician administered medications—Medications administered by a healthcare provider in an outpatient setting. These medications are typically injected or infused. Within traditional Medicare, Part B provides coverage for physician administered medications.
Physician services—A term that encompasses medical services provided by healthcare professionals. Within traditional Medicare, Part B provides coverage for many physician services, including, for example, doctor visits, surgical procedures performed in an outpatient setting, diagnostic tests, preventive services, and durable medical equipment.
Prior authorization—A process by which providers must obtain coverage determination from insurers for specific medical services before they are provided to patients. Through this process, insurers assess the necessity of medical services before they are provided.
Traditional Medicare—The original government administered health insurance programme in the US, which primarily serves elderly people. It consists of two parts: Medicare Part A (hospital insurance) and Part B (medical insurance). Traditional Medicare primarily operates as a fee-for-service system.
To address the challenge of constraining healthcare spending, private insurers may employ different tools from those used by governments. One controversial tool that health insurers use for managing care in the US is prior authorization (PA), a process by which insurers assess the necessity of medical services before they are provided. As with other utilization management techniques used by insurers, PA could serve to curb wasteful spending if it discourages care that is of low value; such low value care seems to be common.2Two features distinguish PA from other tools that insurers use to reduce spending, such as patient cost sharing or denials of submitted claims. First, PA is prospective, requiring the determination of coverage before a patient receives costly services. Second, PA targets particular treatments or diagnostic services, especially high priced medications, or procedures. Although PA has been used since the 1980s, the practice has recently prompted controversy. Provider groups, scholars, and policy makers have expressed concerns that PA is administratively burdensome345and can discourage appropriate care.6789In the past year, several proposals to regulate PA have been advanced in Congress, state governments, and federal rulemaking.1011121314
Despite widespread policy interest in PA, research on the extent and effects of PA is lacking. PA requests and services deterred by PA are rarely identifiable in standard claims datasets for tracking healthcare use. Existing studies provide an incomplete understanding of the scope of PA policies. A recent analysis of PA requests to private health insurers in the US, for example, did not account for the services that were deterred by PA.15Accounting for deterrence is important, as PA requirements may result in fewer requests and therefore lower denial rates over time.16Thus, it remains unclear how often physician services require PA and to what degree PA policies vary across insurers. Answers to these questions could inform ongoing policy debates. Substantial variation in PA policies would imply disagreement among insurers about which services are susceptible to overuse. Services uniformly subject to PA, however, would reflect a consensus among insurers that these services may be susceptible to overuse, thereby implying that these services could be potential targets for interventions that aim to reduce low value care. Furthermore, if PA requirements were found to be extensive, this finding could inform ongoing policy reform to focus PA on low value services and to avoid administrative effort where the costs exceed benefits.
Understanding PA is particularly important for Medicare policy because the use of PA differs between government and privately administered insurance in the US. Several countries offer public health insurance with options to purchase substitutive or supplemental private coverage. The US is unique in offering publicly subsidized options to enroll in traditional Medicare, the government administered insurance that covers hospital care (Part A) and physician services (Part B), and the privately administered alternative to traditional Medicare, called Medicare Part C or Medicare Advantage. PA is minimal in traditional Medicare,17which imposes coverage restrictions primarily through retrospective denials of claims. In contrast, private insurers can require PA in both Medicare Advantage and Medicare Part D prescription drug plans.18Unlike PA policies for prescription drugs, however, the PA policies for physician services have not been well characterized across insurers.1920In this study, we measured and compared the scope of PA policies for physician services, including physician administered medications, among five health insurers that collectively serve most of the Medicare Advantage market, quantifying differences in PA across insurers, physician specialties, and clinical service categories.
Methods
Study design and overview
We measured the scope of PA policies of Medicare Advantage insurers by determining the number of services delivered in traditional Medicare Part B that would have required PA by each insurer, and then calculated the associated spending. This study design allowed us to detect services that would have been subject to PA regardless of whether PA would have been approved, denied, or never sought owing to the sentinel effect of PA policy (ie, deterrence of services without denial of PA).21
Data and study sample
Our initial sample included PA policies among health insurers with Medicare Advantage market shares of 2% or greater, with two exceptions. We excluded Kaiser Permanente because of its integrated insurer-provider organizational model, and we excluded BlueCross and BlueShield affiliates because PA policies were available on independent websites, and these were too numerous to feasibly obtain. The included firms and their 2021 market shares were UnitedHealthcare (27%), Humana (18%), CVS Health (11%), Centene/Wellcare (4%), and Cigna (2%).22
We identified all unique Healthcare Common Procedure Coding System (HCPCS) codes in traditional Medicare claims using the 2021 Centers for Medicare and Medicaid Services National Physician Fee Schedule Relative Value Files.23For each code, we assessed whether the service required PA by each Medicare Advantage insurer in 2021. We extracted data on PA policies from public insurer documents or websites published between July and September 2021.2425262728Each insurer publishes regularly updated PA policies differently for its Medicare Advantage and commercial insurance plans. We manually extracted HCPCS codes subject to PA from available policy documents containing coverage rules for Medicare Advantage plans. If instead the policies were available on insurer websites through search applets, we extracted HCPCS codes subject to PA using an algorithm written in the Selenium package of Python (see supplement file for more details). One insurer, Wellcare, made its PA policies available on a consolidated website across multiple lines of business.
We obtained Part B HCPCS level service counts, or utilization, and allowed charges (ie, spending) from the Medicare Part B National Summary Data File.29Our primary analyses employed 2021 data, the most recent available at the time of our study; in a sensitivity analysis, we used 2019 spending and utilization data to confirm that our findings were not driven by changes in utilization associated with the covid-19 pandemic. We used Medicare administrative enrollment files to calculate spending and utilization for each enrolled beneficiary.30
Analysis of insurer prior authorization policies
To compare the scope of PA policies, for each insurer we totaled Part B spending and utilization counts of HCPCS codes that would have required PA. We refer to these quantities as PA spending and PA utilization, respectively. We also calculated PA spending and PA utilization for three hypothetical policies based on the degree of insurer consensus for each service: the narrowest hypothetical policy required PA for services at all insurers, the broadest hypothetical policy required PA for services by at least one insurer, and an intermediate policy required PA for services requiring PA by any of the insurers with the three largest market shares.
To examine the types of services subject to PA, we assessed the extent of PA across and within clinical service categories. We assigned each HCPCS code to one of five clinical groups: durable medical equipment and transport; radiology, pathology, and laboratory; other medical services; surgery and anesthesia; and medications. We further subcategorized medications into nine therapeutic areas using the Cerner Multum Lexicon database31: autoimmune and musculoskeletal; cardiovascular, diabetes, and hyperlipidemia; genitourinary and sex hormones; hematology and oncology; infectious diseases; neurology; psychiatry; pulmonology; and other.
For each of the five actual and the three hypothetical insurer PA policies and each service category we calculated several quantities. To assess the scope of PA, we calculated the ratio of PA utilization in the service category to all service utilization in the category, and the ratio of PA spending in the category to all spending in the category (eg, the proportion of traditional Medicare Part B medication spending that would require PA). To assess the distribution of PA across service categories, we calculated the ratio of PA utilization in the category to all PA utilization, and the ratio of PA spending in the category to all PA spending (eg, the proportion of all PA spending that was for Part B medications). We summarized these outcomes across insurers by calculating medians and means weighted by insurers’ market share.
To quantify differences in PA by physician specialties, we used the publicly available 2020 Medicare Physician and Other Practitioners-by Provider and Service dataset. We extracted information on services and procedures provided to traditional Medicare Part B beneficiaries by physician and other healthcare professionals, aggregated by provider (based on national provider identifier) and service (based on HCPCS codes).32We combined specialties as appropriate (see supplement file for more details), excluding services attributed to institutions, non-clinicians, or pediatrics (since Medicare covers almost no children). For each physician specialty, we calculated the proportion of total specialty spending for each HCPCS code. We then calculated the total proportion of Part B spending for which each insurer would have required PA for each physician specialty. Finally, we determined the most expensive and most common individual services subject to PA according to annual spending and utilization.
Data analysis employed Python software, version 3.8 (Python Software Foundation) and Stata version 16 (StataCorp).
Patient and public involvement
No members of the public were formally involved in the design or implementation of this study because no funding was set aside for public or patient involvement. However, clinician authors’ experiences caring for patients whose healthcare requires navigating prior authorization policies informed the motivation for and development of the research question and interpretation of the results.
Results
Among the 30 540 086 beneficiaries in traditional Medicare Part B in 2021, mean annual spending was $4649 (£3698; €4338) per beneficiary, or $142bn in total (see supplement table 2). Of the 14 130 Part B clinical services, Medicare Advantage insurers required PA for 944 to 2971 services (median 1899; weighted mean 1429); 4044 services (29%) required PA by at least one Medicare Advantage insurer and 239 (2%) required PA by all Medicare Advantage insurers.
Across the five Medicare Advantage insurers, the proportion of Part B spending requiring PA ranged from 17% to 33% (median 28%; weighted mean 23%) (fig 1); the proportion of Part B utilization requiring PA ranged from 9% to 41% (median 22%; weighted mean 18%; see supplement figure 1). Forty per cent of spending ($57bn) and 48% of service utilization would have required PA by at least one Medicare Advantage insurer. In a sensitivity analysis employing 2019 spending and utilization data, the proportion of spending on PA services did not substantially change (see supplement figure 2). Twelve per cent of spending and 6% of utilization would have required PA by every Medicare Advantage insurer.
For every Medicare Advantage insurer, physician administered medications represented the plurality of PA spending (range 46-62%; median 57%; weighted mean 57%;table 1). Medicare Advantage insurers required PA for 56% to 89% of medication spending (median 86%; weighted mean 72%; see supplement table 5), accounting for 14% to 67% of medication utilization (median 35%; weighted mean 28%; see supplement table 5). Ninety three per cent of Part B medication spending, or 74% of medication use, would require PA by at least one Medicare Advantage insurer. Fifty one per cent of Part B medication spending, or 10% of medication use, would require PA by every Medicare Advantage insurer. Supplement figure 3 and supplement tables 9 and 10 present corresponding calculations for intermediate degrees of consensus about PA policies among insurers. Surgery and anesthesia services accounted for the second greatest proportion of PA spending (range 20-27%; median 21%; weighted average 24%;table 1).
Figure 2shows the distribution of PA spending across medication therapeutic areas. For all Medicare Advantage insurers, hematology and oncology drugs represented the largest proportion of PA spending (range 27-34%; median 33%; weighted mean 30%). The proportion of spending subject to PA ranged widely by physician specialty (fig 3).
Table 2presents individual PA services with the largest associated spending, mostly injectable medications. Among services requiring PA by at least one Medicare Advantage insurer, the 10 costliest accounted for 10% of all Part B spending. Among services uniformly requiring PA by all Medicare Advantage insurers, the 10 costliest accounted for 7% of all Part B spending.
Discussion
Debate in the US about the proper role of PA by private health insurers has occurred amid considerable uncertainty around its extent and variation across insurers, physician specialties, and clinical service categories. In this cross sectional analysis of PA policies across five Medicare Advantage insurers, we found that nearly half of traditional Medicare Part B spending and utilization would have been subject to PA by at least one Medicare Advantage insurer. The Medicare Advantage insurer with the broadest PA policy would encompass one in three dollars spent in Medicare Part B; even the Medicare Advantage insurer with the narrowest policy would encompass one in six dollars. However, the medical services uniformly subject to PA by all five Medicare Advantage insurers accounted for a substantially smaller proportion of spending.
Our study advances efforts to understand the role of private insurers in setting coverage policy and insurers’ use of PA in several ways. We extended methods previously applied to a single Medicare Advantage insurer,21studying several insurers that collectively served more than 60% of the Medicare Advantage market, allowing a more generalizable and nuanced description of PA and its variation. Instead of using data from clinician surveys33or PA applications15to quantify the frequency of encountering PA, we applied coverage policies to the universe of medical services delivered in traditional Medicare Part B. This allowed for consistent and objective quantification of the scope of PA in a national and policy relevant sample. Finally, we studied PA in the setting of clinician services rather than Part D medications, which have been more thoroughly studied.1920Our approach yielded three key findings relevant to the debate over the proper role of PA.
First, we found widespread use of PA for medical services in Medicare Advantage, which is in marked contrast to traditional Medicare. These findings add to previous literature suggesting that Medicare Advantage reduces healthcare use relative to traditional Medicare.343536The results are also consistent with previous studies showing a high prevalence of medical services suspected of being low value in fee-for-service Medicare.23738Furthermore, this finding suggests that insurers’ financial savings from PA programmes exceed insurers’ costs to administer them. However, whether PA of medical services results in substantial aggregate savings remains unclear because our analysis did not account for several costs: the administrative and compliance costs of PA for clinicians, insurers, patients, and regulators; the reduction in spending on restricted services; the increase in spending on services that substitute for restricted services; and costs associated with any adverse events as a result of, or prevented by, PA.
Nonetheless, the large contrast in PA between Medicare Advantage and traditional Medicare suggests widely differing approaches to coverage policy. These divergent approaches to constrain health spending are unlikely to both be optimal, highlighting challenges faced by health systems such as those in the US, where national governments aim to incorporate private firms in the provision of health insurance as well as fully public systems elsewhere. Further research should quantify the relative benefits and harms of the differences in use of PA between Medicare Advantage and traditional Medicare. One recent analysis found that extensive PA policies for prescription drugs produced benefits exceeding their cost.20It is, however, unclear whether extensive PA is similarly valuable for the (non-prescription) medical services we studied. A recent report from the Office of Inspector General found that in some cases Medicare Advantage insurers denied medical services after a PA request that met Medicare coverage criteria.8Though Medicare Advantage insurers are required to follow government coverage determinations, many medical services are implicitly covered by Medicare without undergoing a formal coverage determination process.39Thus, some disagreements about coverage may result from a lack of sufficient criteria to determine when it is appropriate for Medicare Advantage insurers to apply additional coverage restrictions. Beyond Medicare policy, this result highlights the importance of policy efforts to reform PA, such as increasing transparency, streamlining PA processes,40and “gold-carding” rules that allow certain providers to be exempt from PA requirements based on performance measures.41The considerable scope of services requiring PA may lead to reductions in the provision of low value services, but it can impose an administrative burden on the clinicians who must navigate these policies, and the insurer staff who adjudicate the claims. Patients may also face barriers or delays in obtaining appropriate care,424344particularly if therapeutic substitutes are lacking. Because most PA requests are eventually approved, reforms may be particularly promising if they reduce administrative burdens for services most likely to be approved45; to infer whether a PA request is likely to be approved, insurers might employ readily available data about a patient or clinician (eg, the patient’s previous diagnoses or the physicians’ rate of PA approvals).
Second, we found noticeable variation in the use of PA within Medicare Advantage. Only 30% of the Medicare spending subject to PA at any Medicare Advantage insurer was subject to PA at every Medicare Advantage insurer; most of this spending was devoted to a small number of costly services. These findings highlight the challenge of uniformly identifying such low value services, which lack a consistent definition or legal standard. Other explanations for variation in PA across insurers may be variation in practice patterns across the different markets that the insurers serve; differences in bargaining power between insurers and providers across these markets, which could affect negotiations over the use of PA; differences in the use of alternative utilization management or payment policies employed by insurers, such as risk based contracts in which providers are responsible for cost and quality of care provided to patients, which may also impact an insurer’s PA policies; and differences in the degree to which insurers enforce their PA rules. In the present study, the services that consistently required PA across insurers could guide PA policy reforms as a focus for coverage policy: PA in private insurance plans could be narrowed to focus on these services, whereas PA in government administered Medicare could be expanded to include these services. Future research should examine whether the services uniformly subject to PA across most insurers are indeed more likely to represent low value care.
In addition, differing PA policies across insurers might disrupt care for patients switching between health plans with different coverage policies for ongoing diagnostic or treatment plans. Previous studies examining coverage decisions also found substantial variation among private insurers,46but not necessarily variation by clinical factors.47Other studies have found variation in the evidence used by commercial health plans in their coverage policies for specialty drugs, which occasionally diverge from evidence included in FDA required labeling.4849Though these latter studies did not examine PA specifically, variation in the evidence reviewed to determine PA requirements may explain some of the differences we observed among Medicare Advantage insurers. It is possible that dissimilarity of PA policies has also been driven by differences in the use of particular medical services by each insurer’s mix of patients and in-network clinicians.
Third, we found that most spending on medical services requiring PA was concentrated in physician administered drugs, which are typically injected or infused. Nine in 10 dollars spent on medications would be subject to PA by at least one Medicare Advantage insurer. These findings are consistent with a recent study documenting the prevalence of PA policies among large Medicare Advantage insurers for the top 20 drugs by Part B spending in 2020.50Use of PA for such specialty medications may result from their high cost and unit prices, delayed competition and market entry of biosimilars, and opaque interactions between pharmacy benefit managers, insurers, and drug manufacturers incentivizing preferred formulary status and rebates for expensive drugs.51Moreover, assigning appropriateness criteria to non-drug services may be more challenging,52which may explain why the costliest services uniformly subject to PA were mostly medications.
Strengths and limitations of this study
Our study has several limitations. First, we examined the scope of PA only for physician services and did not address other services such as inpatient hospital admission, post-acute care, or Part D prescription drugs. Our research design would not have been feasible for studying inpatient hospital admissions because PA policies typically only apply to elective hospital admissions, which are difficult to identify in administrative claims. Similarly, it was not feasible to examine PA for prescription drugs in Part D because PA is widespread in Part D53and our research design relied on healthcare utilization data in a setting that lacked PA.54Second, our estimates were based on utilization patterns of traditional Medicare beneficiaries, which may differ from those of Medicare Advantage beneficiaries; again, this limitation resulted from our need to employ utilization data from a setting lacking PA. Third, our analysis uses traditional Medicare prices, which can differ slightly from Medicare Advantage. Fourth, we compared the PA policies of five Medicare Advantage insurers, which may differ from those of other insurers. Fifth, we were unable to include data from BlueCross and BlueShield affiliates in our analysis, as many of the affiliates’ PA policies were not available for collection on a centralized website. Nonetheless, the included insurers represent most Medicare Advantage enrollees.22Sixth, we utilized data on insurers’ stated policies on services requiring PA. An evaluation of implementation of such policies, including how frequently insurers initially denied and subsequently approved specific services, was not feasible.
Conclusion
In this cross sectional study, the coverage policies of private insurers in the US would have required PA for a large portion of fee-for-service Medicare Part B spending, particularly spending on medications. There was little consensus among insurers, however, about which medical services required PA. This lack of uniformity suggests it can be challenging to identify and discourage some low value services. Our study informs ongoing efforts to reform PA and reduce its administrative burdens. Given the continued growth of privately administered Medicare insurance plans, PA is likely to remain an important feature of the Medicare programme. This aspect of managed care may be improved if PA policies can be narrowly targeted at the specific services most likely to represent low value care.
In the US, health insurers use a variety of tools to limit the quantity of services provided, including prior authorization (PA), a process by which insurers assess the necessity of planned medical care; however, associated administrative burdens have prompted calls for policy reform
Although minimal PA exists in government administered traditional Medicare, the insurance programme that covers hospital admissions (Part A) and physician services (Part B), PA is more extensive in the privately administered alternative, Medicare Advantage
Evidence is lacking on the extent of PA and its variation across insurers, physician specialties, and clinical services
The coverage policies of private insurers would have required PA for a large portion of fee-for-service Medicare Part B spending, particularly spending on physician administered medications
The lack of consensus among insurers suggests it can be challenging to discourage some low value services; reforms could focus PA on services for which there is the greatest consensus about the risk of overuse
A substantial difference in coverage policies exists between government administered and privately administered Medicare, highlighting the importance of policies governing the role of private health insurers with respect to PA in the US
","Objective: To measure and compare the scope of US insurers’ policies for prior authorization (PA), a process by which insurers assess the necessity of planned medical care, and to quantify differences in PA across insurers, physician specialties, and clinical service categories.
Design: Cross sectional analysis.
Setting: PA policies for five insurers serving most of the beneficiaries covered by privately administered Medicare Advantage in the US, 2021, as applied to utilization patterns observed in Medicare Part B.
Participants: 30 540 086 beneficiaries in traditional Medicare Part B.
Main outcome measures: Proportions of government administered traditional Medicare Part B spending and utilization that would have required PA according to Medicare Advantage insurer rules.
Results: The insurers required PA for 944 to 2971 of the 14 130 clinical services (median 1899; weighted mean 1429) constituting 17% to 33% of Part B spending (median 28%; weighted mean 23%) and 9% to 41% of Part B utilization (median 22%; weighted mean 18%). 40% of spending ($57bn; £45bn; €53bn) and 48% of service utilization would have required PA by at least one insurer; 12% of spending and 6% of utilization would have required PA by all insurers. 93% of Part B medication spending, or 74% of medication use, would have required PA by at least one Medicare Advantage insurer. For all Medicare Advantage insurers, hematology and oncology drugs represented the largest proportion of PA spending (range 27-34%; median 33%; weighted mean 30%). PA rates varied widely across specialties.
Conclusion: PA policies varied substantially across private insurers in the US. Despite limited consensus, all insurers required PA extensively, particularly for physician administered medications. These findings indicate substantial differences in coverage policies between government administered and privately administered Medicare. The results may inform ongoing efforts to focus PA more effectively on low value services and reduce administrative burdens for clinicians and patients.
"
Diagnostic accuracy of magnetically guided capsule endoscopy with a detachable string for detecting oesophagogastric varices in adults with cirrhosis,"Introduction
Oesophagogastric varices occur in about half of patients with cirrhosis and are major causes of morbidity and mortality because of the risk of variceal bleeding.1To identify people with high risk varices who need prophylactic treatment to prevent variceal bleeding, clinical guidelines recommend oesophagogastroduodenoscopy (OGD) as the standard diagnostic modality for screening and periodic surveillance of oesophagogastric varices.123OGD is an unpleasant invasive procedure, however, which usually necessitates sedation, potentially leading to sedation related complications and low adherence to screening programmes in patients with cirrhosis.4An alternative method to OGD that is minimally invasive and of comparable accuracy is thus needed for detecting and grading oesophagogastric varices in patients with cirrhosis.
Capsule endoscopy, involving the ingestion of a small battery powered device with a camera, has been proposed as a promising minimally invasive diagnostic modality for visualising oesophagogastric varices.5678The advantages of capsule endoscopy are that it is minimally invasive, requires no sedation, and is highly acceptable to patients.9Studies have, however, reported that conventional small bowel capsule endoscopy and oesophageal capsule endoscopy are not accurate enough to replace OGD to detect or grade oesophageal varices.579101112In a multicentre study of 330 participants with cirrhosis, the sensitivity of oesophageal capsule endoscopy to diagnose and correctly stage oesophageal varices was only 76% and 64%, respectively.7Moreover, conventional capsule endoscopy has shown poor accuracy in detecting gastric varices and portal hypertensive gastropathy.5613During oesophageal examination, the image capture rate of conventional capsule endoscopy is insufficient compared to the rapid oesophageal transit time, and the passive movement of the capsule means the transit time cannot be controlled. During gastric examination, the passive movement also prevents complete visualisation of the capacious and non-uniform cavity of the stomach. These limitations preclude conventional capsule endoscopy from providing a complete view of areas with suspected oesophagogastric varices. To date, clinical guidelines do not recommend conventional capsule endoscopy for screening or surveillance of oesophagogastric varices.2
To achieve a complete examination of the oesophagus and stomach using capsule endoscopy, we developed magnetically guided capsule endoscopy with a detachable string (ds-MCE). The capsule can be actively controlled in the oesophagus via the string and in the stomach via magnetic force, and therefore offers a minimally invasive method to detect both oesophageal varices and gastric varices. In addition, the capsule has a long battery life, enabling further evaluation for portal hypertensive enteropathy in the small bowel. Our previous small sample studies showed the feasibility and safety of ds-MCE in healthy volunteers and in patients with compensated cirrhosis.1415In this paper, we conducted the CENTERS study to further assess the diagnostic accuracy of ds-MCE in detecting and grading oesophagogastric varices in patients with cirrhosis.
Methods
Study design
CENTERS is a prospective multicentre study reported according to the standards for reporting diagnostic accuracy studies.16The study was performed at 14 hospitals in China between 7 January 2021 and 25 August 2022. Patients with cirrhosis were consecutively recruited to undergo ds-MCE (index test) first, then OGD (the reference standard) within 48 hours of the first procedure. Certified operators performed the ds-MCE and experienced endoscopists performed the OGD, each blinded to the results of the other test. All participants provided written informed consent. The study protocol and statistical analysis plan are included in the supplementary appendix.
Participants
Eligible patients were those aged at least 18 years with a diagnosis of cirrhosis. Exclusion criteria were dysphagia, Zenker’s diverticulum, gastrointestinal obstruction, pregnancy, gastrointestinal bleeding, a cardiac pacemaker or other implanted electromedical device, a life threatening condition, planned magnetic resonance imaging before excretion of the capsule endoscope, participation in another clinical study, and refusal to give informed consent or any condition that precluded compliance with the study. Supplementary method 2.1 lists the inclusion and exclusion criteria.
Study procedures
The ds-MCE system (Ankon Technologies, Wuhan, China) consists of a capsule endoscope with a camera for capturing images, a robotic arm with a magnet for guiding the capsule, a data recorder, a computer workstation with software for real time viewing, two joysticks for controlling the orientation of the capsule (see supplementary figure S1),17181920and a detachable hollow latex string (see supplementary figure S2).14The capsule endoscope measures 27×11.8 mm and has a battery life of more than 10 hours and a viewing field of 150 degrees. Images are captured at an adaptive rate of 0.5-6 frames per second, with a resolution of 480×480 pixels. The latex string is 120 cm in length and can be attached to the capsule endoscope at one end and a syringe at the other end.
Figure 1shows the ds-MCE procedure. Before the examination, participants were asked to fast overnight for 12 hours and to ingest 2.5 g of dimethicone as a defoaming agent 40 minutes before the procedure, followed by 500-1000 mL of water as tolerated before ingestion of the capsule to fill the stomach cavity and aid navigation of the capsule. Participants underwent a standardised gastrointestinal preparation regimen, swallowed the capsule with water, and were not sedated. During examination of the oesophagus, the capsule endoscope was controlled by the string and images were captured at a rate of six frames per second. Once the capsule had reached the gastric cardia by means of the swallowed water, the string was pulled up slowly so that the operator could inspect the oesophagus under real time viewing. This process was repeated at least three times, during which participants drank water to distend the distal oesophagus and to remove saliva and mucous bubbles for better observation. After completion of the examination, the syringe was used to administer 5 mL of air through the string to detach the capsule and the string was removed via the mouth. At this point the capsule endoscope entered the stomach and was controlled by an external magnetic field during the gastric examination. The procedure was performed twice according to standardised protocol, and the gastric cardia, fundus, body, angulus, antrum, and pylorus were fully examined.19On completion of the gastric examination, the capsule endoscope was switched to the mode for examination of the small intestine and allowed to move passively under gastrointestinal peristalsis. In this mode, the frame rate of the capsule varies because the capsule automatically recognises the velocity at which it is moving and can adjust the camera to capture between 0.5-6 frames per second. At each centre, a dedicated certified operator performed the ds-MCE procedures. Staff at an independent core imaging laboratory reviewed the coded videos of the ds-MCE procedures (see supplementary methods 2.2 and 2.3).
At each centre, experienced endoscopists performed conventional forward viewing OGD within 48 hours after ds-MCE. OGD was conducted with or without sedation, according to the centre’s standard procedure and preference of the patient. The endoscopist was blinded to results of the preceding ds-MCE. The oesophagus, stomach, and duodenum were examined routinely. The whole examination of each participant was recorded on video and digital images. An independent core imaging laboratory reviewed the coded videos and images of OGD from each centre (see supplementary methods 2.2 and 2.3).
After completion of both examinations, participants were administered a questionnaire on their satisfaction with the procedures. Two weeks after ds-MCE, participants were followed up to confirm excretion of the capsule endoscope. Any adverse events during the study were reported to the investigators and recorded.
Outcomes measures and definitions
The primary outcomes were the sensitivity and specificity of ds-MCE for detecting oesophagogastric varices in patients with cirrhosis, using OGD as the reference standard. The key secondary outcomes were the sensitivity and specificity of ds-MCE in detecting high risk oesophageal varices compared with OGD. Other secondary outcomes included the diagnostic accuracy of ds-MCE in detecting high risk oesophagogastric varices, oesophageal varices, large oesophageal varices, red colour signs of oesophageal varices, gastric varices, cardiofundal gastric varices, and portal hypertensive gastropathy compared with OGD; the findings of portal hypertensive enteropathy in small bowel under ds-MCE; the examination time of ds-MCE and OGD; assessment of patients’ satisfaction; and evaluation of safety.
Oesophagogastric varices occur in the oesophagus or stomach. Oesophageal varices detected during OGD were classified as large (diameter ≥5 mm) or small according to the Baveno III consensus.21We used the de Franchis method5to grade the size of oesophageal varices detected during ds-MCE according to the percentage of luminal circumference of oesophagus occupied by the largest oesophageal varices (see supplementary figure S3). Oesophageal varices detected during ds-MCE were defined as large when the varix occupied more than the optimal percentage threshold of the luminal circumference of the oesophagus. We defined high risk oesophageal varices as large varices or small varices occurring in the presence of red colour signs.22Gastric varices in the stomach were classified as gastroesophageal varices and isolated gastric varices according to Sarin’s classification,23and cardiofundal gastric varices included type 2 gastroesophageal varices and type 1 isolated gastric varices. We defined high risk oesophagogastric varices as high risk oesophageal varices or any gastric varices.2425Patient satisfaction assessments were based on satisfaction scores, with higher scores representing more comfort. Supplementary method 2.4 provides detailed definitions of outcomes.
Statistical analysis
The primary aims of our single arm diagnostic accuracy study were to test whether the sensitivity and specificity of ds-MCE for detecting oesophagogastric varices would be >85%. With an estimated sensitivity of 90%, specificity of 94%, two sided alpha of 5%, power of 80%, prevalence for oesophagogastric varices of 62%, and dropout rate of 3%, we determined that 591 participants would be needed.26
The diagnostic analyses are based on the results of participants with useable data after ds-MCE and OGD. Using OGD as the reference standard, we assessed the diagnostic accuracy of ds-MCE for detecting oesophagogastric varices, with sensitivity and specificity as the primary outcomes; with the positive predictive value, negative predictive value, and overall diagnostic accuracy of ds-MCE as other measures simultaneously, along with corresponding 95% confidence intervals using Wilson’s method, in per patient analysis. We compared the sensitivity and specificity of detecting oesophagogastric varices with the prespecified 85% threshold using the one sample exact test. The diagnostic performance of ds-MCE for detecting oesophageal varices, red colour signs of oesophageal varices, gastric varices, cardiofundal gastric varices, and portal hypertensive gastropathy was assessed using sensitivity, specificity, positive predictive value, negative predictive value, and overall diagnostic accuracy.
In addition, we divided the sample into development and validation cohorts in a ratio of 2:1 according to the temporal order of first-patient-in dates of each centre, such that centres with earlier first-patient-in dates were allocated to the development cohort (seven centres, n=393, supplementary table S1) and centres with later first-patient-in dates were allocated to the validation cohort (the remaining seven centres, n=189, supplementary table S1). From the development cohort we derived the optimal percentage threshold for luminal circumference of the oesophagus for detecting large oesophageal varices during ds-MCE. We calculated the Youden index, defined as [(sensitivity+specificity)−1], to determine the optimal percentage threshold for oesophageal luminal circumference, rounding down to the nearest whole percentage derived from the development cohort that resulted in the best combination of specificity and sensitivity for detecting large oesophageal varices during ds-MCE. And we internally validated the optimal threshold using the bootstrap method, with 1000 replications. The validation cohort was used to assess the diagnostic accuracy of ds-MCE for detecting high risk oesophageal varices, high risk oesophagogastric varices, and large oesophageal varices on the basis of the optimal threshold, using sensitivity, specificity, positive predictive value, negative predictive value, and diagnostic accuracy. P values <0.05 were considered to indicate statistical significance (see supplementary method 2.5).
Patient and public involvement
No patients or members of the public were directly involved in setting the research question or the outcome measures. At the protocol stage, more than 30 patients with cirrhosis were consulted about the ds-MCE procedure. Participants were aware of the purpose and content of this study during recruitment, although they were not involved in the initial design of the trial. Considering the confidentiality of clinical data, patients did not participate in the subsequent reporting of this research. The results were, however, communicated to patients who expressed an interest during clinic visits.
Results
Participants
A total of 607 adults with cirrhosis from 14 centres in China were enrolled into the study between 7 January 2021 and 25 August 2022. Twenty five patients were excluded—in 17 patients it was not possible to complete the oesophageal and stomach examinations during ds-MCE. Both ds-MCE and OGD examinations were completed in 582 participants (fig 2). These participants were included in the accuracy analysis.Table 1shows the baseline characteristics of the 582 participants.
Primary outcome
Overall, the findings for oesophagogastric varices were concordant between ds-MCE and OGD in 568 of the 582 participants (97.6%). Results were inconsistent between the two procedures in 14 participants, oesophagogastric varices detected by ds-MCE were not confirmed by OGD in four participants, and ds-MCE failed to detect oesophagogastric varices detected by OGD in 10 participants (see supplementary table S3). Using OGD as the reference standard, ds-MCE had a sensitivity of 97.5% (95% confidence interval 95.5% to 98.7%) and specificity of 97.8% (94.4% to 99.1%) for detecting oesophagogastric varices, and both sensitivity and specificity were significantly higher than the prespecified 85% threshold (both P<0.001) (table 2). The positive predictive value and negative predictive value was 99.0% (97.4% to 99.6%) and 94.6% (90.3% to 97.0%), respectively (table 2).Figure 3shows representative oesophagogastric varices observed during ds-MCE and OGD.
Secondary outcomes
The development cohort included 393 participants from seven centres, and the validation cohort included 189 participants from the remaining seven centres (see supplementary table S1). Supplementary table S2 presents the clinical characteristics of participants in both cohorts. In the development cohort, the optimal percentage threshold circumference for the oesophageal lumen of ds-MCE in detecting large oesophageal varices was 18.45%, with a Youden’s index of 0.95 (see supplementary figure S4). Therefore, we chose a threshold of 18% for discriminating large oesophageal varices during ds-MCE. Using the 18% threshold in the development cohort, internal validation showed a sensitivity of 98.3% (94.9% to 100.0%) and a specificity of 97.6% (93.4% to 99.6%) for detecting large oesophageal varices (see supplementary table S4).
Using the 18% threshold in the validation cohort, the sensitivity and specificity of ds-MCE for detecting high risk oesophageal varices were 95.8% (89.7% to 98.4%) and 94.7% (88.2% to 97.7%), respectively (table 2). The diagnostic accuracy of ds-MCE for detecting high risk oesophagogastric varices was 96.3% (92.6% to 98.2%). The sensitivity and specificity of ds-MCE for detecting large oesophageal varices were 95.3% (88.5% to 98.2%) and 93.3% (86.8% to 96.7%), respectively (table 2). Supplementary table S5 presents subgroup analyses of the screening and surveillance populations, and supplementary table S6 presents subgroup analyses of the populations with compensated and decompensated cirrhosis.
Using OGD as the reference standard, the sensitivity and specificity of ds-MCE for detecting oesophageal varices were 96.4% (94.0% to 97.8%) and 98.0% (94.9% to 99.2%), respectively. The sensitivity and specificity of ds-MCE for oesophageal varices with red colour signs were 96.1% (92.5% to 98.0%) and 97.6% (95.5% to 98.7%), respectively (table 2). The sensitivity and specificity of ds-MCE in detecting gastric varices were, respectively, 96.2% (93.0% to 98.0%) and 97.1% (94.7% to 98.4%), cardiofundal gastric varices were 92.2% (85.8% to 95.8%) and 99.6% (98.5% to 99.9%), and portal hypertensive gastropathy were 95.3% (90.9% to 97.6%) and 98.3% (96.5% to 99.2%).
ds-MCE examination of the small bowel was completed in 510 participants. Supplementary table S7 summarises the detailed findings of portal hypertensive enteropathy. Portal hypertensive enteropathy was found in 333 (65.3%) participants, with spontaneous bleeding in three (0.6%) participants (see supplementary figure S5). Supplementary table S8 shows the durations of ds-MCE and OGD. During ds-MCE, the median examination time for the oesophagus and stomach was 4.74 minutes (interquartile range 3.12 to 7.15 minutes) and 15.78 (8.57 to 23.70) minutes, respectively. The examination time for oesophagus, stomach, and duodenum using OGD was 5.50 (4.50 to 7.00) minutes. The median overall satisfaction score for ds-MCE was higher than for OGD both without sedation (3v2), and with sedation (3v3) (see supplementary tables S9 and S10, respectively). A total of six (0.99%) adverse events were reported during the study. Two serious adverse events occurred in association with OGD (variceal oesophageal haemorrhage), with both participants requiring hospital admission and endoscopic band ligation. No serious adverse events were associated with ds-MCE, but four adverse events occurred during ds-MCE: capsule retention in the small bowel, with the capsule excreted spontaneously after 23 days (n=1 participant); capsule retention in the oesophagus owing to unexpected oesophageal stenosis, with the capsule pulled out using the string (n=1); syncope mainly due to glucopenia associated with gastrointestinal preparation (n=1); and rupture and bleeding of haemorrhoids related to small bowel preparation (n=1).
Discussion
In this prospective, multicentre study comparing ds-MCE with the standard reference of OGD among patients with cirrhosis, the results showed that the lower boundary of the 95% confidence interval for the sensitivity and specificity of ds-MCE in detecting oesophagogastric varices were both above the prespecified value of 0.85. This study suggested that ds-MCE has strong discriminative ability to detect oesophagogastric varices owing to technological advances in direct, real time visualisation of oesophagogastric varices under active control. Moreover, ds-MCE showed high accuracy in detecting high risk oesophageal varices in patients both with compensated cirrhosis and with decompensated cirrhosis, which outperformed the non-invasive diagnostic tools of Baveno VII criteria (liver stiffness measurement, spleen stiffness measurement, and platelet count)3and conventional capsule endoscopy.567892728In addition, ds-MCE enabled further exploration of the small bowel and thus provided a more comprehensive evaluation of gastrointestinal changes in patients with cirrhosis.
Current guidelines on the management of oesophagogastric varices stress the importance of detecting high risk oesophageal varices to prevent variceal bleeding.1229Three factors need to be identified for oesophageal varices to be classed as high risk: the presence of oesophageal varices, the detection of red colour signs, and the size grading of the oesophageal varices.2ds-MCE showed high diagnostic accuracy in detecting oesophageal varices and red colour signs compared with OGD, which was higher than reported in previous studies on small bowel capsule endoscopy and oesophageal capsule endoscopy.5789The absence of a specific grading system for oesophageal varices under capsule endoscopy has been one obstacle to grading the size of oesophageal varices.9The standard grading system for oesophageal varices used during OGD is not applicable to capsule endoscopy because air insufflation is required to fully distend the oesophagus2213031; a function that capsule endoscopy lacks. Such a difference may in theory impact the grading of oesophageal varices. Previous studies reported a specific oesophageal varices grading system for capsule endoscopy based on the percentage circumference of the oesophageal lumen occupied by the largest oesophageal varices, which showed good correlation with the standard OGD classification.57828However, the previously reported luminal circumference thresholds (25%, 1/6, 15%, or 12.5%) for grading oesophageal varices under capsule endoscopy were developed on the basis of small sample sizes and limited oesophageal images, as capsule endoscopy could not capture enough images under passive gastrointestinal movement.57828In this large prospective study, ds-MCE provided adequate oesophageal images for each participant to enable detection of the largest oesophageal varices and the luminal circumference percentage. We therefore developed a new threshold of 18%, and both internal and external validation verified that the new threshold presented high sensitivity and specificity in stratifying large oesophageal varices and high risk oesophageal varices compared with OGD, suggesting ds-MCE is an accurate diagnostic modality to determine prophylactic interventions for oesophageal varices.
For the detection of gastric varices and portal hypertensive gastropathy, the reported oesophageal capsule endoscopy and small bowel capsule endoscopy showed poor diagnostic performance owing to the inability to control the capsule endoscope during gastric examination.693233The reported non-invasive tests of the Baveno VI criteria variables had a limited role in predicting the presence of gastric varices or portal hypertensive gastropathy.29In contrast with these non-invasive diagnostic modalities, ds-MCE is comparable to OGD in being able to detect gastric lesions.14171819Notably, in the current study ds-MCE showed robust diagnostic performance for the detection of gastric varices and portal hypertensive gastropathy compared with OGD. Gastric varices were further classified based on Sarin’s classification.23ds-MCE also showed high accuracy in detecting cardiofundal gastric varices, which represent the highest risk of bleeding.21In the current study, the good performance of ds-MCE suggests that, besides detecting oesophageal varices, it is a promising minimally invasive alternative to OGD in detecting gastric varices and portal hypertensive gastropathy.
Portal hypertensive enteropathy is a potential source of gastrointestinal bleeding and may contribute to chronic anaemia.34353637In the current study, ds-MCE enabled the small bowel to be examined, and portal hypertensive enteropathy was detected in 65% of participants with cirrhosis. Importantly, bleeding was observed in the small bowel of three participants. The overall prevalence of portal hypertensive enteropathy was similar to that reported in previous studies on capsule endoscopy.343839The results indicated that ds-MCE is able to recognise small bowel abnormalities and their underlying influence on patients with cirrhosis.
Mild or moderate adverse events specifically associated with ds-MCE occurred in four participants (0.7%). No serious adverse event occurred in association with ds-MCE. Two participants, however, experienced variceal bleeding during OGD examinations and required admission to hospital. In addition, patients’ satisfaction with ds-MCE was better than with OGD either with or without sedation, which may improve adherence to the screening and surveillance programme. Taken together, these results highlight that ds-MCE is safe and well tolerated in the detection of oesophagogastric varices without the need for conscious sedation.
Strengths and limitations of this study
We performed a prospective, multicentre study involving a large number of participants with cirrhosis to evaluate the diagnostic value of ds-MCE in detecting oesophagogastric varices. The sample size was sufficient to meet our objectives, with suitably narrow confidence intervals. Secondly, we proposed and validated a new capsule endoscopy grading standard for risk stratification of oesophageal varices. Besides, the diagnostic performance of ds-MCE in detecting oesophageal varices and red colour signs of oesophageal varices, gastric varices, and portal hypertensive gastropathy was also validated, providing a more comprehensive evaluation of ds-MCE.
Our study has limitations. Firstly, a small number of participants failed to swallow the capsule endoscope and detachable string. A smaller capsule could overcome this problem.40In addition, ds-MCE takes a longer time to perform than OGD and the cost of the disposable ds-MCE is higher than OGD, whereas ds-MCE does not require preoperative anaesthesia and postoperative recovery and enables a more thorough evaluation of the upper gastrointestinal tract and small bowel. Thus, a cost efficacy analysis is required in future studies. Secondly, most of the participants had cirrhosis related to hepatitis B virus, resulting in unavoidable selection bias. The application value of ds-MCE in other populations requires validation. Thirdly, OGD was always carried out after ds-MCE in this study. This order effect may have influenced the evaluation of patient satisfaction. Fourthly, the size of oesophageal varices was clarified using the oesophageal varices diameter under insufflation of the oesophagus by OGD, whereas the size of oesophageal varices was graded using the luminal circumference percentage grading system during ds-MCE. The effectiveness of this specific grading system and the proposed threshold to detect large oesophageal varices under capsule endoscopy needs to be validated in further studies. Finally, other non-invasive tools, including Baveno VI and VII criteria, were not compared with ds-MCE, and further studies are warranted.
Implications for clinical practice
As the population with cirrhosis increases worldwide, the incidence of oesophagogastric varices will increase. It is therefore vital to improve the screening and surveillance of oesophagogastric varices and thus possibly to intervene at an early stage to prevent variceal haemorrhage. Our study has several important implications for clinical practice. Firstly, ds-MCE can be used as a minimally invasive screening tool for detecting oesophagogastric varices in patients with cirrhosis. As ds-MCE uses direct visualisation, it is able to provide clear images of oesophageal varices, gastric varices, and portal hypertensive gastropathy. Thus, as a screening tool, ds-MCE showed an advantage over other available non-invasive tests, including serum markers, doppler ultrasonography, contrast enhanced ultrasonography, and ultrasound elastography. Secondly, the accurate detection of high risk oesophageal varices during ds-MCE could provide evidence for clinical treatment decisions, which depends on risk stratification for the prevention of variceal bleeding. Therefore, ds-MCE also serves as a promising surveillance tool for stratification of variceal bleeding risk in patients with cirrhosis. Thirdly, apart from the oesophagus and stomach, ds-MCE can be further used to evaluate small bowel lesions in one process. Finally, ds-MCE is well tolerated, easy to perform, does not require sedation, and is potentially cost effective, and as such is suitable for use in primary, secondary, and tertiary healthcare settings, providing clear potential to optimise the screening and monitoring of oesophagogastric varices.
Conclusions
This study found that ds-MCE is a highly accurate, safe, and minimally invasive method for detecting and grading oesophagogastric varices in patients with cirrhosis. Moreover, ds-MCE is a reliable tool to evaluate portal hypertensive gastropathy and small bowel abnormalities during one examination. The better tolerance of patients to ds-MCE may also improve adherence to endoscopic follow-up. Therefore, ds-MCE provides a promising alternative to OGD for screening and surveillance of gastrointestinal lesions in patients with cirrhosis.
Oesophagogastroduodenoscopy (OGD) is used to diagnose oesophagogastric varices in patients with cirrhosis, but it is invasive and involves sedation leading to patient discomfort, poor adherence, and potentially serious complications
Capsule endoscopy is a less invasive test than OGD as patients swallow a small device that produces images of the gastrointestinal tract
Conventional small bowel capsule endoscopy and oesophageal capsule endoscopy, however, are not accurate enough to replace OGD to detect oesophageal or gastric varices
In this diagnostic accuracy study of 607 patients with cirrhosis, magnetically controlled capsule endoscopy with a detachable string (ds-MCE) showed high sensitivity and specificity in diagnosing oesophagogastric varices, with OGD as the reference standard
The accuracy of ds-MCE for detecting high risk oesophageal varices was also comparable to OGD
The results indicate ds-MCE is a highly accurate and safe method for detecting and grading oesophagogastric varices, and it is a promising alternative to OGD for screening and surveillance of oesophagogastric varices in patients with cirrhosis
","Objective: To evaluate the diagnostic accuracy and safety of using magnetically guided capsule endoscopy with a detachable string (ds-MCE) for detecting and grading oesophagogastric varices in adults with cirrhosis.
Design: Prospective multicentre diagnostic accuracy study.
Setting: 14 medical centres in China.
Participants: 607 adults (>18 years) with cirrhosis recruited between 7 January 2021 and 25 August 2022. Participants:  underwent ds-MCE (index test), followed by oesophagogastroduodenoscopy (OGD, reference test) within 48 hours. The participants were divided into development and validation cohorts in a ratio of 2:1.
Main outcome measures: The primary outcomes were the sensitivity and specificity of ds-MCE in detecting oesophagogastric varices compared with OGD. Secondary outcomes included the sensitivity and specificity of ds-MCE for detecting high risk oesophageal varices and the diagnostic accuracy of ds-MCE for detecting high risk oesophagogastric varices, oesophageal varices, and gastric varices.
Results: ds-MCE and OGD examinations were completed in 582 (95.9%) of the 607 participants. Using OGD as the reference standard, ds-MCE had a sensitivity of 97.5% (95% confidence interval 95.5% to 98.7%) and specificity of 97.8% (94.4% to 99.1%) for detecting oesophagogastric varices (both P<0.001 compared with a prespecified 85% threshold). When using the optimal 18% threshold for luminal circumference of the oesophagus derived from the development cohort (n=393), the sensitivity and specificity of ds-MCE for detecting high risk oesophageal varices in the validation cohort (n=189) were 95.8% (89.7% to 98.4%) and 94.7% (88.2% to 97.7%), respectively. The diagnostic accuracy of ds-MCE for detecting high risk oesophagogastric varices, oesophageal varices, and gastric varices was 96.3% (92.6% to 98.2%), 96.9% (95.2% to 98.0%), and 96.7% (95.0% to 97.9%), respectively. Two serious adverse events occurred with OGD but none with ds-MCE.
Conclusion: The findings of this study suggest that ds-MCE is a highly accurate and safe diagnostic tool for detecting and grading oesophagogastric varices and is a promising alternative to OGD for screening and surveillance of oesophagogastric varices in patients with cirrhosis.
Trial registration: ClinicalTrials.govNCT03748563.
"
Ultra-processed food exposure and adverse health outcomes,"Introduction
Ultra-processed foods, as defined using the Nova food classification system, encompass a broad range of ready to eat products, including packaged snacks, carbonated soft drinks, instant noodles, and ready-made meals.1These products are characterised as industrial formulations primarily composed of chemically modified substances extracted from foods, along with additives to enhance taste, texture, appearance, and durability, with minimal to no inclusion of whole foods.2Analyses of worldwide ultra-processed food sales data and consumption patterns indicate a shift towards an increasingly ultra-processed global diet,34although considerable diversity exists within and between countries and regions.56Across high income countries, the share of dietary energy derived from ultra-processed foods ranges from 42% and 58% in Australia and the United States, respectively, to as low as 10% and 25% in Italy and South Korea.56In low and middle income countries such as Colombia and Mexico, for example, these figures range from 16% to 30% of total energy intake, respectively.5Notably, over recent decades, the availability and variety of ultra-processed products sold has substantially and rapidly increased in countries across diverse economic development levels, but especially in many highly populated low and middle income nations.3
The shift from unprocessed and minimally processed foods to ultra-processed foods and their subsequent increasing contribution to global dietary patterns in recent years have been attributed to key drivers including behavioural mechanisms, food environments, and commercial influences on food choices.7891011These factors, combined with the specific features of ultra-processed foods, raise concerns about overall diet quality and the health of populations more broadly. For example, some characteristics of ultra-processed foods include alterations to food matrices and textures, potential contaminants from packaging material and processing, and the presence of food additives and other industrial ingredients, as well as nutrient poor profiles (for example, higher energy, salt, sugar, and saturated fat, with lower levels of dietary fibre, micronutrients, and vitamins).612Although mechanistic research is still in its infancy, emerging evidence suggests that such properties may pose synergistic or compounded consequences for chronic inflammatory diseases and may act through known or plausible physiological mechanisms including changes to the gut microbiome and increased inflammation.1213141516Researchers, public health experts, and the general public have shown considerable interest in ultra-processed dietary patterns, foods, and their constituent parts given their potential role as modifiable risk factors for chronic diseases and mortality.
Although several meta-analyses have made efforts to consolidate the many individual original research articles that have investigated the associations between exposure to ultra-processed foods and the risk of adverse health outcomes in the past decade,1718no comprehensive umbrella review has offered a broad overview and assessment of the existing meta-analytic evidence. Undertaking such a comprehensive review has the potential to enhance our understanding of these associations and provide valuable insights for better informing public health policies and strategies. This is particularly pertinent as the global debate continues regarding the need for public health measures to tackle exposure to ultra-processed foods in general populations.1920To bridge this gap in evidence and contribute to the ongoing discussion on the role of ultra-processed food exposure in chronic diseases, we did an umbrella review to evaluate the evidence provided by meta-analyses of observational epidemiological studies exploring the associations between exposure to ultra-processed food and the risk of adverse health outcomes.
Methods
We conducted and reported this systematic umbrella review of meta-analyses (herein referred to as “meta-analysis studies”) in line with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines.21
Inclusion criteria and searches
We found no existing pooled analyses of randomised controlled trials during the pilot phase of this review. Consequently, we refined our search approach and scope to focus on observational epidemiological studies. Thus, we outlined inclusion criteria in accordance with the population, exposure, comparisons, outcomes, and study design (PECOS) reporting structure.22Eligible meta-analysis studies comprised human populations across the life course, irrespective of health status (population). We also considered meta-analysis studies that examined associations of dietary intake of ultra-processed foods, as defined by the Nova food classification system (exposure), comparing dose-response (continuous exposure) and/or non-dose-response (categorical only or categorical and continuous exposure) associations of dietary intake of ultra-processed foods (comparison), with any adverse health endpoint (outcome). Included in our review were observational epidemiological study designs (for example, prospective cohort, case-control, and/or cross sectional) that pooled categorical or continuous outcome data by using meta-analysis (study design).
The lead author (MML) did a systematic search across MEDLINE, PsycINFO, Embase, and the Cochrane databases for studies spanning the period from 2009 to June 2023 (last update). The year 2009 aligns with the initial publication of the details and principles of the Nova food classification system, which introduced the concept of ultra-processed foods.23We applied no language limitations.
To identify relevant meta-analysis studies, we used key search terms and variations of text words related to ultra-processed food or Nova and meta-analysis study design: (“ultra-processed food” OR UPF OR “Nova food classification system”) AND (“meta-analysis” OR “systematic review”). The specific search strings for each database can be found in supplementary table A. We used Covidence systematic review software to do duplicate primary screening based on titles and abstracts (MML and EG) and duplicate secondary screening based on full text articles (MML and WM). We screened references cited within the eligible meta-analysis studies to identify any additional relevant meta-analysis studies (EG). Any disagreements between authors conducting eligibility screening were resolved through consensus. We included the most recent and/or largest meta-analysis study when multiple pooled analyses were available for the same adverse health outcome. This is consistent with the methods used in previous umbrella reviews.242526In cases in which the most recent meta-analysis study examined non-dose-response and dose-response exposure to ultra-processed foods, we included both meta-analysed effect estimates.
Data extraction
We extracted characteristics of the original research articles included in the retained meta-analysis studies in duplicate by using a pre-piloted custom Microsoft Excel spreadsheet (MML, EG, SD, DNA, AJM, and SG). These data included details such as the outcome, spanning health domains such as mortality, cancer, and mental, respiratory, cardiovascular, gastrointestinal, and metabolic health outcomes. In addition, the data extraction encompassed details on the level of exposure comparison, distinguishing between dose-response (involving each additional serving per day or a 10% increment) and non-dose-response (encompassing categories such as high versus low, daily consumers versus not daily consumers, and frequent consumption versus no frequent consumption, as well as combinations of categories with continuous exposure including 1% or 10% increments). It also covered the total number of studies (original research articles), participants, and cases included in the pooled analysis. The extraction included effect estimates with 95% confidence intervals from both separate original research articles and those pooled from the meta-analysis studies, as well as the pooled effect size metric (hazard ratios, odds ratios, and risk ratios). Furthermore, we extracted details about the meta-analysis study, including the first author’s name, publication year, original research study design, and competing interests and funding disclosures of meta-analysis study authors. We prioritised pooled estimates with the largest number of prospective cohorts, given that prospective studies guarantee temporality in epidemiological associations and strongly limit reverse causality bias.27Additionally, we extracted pooled estimates for related health outcomes that were meta-analysed together and separately (for example, metabolic syndrome and its individual components including low high density lipoprotein cholesterol and hypertriglyceridaemia). If information was missing or unclear in the meta-analysis studies, we obtained the data from the original research articles or directly requested it from the corresponding author(s) of those meta-analysis studies. If discrepancies existed between the data reported in the original research articles and the meta-analyses, we prioritised extracting data from the original research article.
Data analysis
Operating according to previously published methods and guidance,2829we used a random effects meta-analysis model to reanalyse the effect estimates for each outcome. As part of our main reanalysis, we took the following steps: entry of separate effect estimates and the total number of participants and cases from the original research articles; recalculation of the pooled effect estimates using the original metric used by the meta-analysis study authors (hazard ratio, odds ratio, and risk ratio with 95% confidence intervals); recalculation of the P value; and recalculation of the between study heterogeneity using the I2statistic. We also calculated 95% prediction intervals and assessed excess significance bias, small study effects, and the largest study significance (detailed below) as part of our reanalysis.
We used the I2statistic to assess the proportion of variability in a pooled analysis that was explained by between study heterogeneity, rather than by sampling error, and to reflect the extent to which 95% confidence intervals from the different original research articles overlapped with each other.30We considered a value of 50% to be moderate heterogeneity and a value of 75% or more to be high heterogeneity.30
Unlike 95% confidence intervals, which give a range within which we can reasonably expect the true population parameter to fall based on our sample, 95% prediction intervals provide a range in which we can anticipate the value of an individual observation from future studies to fall.31In an umbrella review, if the 95% prediction intervals exclude the null, it indicates a statistically significant range of effect estimates.31Notably, outputs for tests of 95% prediction intervals, as well as the small study effects and excess significance bias (as described below), were available only for pooled analyses involving three or more original research articles (n=28).
We did a test for excess significance to determine whether the number of studies with nominally significant results (P<0.05) was higher than expected, based on statistical power.32
We used Egger's regression asymmetry test to detect potential small study effects, whereby smaller studies sometimes show different, often larger, effect estimates than large studies.33
We assessed whether effect estimates from the largest original research article (that is, the study with the highest participant count) included in the pooled analyses had a P value below 0.05. This evaluation is expected to provide the most reliable and precise estimation considering the statistical power involved.35We evaluated the significance of the largest study across all 45 unique pooled analyses.
For visually comparative purposes, we developed forest plots whereby pooled effect estimates were harmonised to equivalent odds ratios with 95% confidence intervals by using methods presented in table 1 of Fusar-Poli et al (2018).36In this instance, an equivalent odds ratio >1 indicates higher odds, whereas an equivalent odds ratio <1 indicates lower odds, of an outcome.
We used the online version of the R statistical package calledmetaumbrella(https://metaumbrella.org/app) for data analyses.37The corresponding code repository is publicly accessible at GitHub (https://github.com/cran/metaumbrella).37Furthermore, the raw data are available at the Open Science Framework (https://osf.io/8j2gt/), and a step-by-step analysis usingmetaumbrellais provided in supplementary table B.
Terminology
We used the terms “direct” and “inverse” to describe the direction of observed associations between ultra-processed food exposure and adverse health outcomes, with “direct” referring to a higher risk associated with greater exposure and “inverse” referring to a lower risk. We chose these terms over “positive” or “negative” associations to avoid ambiguous interpretations.
Credibility and quality assessment of evidence and methods
Using the data derived from our reanalyses, such as the P value, I2statistic, 95% prediction intervals, small study effects, excess significance bias, and largest study significance, we categorised each re-meta-analysed result of our umbrella review as convincing (“class I”), highly suggestive (“class II”), suggestive (“class III”), weak (“class IV”), or no evidence (“class V”) by following evidence classification criteria and previous umbrella reviews.24252636We determined these classifications on the basis of the criteria outlined in supplementary table C.
We used the GRADE (Grading of Recommendations, Assessment, Development, and Evaluation) system to evaluate the quality of evidence for each unique pooled analysis, and categorised them as either “high,” “moderate,” “low,” or “very low” (supplementary table D).38The GRADE approach initially considers all observational studies as evidence of low quality.38Of the eight criteria put forth in the GRADE method, five have the potential to diminish confidence in the accuracy of effect estimates, leading to downgrading: risk of bias, inconsistency of results across studies, indirectness of evidence, imprecision, and publication bias.38Additionally, three criteria are proposed to enhance confidence or upgrade it: a substantial effect size with no plausible confounders, a dose-response relation, and a conclusion that all plausible residual confounding would further support inferences regarding exposure effect.38
We used the AMSTAR 2 (A Measurement Tool to Assess Systematic Reviews – second edition) quality assessment tool to evaluate the quality of the included meta-analysis studies (supplementary table E).39This tool emphasises certain critical domains that could affect the reliability of a review.39The critical domains considered pertinent to our review included pre-specified review methods, the adequacy of the literature search, the rationale for excluding specific studies, the risk of bias in the included studies, the appropriateness of the meta-analytic methods, and the consideration of bias when interpreting the results (domains bolded in supplementary table E).39Following a recommendation from a recent review,39we used the AMSTAR 2 tool to do a qualitative assessment, considering the potential impact of a low rating for each item, particularly the critical domains outlined in supplementary table E. This meant that we did not quantify individual item ratings or combine them to create an overall score.39
Patient and public involvement
The study and manuscript development did not involve patients or the public owing to the absence of funding for this research.
Results
The systematic search identified 430 de-duplicated articles (fig 1). After applying the eligibility criteria, we included 14 meta-analysis studies with 45 distinct pooled analyses.1718404142434445464748495051
Study characteristics
The range of adverse health outcomes reviewed across the 45 discrete pooled analyses included mortality, cancer, and mental, respiratory, cardiovascular, gastrointestinal, and metabolic health outcomes. All meta-analysis studies were published in the past three years, and none was funded by a company involved in the production of ultra-processed foods. The number of original research articles included in the pooled analyses was four on average and ranged from two to nine. The sum total number of participants included across the pooled analyses was 9 888 373 (ranging from 111317to 962 59348). Supplementary table F details the characteristics of the original research articles included in each of the pooled analyses, such as study design, population, and exposure measurement. Pooled analyses included estimates from original research articles that comprised either prospective cohorts (n=18), mixed study designs (n=15), or cross sectional designs (n=12). Most pooled analyses included adults as the main population, except for five, which included children and adolescents in examining mental health outcomes and respiratory conditions.184950In 87% of pooled analyses, estimates of exposure to ultra-processed food were obtained from a combination of tools, including food frequency questionnaires, 24 hour dietary recalls, and dietary history, as reported in the meta-analysis studies. Six pooled analyses, pertaining to heart disease related mortality,51cancer related mortality,51respiratory conditions,18and non-alcohol fatty liver disease,46included estimates of exposure from food frequency questionnaires alone.
Each of the meta-analysis studies examined the non-dose-response associations between exposure to ultra-processed foods and adverse health outcomes. However, an additional analysis involving dose-response modelling of the ultra-processed food exposure variable was conducted in 13 pooled analyses across five meta-analysis studies.4042434751The outcomes considered using this approach included all cause mortality and cardiovascular disease events, such as cardiovascular disease morbidity and mortality, associated with each increase in daily servings of ultra-processed food.43One meta-analysis study specifically pooled heart disease related deaths, such as ischaemic heart disease related mortality and cerebrovascular disease related mortality, with each 10% increase in total ultra-processed food exposure.51Additionally, associations for other outcomes, such as abdominal obesity,42overweight and obesity,42type 2 diabetes,47and breast, colorectal, and prostate cancers,40were modelled on the basis of each 10% increase in ultra-processed food exposure.
Results of syntheses
Figure 2andfigure 3show the direction and sizes of effect estimates using equivalent odds ratios for both the non-dose-response and dose-response relations between exposure to ultra-processed foods and each adverse health outcome, respectively.
On the basis of the random effects model, 32 (71%) distinct pooled analyses showed direct associations between greater ultra-processed food exposure and a higher risk of adverse health outcomes at the significance level of P≤0.05 (supplementary table G). Additionally, of these combined analyses, 11 (34%) showed continued statistical significance when a more stringent threshold was applied (P<1×10−6) (data not shown). These included the incidence of all cause mortality,43cardiovascular disease related mortality,43heart disease related mortality,51type 2 diabetes (dose-response and non-dose-response),47and depressive outcomes,50as well as the prevalence of anxiety and combined common mental disorder outcomes,50adverse sleep related outcomes,49and wheezing.18
We found evidence of moderate (I2=50-74.9%) to high (I2≥75%) heterogeneity in 13 (29%) and eight (18%) of the 45 discrete pooled analyses, respectively (supplementary table G). The 95% prediction intervals were statistically significant for seven (25%) of the 28 pooled analyses with three or more original research articles (supplementary table G), including direct associations of greater ultra-processed food exposure with higher risks of all cause mortality,43cardiovascular disease related mortality,43common mental disorder outcomes,50Crohn’s disease,48obesity,42and type 2 diabetes (dose-response).47Additionally, we found evidence of excess significance bias in nine (32%) of the 28 pooled analyses with three or more original research articles listed in supplementary table G. This bias was evident in associations between higher ultra-processed food exposure and all cause mortality (dose-response and non-dose-response),43hypertension,44abdominal obesity,42metabolic syndrome,45non-alcoholic fatty liver disease,46obesity (dose-response and non-dose-response),42and type 2 diabetes.47Small study effects were evident in five (18%) of the 28 pooled analyses with three or more original research articles, as indicated in supplementary table G. We observed these effects in associations between higher ultra-processed food exposure and all cause mortality (dose-response and non-dose-response),43breast cancer,40metabolic syndrome,45and obesity (dose-response).42
Effect estimates from the largest original research article were nominally statistically significant for 28 (62%) pooled analyses (supplementary table G) and pertained to associations of greater ultra-processed food exposure with higher risks of all cause mortality (dose-response and non-dose-response),43cardiovascular disease related mortality (dose-response and non-dose-response),43heart disease related mortality (dose-response and non-dose-response),51central nervous system tumours,40adverse sleep outcomes,49common mental disorder outcomes,50asthma,18wheezing,18cardiovascular disease events (dose-response and non-dose-response),43low high density lipoprotein concentrations,17abdominal obesity (dose-response and non-dose-response),42hyperglycaemia,17metabolic syndrome,45non-alcoholic fatty liver disease,46obesity and overweight (dose-response and non-dose-response),42and type 2 diabetes (dose-response and non-dose-response).47
Credibility and GRADE quality assessments
Pooled effect estimates from nine dose-response and seven non-dose-response cohorts showed direct associations between greater exposure to ultra-processed foods and higher risks of incident all cause mortality (dose-response risk ratio 1.02, 95% confidence interval 1.01 to 1.03; credibility assessment class III; GRADE assessment moderate and non-dose-response risk ratio 1.21, 1.15 to 1.27; class II; low)43(fig 4; supplementary tables D and G). Four dose-response and five non-dose-response cohorts informed the synthesis of associations between greater exposure to ultra-processed foods and higher risks of incident cardiovascular disease related mortality (dose-response risk ratio 1.05, 1.02 to 1.08; class IV; low and non-dose-response risk ratio 1.50, 1.37 to 1.63; class I; very low).43Effect estimates from two cohorts were pooled and showed limited evidence supporting direct associations between greater ultra-processed food exposure and a higher risk of incident cancer related mortality (hazard ratio 1.00, 0.81 to 1.24; class V; low).51We found further limited evidence for an association between greater ultra-processed food exposure and incident heart disease related mortality (dose-response hazard ratio 1.18, 0.95 to 1.47; class V; low and non-dose-response hazard ratio 1.66, 1.51 to 1.84; class II; low).51
Pooled analyses from seven cohort studies showed direct associations between greater exposure to ultra-processed foods and higher risks of incident cancer overall (hazard ratio 1.12, 1.06 to 1.19; class III; very low).41Synthesised analyses including mixed cohort and case-control study designs additionally showed direct associations with a risk of colorectal cancer (dose-response odds ratio 1.04, 1.01 to 1.07; class IV; low and non-dose-response odds ratio 1.23, 1.10 to 1.38; class III; very low).40
We found limited evidence for pooled analyses, including mixed cohort and case-control study designs, of the association between greater ultra-processed food exposure and higher risks of breast cancer (dose-response odds ratio 1.03, 0.98 to 1.09; class V; low and non-dose-response odds ratio 1.15, 0.99 to 1.34; class V; very low),40central nervous system tumours (odds ratio 1.20, 0.87 to 1.65; class V; very low), chronic lymphocytic leukaemia (odds ratio 1.08, 0.80 to 1.45; class V; very low), pancreatic cancer (odds ratio 1.24, 0.85 to 1.79; class V; very low), and prostate cancer (dose-response odds ratio 0.99, 0.97 to 1.02; class V; moderate and non-dose-response odds ratio 1.02, 0.93 to 1.12; class V; low).
Examining data from two to four cross sectional designs, we found evidence supporting direct associations between greater exposure to ultra-processed foods and a higher risk of the prevalence of adverse sleep related outcomes (odds ratio 1.41, 1.24 to 1.61; class II; low),49as well as anxiety outcomes (odds ratio 1.48, 1.37 to 1.59; class I; low).50We observed similar associations in separate assessments of prevalent combined common mental disorder outcomes across six cross sectional designs (odds ratio 1.53, 1.43 to 1.63; class I; low)50and incident depressive outcomes across two cohorts (odds ratio 1.22, 1.16 to 1.28; class II; low).50
Pooled analyses that included two cross sectional studies provided limited evidence of an association between greater exposure to ultra-processed foods and risks of prevalent asthma (risk ratio 1.20, 0.99 to 1.46; class V; very low)18and wheezing (risk ratio 1.40, 1.27 to 1.55; class II; low).18
Pooled analyses from six cohorts showed direct associations between greater ultra-processed food exposure and higher risks of incident cardiovascular disease events such as morbidity and mortality (dose-response risk ratio 1.04, 1.02 to 1.06; class III; low and non-dose-response risk ratio 1.35, 1.18 to 1.54; class III; very low),43as well as incident cardiovascular disease morbidity (dose-response risk ratio 1.04, 1.02 to 1.06; class III; low and non-dose-response risk ratio 1.20, 1.09 to 1.33; class III; low).43The higher risk of hypertension associated with greater ultra-processed food exposure was assessed using data from nine mixed cohorts and cross sectional study designs (odds ratio 1.23, 1.11 to 1.37; class III; very low).44We found weak to no evidence for associations between exposure to ultra-processed foods and hypertriglyceridaemia (odds ratio 0.95, 0.60 to 1.50; class V; very low)17and low high density lipoprotein concentrations (odds ratio 2.02, 1.27 to 3.21; class IV; very low).17
We found weak or no evidence in pooled analyses incorporating data from four cohorts for associations between greater exposure to ultra-processed foods and higher risks of incident Crohn’s disease (hazard ratio 1.71, 1.37 to 2.14; class IV; low)48and ulcerative colitis (hazard ratio 1.17, 0.86 to 1.61; class V; very low).48
The risk of abdominal obesity was examined by synthesising effect estimates from mixed cohort and cross sectional study designs, which showed direct associations with greater ultra-processed food exposure (dose-response odds ratio 1.05, 1.02 to 1.07; class III; low and non-dose-response odds ratio 1.41, 1.18 to 1.68; class III; very low).42We found weak to no evidence for associations between exposure to ultra-processed foods and hyperglycaemia (odds ratio 1.10, 0.34 to 3.52; class V; very low),17metabolic syndrome (risk ratio 1.25, 1.09 to 1.42; class IV; very low),45non-alcoholic fatty liver disease (risk ratio 1.23, 1.03 to 1.46; class IV; very low),46and overweight and obesity (assessed together: dose-response odds ratio 1.03, 1.01 to 1.06; class IV; moderate and non-dose-response odds ratio 1.29, 1.05 to 1.58; class IV; low).42Effect estimates from four cross sectional studies informed pooled analyses of direct associations between greater ultra-processed food exposure and higher risk of the prevalence of overweight (dose-response odds ratio 1.06, 1.03 to 1.10; class III; low and non-dose-response odds ratio 1.36, 1.14 to 1.63; class III; very low).42Pooled analyses including seven cross sectional study designs further showed direct associations between greater ultra-processed food exposure and a higher prevalence of obesity (dose-response odds ratio 1.07, 1.03 to 1.11; class III; low and non-dose-response odds ratio 1.55, 1.36 to 1.77; class II; low).42The combined analysis of seven cohorts showed direct associations between greater exposure to ultra-processed foods and higher risk of incident type 2 diabetes (dose-response risk ratio 1.12, 1.11 to 1.13; class I; moderate and non-dose-response odds ratio 1.40, 1.23 to 1.59; class II; very low).47
Quality assessment of individual meta-analysis studies using AMSTAR 2 tool
Although all of the authors of the meta-analysis studies used satisfactory literature search techniques (AMSTAR critical item 4) and accounted for the potential risk of bias in original research articles when interpreting and discussing their results (AMSTAR critical item 9), we considered the overall confidence in the results of seven meta-analysis studies to be low owing to lack of clarity as to whether the review methods were established before the conduct of the review (AMSTAR critical item 2) (supplementary table E).17404142444551Based on non-critical items, the confidence in the results of all meta-analysis studies was assessed as moderate. Notably, the most considerable limitations, for which all meta-analysis studies scored zero, were related to the review authors’ failure to provide an explanation for their selection of study designs for inclusion in the review (AMSTAR item 3) and their omission of information on funding sources for the studies included in the review (AMSTAR item 10).39
Discussion
Principal findings
Our umbrella review provides a comprehensive overview and evaluation of the evidence for associations between dietary exposure to ultra-processed foods and various adverse health outcomes. Our review included 45 distinct pooled analyses, encompassing a total population of 9 888 373 participants and spanning seven health parameters related to mortality, cancer, and mental, respiratory, cardiovascular, gastrointestinal, and metabolic health outcomes. Across the pooled analyses, greater exposure to ultra-processed foods, whether measured as higher versus lower consumption, additional servings per day, or a 10% increment, was consistently associated with a higher risk of adverse health outcomes (71% of outcomes).
Considering the evidence classification criteria assessments, we graded 9% of the pooled analyses as providing convincing evidence (class I), including those measuring risks of cardiovascular disease related mortality, common mental disorder outcomes, and type 2 diabetes (dose-response) (fig 4). We graded 16% of pooled analyses (all non-dose-response) as providing highly suggestive evidence (class II), encompassing risks of all cause mortality, heart disease related mortality, adverse sleep related outcomes, wheezing, obesity, and type 2 diabetes. We graded approximately 29% of the pooled analyses as providing suggestive evidence (class III), covering a range of conditions from risks of abdominal obesity to overweight, with 18% graded as weak evidence (class IV), encompassing outcomes such as risks of colorectal cancer and overweight and obesity (evaluated together as single outcome). We graded the remaining 29% of pooled analyses as lacking evidence (class V), spanning conditions from asthma to ulcerative colitis. As previously noted, moderate to high levels of heterogeneity were observed across 45% of pooled analyses. Using GRADE assessments, which initially assign observational epidemiological studies as “low” quality evidence,38approximately 29% of the pooled analyses remained unchanged, indicating that no additional concerns were identified based on GRADE criteria, with a further 9% upgraded to a “moderate” rating owing to a dose-response gradient (fig 4). Dose-response pooled analyses upgraded to “moderate” quality evidence related to all cause mortality, prostate cancer, overweight and obesity (assessed together), and type 2 diabetes. Associations were downgraded largely owing to inconsistencies or heterogeneity in the effect estimates found across the original research articles or owing to imprecision (that is, wide confidence intervals).
The heterogeneity and imprecision noted across several of the pooled analyses, as shown by both the evidence classification criteria and GRADE assessme","Objective: To evaluate the existing meta-analytic evidence of associations between exposure to ultra-processed foods, as defined by the Nova food classification system, and adverse health outcomes.
Design: Systematic umbrella review of existing meta-analyses.
Data sources: MEDLINE, PsycINFO, Embase, and the Cochrane Database of Systematic Reviews, as well as manual searches of reference lists from 2009 to June 2023.
Eligibility criteria for selecting studies: Systematic reviews and meta-analyses of cohort, case-control, and/or cross sectional study designs. To evaluate the credibility of evidence, pre-specified evidence classification criteria were applied, graded as convincing (“class I”), highly suggestive (“class II”), suggestive (“class III”), weak (“class IV”), or no evidence (“class V”). The quality of evidence was assessed using the GRADE (Grading of Recommendations, Assessment, Development, and Evaluations) framework, categorised as “high,” “moderate,” “low,” or “very low” quality.
Results: The search identified 45 unique pooled analyses, including 13 dose-response associations and 32 non-dose-response associations (n=9 888 373). Overall, direct associations were found between exposure to ultra-processed foods and 32 (71%) health parameters spanning mortality, cancer, and mental, respiratory, cardiovascular, gastrointestinal, and metabolic health outcomes. Based on the pre-specified evidence classification criteria, convincing evidence (class I) supported direct associations between greater ultra-processed food exposure and higher risks of incident cardiovascular disease related mortality (risk ratio 1.50, 95% confidence interval 1.37 to 1.63; GRADE=very low) and type 2 diabetes (dose-response risk ratio 1.12, 1.11 to 1.13; moderate), as well as higher risks of prevalent anxiety outcomes (odds ratio 1.48, 1.37 to 1.59; low) and combined common mental disorder outcomes (odds ratio 1.53, 1.43 to 1.63; low). Highly suggestive (class II) evidence indicated that greater exposure to ultra-processed foods was directly associated with higher risks of incident all cause mortality (risk ratio 1.21, 1.15 to 1.27; low), heart disease related mortality (hazard ratio 1.66, 1.51 to 1.84; low), type 2 diabetes (odds ratio 1.40, 1.23 to 1.59; very low), and depressive outcomes (hazard ratio 1.22, 1.16 to 1.28; low), together with higher risks of prevalent adverse sleep related outcomes (odds ratio 1.41, 1.24 to 1.61; low), wheezing (risk ratio 1.40, 1.27 to 1.55; low), and obesity (odds ratio 1.55, 1.36 to 1.77; low). Of the remaining 34 pooled analyses, 21 were graded as suggestive or weak strength (class III-IV) and 13 were graded as no evidence (class V). Overall, using the GRADE framework, 22 pooled analyses were rated as low quality, with 19 rated as very low quality and four rated as moderate quality.
Conclusions: Greater exposure to ultra-processed food was associated with a higher risk of adverse health outcomes, especially cardiometabolic, common mental disorder, and mortality outcomes. These findings provide a rationale to develop and evaluate the effectiveness of using population based and public health measures to target and reduce dietary exposure to ultra-processed foods for improved human health. They also inform and provide support for urgent mechanistic research.
Systematic review registration: PROSPERO CRD42023412732.
"
Added benefit and revenues of oncology drugs approved by the EMA,"Introduction
The share of cancer care expenditures allocated to oncology drugs is consistently rising, primarily driven by increasing volumes of innovative drugs reaching the market and the high prices associated with these treatments.12345Correspondingly, global spending for oncology drugs is estimated to rise from $167bn (£132bn; €155bn) in 2020 to $269bn in 2025.6High prices for oncology drugs are often justified by the need to earn back research and development (R&D) expenses, and by the value these drugs aim to deliver to patients.78Whether prices are truly justified by the required earnings and the value—or added benefit—that these drugs deliver to patients has been subject to extensive debate.1291011
Health technology assessment (HTA) agencies are among the various organisations that conduct and publish added benefit assessments. The primary objective of HTA is to inform decision makers on the implementation of new health technologies to ensure that the finite resources of a healthcare system are used in an efficient and effective manner. In this context, added benefit assessments are a key tool for evaluating the value of new drugs, informing clinical practice, and guiding reimbursement decisions.12These assessments are based on comparing a drug’s effects with those of the best available alternative, informed by relevant evidence. Added benefit assessments go beyond benefit-risk assessments performed by regulatory authorities because benefit-risk assessments are not necessarily comparing a drug’s effects with those of the best (nationally) available alternative. The differences between these two types of assessments might lead to drugs with a positive benefit-risk balance but negative added benefit, which is often the case if robust comparative evidence is lacking.13141516
Increasingly, oncology drugs are approved based on less comprehensive evidence, such as evidence obtained from non-randomised or single arm trials, or based only on surrogate endpoints that do not directly represent a clinical benefit but might predict one.21718A study by Naci and colleagues found that 13 (24%) of the 54 pivotal trials that supported the 32 new oncology drugs approved by the European Medicines Agency (EMA) between 2014 and 2016 were non-randomised or single arm trials.19Regulatory authorities acknowledge the unmet medical needs that new innovative treatments might address and have adopted expedited approval pathways to enable patient access, resulting in an increase in the approval of drugs that are associated with less comprehensive evidence.20This approach leads to substantial uncertainty at the time reimbursement decisions are made, inherently hampering assessments of added benefit. HTA bodies tend to show greater reluctance in recommending drugs for which there is less comprehensive evidence available, and previous research has shown high proportions of negative added benefit ratings of (oncology) drugs approved through expedited approval pathways.1415182122
With high prices, increased use of expedited approval pathways, and the consequential difficulties for added benefit assessments, concern is growing that incentives within the pharmaceutical market are not in line with the interests of patients, namely fast and sustainable access to drugs that provide clinical benefits.31423Previous research has shown that no statistically significant association exists between estimates of added benefit and drug prices, implying that drugs are not necessarily rewarded for the value they deliver.21417232425Drugs lacking added benefit are not found to be associated with lower prices compared with drugs that provide greater benefit.1However, an important limitation of these studies is that their analyses are often based on public list prices, which are arguably an imperfect measure of financial incentives because they only provide information for a single country and they are usually subject to confidential discounts negotiated by hospitals, insurers, governments, or HTA agencies.1423Focusing on drug revenues might be a valid alternative because these are globally relevant and provide a better reflection of the earnings associated with a drug.
The objectives of this study were to investigate the added benefit of oncology drugs approved by the EMA between 1995 and 2020; assess corresponding cumulative revenues compared with estimated R&D costs; and explore the association between added benefit and revenues. Additionally, we aimed to examine whether discrepancies in added benefit or revenues exist across the various EMA approval pathways; that is, standard marketing authorisation (SMA), conditional marketing authorisation (CMA), and authorisation under exceptional circumstances (AEC).Box 1presents definitions of key terms.
Definitions of key terms used throughout this study
The added benefit of a health technology can be defined as its therapeutic value compared with one or more alternative treatments, typically the standard of care within the assessed indication. Added benefit ratings serve different purposes, primarily enabling treatment prioritisation and informing drug related decision making.
Oncology drugs can be approved for and used in several indications. The extent of added benefit for a drug can differ considerably across indications owing to, for example, variations in standards of care. Consequently, evaluations of added benefit apply to specific drug-indication combinations.
SMA is a type of marketing authorisation that is granted by the European Medicines Agency (EMA) when comprehensive data are available that indicate a positive benefit-risk balance.
CMA is a type of marketing authorisation that can be granted by the EMA before comprehensive clinical data are available. CMA is intended for drugs that target seriously debilitating or life threatening diseases for which an unmet medical need exists. The drug needs to have a positive benefit-risk balance and the benefit of immediate availability needs to outweigh the risks associated with the lack of comprehensive clinical evidence. A CMA is subject to requirements to conduct further studies after authorisation. Once comprehensive data are provided and the benefit-risk balance remains positive, a CMA can be converted into a standard marketing authorisation.26
AEC is a type of marketing authorisation that can be granted by the EMA for drugs for which comprehensive clinical or non-clinical data cannot be provided, such as for very rare diseases, because it is considered unethical to collect these data, or because the current state of scientific knowledge does not allow it. An AEC is also subject to requirements to conduct further studies after authorisation, but is not normally converted into a standard marketing authorisation.27
Methods
To quantify added benefit, we extracted ratings from evaluation reports by several organisations, including HTA agencies from Europe and the United States, medical oncology societies, and a drug bulletin. We analysed the development of global revenues based on publicly available financial reports from pharmaceutical companies and compared them with previously published estimates of R&D expenses. Finally, we integrated these analyses by linking added benefit ratings to corresponding revenue data.
Study cohort and setting
All drugs and their initial indications approved in the European Union since the inception of the EMA in 1995 up to 2020 were retrieved from the EMA’s register of European public assessment reports.28Veterinary drugs, non-oncology drugs, generics, biosimilars, refused drugs, diagnostics, and duplicates were excluded from the cohort. Non-oncology drugs were identified based on the Anatomical Therapeutical Chemical (ATC) classification system of the World Health Organization.29
Ratings of added benefit were obtained from evaluation reports published by organisations including HTA agencies, medical oncology societies, and drug bulletins, with the final selection based on four criteria: the organisations should publish an appraisal or judgment of added benefit; the organisations had to use a multiple category scale to quantify the level of added benefit (eg, absent, minor, moderate, major); the organisations should not incorporate any cost related aspects in their added benefit appraisals (ie, the added benefit rating should not be confounded by costs); and their reports had to be in English, Dutch, French, German, or Italian. Ultimately, this led to the consideration of evaluation reports from the following organisations:
Institute for Clinical and Economic Review (ICER, a non-profit organisation in the US). The US lacks a centralised HTA agency. While ICER is not formally designated as an HTA agency, it operates as an HTA-like organisation, conducting assessments similar to traditional HTA processes30
Haute Autorité de Santé (HAS, HTA agency of France)
Gemeinsamer Bundesausschuss (G-BA, HTA agency of Germany). Germany has two HTA organisations: G-BA and the Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen. They each publish separate assessments, which can lead to differing conclusions. This study focuses only on G-BA’s assessments because of their responsibility for final appraisals and the inclusion of orphan drugs in their evaluations (unlike Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen)31
Agenzia Italiana del Farmaco (HTA agency of Italy)
European Society for Medical Oncology (ESMO; developed the magnitude of clinical benefit scale for grading the added benefit of oncology drug-indication combinations)
American Society of Clinical Oncology (ASCO; developed the value framework for grading the added benefit of oncology drug-indication combinations through net health benefit scores)
Prescrire (French independent drug bulletin). Despite a seemingly different scope than the other organisations, Prescrire is internationally renowned for its high quality and comprehensive drug evaluations conducted collaboratively by a team of physicians and pharmacists. The outcomes of Prescrire’s assessments play an important part in informing drug related decision making.3233
After retrieving all oncology drugs and their initial indications that had been approved by the EMA between 1995 and 2020, three cohorts were formed in line with the three objectives of the study. The added benefit cohort included all added benefit ratings of the drug-indication combinations that were evaluated by at least one of the organisations listed above. To ensure that our study's findings were grounded in comparable evidence conducted near the EMA approval date, we excluded evaluations performed more than 1.5 years before or after the EMA approval date. This decision was made to limit potential discrepancies in the evaluations of added benefits owing to the availability of additional evidence over time. HTA organisations generally perform assessments within 1.5 years after EMA approval, which makes this timeframe appropriate for our study.3435The decision to also exclude evaluations conducted more than 1.5 years before the EMA approval date was essential because there are differences in timelines between regulatory authorities in the US and Europe, and we included organisations from both regions in our study.
The second cohort, the revenue cohort, included all oncology drugs for which revenue data were available. The third cohort, the combined cohort, comprised all oncology drugs with at least three years of revenue data available that had been evaluated for added benefit by at least one organisation. We excluded all drugs with several initial indications or that received new indications before the end of follow-up, which was checked by comparing the initial and most recent European public assessment reports. This strategy ensured that the revenue data were correctly attributed to the indications on which the added benefit ratings were based, which is important because added benefit evaluations apply to drug-indication combinations, whereas revenue data are relevant at the product level.
Data collection
Added benefit evaluation reports were collected by following a standardised data extraction guide developed by FB and discussed with LTB and Rick Vreman to ensure consistent extraction of the ratings from each organisation (see supplementary materials box S1). Added benefit ratings relate to specific drug-indication combinations. When added benefit ratings were assigned to subindications (eg, specific subpopulations) of the initial indication, these were treated as distinct drug-indication combinations in the study cohort. All retrieved added benefit ratings were recategorised using a four point ranking scale to indicate negative or non-quantifiable, minor, substantial, or major added benefit (seetable 1), based on previous work.1537
We retrieved global revenue data up to 2020 from publicly available financial reports of pharmaceutical companies on the level of the brand names of the included drugs. When financial reports indicated that only revenues of major or bestselling products were disclosed, we inferred that products of that company with missing revenue data were minor or less successful and made a note of them. The impact of these missing products was studied in a sensitivity analysis. All revenue data were expressed in US dollars through historical exchange rates of the date that the fiscal year ended and were converted to 2020 values using historical consumer price indices.3839We calculated yearly cumulative revenues for each individual drug, starting from the year in which revenues were first generated (year 1 after market entry).
To assess potential discrepancies between added benefit and revenue among different approval pathways of the EMA, we categorised the study cohorts based on approval type, including SMAs, CMAs, and AECs. Information about approval types was retrieved from the European Commission’s Union Register of medicinal products for human use.40
All data collection was performed until 31 August 2021. Extraction of all data was performed by FB and validated by Jan-Willem Versteeg through independent extraction of a random sample of 10% of the study cohort. Additionally, our extraction of revenue data was further validated using a previously developed dataset consisting of revenue data from a selection of orphan drugs.41
Data analyses
To evaluate the obtained ratings of added benefit in the added benefit cohort, we used descriptive statistics. We also assessed the number of drug-indication combinations that were evaluated across several organisations. We did not consolidate multiple added benefit ratings for a specific drug-indication combination into a single rating. Instead, we performed the analyses based on all the extracted added benefit ratings to maintain proximity to the original data and preclude the risk of losing the valuable variation found across scores for individual drug-indication combinations.
We assessed cumulative revenues of the revenue cohort for a maximum of eight years after market entry, in line with the estimated remaining patent exclusivity period of 7-10 years after market approval.42We compared the cumulative revenues obtained for individual drugs with estimates of R&D costs of a single oncology drug to analyse the time required for cumulative revenues to equal (ie, offset) R&D costs. For this comparison, we used estimates from a study by Prasad and Mailankody in which the median risk adjusted R&D costs of a single oncology drug were estimated to be $684m (range $166m to $2060m, adjusted to 2020 values).7These estimates also include the costs of failure and are in line with other estimates quoted by the pharmaceutical industry.343Additionally, we conducted a sensitivity analysis using alternative R&D estimates reported by Prasad and Mailankody, which incorporated 7% opportunity costs (median $800m, range $215m to $2747m, adjusted to 2020 values).7
To account for missing revenue data, we conducted a sensitivity analysis that corrected for the drugs for which revenue data were not available because the company only disclosed revenues of its major or bestselling products. Taking a conservative approach, we assumed that these missing products did not offset the estimated R&D expenses during the follow-up period in our study, thereby lowering the proportion of drugs that offset R&D expenses.
We visualised the cumulative revenues for different levels of added benefit in the combined cohort using boxplots. We also performed a linear regression analysis to estimate the association between added benefit ratings of the included drugs and corresponding cumulative revenues three years after market entry. The three year cumulative revenue cut off ensures an appropriate balance between sufficient market penetration and minimal data loss owing to more recently approved drugs, which was particularly important given the small sample size of the combined cohort (149 added benefit ratings of 43 drugs with corresponding revenue data).
We estimated the association between each individual added benefit rating and the revenue datapoint of the corresponding drug. When a drug had been evaluated across different organisations, its revenue datapoint was linked to several added benefit ratings. Using this approach, we preserved the original data because using the median or mean added benefit rating of a drug could have resulted in invalid results owing to the large variation in added benefit ratings for the same drug.
We performed the linear regression analysis in R (version 4.1.0) and RStudio (version 1.4.1717) and used the lm.cluster function of the miceadds package to incorporate a cluster effect in the analysis to correct for linking revenue datapoints to several added benefit ratings. We checked the assumptions of linear regression and evaluated the robustness of our estimates by removing outliers in a sensitivity analysis.
In a subgroup analysis of the added benefit cohort, we calculated risk ratios with 95% confidence intervals to evaluate the association between the EMA approval pathway and level of added benefit. To distinguish between added benefit and negative or non-quantifiable added benefit, we combined the ratings categorised as major, substantial, and minor added benefit.
We assessed whether cumulative revenues were higher for certain approval pathways in the revenue cohort. AECs were excluded from the analysis owing to their small numbers (n=6). Cumulative revenues five years after market entry (drugs with less than five years of revenue data available were excluded for this analysis) were compared between SMAs and CMAs by performing a Mann-Whitney U test, in which P<0.05 was considered statistically significant. A period of five years was chosen because this strikes an appropriate balance between a sufficiently long follow-up—surpassing the duration of most budget impact predictions by HTA agencies—and a follow-up short enough to ensure that none of the drugs would have patent expiration, which would hamper comparison of the cumulative revenues.44
Finally, we repeated the linear regression analysis in the combined cohort to estimate the association between added benefit ratings of the included drugs and corresponding cumulative revenues three years after market entry for different approval pathways. AECs were excluded because of their small numbers (n=2).
Patient and public involvement
Because of lack of funding, patients and members of the public were not involved in the design and conduct of this study. However, the authors plan to involve patient representatives during dissemination of the study findings.
Results
Study cohort
Figure 1presents a flowchart of the inclusion process and the characteristics of the three distinct study cohorts. There were 131 oncology drugs with 166 indications which had been evaluated for their added benefit by at least one organisation within the required timeframe, yielding a total of 458 added benefit ratings (added benefit cohort). Revenue data were available for 109 drugs (revenue cohort), of which 43 were evaluated by at least one organisation, had at least three years of revenue data, and were associated with a single indication at the end of the follow-up period. A total of 149 added benefit ratings corresponded to these 43 drugs (combined cohort). Supplementary materials table S1 presents a more detailed overview of the characteristics of the drugs and drug-indication combinations in the respective study cohorts.
Added benefit
Of the acquired 458 added benefit ratings, 59 (13%) were classified as major benefit, 107 (23%) as substantial benefit, 103 (23%) as minor benefit, and 189 (41%) as negative or non-quantifiable benefit. The 166 drug-indication combinations included were most commonly assessed across one, two, or three organisations (n=39, 23%; n=41, 25%; n=33, 20%, respectively), whereas none of the drug-indication combinations were evaluated by all seven organisations. Supplementary materials table S2 presents the distribution of added benefit ratings for each organisation.
Comparison of revenues to estimated R&D costs
Figure 2shows the median cumulative revenues of the revenue cohort from years 1 to 8 after market entry, and the estimated R&D costs (supplementary materials table S3 gives more details on the number of drugs available for yearly follow-up). The median cumulative revenues exceeded the minimum R&D costs of $166m within two years, the median R&D costs of $684m within three years, and the maximum R&D costs of $2060m within just over five years after market entry.Figure 3(upper panel) shows the proportion of drugs that have offset the median estimated R&D costs of $684m for each year after market entry. Within eight years of market entry, 50 of 55 (91%) drugs surpassed the median R&D costs. In a sensitivity analysis that assessed the impact of missing data of minor or less successful drugs (fig 3, lower panel), a similar trend was found, and 50 of 61 (82%) drugs exceeded the median R&D costs within eight years. Supplementary materials table S3 presents more details on the number of drugs available for yearly follow-up. The sensitivity analysis that used alternative R&D estimates and included opportunity costs produced similar results to the main analysis (data not shown).
Association between added benefit and revenues
Figure 4shows that cumulative revenues three years after market entry generally increased with the level of added benefit, although cumulative revenues varied, in particular for drugs with substantial and major added benefit. The linear regression analysis estimated that the median cumulative revenues three years after market entry for drugs with major and substantial added benefit were $502m and $506m higher than drugs without benefit, respectively. These results were not statistically significant, probably owing to the large variance and the relatively small sample size available for this analysis (149 added benefit ratings for 43 drugs). Supplementary materials table S4 gives more detailed results of the linear regression analysis.
Subgroup analyses: standard versus expedited approvals
Of the 341 added benefit ratings for drug-indication combinations approved through SMAs, 124 (36%) were classified as negative or non-quantifiable compared with 56 of 98 (57%) and 9 of 19 (47%) added benefit ratings for drug-indication combinations approved through CMAs and AECs, respectively. CMAs alone (risk ratio 1.57, 95% confidence interval 1.26 to 1.96) and in combination with AECs (1.53, 1.23 to 1.89) were more likely to receive a rating of negative or non-quantifiable added benefit compared with SMAs. AECs alone also had a point estimate greater than 1.0 for a negative added benefit rating, but owing to the small sample size, this should be interpreted with caution (see supplementary materials table S5).
Cumulative revenues of CMAs were distinctly lower than those of SMAs (fig 5). Five years after market entry, the median cumulative revenues of CMAs (n=17) were $1105m lower compared with SMAs (n=58), or almost twice as low ($1196mv$2301m, respectively), although this difference was not statistically significant (P=0.07).
The median cumulative revenues exceeded the minimum R&D costs of $166m within two years for SMAs and CMAs, the median R&D costs of $684m within three years for SMAs and four years for CMAs, and the maximum R&D costs of $2060m within five years for SMAs and eight years for CMAs (fig 5). At the end of the eight year study period, 37 of 41 (90%) SMAs and 8 of 9 (89%) CMAs had offset the median R&D costs (see supplementary materials figure S1), and median cumulative revenues of SMAs (n=41) were more than $3bn higher than those of CMAs (n=9; $5306mv$2276m, respectively). The observed decline in median cumulative revenues for CMAs between years 6 and 7 can be attributed to the varying market durations of the included drugs. Supplementary materials table S3 presents more details on the number of drugs available for yearly follow-up.
Association between added benefit and revenues
Revenues increased similarly for SMAs and CMAs, along with higher levels of added benefit, although these associations were not statistically significant. The linear regression analysis estimated that the median cumulative revenues three years after market entry for drugs with major and substantial added benefit were $429m and $413m higher than for drugs without added benefit, respectively (see supplementary materials table S6).
Discussion
Our study showed that oncology drugs approved by the EMA between 1995 and 2020 were often found to provide little or no added benefit. Our results on revenues showed that the median time to offset the median estimated R&D costs of $684m was three years, and 50 of 55 (91%) of the included drugs had recovered these costs within eight years. We found that higher added benefit ratings were generally accompanied by greater revenues. Moreover, negative added benefit ratings were more common for drugs initially approved through CMA and AEC compared with SMA, and cumulative drug revenues were found to be distinctly lower for CMAs than for SMAs. Correspondingly, initial CMAs took longer to offset the median estimated R&D spending in comparison to SMAs (four years versus three years).
Despite claims from the pharmaceutical industry that high drug prices are necessary to sustain the costs of R&D, studies have found no correlation between drug prices and R&D expenses.45A recent study by Angelis and colleagues showed that the world's largest biopharmaceutical companies spent more on selling, general, and administrative activities than on R&D, with only 16-21% of revenues allocated to R&D between 1999 and 2018.8Additionally, Tay-Teo and colleagues found a median income return of $14.50 for every $1 spent on R&D costs.46Our findings complement these studies by showing that R&D costs are typically recovered within a few years of a drug's market entry, with the median time to recover the median and maximum estimated R&D costs being three and five years, respectively. Even for drugs with considerably lower added benefits at the time of initial approval (ie, CMAs), the median time to recover the median estimated R&D costs is four years (typically the timeframe within which more comprehensive evidence becomes available2147).
Previous studies have extensively focused on the relation between added benefit and drug prices, in which no statistically significant associations were found.214232425However, drug prices might be an imperfect measure as they only reflect information for a single country and are often subject to confidential discounts; therefore, these findings should be interpreted with caution. Our study explores the association between added benefit and drug revenues. We view revenues as a more relevant measure because these hold global relevance and better reflect the earnings of a drug. Our findings imply that—irrespective of approval type—revenues of oncology drugs are generally in line with their added benefit, which is in concordance with our expectations.214232425This indicates that pharmaceutical companies might be incentivised to develop drugs with high levels of added benefit because these drugs are associated with higher revenues. However, we also observed that drugs with lower levels of added benefit were still able to recover their estimated R&D expenses within a relatively short time period. This finding indicates that while lower levels of added benefit might result in lower revenues, these are still sufficiently high to recover R&D expenses. Additionally, our results revealed a striking similarity in the revenues generated by drugs with substantial and major added benefits. These mechanisms in obtaining revenues might attenuate the potential incentive to develop high value drugs because pharmaceutical companies might be satisfied with the revenue generated from lower value drugs.
In subgroup analyses, we assessed whether added benefit and revenues were higher for certain approval types. CMAs and AECs are approval pathways intended for patients with unmet medical needs, for which the EMA determined that the benefit of immediate access outweighs the risks of increased uncertainty. Our results indicate that this potential to address unmet medical needs might be negated by the lack of comprehensive evidence inherent to these approval pathways, often resulting in negative added benefit ratings at the time of initial approval. Similarly, other studies found high numbers of negative added benefit ratings for expedited approval drugs.15182122All these findings imply that drugs approved through expedited pathways, which are meant to allow access to promising drugs, do not necessarily show an added benefit at the time of initial evaluation. Our study extends previous research by revealing that CMAs not only have more negative added benefit ratings than SMAs but also generate substantially lower revenues, and accordingly, take longer to offset estimated R&D costs. The median time to recover R&D expenses ranged between three and four years for SMAs and CMAs, respectively. Because CMA is a pathway that aims to speed up drug approval, it might also lead to earlier market entry and therefore an increased period of time to generate revenues before patent expiration. Conversion from CMA to SMA takes place once the marketing authorisation holder fulfils the obligation to provide more comprehensive evidence, a process that typically occurs within four years of receiving regulatory approval.2147This conversion point can be considered the potential starting point for generating revenues by SMAs, coinciding with the approximate offsetting of R&D costs by initial CMAs. This prompts the question of whether it is desirable for CMAs to have already offset their estimated R&D costs at this particular stage. Nevertheless, the difficulties in showing added benefit during reimbursement processes might negate the effects gained through earlier regulatory approval, given that after eight years, the difference in the median cumulative revenues between CMAs and SMAs is more than $3bn.
Limitations of this study
This study has several limitations. We focused solely on the initial indications of the included oncology drugs and evaluated the added benefit that was based on data submitted for initial approval because this reflects the entry into the market. We did not assess how this benefit evolves over time, including potential new indications, because this fell beyond the scope of our study. Additionally, to avoid potential discrepancies resulting from additional evidence becoming available over time, we excluded evaluations performed more than 1.5 years after the EMA approval date from the added benefit cohort. However, because some organisations from the US (ICER and ASCO) follow timelines of the US Food and Drug Administration for drug approval, which are typically earlier than EMA approval dates, we also excluded evaluations conducted more than 1.5 years before EMA approval to ensure that our findings were based on comparable evidence. While it would have been more accurate to consider US Food and Drug Administration approval dates for ICER and ASCO assessments, we chose to focus on EMA approval dates for our study.
We obtained ratings of added benefit from seven different organisations, each using a distinct scoring system. To ensure consistency, we converted these scores to a four point rating scale; however, alternative categorisations might have been possible, which could have produced different outcomes. Nevertheless, we attempted to reduce the ","Objectives: To evaluate the added benefit and revenues of oncology drugs, explore their association, and investigate potential discrepancies between added benefit and revenues across different approval pathways of the European Medicines Agency (EMA).
Design: Retrospective cohort study.
Setting: Oncology drugs and their indications approved by the EMA between 1995 and 2020.
Main outcome measures: Added benefit was evaluated using ratings published by seven organisations: health technology assessment agencies from the United States, France, Germany, and Italy, two medical oncology societies, and a drug bulletin. All retrieved ratings were recategorised using a four point ranking scale to indicate negative or non-quantifiable, minor, substantial, or major added benefit. Revenue data were extracted from publicly available financial reports and compared with published estimates of research and development (R&D) costs. Finally, the association between added benefit and revenue was evaluated. All analyses were performed within the overall study cohort, and within subgroups based on the EMA approval pathway: standard marketing authorisation, conditional marketing authorisation, and authorisation under exceptional circumstances.
Results: 131 oncology drugs with 166 indications were evaluated for their added benefit by at least one organisation within the required timeframe, yielding a total of 458 added benefit ratings; 189 (41%) were negative or non-quantifiable. The median time to offset the median R&D costs ($684m, £535m, €602m, adjusted to 2020 values) was three years; 50 of 55 (91%) drugs recovered these costs within eight years. Drugs with higher added benefit ratings generally had greater revenues. Negative or non-quantifiable added benefit ratings were more frequent for conditional marketing authorisations and authorisations under exceptional circumstances than for standard marketing authorisations (relative risk 1.53, 95% confidence interval 1.23 to 1.89). Conditional marketing authorisations generated lower revenues and took longer to offset R&D costs than standard marketing authorisations (four years compared with three years).
Conclusions: While revenues seem to align with added benefit, most oncology drugs recover R&D costs within a few years despite providing little added benefit. This is particularly true for drugs approved through conditional marketing authorisations, which inherently appear to lack comprehensive evidence. Policy makers should evaluate whether current regulatory and reimbursement incentives effectively promote development of the most effective drugs for patients with the greatest needs.
"
Exposure to air pollution and hospital admission for cardiovascular diseases,"Introduction
Particulate matter with an aerodynamic diameter of ≤2.5 µm (PM2.5) is a major component of ambient air pollution.1These particles are small enough to be inhaled deeply into the lungs, and they can enter the bloodstream, leading to a range of health problems such as systematic inflammation, vasoconstriction, cardiac electrical changes, and formation of blood clots, all of which can contribute to the development of cardiovascular disease (CVD).2Numerous studies, including large scale epidemiological studies, have consistently found that exposure to PM2.5can trigger adverse cardiovascular conditions and increase the risk of cardiovascular related hospital admission and mortality.345678910111213Chronic exposures to PM2.5, lasting one year or more, have been found to pose a much greater risk to cardiovascular health than short term exposures spanning only a few days.14
However, major gaps in knowledge remain. First, most published studies focused on one or a small number of CVD subtypes, or treated CVD as a composite endpoint, which can fail to identify subtypes most vulnerable to PM2.5. With the rapidly growing burden of CVD worldwide, a direct comparison of the effect sizes across subtypes from within the same cohort would help to better understand the underlying mechanisms and inform targeted interventions to reduce the impact of PM2.5.15Additionally, errors in exposure measurement can bias the estimated exposure-response association between PM2.5and CVD, limiting the ability to know whether a safe threshold exists below which toxicity does not occur.16This is of particular importance considering that the US Environmental Protection Agency has expressed uncertainty about the effect of annual PM2.5levels <9 µg/m3.317Consequently, an assessment below that threshold becomes crucial for public health considerations.18Further, most studies have reported effects only on a multiplicative scale (eg, relative risk), which might obscure the actual size of the risk.19Moreover, fewer studies have used causal methods, which provide greater assurance against confounding. Finally, although concerns about environmental justice are growing, the role of co-occurrence of individual level and neighborhood level factors in the association between PM2.5and CVD has not been comprehensively studied.7
We estimated the exposure-response associations between three year average exposure to PM2.5and the risks of a first hospital admission for seven major subtypes of CVD—ischemic heart disease, cerebrovascular disease, heart failure, cardiomyopathy, arrhythmia, valvular heart disease, and thoracic and abdominal aortic aneurysms, for Medicare fee-for-service beneficiaries aged ≥65 years in the contiguous US during 2000-16. We also examined the risk of a first hospital admission for the composite of these CVD subtypes. Both relative and absolute effects were assessed to provide a comprehensive understanding of the risks posed by PM2.5. Extensive subgroup analyses by individual and neighborhood level factors were performed to identify susceptible groups, in particular socioeconomically deprived neighborhoods.
Methods
Study population
The study data included all Medicare beneficiaries aged ≥65 years who resided in the contiguous US and enrolled in the fee-for-service programme at some point during 2000-16. From the Medicare denominator file, we obtained a unique identifier for each beneficiary and details of sex, race/ethnicity, age, enrollment status in Medicaid, residential zip code, and date of death if it occurred during the study period. Information on age, residential zip code, and enrollment status in Medicaid are updated annually. Medicaid provided additional coverage for participants with a low income and therefore we used enrollment status in Medicaid as a proxy for personal socioeconomic status. From the Medicare Provider Analysis and Review file, we obtained the date of hospital admission, ICD-9 and ICD-10 (international classification of diseases, ninth and 10th revisions, respectively) codes for principal discharge diagnosis, and a unique identifier used to match to the denominator file for each beneficiary. For each subtype of CVD, we constructed a separate cohort by following each beneficiary each year until the first hospital admission for that subtype of CVD, death, or end of the study period in 2016, whichever occurred earliest.20To examine the risk of first hospital admission for the composite CVD outcome, we constructed another cohort by following each beneficiary each year until the first hospital admission for any of the studied CVD subtypes, death, or end of the study period in 2016, whichever occurred earliest. The supplementary material lists the specific ICD-9 and ICD-10 codes used for the identification of each CVD subtype related hospital admission.
PM2.5exposure estimation, calibration, and aggregation
We used geographically weighted regressions to predict ambient daily PM2.5levels at a spatial resolution of 1-km2grid cells across the contiguous US during 2000-16. Our ensemble model combined predictions from multiple machine learning algorithms, including random forests, gradient boosting, and neural networks. To generate these predictions, we integrated various data sources, such as satellite data, meteorology, land use variables, monitoring data, and simulations of chemical transport model.21The ensemble model showed better predictive performance than the individual machine learners, with a 10-fold cross validated R2value of 0.86.
We employed a regression calibration approach to enhance the precision of grid level PM2.5estimates and reduce biases in the health effect estimates due to exposure error.22First, we used a logistic regression to predict the probability of having at least one PM2.5monitoring site for each zip code in each year, given population density, per cent of the population who identified as being of Black race, per cent of the population who identified as Hispanic, per cent of the population who identified as Asian, per cent below the threshold for poverty, distance between industrial facility and zip code centroid, road density, normalized difference vegetation index, and tree canopy. We did this to adjust for the uneven distribution of the monitored locations via inverse probability weights, thus making the non-monitored locations more representative. Next, we stratified the monitored, seasonal averaged PM2.5levels by season, elevation, and census division and generated 1000 bootstrap samples for each stratum. For each bootstrap sample, we fitted a linear regression of monitored PM2.5levels on the predicted PM2.5at grid cells where the monitoring sites were located, adjusted for wind speed, humidity, imperviousness, and calendar year, and weighted by the inversed probabilities of having at least one monitoring site derived from the earlier logistic regression. This allowed higher weights in strata that had lower probabilities of having monitoring sites. Then we stratified, across the contiguous US, the grid level PM2.5estimates by season, elevation, and census division; the same as for the monitored PM2.5. For each stratum, we used the fitted regressions to estimate 1000 sets of calibrated PM2.5levels, based on the grid level PM2.5estimates and covariates. This process produced 1000 regression calibrated, seasonal averaged PM2.5levels at all grid cells. By averaging seasonal averages within a year, we obtained 1000 calibrated, annual PM2.5levels for each grid cell per year. The supplementary material provides a comprehensive description of the calibration process.
To match the spatial resolution of the Medicare cohort, we aggregated the calibrated PM2.5estimates at grid cells to zip codes. Briefly, we estimated zip code level PM2.5by averaging the predictions at grid cells with centroids inside the polygonal area for general zip codes, and by assigning the prediction at the nearest grid for other zip codes that lacked polygon representations, such as apartment buildings, military bases, and post offices.23This method produced 1000 calibrated PM2.5levels for each zip code per year.
The exposure of primary interest was three year average exposure to PM2.5over the current and previous two years (lag 0-2). We also examined exposures to PM2.5at single lag years, specifically lag 0, lag 1, and lag 2.
Covariates
We adjusted for calendar year; individual level characteristics, including sex, race/ethnicity, age group, and Medicaid enrollment; and various neighborhood level covariates as potential confounders based on substantive knowledge and the most relevant literature.14242526Neighborhood level socioeconomic and personal covariates, including proportion of the population aged ≥65 years living below the threshold for poverty, population density, median value of owner-occupied properties, proportion of housing units occupied by owners, proportion of the population who identified as Hispanic, and proportion of the population aged ≥65 years who had not graduated from high school, were obtained from the US Census and the American Community Survey.2728These data were available in 2000 and from 2010 to 2016 by zip code and were linearly interpolated to estimate missing values for all other years.
County level risk factors, including mean body mass index and the prevalence of ever smokers, were obtained from the Behavioral Risk Factor Surveillance System of the Centers for Disease Control and Prevention and were linked to respective zip codes.29Additionally, we computed the annual rate of lung cancer related hospital admissions in each zip code from the Medicare data as a surrogate for smoking and tobacco use.
Healthcare covariates, including the proportion of Medicare beneficiaries who received at least one test for glycated hemoglobin each year, the proportion of beneficiaries who had at least one ambulatory visit to a doctor in a year, and the proportion of female beneficiaries who received mammography within a two year period, were obtained from the Dartmouth Atlas of Health Data.30These covariates represented the most commonly utilized healthcare services. We collected data at the hospital service area level and linked them to zip codes. Further, we computed the distance from the centroid of each zip code to the nearest hospital, using hospital locations sourced from a dataset produced by Esri (Redlands, CA) as an overall measure of access to healthcare.31
Similar to PM2.5, we aggregated concurrent air pollutants and weather data, including annual warm season ozone (April-September), annual nitrogen dioxide levels, and annual temperature at grid cells, to zip codes and linked to the Medicare cohort.233233
In analyses examining effects of exposure to PM2.5in a single year, we adjusted the PM2.5levels from the other two lag years as covariates to obtain independent effect estimates.
Statistical analysis
We divided the PM2.5exposure levels into 11 disjoint intervals. These intervals included [0, 5), [5, 6), . . . , [13, 14), [14, ∞), two wider intervals for the extremely low and high levels, and nine 1 unit intervals for those in between. For each outcome, we prepared a dataset with person year representations of follow-up to allow for time varying exposures and covariates. We aggregated person years by all the categorical variables, including zip code, year, sex (male or female), race/ethnicity (white or non-white), age group (65-74 years or ≥75 years), and Medicaid enrollment status (yes or no), so that each observation represented a stratum of a unique combination of those variables. Numbers of hospital admissions and person years were cumulated for each stratum.20
Assuming the 1000 calibrated PM2.5exposure levels for each stratum were normally distributed, we calculated the probability of a PM2.5level falling within an exposure interval, based on the mean and standard deviation of the 1000 calibrated PM2.5levels. This allowed us to directly incorporate uncertainty about the exposure into the exposure-response estimations. Using the logit link function, we fitted a beta regression to predict the probability of the PM2.5level falling within that exposure interval, given the covariates delineated above as predictors and weighted by the number of person years. We used the predicted probability to estimate stabilized inverse probability weight, defined as:swij=p(Xj)divided byexpit(g(Xij|C))whereswijrepresented the stabilized inverse probability weight for stratumiin exposure intervalj;P(Xj)the probability of any stratum with an exposure level ofXin intervalj;expit(·)=exp(·)divided by1+exp(·), the inverse logit function; andg(Xij|C)the beta regression for predicting the probability of being exposed at PM2.5level within intervalj, given covariatesC. This generated 11 inverse probability weights per stratum, each corresponding to a distinct exposure interval, thus allowing for different impacts of confounders across intervals.
Then we categorized the mean of the 1000 calibrated PM2.5exposure levels per stratum into one of the exposure intervals and assigned the inverse probability weight corresponding to that exposure interval for estimation of the exposure-response association. Assuming the inverse probability weight resulted in an exposure in each interval that was independent of all covariates, the exposure effect for that exposure interval was a valid substitute for the counterfactual scenario, in which all strata would be exposed to PM2.5levels falling within that particular interval.34Finally, we fitted a quasi-Poisson regression for the count of hospital admissions against the categorized exposure interval, weighted by the inverse probability weight. The model also included an offset for the number of person years to account for the difference in size of population at risk between strata. This quasi-Poisson regression provided the relative risks of the first hospital admission for all exposure intervals, using the lowest exposure interval as the reference. The entire process was repeated for each CVD outcome. For all effect estimates within an exposure interval, considering the inherent correlations among the eight CVD outcomes, we used the Bonferroni correction to adjust 95% confidence intervals. Absolute scale exposure-response associations were obtained based on the cohort’s baseline risks of hospital admission for the lowest exposure interval of [0, 5) combined with the estimated relative risks.
To identify susceptible subpopulations, we conducted separate analyses by age group and sex. Further, to identify neighborhood level factors that contributed to susceptibility, we performed subgroup analyses by upper or lower fourths of per cent population aged ≥65 years who did not graduate from high school, distance from the centroid of each zip code to the nearest hospital, and the national area deprivation index as a composite metric for level of neighborhood disadvantage.35For consistency across CVD outcomes, we used the same upper or lower fourth of the neighborhood level variables based on the entire Medicare fee-for-service cohort.
Patient and public involvement
The study population was not involved in this study owing to the deidentified nature of the Medicare data and restricted access to these data. However, one author, through regular clinical interactions with patients admitted to hospital for cardiovascular conditions, discussed the potential impacts of air pollution. These discussions informed our study design and interpretation of the results.
Results
Table 1shows the characteristics of the studied Medicare fee-for-service cohort during 2000-16. The cohort comprised 59 761 494 adults with 476 953 892 person years of follow-up and consisted of mostly white adults (84.3%, n=50 382 526), a higher proportion of female beneficiaries (55.0%, n=32 894 731), and individuals mostly aged 65-74 years at study entry (75.1%, n=44 858 373). In total, 17.6% (n=10 542 856) of the beneficiaries enrolled in Medicaid at some point during the study period. Overall, 21.8% (n=13 001 853) of the beneficiaries had a composite CVD related hospital admission (ie, any of the seven CVD subtypes). Ischemic heart disease was the most prevalent CVD subtype resulting in hospital admission, with 8.8% (n=5 248 669) of beneficiaries experiencing the condition, followed by cerebrovascular disease (7.7%, n=4 580 000), heart failure (6.6%, n=3 964 836), and arrhythmia (6.5%, n=3 885 622). Given that the exposure of primary interest was the three year average PM2.5, for which data have been available since 2000, the earliest CVD related hospital admissions occurred in 2002. The supplementary materials provide summary statistics of the neighborhood level variables.
Figure 1andfigure 2show the distributions of mean and standard deviation for 1000 calibrated lag 0-2 PM2.5concentration per zip code-year combination, respectively. The distribution covers all zip codes in the contiguous US during 2000-16. The mean of calibrated lag 0-2 PM2.5concentration followed an approximate normal distribution, averaging at 9.7 µg/m3, substantially below the current national standard of 12 µg/m3. The standard deviation of the calibrated PM2.5values showed a right skewed distribution, with a minimum of 0.03 μg/m3. Out of the total 39 188 zip codes in our analysis, 3871 (9.9%) contained monitoring sites.
Figure 3shows the estimated exposure-response associations between the calibrated lag 0-2 PM2.5exposure level and relative risks of first CVD related hospital admissions. For each effect estimate, the exposure level corresponds to the median value (see supplementary table S3) within that exposure interval for the entire Medicare fee-for-service cohort. The exposure-response curves for composite CVD, ischemic heart disease, cerebrovascular disease, heart failure, and cardiomyopathy monotonically increased. Notably, the curve for heart failure displayed the steepest slope, with the relative risk for a first hospital admission associated with calibrated lag 0-2 PM2.5concentration ranging from 1.19 (95% confidence interval 1.17 to 1.22) for the [5, 6) exposure interval to 2.31 (2.28 to 2.35) for the highest exposure interval of [14, ∞). The exposure-response curve for valvular heart disease fluctuated around 1, with a substantially smaller risk compared to other CVD subtypes. For arrhythmia and thoracic and abdominal aortic aneurysms, the exposure-response curves monotonically increased until the [13, 14) exposure interval, and decreased slightly at the highest exposure interval.
Figure 4shows the estimated exposure-response associations between exposure to the calibrated lag 0-2 PM2.5concentration and absolute risks of first CVD related hospital admissions. Among CVD subtypes, ischemic heart disease, cerebrovascular disease, heart failure, and arrhythmia showed relatively high baseline risks of hospital admission and were clearly positively associated with lag 0-2 PM2.5concentration. For composite CVD, the absolute risk monotonically increased from 2.59% at baseline to 4.69% (95% confidence interval 4.65% to 4.73%) at the highest exposure interval. Cardiomyopathy, valvular heart disease, and thoracic and abdominal aortic aneurysms presented considerably lower risks compared with the composite CVD and other subtypes.
Figure 5shows the estimated exposure-response associations between exposures to calibrated PM2.5concentration at lag 0, lag 1, and lag 2 and relative risks of first CVD related hospital admissions. The effect estimates of these three lagged exposures were mutually adjusted and therefore independent. For composite CVD and the three most prevalent CVD subtypes—ischemic heart disease, cerebrovascular disease, and heart failure—the highest risk was associated with immediate exposure to PM2.5at lag 0. This was followed by a slightly reduced effect at lag 1 PM2.5level and a further reduction at lag 2 PM2.5level. For each outcome, the single year lagged exposure-response curves were similar to each other and were consistent with the curve for lag 0-2 PM2.5level (fig 3).
Figure 6,figure 7,figure 8,figure 9, andfigure 10show the estimated exposure-response associations between exposure to lag 0-2 PM2.5level and relative risks of first CVD related hospital admissions from subgroup analyses. Female beneficiaries exhibited a higher risk for composite CVD, ischemic heart disease, and heart failure, whereas their risk of cardiomyopathy was lower (fig 6). Younger beneficiaries aged 65 to 74 years experienced a greater risk of hospital admission for composite CVD and most individual CVD subtypes (fig 7). Beneficiaries who lived in zip codes with lower rates of high school completion (≤25th centile), longer distance to hospital (>75th centile), or higher deprivation level (area deprivation index >75th centile) experienced higher risk for most outcomes (fig 8,fig 9, andfig 10). The supplementary material shows the numerical results.
Discussion
In this cohort study of 59 million Medicare beneficiaries across the contiguous US, we found that three year average exposure to PM2.5was associated with increased relative risks of a first hospital admission for ischemic heart disease, cerebrovascular disease, heart failure, cardiomyopathy, arrhythmia, and thoracic and abdominal aortic aneurysms. For the composite CVD outcome, the exposure-response curve monotonically increased, suggesting that no safe threshold exists for overall cardiovascular health; compared with the lowest exposure interval between 0 and 5 µg/m3, the risk of hospital admission was already raised at exposures between 5 and 6 µg/m3and increased by 1.29 times at exposures between 9 and 10 µg/m3. This latter range encompassed the national average of 9.7 µg/m3and is being evaluated by the US Environmental Protection Agency for the upcoming National Ambient Air Quality Standards.18On an absolute scale, the risk of hospital admission for composite CVD increased from 2.59% with exposures ≤5 µg/m3to 3.35% at exposures between 9 and 10 µg/m3; ischemic heart disease, cerebrovascular disease, heart failure, and arrhythmia presented considerable increases in absolute risk of hospital admission associated with PM2.5, in comparison with other CVD subtypes. These results were crucial for informing targeted interventions to specifically mitigate the adverse impacts of PM2.5on these conditions. In the single year lagged models, the effect of PM2.5persisted at least for three years after exposure, suggesting a long lasting impact of PM2.5on cardiovascular health. It also implied that the benefits of policies to reduce PM2.5would be realized immediately after implementation and would continue to accrue for years. Overall, our findings highlighted important benefits that could be attained through adherence to the newly published WHO air quality guideline of ≤5 µg/m3.36Our study is timely, considering the ongoing review of the US national standards.18Nevertheless, the upcoming US standard, proposed to be between 9 and 10 µg/m3, is still considerably higher than the 5 µg/m3set by WHO, and not sufficient for the protection of public health.
Although PM2.5has been recognized as a primary environmental risk factor for CVD, its specific impact on different CVD subtypes, as well as the exposure-response associations, remained largely unquantified.7Moreover, studies correcting for exposure measurement error, assessing absolute effects, and using causal methods, are limited or non-existent. In this cohort study, we estimated exposure-response associations between chronic exposure to PM2.5and risks of first hospital admission for seven major CVD subtypes, as well as the composite of those subtypes. We developed a causal framework that allowed confounders to have different impacts in different exposure ranges, thereby strengthening the validity of the associations between chronic exposure to PM2.5and CVD outcomes. To reduce the bias in exposure-response estimations from errors in measuring those exposures, we implemented two strategies: in the first we employed a regression calibration approach to refine our spatiotemporally resolved PM2.5exposure estimates, and in the second we incorporated the uncertainty of exposure estimates into exposure-response estimations by assigning greater weights to exposures with less uncertainty, thus improving the overall accuracy of health effect estimates.
Comparison with other studies
The exposure-response estimations for ischemic heart disease and cerebrovascular disease were similar on both relative and absolute scales, possibly as a result of the shared underlying pathophysiological mechanisms involving atherosclerosis and thromboembolism.37This finding aligned with a recent review,6which reported highly comparable associations between long term exposure to PM2.5and mortality for ischemic heart disease and cerebrovascular diseases (23% and 24% increased risk, respectively, for each 10 µg/m3increase in long term exposure to PM2.5). Building upon previous studies showing positive associations between chronic exposure to PM2.5and risks for heart failure and arrhythmia,38938the results on our absolute scale show that these are among the most susceptible CVD subtypes to develop in people exposed to PM2.5. For arrhythmia specifically, a subtype analysis suggested that people with atrial fibrillation and other subtypes of arrhythmia were susceptible to chronic exposure to PM2.5(see supplementary fig S1). Given the high prevalence, substantial healthcare costs, impact on daily activities, and influence on mortality, heart failure and arrhythmia emerge as major public health concerns.39Thus, alongside ischemic heart disease and cerebrovascular disease, which are the leading causes of cardiovascular mortality, it is crucial to emphasize heart failure and arrhythmia as primary targets for population level interventions to mitigate the adverse effects of air pollution on cardiovascular health.
Consistent with the findings of a multiethnic study,40we found an ambiguous association between PM2.5and valvular heart disease, suggesting that PM2.5might play a less important role than other primary risk factors such as genetics, infections, and other cardiovascular conditions.41The ambiguous association could also result from data variability. When these subtypes were assessed in more detail, this unclear association appeared to be driven by aortic valve disease (see supplementary fig S2). PM2.5was, however, clearly not associated with mitral valve disease, suggesting that the pollutant might be more specifically associated with the calcification of aortic valve rather than with valvular calcification in general.42The positive association between PM2.5and tricuspid and pulmonary valve diseases suggested that a longer follow-up period might be needed to better understand the associations of PM2.5with mitral valve disease and aortic valve disease, since the left side of the heart has greater flow and calcium deposition may take longer to build up.
In subgroup analyses, we found that older beneficiaries were consistently less susceptible to PM2.5for all outcomes, which was consistent with the findings of several studies.14242543This may potentially be explained by older aged people having lower average body mass index, which may be associated with decreased vulnerability to cardiovascular events.44Another plausible explanation is that beneficiaries susceptible to PM2.5might have experienced their first hospital admission when young. Consequently, they were not included in the older group.45Our findings on sex differences varied across CVD subtypes, consistent with the mixed results reported by others.62526
At the neighborhood level, we found a higher risk of hospital admission associated with PM2.5for beneficiaries living in zip codes with lower rates of high school completion, longer distance to the nearest hospital, or greater neighborhood deprivation. Consistent with four studies,10111213these identified differences in susceptibility highlight factors that may further contribute to inequalities in the impact of air pollution, in addition to inequalities in exposure.46Longer distance to the nearest hospital is indicative of limited access to healthcare services, which is particularly relevant in the context of cardiovascular related hospital admission. Indeed, timely and effective outpatient care plays a vital role in minimizing the risk of hospital admission for cardiovascular conditions.47Greater neighborhood deprivation and lower education attainment characterize various social, economic, and environmental factors that can contribute to increased susceptibility to the impact of air pollution. These factors include poverty, poorly maintained walkways, access to parks, shopping areas, and neighborhood organizations, as well as substandard quality of housing.35
Strengths and limitations of this study
Our study has several strengths. First, we addressed a central concern of exposure measurement error that was pervasive in large scale studies using spatiotemporally resolved air pollution predictions.16Although some methods for estimating uncertainty of exposure have been developed and integrated into health effect estimates, most existing studies have not accounted for this uncertainty.48In the present study, we developed a regression calibration approach to improve the accuracy of exposure measures. We used this calibration approach and further estimated the variability of exposure estimates as a measure of uncertainty, and we incorporated this uncertainty into our exposure-response estimations to reduce bias in health effect estimates from exposure error. Specifically, we assigned lower weights to strata associated with exposure measures with higher uncertainty—thereby these strata contributed less to the overall effect estimates. Another strength was the categorization of continuous exposure for inverse probability weight estimation, improving the robustness against outliers. Estimating inverse probability weight for continuous exposures can be challenging as excessively large or small weights for outliers need to be avoided.20This approach also relaxed the strong assumptions of distribution form and homoscedasticity inherent in continuous exposures.49Further, the causal approach fitted separate propensity score models for each PM2.5exposure interval, allowing the confounders to have different impacts in different exposure ranges and thus reducing confounding bias. Finally, we provided absolute effect estimates of PM2.5on risks of hospital admission for CVD subtypes. The results could inform targeted interventions to mitigate the adverse impacts of PM2.5on ischemic heart disease, cerebrovascular disease, heart failure, and arrhythmia, as risks from these conditions appeared to be higher compared with other CVD subtypes.
This study has some limitations. First, owing to restricted available data sources, we were unable to account for individual level risk factors such as body mass index and smoking for cardiovascular related hospital admissions. However, ambient air pollution levels in neighborhoods are unlikely to be substantially influenced by individual covariates. This is because, for example, neighborhoods with higher levels of poverty and population density may also be neighborhoods with higher air pollution levels. In other words, ambient air pollution levels are spatially patterned in neighborhoods based on neighborhood level characteristics. If people with low incomes were to move to high income neighborhoods, they would receive an ambient level of exposure associated with characteristics of the high income neighborhood rather than their own individual characteristics. Hence, only neighborhood level covariates are likely confounders, which we have controlled for. Supporting this argument, we performed a sensitivity analysis that excluded all individual level covariates (sex, race/ethnicity, age, enrollment status in Medicaid), and found that the results remained highly consistent with our main analysis (see supplementary fig S3). Second, our framework did not account for unobserved confounders, particularly those at the neighborhood level, which could potentially influence the findings. Third, the exposure assessment was still subject to measurement error because zip code was the finest geographic unit available to match PM2.5with each beneficiary, rather than individual residential address. A recent simulation study suggested that performing the analysis at a finer spatial resolution may result in larger effect estimates.16Fourth, the generalizability of our findings was constrained by the demographic profile ","Objective: To estimate exposure-response associations between chronic exposure to fine particulate matter (PM2.5) and risks of the first hospital admission for major cardiovascular disease (CVD) subtypes.
Design: Population based cohort study.
Setting: Contiguous US.
Participants: 59 761 494 Medicare fee-for-service beneficiaries aged ≥65 years during 2000-16. Calibrated PM2.5predictions were linked to each participant’s residential zip code as proxy exposure measurements.
Main outcome measures: Risk of the first hospital admission during follow-up for ischemic heart disease, cerebrovascular disease, heart failure, cardiomyopathy, arrhythmia, valvular heart disease, thoracic and abdominal aortic aneurysms, or a composite of these CVD subtypes. A causal framework robust against confounding bias and bias arising from errors in exposure measurements was developed for exposure-response estimations.
Results: Three year average PM2.5exposure was associated with increased relative risks of first hospital admissions for ischemic heart disease, cerebrovascular disease, heart failure, cardiomyopathy, arrhythmia, and thoracic and abdominal aortic aneurysms. For composite CVD, the exposure-response curve showed monotonically increased risk associated with PM2.5: compared with exposures ≤5 µg/m3(the World Health Organization air quality guideline), the relative risk at exposures between 9 and 10 µg/m3, which encompassed the US national average of 9.7 µg/m3during the study period, was 1.29 (95% confidence interval 1.28 to 1.30). On an absolute scale, the risk of hospital admission for composite CVD increased from 2.59% with exposures ≤5 µg/m3to 3.35% at exposures between 9 and 10 µg/m3. The effects persisted for at least three years after exposure to PM2.5. Age, education, accessibility to healthcare, and neighborhood deprivation level appeared to modify susceptibility to PM2.5.
Conclusions: The findings of this study suggest that no safe threshold exists for the chronic effect of PM2.5on overall cardiovascular health. Substantial benefits could be attained through adherence to the WHO air quality guideline.
"
"Short term exposure to low level ambient fine particulate matter and natural cause, cardiovascular, and respiratory morbidity","Introduction
A large number of epidemiological studies have consistently reported that exposure to ambient fine particulate matter (PM2.5) is associated with increased risk of morbidity and mortality.123456According to the Global Burden of Disease study,7exposure to PM2.5accounts for an estimated 7.6% of total global mortality and 4.2% of global disability adjusted life years. In light of this extensive body of evidence, the World Health Organization recently introduced an ambitious new air quality guideline limit in 2021, recommending that the 24 hour average PM2.5levels should not exceed 15 μg/m3on more than 3-4 days each year.8In the US, the current national ambient air quality standards for the 24 hour average PM2.5(calculated as 98th centile concentrations averaged over a three year period) were set at 35 μg/m3in 2012, and a revision of these standards is currently being considered.9
Although the literature on the adverse health effects of short term exposure to PM2.5is vast,123456several key knowledge gaps remain. Specifically, owing to the accessibility of national vital statistics data and Medicare data, most large scale studies in the US have focused on the health effects of PM2.5among adults aged ≥65 years,15with relatively few studies including young or middle aged adults; most studies have focused on mortality and hospital admissions as the outcomes,1235610with relatively less information available on the impact of PM2.5on emergency department visits (ie, where individuals are treated in the emergency department room but not admitted to hospital for inpatient care)11; and no study has specifically examined whether the observed associations persist at daily PM2.5levels below the 2021 WHO air quality guideline limit.
To tackle these knowledge gaps, we estimated the association between short term exposure to PM2.5at concentrations below the 2021 WHO air quality guideline limit and the risks of hospital admissions and emergency department visits for natural causes, cardiovascular disease, and respiratory disease among adults with health insurance in the contiguous US from 2010 to 2016 using the healthcare utilization deidentified claims data from the Optum Laboratories Data Warehouse (OLDW). We also examined whether the observed associations of PM2.5level and morbidity differed across strata defined by age, sex, insurance type, and geographic region.
Methods
Study population
This study used deidentified medical claims data between 1 January 2010 and 31 December 2016 from OLDW.12This database includes facility and physician claims and enrollment records for enrollees with either commercial or Medicare Advantage health insurance. The database contains longitudinal health information on more than 200 million enrollees, representing a diversity of ages and geographic regions across the US.
Medical claims in OLDW are classified using ICD-9 and ICD-10 (international classification of diseases, ninth revision and 10th revision, respectively) codes, revenue codes, current procedure terminology codes, and place of service codes (see supplementary table S1). We used claims to identify clinical encounters classified as either hospital admissions or emergency department visits. For each patient encounter, we extracted information on age, sex, county of residence, date of service, insurance type, and principal diagnoses. Analyses were limited to adults aged ≥18 years who had hospital admissions or emergency department visits for natural causes (ICD-9: 0-799 or ICD-10: A0-R99), cardiovascular disease (ICD-9: 390-459 or ICD-10: I00-I99), or respiratory disease (ICD-9: 460-519 or ICD-10: J00-J99).
For each health outcome, we aggregated medical claims into daily counts of hospital admissions or emergency department visits by age (18-29, 30-39, 40-49, 50-64, 65-74, or ≥75 years), sex (malevfemale), insurance type (commercial insurancevMedicare Advantage), and geographic regions of the country, as defined by the US Global Change Research Program’s Fourth National Climate Assessment (NCA4) for each county (see supplementary figure S1).
Environmental data
To estimate daily concentrations of 24 hour average PM2.5at 1 km × 1 km grid cells in the contiguous US, we used a previously developed spatiotemporal ensemble model.13The model incorporates three machine learning algorithms: neural network, random forest, and gradient boosting. These algorithms relied on multiple predictor variables, including satellite data at 1 km × 1 km resolution, meteorological conditions, such as ambient temperature, land use variables, such as elevation and road density, and predictions from chemical transport models.13The predictions of daily PM2.5level from each algorithm were then combined with a geographically weighted generalized additive model.1314Additionally, we obtained monitoring data for PM2.5from 2156 surveillance sites operated by the US Environmental Protection Agency. To account for spatial and temporal autocorrelation, we calculated lagged PM2.5levels from monitoring sites and incorporated these as supplementary inputs into the ensemble models, along with the aforementioned mentioned predictor variables.1314The final ensemble model showed good performance when predicted values were compared with monitored data, achieving a 10-fold cross validation R2of 0.86.1314
We also derived meteorological variables, including daily outdoor ambient air temperature and relative humidity, from the Parameter-elevation Relationships on Independent Slopes model (PRISM) dataset, which is a publicly available gridded climate dataset.1516The dataset we used provides daily estimates of several meteorological variables at a horizontal grid spacing of about 4 km across the contiguous US.1516
Daily PM2.5concentrations and meteorological variables were calculated at the county level by extracting the grid values at the population centroids for each census tract (based on the 2000 US Census) within a given county. We then calculated the population weighted average values based on the proportion of the county’s population residing in each census tract.1517The concentration of PM2.5and meteorological variables were assigned to each county and the exposure data linked with time series data of hospital admissions and emergency department visits in each respective county.
Statistical analysis
To estimate the association between short term exposure to PM2.5and risks of hospital admissions and emergency department visits for natural causes, cardiovascular disease, and respiratory disease, we used a case time series design—a novel approach suitable for analyzing small area data.1819This design incorporates the self-matched structure in case only models into a traditional time series form, providing a flexible and computationally efficient tool for complex longitudinal data.1819In the case time series design, observations defined as cases are collected longitudinally over a period of time at equally spaced time intervals, forming a set of case level time series data.18The time series of cases can be either individual level outcomes or aggregated measurements over small geographic areas.1819In our study, we aggregated medical claims into county specific daily time series of hospital admission and emergency department visit counts and linked these case time series with county level PM2.5concentrations and meteorological conditions. We chose a case time series design because of its suitability to accommodate the longitudinal time series structure of health outcomes at small area level, strict control for time invariant confounding, and flexibility to adjust time varying confounding in the modeling framework.1819
We used a conditional quasi-Poisson log-link regression model stratified by year, month, day of week, and county of residence, therefore adjusting for the differential baseline morbidity risk and trends across the different spatiotemporal strata. Meanwhile, we controlled the residual temporal variations with time varying covariates, such as ambient temperature and relative humidity. Consistent with previous studies,126our primary analyses considered the moving average of the current and previous day (lag 0-1) PM2.5as the exposure of interest and assumed a linear exposure-response association between exposure to PM2.5and risk of morbidity. In the models, we adjusted for the two day moving average of daily mean ambient temperature using a natural cubic spline function with three degrees of freedom, and the two day moving average of daily mean relative humidity using a natural cubic spline function with three degrees of freedom.1To account for spatial heterogeneity of ambient temperature, we interacted the temperature splines with NCA4 regions. We additionally adjusted for federal holiday (dummy variable). Low level PM2.5was defined as daily PM2.5concentrations below the 2021 WHO air quality guideline limit of 15 μg/m3. To estimate the association between short term exposure to low level PM2.5and risk of morbidity, we restricted our analyses to days with daily PM2.5concentrations <15 μg/m3. To examine the lag structure of the association, we also fitted the models with separate terms for PM2.5on the same day of admission (lag 0) and 1 day previously (lag 1).
We presented results as both the excess relative risk and the excess absolute risk of hospital admissions or emergency department visits associated with a 10 µg/m3increase in PM2.5concentrations.1420Excess relative risk was defined as (relative risk−1)×100%, and excess absolute risk was calculated as α×(relative risk−1)/relative risk, where α is the incidence rate of cause specific hospital admissions or emergency department visits (see supplementary eAppendix).
Several sensitivity analyses were performed to examine the robustness of our findings. First, to examine the exposure-response association between exposure to PM2.5and risk of morbidity, we used a natural cubic spline with three degrees of freedom for lag 0-1 PM2.5. Second, to assess whether our findings were robust to the choice of temperature metric, we replaced the daily mean ambient temperature in the models with daily maximum ambient temperature or daily minimum ambient temperature. Third, to account for spatial heterogeneity in relative humidity, we additionally included an interaction term between relative humidity splines and NCA4 regions in the models. Fourth, to examine the association between exposure to PM2.5and morbidity across the range of PM2.5levels, we repeated the main analysis without restricting our analyses to days with daily PM2.5concentrations <15 μg/m3.
To evaluate potential differences in susceptibility, we examined whether the association between exposure to PM2.5and risk of morbidity differed across subgroups of the population defined by age, sex, insurance type, and geographic region. We fit separate models for each stratum and conducted Wald tests to evaluate the heterogeneity across these strata.21
All statistical analyses were conducted in R software (version 4.0.2). We used the “gnm” (version 1.1.1) package22to fit the conditional Poisson regression models and the “dlnm” (version 2.4.2) package23to model the non-linear exposure-response functions.
Patient and public involvement
As the study used deidentified medical claims data, no patients or members of the public were involved in implementing the study design.
Results
Descriptive statistics
Of the 3144 counties in the contiguous US, we excluded those with no recorded hospital admissions or emergency department visits during the study period, resulting in a total of 2935 to 2939 counties included in the analysis, depending on the specific outcome (see supplementary table S2). Between 1 January 2010 and 31 December 2016, a total of 10.3 million hospital admissions and 24.1 million emergency department visits for natural causes were recorded among 50.1 million adults aged ≥18 years with commercial or Medicare Advantage health insurance (see supplementary table S3). The incidence rates of hospital admissions and emergency department visits for natural causes were 207.9 and 485.7 per million enrollees per day during the study period, respectively (see supplementary table S4).
Of these healthcare encounters, more than 50% of hospital admissions and 28% of emergency department visits were for cardiovascular and respiratory diseases, and the distribution varied considerably across different age groups (fig 1andfig 2). For example, among adults aged <30 years, only 11.3% of all hospital admissions were attributed to cardiovascular or respiratory diseases, but this percentage increased to 92.7% among those aged ≥75 years. In terms of absolute numbers, the incidence rates for hospital admissions and emergency department visits increased with age and tended to be higher in women compared with men, except for cardiovascular disease (see supplementary table S4).
Incidence rates of hospital admissions and emergency department visits also varied across geographic regions. The highest incidence rates for hospital admissions related to natural causes were observed in the northern Great Plains and the northeast, whereas the highest incidence rates for emergency department visits for natural causes were documented in the southeast and midwest (fig 3). Supplementary tables S3 and S4 show the total number and incidence rates of hospital admissions and emergency department visits for natural causes and for cardiovascular and respiratory diseases across different geographic areas, respectively.
During the study period, only 0.1% of county days (8344 out of 7 949 713) recorded daily PM2.5concentrations that exceeded the current national ambient air quality standards of 35 μg/m3; these counties were primarily located in central California, northwestern Utah, southwestern Montana, and east Idaho. Daily PM2.5levels were below the new WHO air quality guideline limit of 15 μg/m3in 92.6% of county days (7 360 725 out of 7 949 713) (see supplementary figure S2). Restricting our sample of events to days below this level resulted in the exclusion of 9.4% of hospital admissions and 9.1% of emergency department visits.
Regression results
Exposure to PM2.5at concentrations below the new WHO air quality guideline limit was associated with an increased risk of hospital admissions for natural causes, cardiovascular disease, and respiratory disease. Specifically, each 10 μg/m3increase in lag 0-1 PM2.5was associated with a 0.91% (95% confidence interval 0.55% to 1.26%) higher relative risk of hospital admissions for natural causes, 1.39% (0.81% to 1.98%) higher relative risk of hospital admissions for cardiovascular disease, and 1.90% (1.15% to 2.66%) higher relative risk of hospital admissions for respiratory disease (fig 4, also see supplementary table S5). The corresponding excess absolute risk was 1.87 (95% confidence interval 1.14 to 2.59), 1.04 (0.61 to 1.48), and 0.85 (0.52 to 1.18) per million enrollees per day for hospital admissions related to natural causes, cardiovascular disease, and respiratory disease, respectively (fig 4, also see supplementary table S6).
The association between exposure to PM2.5and risk of hospital admissions was most pronounced at lag 0, but with little evidence of continued higher risk at lag 1, except for respiratory diseases. For example, a 10 μg/m3increase in PM2.5was associated with a 0.86% (95% confidence interval 0.52% to 1.19%) higher relative risk at lag 0 and a 0.04% (−0.31% to 0.38%) higher relative risk at lag 1 for hospital admissions related to natural causes. The corresponding excess relative risk for respiratory disease was 0.86% (95% confidence interval 0.16% to 1.57%) at lag 0 and 1.03% (0.31% to 1.76%) at lag 1 (see supplementary table S7).
For emergency department visits, a 10 μg/m3increase in lag 0-1 PM2.5was associated with a 1.34% (95% confidence interval 0.73% to 1.94%) excess relative risk of emergency department visits for respiratory disease (fig 4, also see supplementary table S5), corresponding to 0.93 (95% confidence interval 0.52 to 1.35) additional emergency department visits per million enrollees per day (fig 4, also see supplementary table S6). The estimated association between exposure to PM2.5and emergency department visits for natural causes or for cardiovascular disease was weaker and not statistically significant.
We performed a series of sensitivity analyses to evaluate the robustness of our findings. In analyses that allowed for a flexible exposure-response relation for the association between exposure to PM2.5and morbidity, we found a monotonic association between exposure to PM2.5and the relative risk of hospital admissions for natural causes, cardiovascular disease, and respiratory disease, with no indication of a threshold at lower concentrations (fig 5). We found a monotonic association between exposure to PM2.5and relative risk of emergency department visits for respiratory disease, with the association appearing more pronounced at lower PM2.5levels (fig 5). When we adjusted for daily maximum ambient temperature or daily minimum ambient temperature instead of daily mean temperature, the results remain consistent with our main findings, except for the association between PM2.5level and emergency department visits for natural causes and cardiovascular disease, which became statistically significant when we adjusted for daily minimum temperature (see supplementary table S8). Our results were not materially different when we additionally included an interaction term between relative humidity splines and NCA4 regions in the models (see supplementary table S9). Additionally, when we expanded our analysis beyond days with daily PM2.5concentrations <15 μg/m3, we generally found an attenuation in the association for PM2.5compared with low level exposure to PM2.5, suggesting that PM2.5concentrations are even more strongly associated with adverse outcomes <15 μg/m3versus >15 μg/m3(see supplementary table S10).
We found that the association between exposure to PM2.5and hospital admissions for natural causes was statistically significant only among adults aged ≥65 years (fig 6). For example, a 10 μg/m3increase in PM2.5was associated with an excess relative risk of 0.36% (95% confidence interval −0.72% to 1.45%) among adults aged 18-29 years compared with 1.43% (0.60% to 2.26%) and 2.21% (1.52% to 2.91%) among those aged 65-74 years and ≥75 years, respectively. The corresponding excess absolute risks were 0.41 (95% confidence interval −0.84 to 1.67), 4.76 (2.04 to 7.48), and 14.57 (10.09 to 19.06) per million enrollee per day for adults aged 18-29 years, 65-74 years, and ≥75 years, respectively. The association between exposure to PM2.5and hospital admissions for natural causes was more pronounced among men, those residing in the northeast US, and those with Medicare Advantage health insurance.
For emergency department visits, we found a statistically significant association between exposure to PM2.5and respiratory disease (see supplementary figure S3). This association was most pronounced in young and middle aged adults and in the southern Great Plains. For example, we found that adults aged 40-49 years had the highest excess relative risk of 2.57% (95% confidence interval 0.87% to 4.30%) compared with the older population among whom the association was attenuated and not statistically significant. A 10 μg/m3increase in PM2.5was associated with an excess relative risk of 5.64% (3.77% to 7.54%) in the southern Great Plains versus 1.07% (−0.43% to 2.59%) in the US northeast.
Discussion
Using data encompassing more than 10 million hospital admissions and 24 million emergency department visits across the contiguous US from 2010 to 2016, we found that short term exposure to PM2.5, even at concentrations below the new WHO air quality guideline limit of 15 μg/m3, was statistically significantly associated with a higher risk of hospital admissions for natural causes, cardiovascular disease, and respiratory disease, as well as emergency department visits for respiratory disease.
Comparison with other studies
Studies of the potential health effects of PM2.5at low levels (by today’s standards) provide valuable insights and critically inform national health policies. However, relatively few such studies have been conducted.12425For example, in a pooled analysis of multiple European cohorts, one study evaluated the association between long term exposure to PM2.5and mortality, with a focus on the health effects of such exposure below the current standards and guidelines of the European Union (25 μg/m3) and US (12 μg/m3) and previous guidelines from WHO (10 μg/m3).24Another study used mortality data from the US Medicare fee-for-service population to estimate the association between short term exposure to PM2.5below the current daily national ambient air quality standard (35 μg/m3) and mortality in the US.1In a recent study, researchers found that the adverse health effects of PM2.5on all cause mortality persisted at lower levels of PM2.5below the previous WHO air quality guideline limit of 25 μg/m3.326Our study investigated whether the risk of morbidity occurs at levels of PM2.5below the newly revised WHO air quality guideline limit of 15 μg/m3, using data on both hospital admissions and emergency department visits among younger and older adults in the US.
The new WHO air quality guideline calls for a limit on 24 hour mean PM2.5concentrations to 15 μg/m3, a reduction from the previous limit of 25 μg/m3set in 2005, in response to compelling evidence of substantial health effects of PM2.5even at concentrations below the earlier limit. Our study identified a consistent monotonic exposure-response association between exposure to PM2.5at levels below the new WHO air quality guideline limit and hospital admissions for natural causes, cardiovascular disease, and respiratory disease, and emergency department visits for respiratory diseases. Notably, when examining emergency department visits, we noted that the association between PM2.5level and respiratory disease appears to be more pronounced at lower PM2.5levels. These findings provided evidence that PM2.5continues to pose adverse health risks even when concentrations are below the newly revised WHO air quality guidelines, which corroborates the conclusion that probably no safe level of exposure to PM2.5exists—that is, the level below which no adverse health effects are observed.1242526
Our findings that PM2.5level was associated with hospital admissions among adults ≥aged 65 years but not among adults aged <65 years were consistent with previous studies.1027For example, a nationwide study in Italy, examining the link between air pollution and hospital admissions for respiratory disease, found that PM2.5level was associated with higher risk of hospital admissions among adults aged ≥75 years but not among adults aged <75 years.10
We found statistically significant associations between exposure to PM2.5and increased risk of emergency department visits for respiratory disease exclusively among adults aged <50 years. These findings indicate that previous studies that focused on older populations may not have fully captured the adverse health effects of PM2.5on respiratory related emergency department visits. Relying on such studies could potentially lead to an underestimation of the health effects of PM2.5, particularly among younger age groups. Our findings were consistent with a US nationwide study among 40 million respiratory related emergency department visits collected through the Centers for Disease Control and Prevention’s national environmental public health tracking network.11This study found a strong association between PM2.5level and emergency department visits for respiratory disease among children and young people aged 0-18 years, a moderate association among adults aged 19-64 years, and no significant association among older adults aged ≥65 years.11
We found that most of the adverse health effects of PM2.5were more pronounced among adults with Medicare Advantage health insurance than among adults with commercial health insurance. Enrollees in the Medicare Advantage programme are largely adults aged >65 years. Thus, the health effect of PM2.5among Medicare enrollees is similar to that among people aged ≥65 years. For example, no association was found between short term exposure to PM2.5and emergency department visits for respiratory disease among both Medicare enrollees and older adults. We observed geographic differences in the health effects of PM2.5, with beneficiaries residing at higher latitudes tending to show higher risk of hospital admissions and emergency department visits for cardiovascular disease associated with exposure to PM2.5. We speculate that geographic locations may modify the association between exposure to PM2.5and morbidity, possibly through factors such as differences in local temperature,2829and particle composition contributing to varying levels of oxidative potential.30
Limitations and strengths of this study
Our study has several limitations. First, we used county level PM2.5level as a proxy for personal exposure to PM2.5, which could potentially lead to misclassification of exposure. Additionally, the absence of information on patients’ time-activity patterns might introduce additional misclassification of exposure. Nevertheless, we expect that any potential misclassification would likely be non-differential and would tend to bias our results toward the null hypothesis of no association.31Second, our study population was limited to US adults with health insurance, which may limit the generalizability of our findings to individuals without medical insurance, children and adolescents, and individuals living outside the contiguous US. Third, our analysis was conducted using data available up to 2016. Future studies with more recent data on exposure to air pollution and morbidity are warranted to provide further evaluation of the evidence on the association between exposure to PM2.5and morbidity, particularly at daily PM2.5levels below the 2021 WHO air quality guideline limit.
Our study has three main strengths. First, our study population included more than 10 million hospital admissions and 24 million emergency department visits among beneficiaries with commercial and Medicare Advantage insurance. The enrollees span a broad spectrum of age groups older than 18 years, and they are distributed across different climate regions in the contiguous US. Second, we not only assessed the excess relative risk of morbidity for natural causes, cardiovascular disease, and respiratory disease but also quantified the absolute risk associated with short term exposure to PM2.5. Therefore, our study provides a comprehensive evaluation of the health effects of daily PM2.5at levels below the new WHO air quality guideline limit of 15 µg/m3. Third, we included both hospital admissions and emergency department visits as our primary health outcomes, allowing us to compare the health impacts of PM2.5levels on different morbidity metrics.
Conclusions and policy implications
Short term exposures to PM2.5below the new WHO air quality guideline limit of 15 µg/m3are associated with higher risks of hospital admission for natural causes, cardiovascular disease, and respiratory disease as well as for emergency department visits for respiratory disease among adults with health insurance in the contiguous US. Our study contributes to the evidence that ambient air pollution is associated with morbidity even at PM2.5levels below the current WHO air quality guideline limit. Our study provides an initial assessment of the newly revised WHO guidelines for PM2.5and provides valuable reference for future national air pollution standards.
Short term exposure to fine particulate matter (PM2.5) has been associated with increased risk of morbidity and mortality
Previous studies have primarily focused on older adults or used data on hospital admissions
Evidence for the association between short term exposure to PM2.5and morbidity at levels below the new World Health Organization air quality guideline limit remains unclear
In this nationwide study in the US, exposure to ambient PM2.5at levels below the new WHO air quality guideline limit of 15 µg/m3was statistically significantly associated with hospital admissions for natural causes, cardiovascular disease, and respiratory disease, and emergency department visits for respiratory disease
These findings provide evidence of health harms associated with short term exposure to PM2.5even at levels below the new WHO air quality guideline limit
","Objective: To estimate the excess relative and absolute risks of hospital admissions and emergency department visits for natural causes, cardiovascular disease, and respiratory disease associated with daily exposure to fine particulate matter (PM2.5) at concentrations below the new World Health Organization air quality guideline limit among adults with health insurance in the contiguous US.
Design: Case time series study.
Setting: US national administrative healthcare claims database.
Participants: 50.1 million commercial and Medicare Advantage beneficiaries aged ≥18 years between 1 January 2010 and 31 December 2016.
Main outcome measures: Daily counts of hospital admissions and emergency department visits for natural causes, cardiovascular disease, and respiratory disease based on the primary diagnosis code.
Results: During the study period, 10.3 million hospital admissions and 24.1 million emergency department visits occurred for natural causes among 50.1 million adult enrollees across 2939 US counties. The daily PM2.5levels were below the new WHO guideline limit of 15 μg/m3for 92.6% of county days (7 360 725 out of 7 949 713). On days when daily PM2.5levels were below the new WHO air quality guideline limit of 15 μg/m3, an increase of 10 μg/m3in PM2.5during the current and previous day was associated with higher risk of hospital admissions for natural causes, with an excess relative risk of 0.91% (95% confidence interval 0.55% to 1.26%), or 1.87 (95% confidence interval 1.14 to 2.59) excess hospital admissions per million enrollees per day. The increased risk of hospital admissions for natural causes was observed exclusively among adults aged ≥65 years and was not evident in younger adults. PM2.5levels were also statistically significantly associated with relative risk of hospital admissions for cardiovascular and respiratory diseases. For emergency department visits, a 10 μg/m3increase in PM2.5during the current and previous day was associated with respiratory disease, with an excess relative risk of 1.34% (0.73% to 1.94%), or 0.93 (0.52 to 1.35) excess emergency department visits per million enrollees per day. This association was not found for natural causes or cardiovascular disease. The higher risk of emergency department visits for respiratory disease was strongest among middle aged and young adults.
Conclusions: Among US adults with health insurance, exposure to ambient PM2.5at concentrations below the new WHO air quality guideline limit is statistically significantly associated with higher rates of hospital admissions for natural causes, cardiovascular disease, and respiratory disease, and with emergency department visits for respiratory diseases. These findings constitute an important contribution to the debate about the revision of air quality limits, guidelines, and standards.
"
Optimal timing of influenza vaccination in young children,"Introduction
The US Centers for Disease Control and Prevention (CDC) recommends that annual influenza vaccinations be administered in September or October to maximize vaccine induced immunity over the coming influenza season.12Although the upcoming season’s vaccine may be available in the summer months, receiving the vaccine too early may result in waning effectiveness, particularly among children and older adults, before the end of the flu season.3456789Meanwhile, delaying vaccination may result in exposure to circulating influenza without the protection of vaccine induced immunity.10As such, the CDC recommends that vaccination programs “balance maximizing the likelihood of persistence of vaccine-induced protection through the season with avoiding missed opportunities to vaccinate or vaccinating after onset of influenza circulation occurs.”1
Annual influenza vaccination is particularly important for young children, who are at elevated risk of influenza and severe infection necessitating admission to hospital1112; however, little clinical evidence exists to precisely guide the timing of vaccination in this population. One way to study the optimal timing of vaccination would be to measure antibody titers or in vitro antiviral activity of immunized patients at different times following vaccination111314151617; however, this method focuses on an intermediate outcome that is imperfectly correlated with real world effectiveness and may be limited by study size and representativeness.18Another way would be to examine vaccine effectiveness as a function of time since vaccination in observational data. Such studies in other age groups suggest that effectiveness peaks within several weeks of vaccination and wanes with each month519202122; children, however, may have a longer duration of vaccine effectiveness.23Such observational studies are often limited by generalizability and selection bias (for example, patients at higher risk of clinically significant infection may choose to get vaccinated earlier).
A child’s birthday may influence the timing of influenza vaccination. Preventive care visits for young children are a convenient time to administer vaccines; these visits are often purposefully timed around birthdays and coordinate with the recommended childhood vaccine schedule, which includes annual influenza vaccination.2425As such, children who happen to be born in the fall and early winter, when the seasonal influenza vaccine becomes available, are both more likely to be vaccinated against influenza and less likely to have a diagnosis of influenza infection.26Moreover, among children who are vaccinated, those born in August or September may also be vaccinated earlier owing to earlier preventive care visits. If so, earlier vaccination may be associated with waning vaccine induced immunity toward the end of the flu season, whereas children vaccinated later may have insufficient vaccine induced immunity before being exposed to the influenza virus.
Because a child’s birthday is as good as random with respect to influenza outcomes, the timing of a child’s birthday, through its influence on the timing of preventive care visits and influenza vaccination, provides a unique opportunity to assess the optimal timing of influenza vaccination by using observational data. We examined patterns of influenza vaccination and infection, by birth month, among vaccinated children born in months when the annual influenza vaccine is typically available.
Methods
Data sources
Data came from the MarketScan Research Database, which contains insurance claims for approximately 30-40 million Americans covered by employer sponsored health insurance plans each year. Data are generated when a healthcare provider or organization submits a claim to insurance to pay for services or when insurance is used to pay for prescription drugs. Care for which a claim is not submitted to insurance will not appear in the database. These data have been used to study patterns of childhood vaccination, including influenza.26272829303132
Study population
The study cohort comprised children aged 2-5 years during 2011-18 who were continuously enrolled in insurance over the course of at least one influenza season, defined as September to May of the subsequent year. We focused on these ages because previous research suggested that children of this age are most likely to have preventive care visits near their birthday and that influenza vaccination commonly occurs during these visits.26We restricted our analysis to children with birthdays between August 1 and January 31, as the timing of influenza vaccination in these children could plausibly be influenced by the timing of preventive visits; vaccinated children born in other months so far outside the typical vaccination window would be unlikely to have their birthday influence the timing of their vaccination in an as good as random fashion.
Because our objective was to examine the optimal timing of influenza vaccination among children who are vaccinated, the cohort included only children who were vaccinated against influenza, defined by an insurance claim with any of the Current Procedural Terminology (CPT) codes for influenza vaccination (supplementary table A) or ICD-9 (international classification of diseases, 9th revision) code V04.81, between August and January of the subsequent year.
Study measures and covariates
As the flu season spans two calendar years, we defined a child’s age as the maximum age achieved between August 1 and December 31 of a given influenza season (for example, for the 2012-13 season, we considered a child’s age to be their maximum age in 2012). We based a child’s birth month on monthly enrollment files and determined it as the month in which age changed in those files.
We defined influenza infection by the presence of at least one insurance claim for an ambulatory, emergency department, or inpatient visit containing any of several ICD-9 or ICD-10 influenza diagnosis codes (supplementary table A) or a prescription claim for oseltamivir, the preferred anti-influenza drug for young children. Studies estimate that approximately one third to one half of children with influenza-like illness present for care,3334although this proportion may be higher for young children.
We obtained characteristics of children, siblings, and parents covered by the same insurance policy from the MarketScan database. Model covariates included the child’s age, sex, healthcare use (mean number of office visits, emergency department visits, and hospital admissions per flu season over the study period), medical comorbidities, and family size (number of beneficiaries on the same policy) and the medical comorbidities and healthcare use of siblings and parents. Comorbidities included an indicator for previous pulmonary disease and the overall number of comorbidities, both defined using Elixhauser Comorbidity Software,35for the duration of a person’s inclusion in the database before flu season.
Overview of study design
Using observational data to assess the optimal timing of influenza vaccination is complicated by the possibility that timing of vaccination is not random and may be affected by observed or unobserved characteristics of children or their families. For example, children at higher risk of influenza related complications (or with family members at higher risk) may get vaccinated earlier, which could spuriously suggest that earlier vaccination causes worse influenza related outcomes. Alternatively, children who get vaccinated earlier may belong to families that are highly motivated to receive the vaccine, are wealthier, or are more educated, which may be correlated with better outcomes.
To counter this, we exploited the natural experiment created when children, by chance, are vaccinated at different points during influenza season simply because of the month in which they happened to be born, as preventive care visits timed near birthdays are occasions to administer influenza vaccinations when they are available. Previous work suggested that among children aged 2-5 a child’s birth month affects the probability that a child is vaccinated at all, as many children (for example, those with birth months during January-July) have annual visits during months when the upcoming vaccine is unavailable.26Moreover, children with birthdays in October, a month when vaccination is encouraged and vaccines are widely available, are both the most likely to be vaccinated and the least likely to have a diagnosis of influenza compared with children born in other months.26
If birth month affects the likelihood of vaccination at all, it may also influence the timing of vaccination among vaccinated children in a way that is as good as random with respect to a patient’s clinical characteristics. For this analysis, birth month functions as an instrumental variable (supplementary figure A).
Statistical analyses
We studied the optimal timing of influenza vaccination by examining how rates of influenza diagnosis vary according to a vaccinated child’s birth month, as birth month may influence the timing of vaccination but be otherwise unrelated to risk of influenza. To motivate the need for this quasi-experimental design, we first explored how risk of influenza varied according to the actual month of vaccination, estimating a logistic model of influenza diagnosis as a function of vaccination month (indicator variables) and covariates described above at the child-influenza season level.
We next examined whether young children tend to have annual visits near their birthday, calculating the proportion of children whose visits occurred within two weeks of their birth month. Among vaccinated children, we compared rates of influenza vaccination on the day of such preventive visits compared with surrounding days, to establish whether the actual visit presents an important opportunity for influenza vaccination. We then analyzed how the timing of vaccination varied according to birth month, to assess whether children with earlier birth months (for example, August) received vaccines earlier, on average, than children with later birth months (for example, December). For each vaccinated child, we calculated the time elapsed from August 1 to the day of vaccination in each influenza season. We compared mean differences in weeks elapsed between birth month cohorts by using analysis of variance and did a time to event Kaplan-Meier analysis to visualize differences in vaccine timing between birth month groups, comparing differences by using the log-rank test. We also compared distributions of vaccination timing across birth months.
Next, we assessed whether vaccinated children born in different months are similar to each other, as the validity of using birth month as a quasi-experimental device to influence timing of vaccination requires that a child’s birth month not be otherwise correlated with risk of influenza infection. We compared demographic, clinical, and healthcare use characteristics of vaccinated children, their siblings, and their parents between children’s birth months by using standardized mean differences, with differences <0.1 not considered clinically important.36373839We also estimated the predicted risk of influenza infection for a given child in each influenza season by using a logistic regression of influenza infection as a function of the covariates described above. We calculated predicted rates of influenza by birth month, hypothesizing that predicted risk would not vary by birth month, consistent with a natural experiment.
Next, we analyzed how observed rates of influenza diagnosis varied according to a child’s birth month, our primary objective. Firstly, we measured the unadjusted rate of influenza diagnosis by birth month, at the level of the child-season (for example, a child present over four influenza seasons contributed four child-seasons to the analysis). Secondly, we estimated a logistic regression of influenza diagnosis as a function of birth month and the covariates above, at the child-season level in the primary analysis that combined all influenza seasons as well as models for each individual influenza season conducted at the child level. In this model, we assumed birth month to influence the timing of vaccination and to be associated with the risk of influenza only through its influence on vaccine timing.
Additional analyses
We did several analyses to evaluate whether the relation between birth month and influenza risk was due to confounding or chance. In falsification analyses, we examined the relation between birth month and rate of superficial injury visits, an outcome that is unrelated to influenza but might be related to a propensity of a child’s family to seek medical care. We also did similar analyses examining the relation between birth month and diagnosis of two common non-influenza infectious diseases in this age group, conjunctivitis and viral gastroenteritis, to evaluate for a broader pattern for infectious diseases beyond influenza. ICD code based definitions for these falsification conditions are listed in supplementary table A. Next, we repeated our primary analysis after assigning children random birth months, to evaluate whether findings were due to chance alone.
To evaluate whether the magnitude of benefit associated with optimal vaccination timing may vary depending on the child’s health, we did subgroup analyses according to presence of previous pulmonary disease and number of Elixhauser comorbidities (categorized into thirds). As the magnitude of benefit associated with optimal vaccine timing may also be greater in periods or regions (for example, metropolitan statistical areas, or MSAs) with greater severity of influenza, we did subgroup analyses based on the third of influenza severity in an MSA-season (calculated from influenza diagnoses among all patients in the MarketScan database during an MSA-season).
Research suggests that experiences in one flu season can affect vaccination rates in subsequent flu seasons.40To evaluate whether a child’s experience with the influenza vaccine and/or influenza disease in one season might lead to different behaviors in subsequent seasons in a way that could differ by birth month, we repeated the primary analysis while including only one influenza season per child, which we chose at random for children who were vaccinated in more than one season.
We used R and Stata version 15 for all analyses. The 95% confidence interval around adjusted estimates represents a two tailed α level of 0.05.
Patient and public involvement
This study was a retrospective observational study. No patients were involved in setting the research question or the outcome measures, nor were they involved in developing plans for design or implementation of the study. No patients were asked to advise on interpretation or writing up of results.
Results
The study included 819 223 children aged 2-5 who received influenza vaccination between August 1 and January 31 of a given flu season, for a total of 1 261 164 child-seasons (48.9% (616 162/1 261 164) female; mean age 3.5 years) (table 1). As expected, the overall timing of influenza vaccination followed a similar pattern each year, whereas timing of the peak of influenza diagnoses varied (supplementary figure B).
October was the most common month for children to be vaccinated (37.3% (469 809/1 261 164) of child-seasons) (supplementary figure C). Children vaccinated in November and December had the lowest probability having a diagnosis of influenza compared with children vaccinated in other months (supplementary figure D). However, because the timing of vaccination may be influenced by both measured and unmeasured factors, a causal relation between vaccine timing and influenza infection cannot be inferred from this analysis alone. We therefore investigated the relations between birth month, timing of preventive care visits and influenza vaccination, and influenza risk.
Birth month and timing of influenza vaccination
Overall, 90.2% (1 137 983/1 261 164) of children who were vaccinated between August 1 and January 31 in a given flu season had a preventive care visit during that period; 716 751 (56.8%) had a visit in the two weeks surrounding their birth month (73.6% (241 484/328 239) of 2 year olds, 59.6% (189 730/318 102) of 3 year olds, 51.4% (161 246/313 686) of 4 year olds, and 41.3% (124 291/301 137) of 5 year olds). Among children with preventive visits, the most common day to be vaccinated was the day of that visit, consistent with these visits being a convenient time for vaccination (fig 1). The likelihood of being vaccinated on the day of a preventive visit was lower for children born in December or January, as many of these children may receive vaccination earlier in the fall.
We observed no meaningful differences across birth months in children’s demographic and clinical characteristics, including the predicted risk of influenza infection (table 1; supplementary figure E). For example, children born in August, October, and December had similar age, sex, comorbidities, healthcare use, predicted influenza risk, family size, and comorbidities and healthcare use among family members.
However, timing of influenza vaccination differed across birth months. Vaccination occurred, on average, earlier for children with earlier birth months (fig 2; supplementary figure F; log-rank P<0.001 in unadjusted time-to-event analysis). For instance, the mean time between August 1 and date of vaccination was 10.8 weeks for children born in August, 12.7 weeks for those born in October, and 14.1 weeks for those born in December (P<0.001 for difference). October vaccinations were also disproportionately greater in children with October birth months compared with other birth months (for example, 48.9% (109 721/224 520) of children born in October were vaccinated in October compared with 34.0% (72 230/212 622) of children born in August, 40.2% (88 534/220 235) of those born in September, 27.3% (57 044/209 014) of those born in November, and 33.7% (70 130/208 012) of those born in December) (supplementary figure G).
Influenza diagnosis by birth month
Unadjusted rates of diagnosis of influenza varied by birth month and were lowest for children born in October (fig 3). For example, among children born in August, the average rate of influenza diagnosis across flu seasons studied was 3.0% (6462/212 622), compared with 2.7% (6016/224 540) for children born in October and 2.9% (6041/208 012) for those born in December. We observed a similar relation between birth month and influenza risk in adjusted analysis (fig 3). Children born in October were least likely to have a diagnosis of influenza compared with other birth months (adjusted odds ratio compared with children born in August 0.88, 95% confidence interval 0.85 to 0.92) (supplementary table B). This was the most common pattern in analyses of individual influenza seasons (supplementary table C).
Additional analyses
In falsification analyses, we observed no meaningful patterns between birth months when substituting randomly generated birth months or when substituting superficial injury, conjunctivitis, or viral gastroenteritis as the outcome (supplementary figures H and I). We observed a larger absolute reduction in influenza infection among children born in October compared with other birth months in subgroup analyses that focused on children with previous pulmonary disease (fig 4; supplementary table B) or in higher thirds of Elixhauser comorbidities (fig 4) and in MSA regions with greater influenza severity (fig 4), consistent with optimal timing of influenza vaccination having larger absolute benefit in these subgroups. We observed no meaningful differences in the results of an analysis that included only one influenza season per child (supplementary table B).
Discussion
In an analysis of children aged 2-5 years who were vaccinated against influenza, birth month was associated with both timing of influenza vaccination and the likelihood of diagnosis of influenza. Children born in October, who were disproportionately likely to be vaccinated in October compared with children with other birth months, were least likely to have a diagnosis of influenza, particularly compared with children born in August, who tended to be vaccinated sooner, and those born in December, who tended to be vaccinated later. These quasi-experimental results, which rely on the observation that preventive care visits tend to occur during birth months and are a convenient time to receive the influenza vaccine, support current recommendations for October being the optimal month for influenza vaccination in young children in typical influenza seasons. Notably, the optimal timing of influenza vaccination that can be inferred from using birth month as a natural experiment differs from what would be inferred from analyses of influenza risk by actual month of vaccination.
Studies of vaccine effectiveness suggest that immunity against influenza wanes over the course of the flu season; meanwhile, vaccinated patients who are exposed to the virus before acquiring vaccine induced immunity are at higher risk of infection.345678910111216171819212223This suggests that an optimal time may exist for children to be vaccinated before the flu season and how this question could be evaluated using observational data and quasi-experimental empirical methods. Under the assumption that children born in October are otherwise similar to children born in other months, our findings suggest that the specific timing of influenza vaccination among children born in October may lead to lower rates of influenza infection.
Comparison with other studies
Previous work has suggested that the convenience of being able to be vaccinated at preventive care visits—frequently timed near birthdays—likely leads to increased vaccination rates.26This same convenience may also affect the timing of vaccination among children who are vaccinated. Taking these studies together, children born in October seem to have two advantages when it comes to influenza vaccination that likely lead to lower infection rates: they are more likely to be vaccinated at all because it is more convenient, and, conditional on vaccination, they are more likely to have optimal vaccine induced immunity because of the specific timing of their vaccination. This study’s approach complements those of previous studies examining optimal timing of vaccination by measuring antibody concentrations, which may not correspond to real world vaccine effectiveness, and other observational studies of influenza outcomes using methods that cannot account for unmeasured confounding factors.51617181920212223Our findings suggest that US public health interventions focused on vaccination of young children in October may yield the best protection in typical flu seasons.
This study adds to the growing evidence base establishing and characterizing links between birth month—an arbitrary characteristic—and specific healthcare behaviors and outcomes across the lifespan.264142In young children, for example, arbitrary age cut-offs for starting school create a relative age effect wherein children born in certain months who are relatively younger than their peers are more likely to receive a diagnosis of and treatment for attention deficit/hyperactivity disorder.434445Although a variety of birth month associated outcomes later in life may have origins in early childhood, because this study focuses on an outcome that is so proximal to the exposure (diagnosis following vaccination within the same influenza season), other birth month related differences are highly unlikely to introduce bias that would affect our interpretation of the results.
Strengths and limitations of study
Our study has several limitations. Firstly, despite its quasi-experimental approach and the similarity of children across birth months, including predicted influenza risk, residual confounding is possible. Secondly, claims data limit our ability to measure care that was not submitted to insurance, including vaccinations and influenza cases in which patients did not seek medical care. To account for this, we restricted this analysis to patients with a confirmed vaccination defined by the presence of a claim; meanwhile, care seeking behavior for influenza diagnosis should not differ across birth month groups in a way that would bias our results. Additionally, influenza infections included in the database for which patients sought medical care are the most clinically significant from a standpoint of public health policy. Thirdly, the availability of influenza vaccines and the timing and peak of seasonal influenza infections vary across years and geography, limiting generalizability to any specific influenza season, region, or country. Fourthly, we analyzed children with employer sponsored insurance, limiting generalizability to other populations.
Conclusions
In a large quasi-experimental study of children aged 2-5 years vaccinated against influenza, children who happened to have been born in October tended to be vaccinated later than children born in August and earlier than those born in December, were more likely than other children to be vaccinated in the month of October, and were least likely to have a diagnosis of influenza in the following flu season. The findings support current recommendations that children be vaccinated in October preceding a typical influenza season.
","Objective: To assess optimal timing of influenza vaccination in young children.
Design: Population based cohort study.
Setting: United States.
Participants: Commercially insured children aged 2-5 years who were vaccinated against influenza during 2011-18.
Main outcome measure: Rates of diagnosis of influenza among children who were vaccinated against influenza, by birth month.
Results: Overall, 819 223 children aged 2-5 received influenza vaccination. Children vaccinated in November and December were least likely to have a diagnosis of influenza, a finding that may be confounded by unmeasured factors that influence the timing of vaccination and risk of influenza. Vaccination commonly occurred on days of preventive care visits and during birth months. Children born in October were disproportionately vaccinated in October and were, on average, vaccinated later than children born in August and earlier than those born in December. Children born in October had the lowest rate of influenza diagnosis (for example, 2.7% (6016/224 540) versus 3.0% (6462/212 622) for those born in August; adjusted odds ratio 0.88, 95% confidence interval 0.85 to 0.92).
Conclusions: In a quasi-experimental analysis of young children vaccinated against influenza, birth month was associated with the timing of vaccination through its influence on the timing of preventive care visits. Children born in October were most likely to be vaccinated in October and least likely to have a diagnosis of influenza, consistent with recommendations promoting October vaccination.
"
Effect of exercise for depression,"Introduction
Major depressive disorder is a leading cause of disability worldwide1and has been found to lower life satisfaction more than debt, divorce, and diabetes2and to exacerbate comorbidities, including heart disease,3anxiety,4and cancer.5Although people with major depressive disorder often respond well to drug treatments and psychotherapy,67many are resistant to treatment.8In addition, access to treatment for many people with depression is limited, with only 51% treatment coverage for high income countries and 20% for low and lower-middle income countries.9More evidence based treatments are therefore needed.
Exercise may be an effective complement or alternative to drugs and psychotherapy.1011121314In addition to mental health benefits, exercise also improves a range of physical and cognitive outcomes.151617Clinical practice guidelines in the US, UK, and Australia recommend physical activity as part of treatment for depression.18192021But these guidelines do not provide clear, consistent recommendations about dose or exercise modality. British guidelines recommend group exercise programmes2021and offer general recommendations to increase any form of physical activity,21the American Psychiatric Association recommends any dose of aerobic exercise or resistance training,20and Australian and New Zealand guidelines suggest a combination of strength and vigorous aerobic exercises, with at least two or three bouts weekly.19
Authors of guidelines may find it hard to provide consistent recommendations on the basis of existing mainly pairwise meta-analyses—that is, assessing a specific modality versus a specific comparator in a distinct group of participants.121322These meta-analyses have come under scrutiny for pooling heterogeneous treatments and heterogenous comparisons leading to ambiguous effect estimates.23Reviews also face the opposite problem, excluding exercise treatments such as yoga, tai chi, and qigong because grouping them with strength training might be inappropriate.23Overviews of reviews have tried to deal with this problem by combining pairwise meta-analyses on individual treatments. A recent such overview found no differences between exercise modalities.13Comparing effect sizes between different pairwise meta-analyses can also lead to confusion because of differences in analytical methods used between meta-analysis, such as choice of a control to use as the referent. Network meta-analyses are a better way to precisely quantify differences between interventions as they simultaneously model the direct and indirect comparisons between interventions.24
Network meta-analyses have been used to compare different types of psychotherapy and pharmacotherapy for depression.62526For exercise, they have shown that dose and modality influence outcomes for cognition,16back pain,15and blood pressure.17Two network meta-analyses explored the effects of exercise on depression: one among older adults27and the other for mental health conditions.28Because of the inclusion criteria and search strategies used, these reviews might have been under-powered to explore moderators such as dose and modality (κ=15 and κ=71, respectively). To resolve conflicting findings in existing reviews, we comprehensively searched randomised trials on exercise for depression to ensure our review was adequately powered to identify the optimal dose and modality of exercise. For example, a large overview of reviews found effects on depression to be proportional to intensity, with vigorous exercise appearing to be better,13but a later meta-analysis found no such effects.22We explored whether recommendations differ based on participants’ sex, age, and baseline level of depression.
Given the challenges presented by behaviour change in people with depression,29we also identified autonomy support or behaviour change techniques that might improve the effects of intervention.30Behaviour change techniques such as self-monitoring and action planning have been shown to influence the effects of physical activity interventions in adults (>18 years)31and older adults (>60 years)32with differing effectiveness of techniques in different populations. We therefore tested whether any intervention components from the behaviour change technique taxonomy were associated with higher or lower intervention effects.30Other meta-analyses found that physical activity interventions work better when they provide people with autonomy (eg, choices, invitational language).33Autonomy is not well captured in the taxonomy for behaviour change technique. We therefore tested whether effects were stronger in studies that provided more autonomy support to patients. Finally, to understand the mechanism of intervention effects, such as self-confidence, affect, and physical fitness, we collated all studies that conducted formal mediation analyses.
Methods
Our findings are presented according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses-Network Meta-analyses (PRISMA-NMA) guidelines (see supplementary file, section S0; all supplementary files, data, and code are also available athttps://osf.io/nzw6u/).34We amended our analysis strategy after registering our review; these changes were to better align with new norms established by the Cochrane Comparing Multiple Interventions Methods Group.35These norms were introduced between the publication of our protocol and the preparation of this manuscript. The largest change was using the confidence in network meta-analysis (CINeMA)35online tool instead of the Grading of Recommendations, Assessment, Development and Evaluation (GRADE) guidelines and adopting methods to facilitate assessments—for example, instead of using an omnibus test for all treatments, we assessed publication bias for each treatment compared with active controls. We also modelled acceptability (through dropout rate), which was not predefined but was adopted in response to a reviewer’s comment.
Eligibility criteria
To be eligible for inclusion, studies had to be randomised controlled trials that included exercise as a treatment for depression and included participants who met the criteria for major depressive disorder, either clinician diagnosed or identified through participant self-report as exceeding established clinical thresholds (eg, scored >13 on the Beck depression inventory-II).36Studies could meet these criteria when all the participants had depression or when the study reported depression outcomes for a subgroup of participants with depression at the start of the study.
We defined exercise as “planned, structured and repetitive bodily movement done to improve or maintain one or more components of physical fitness.”37Unlike recent reviews,1222we included studies with more than one exercise arm and multifaceted interventions (eg, health and exercise counselling) as long as they contained a substantial exercise component. These trials could be included because network meta-analysis methods allows for the grouping of those interventions into homogenous nodes. Unlike the most recent Cochrane review,12we also included participants with physical comorbidities such as arthritis and participants with postpartum depression because theDiagnostic Statistical Manual of Mental Health Disorders, fifth edition, removed the postpartum onset specifier after that analysis was completed.23Studies were excluded if interventions were shorter than one week, depression was not reported as an outcome, and data were insufficient to calculate an effect size for each arm. Any comparison condition was included, allowing us to quantify the effects against established treatments (eg, selective serotonin reuptake inhibitors (SSRIs), cognitive behavioural therapy), active control conditions (usual care, placebo tablet, stretching, educational control, and social support), or waitlist control conditions. Published and unpublished studies were included, with no restrictions on language applied.
Information sources
We adapted the search strategy from the most recent Cochrane review,12adding keywords for yoga, tai chi, and qigong, as they met our definition for exercise. We conducted database searches, without filters or date limits, in The Cochrane Library via CENTRAL, SPORTDiscus via Embase, and Medline, Embase, and PsycINFO via Ovid. Searches of the databases were conducted on 17 December 2018 and 7 August 2020 and last updated on 3 June 2023 (see supplementary file section S1 for full search strategies). We assessed full texts of all included studies from two systematic reviews of exercise for depression.1222
Study selection and data collection
To select studies, we removed duplicate records in Covidence38and then screened each title and abstract independently and in duplicate. Conflicts were resolved through discussion or consultation with a third reviewer. The same methods were used for full text screening.
We used the Extraction 1.0 randomised controlled trial data extraction forms in Covidence.38Data were extracted independently and in duplicate, with conflicts resolved through discussion with a third reviewer.
Data items
For each study, we extracted a description of the interventions, including frequency, intensity, and type and time of each exercise intervention. Using the Compendium of Physical Activities,39we calculated the energy expenditure dose of exercise for each arm as metabolic equivalents of task (METs) min/week. Two authors evaluated each exercise intervention using the Behaviour Change Taxonomy version 130for behaviour change techniques explicitly described in each exercise arm. They also rated the level of autonomy offered to participants, on a scale from 1 (no choice) to 10 (full autonomy). We also extracted descriptions of the other arms within the randomised trials, including other treatment or control conditions; participants’ age, sex, comorbidities, and baseline severity of depressive symptoms; and each trial’s location and whether or not the trial was funded.
Risk of bias in individual studies
We used Cochrane’s risk of bias tool for randomised controlled trials.40Risk of bias was rated independently and in duplicate, with conflicts resolved through discussion with a third reviewer.
Summary measures and synthesis
For main and moderation analyses, we used bayesian arm based multilevel network meta-analysis models.41All network meta-analytical approaches allow users to assess the effects of treatments against a range of comparisons. The bayesian arm based models allowed us to also assess the influence of hypothesised moderators, such as intensity, dose, age, and sex. Many network meta-analyses use contrast based methods, comparing post-test scores between study arms.41Arm based meta-analyses instead describe the population-averaged absolute effect size for each treatment arm (ie, each arm’s change score).41As a result, the summary measure we used was the standardised mean change from baseline, calculated as standardised mean differences with correction for small studies (Hedges’ g). In keeping with the norms from the included studies, effect sizes describe treatment effects on depression, such that larger negative numbers represent stronger effects on symptoms. Using National Institute for Health and Care Excellence guidelines,42we standardised change scores for different depression scales (eg, Beck depression inventory, Hamilton depression rating scale) using an internal reference standard for each scale (for each scale, the average of pooled standard deviations at baseline) reported in our meta-analysis. Because depression scores generally show regression to the mean, even in control conditions, we present effect sizes as improvements beyond active control conditions. This convention makes our results comparable to existing, contrast based meta-analyses.
Active control conditions (usual care, placebo tablet, stretching, educational control, and social support) were grouped to increase power for moderation analyses, for parsimony in the network graph, and because they all showed similar arm based pooled effect sizes (Hedges’ g between −0.93 and −1.00 for all, with no statistically significant differences). We separated waitlist control from these active control conditions because it typically shows poorer effects in treatment for depression.43
Bayesian meta-analyses were conducted in R44using thebrmspackage.45We preregistered informative priors based on the distributional parameters of our meta-analytical model.46We nested effects within arms to manage dependency between multiple effect sizes from the same participants.46For example, if one study reported two self-reported measures of depression, or reported both self-report and clinician rated depression, we nested these effect sizes within the arm to account for both pieces of information while controlling for dependency between effects.46Finally, we compared absolute effect sizes against a standardised minimum clinically important difference, 0.5 standard deviations of the change score.47From our data, this corresponded to a large change in before and after scores (Hedges’ g −1.16), a moderate change compared with waitlist control (g −0.55), or a small benefit when compared with active controls (g −0.20). For credibility assessments comparing exercise modalities, we used thenetmetapackage48and CINeMA.49We also usednetmetato model acceptability, comparing the odds ratio for drop-out rate in each arm.
Additional analyses
All prespecified moderation and sensitivity analyses were performed. We moderated for participant characteristics, including participants’ sex, age, baseline symptom severity, and presence or absence of comorbidities; duration of the intervention (weeks); weekly dose of the intervention; duration between completion of treatment and measurement, to test robustness to remission (in response to a reviewer’s suggestion); amount of autonomy provided in the exercise prescription; and presence of each behaviour change technique. As preregistered, we moderated for behaviour change techniques in three ways: through meta-regression, including all behaviour change techniques simultaneously for primary analysis; including one behaviour change technique at a time (using 99% credible intervals to somewhat control for multiple comparisons) in exploratory analyses; and through meta-analytical classification and regression trees (metaCART), which allowed for interactions between moderating variables (eg, if goal setting combined with feedback had synergistic effects).50We conducted sensitivity analyses for risk of bias, assessing whether studies with low versus unclear or high risk of bias on each domain showed statistically significant differences in effect sizes.
Credibility assessment
To assess the credibility of each comparison against active control, we used CINeMA.3549This online tool was designed by the Cochrane Comparing Multiple Interventions Methods Group as an adaptation of GRADE for network meta-analyses.35In line with recommended guidelines, for each comparison we made judgements for within study bias, reporting bias, indirectness, imprecision, heterogeneity, and incoherence. Similar to GRADE, we considered the evidence for comparisons to show high confidence then downgraded on the basis of concerns in each domain, as follows:
Within study bias—Comparisons were downgraded when most of the studies providing direct evidence for comparisons were unclear or high risk.
Reporting bias—Publication bias was assessed in three ways. For each comparison with at least 10 studies51we created funnel plots, including estimates of effect sizes after removing studies with statistically significant findings (ie, worst case estimates)52; calculated an s value, representing how strong publication bias would need to be to nullify meta-analytical effects52; and conducted a multilevel Egger’s regression test, indicative of small study bias. Given these tests are not recommended for comparisons with fewer than 10 studies,51those comparisons were considered to show “some concerns.”
Indirectness—Our primary population of interest was adults with major depression. Studies were considered to be indirect if they focused on one sex only (>90% male or female), participants with comorbidities (eg, heart disease), adolescents and young adults (14-20 years), or older adults (>60 years). We flagged these studies as showing some concerns if one of these factors was present, and as “major concerns” if two of these factors were present. Evidence from comparisons was classified as some concerns or major concerns using majority rating for studies directly informing the comparison.
Imprecision—As per CINeMA, we used the clinically important difference of Hedges’ g=0.2 to ascribe a zone of equivalence, where differences were not considered clinically significant (−0.2<g<0.2). Studies were flagged as some concerns for imprecision if the bounds of the 95% credible interval extended across that zone, and they were flagged as major concerns if the bounds extended to the other side of the zone of equivalence (such that effects could be harmful).
Heterogeneity—Prediction intervals account for heterogeneity differently from credible intervals.35As a result, CINeMA accounts for heterogeneity by assessing whether the prediction intervals and the credible intervals lead to different conclusions about clinical significance (using the same zone of equivalence from imprecision). Comparisons are flagged as some concerns if the prediction interval crosses into, or out of, the zone of equivalence once (eg, from helpful to no meaningful effect), and as major concerns if the prediction interval crosses the zone twice (eg, from helpful and harmful).
Incoherence—Incoherence assesses whether the network meta-analysis provides similar estimates when using direct evidence (eg, randomised controlled trials on strength training versus SSRI) compared with indirect evidence (eg, randomised controlled trials where either strength training or SSRI uses waitlist control). Incoherence provides some evidence the network may violate the assumption of transitivity: that the only systematic difference between arms is the treatment, not other confounders. We assessed incoherence using two methods: Firstly, a global design-by-treatment interaction to assess for incoherence across the whole network,3549and, secondly, separating indirect and direct evidence (SIDE method) for each comparison through netsplitting to see whether differences between those effect estimates were statistically significant. We flagged comparisons as some concerns if either no direct comparisons were available or direct and indirect evidence gave different conclusions about clinical significance (eg, from helpful to no meaningful effect, as per imprecision and heterogeneity). Again, we classified comparisons as major concerns if the direct and indirect evidence changed the sign of the effect or changed both limitsof the credible interval.3549
Patient and public involvement
We discussed the aims and design of this study with members of the public, including those who had experienced depression. Several of our authors have experienced major depressive episodes, but beyond that we did not include patients in the conduct of this review.
Results
Study selection
The PRISMA flow diagram outlines the study selection process (fig 1). We used two previous reviews to identify potentially eligible studies for inclusion.1222Database searches identified 18 658 possible studies. After 5505 duplicates had been removed, two reviewers independently screened 13 115 titles and abstracts. After screening, two reviewers independently reviewed 1738 full text articles. Supplementary file section S2 shows the consensus reasons for exclusion. A total of 218 unique studies described in 246 reports were included, totalling 495 arms and 14 170 participants. Supplementary file section S3 lists the references and characteristics of the included studies.
Network geometry
As preregistered, we removed nodes with fewer than 100 participants. Using this filter, most interventions contained comparisons with at least four other nodes in the network geometry (fig 2). The results of the global test design-by-treatment interaction model were not statistically significant, supporting the assumption of transitivity (χ2=94.92, df=75, P=0.06). When net-splitting was used on all possible combinations in the network, for two out of the 120 comparisons we found statistically significant incoherence between direct and indirect evidence (SSRIvwaitlist control; cognitive behavioural therapyvtai chi or qigong). Overall, we found little statistical evidence that the model violated the assumption of transitivity. Qualitative differences were, however, found for participant characteristics between different arms (see supplementary file, section S4). For example, some interventions appeared to be prescribed more frequently among people with severe depression (eg, 7/16 studies using SSRIs) compared with other interventions (eg, 1/15 studies using aerobic exercise combined with therapy). Similarly, some interventions appeared more likely to be prescribed for older adults (eg, mean age, tai chi=59vdance=31) or women (eg, per cent female: dance=88%vcycling=53%). Given that plausible mechanisms exist for these systematic differences (eg, the popularity of tai chi among older adults),53there are reasons to believe that allocation to treatment arms would be less than perfectly random. We have factored these biases in our certainty estimates through indirectness ratings.
Risk of bias within studies
Supplementary file section S5 provides the risk of bias ratings for each study. Few studies explicitly blinded participants and staff (fig 3). As a result, overall risk of bias for most studies was unclear or high, and effect sizes could include expectancy effects, among other biases. However, sensitivity analyses suggested that effect sizes were not influenced by any risk of bias criteria owing to wide credible intervals (see supplementary file, section S6). Nevertheless, certainty ratings for all treatments arms were downgraded owing to high risk of bias in the studies informing the comparison.
Synthesis of results
Supplementary file section S7 presents a forest plot of Hedges’ g values for each study.Figure 4shows the predicted effects of each treatment compared with active controls. Compared with active controls, large reductions in depression were found for dance (n=107, κ=5, Hedges’ g −0.96, 95% credible interval −1.36 to −0.56) and moderate reductions for walking or jogging (n=1210, κ=51, g −0.63, −0.80 to −0.46), yoga (n=1047, κ=33, g=−0.55, −0.73 to −0.36), strength training (n=643, κ=22, g=−0.49, −0.69 to −0.29), mixed aerobic exercises (n=1286, κ=51, g=−0.43, −0.61 to −0.25), and tai chi or qigong (n=343, κ=12, g=−0.42, −0.65 to −0.21). Moderate, clinically meaningful effects were also present when exercise was combined with SSRIs (n=268, κ=11, g=−0.55, −0.86 to −0.23) or aerobic exercise was combined with psychotherapy (n=404, κ=15, g=−0.54, −0.76 to −0.32). All these treatments were significantly stronger than the standardised minimum clinically important difference compared with active control (g=−0.20), equating to an absolute g value of −1.16. Dance, exercise combined with SSRIs, and walking or jogging were the treatments most likely to perform best when modelling the surface under the cumulative ranking curve (fig 4). For acceptability, the odds of participants dropping out of the study were lower for strength training (n=247, direct evidence κ=6, odds ratio 0.55, 95% credible interval 0.31 to 0.99) and yoga (n=264, κ=5, 0.57, 0.35 to 0.94) than for active control. The rate of dropouts was not significantly different from active control in any other arms (see supplementary file, section S8).
Consistent with other meta-analyses, effects were moderate for cognitive behaviour therapy alone (n=712, κ=20, g=−0.55, −0.75 to −0.37) and small for SSRIs (n=432, κ=16, g=−0.26, −0.50 to −0.01) compared with active controls (fig 4). These estimates are comparable to those of reviews that focused directly on psychotherapy (g=−0.67, −0.79 to −0.56)7or pharmacotherapy (g=−0.30, –0.34 to −0.26).25However, our review was not designed to find all studies of these treatments, so these estimates should not usurp these directly focused systematic reviews.
Credibility assessment
Despite the large number of studies in the network, confidence in the effects were low (fig 5). This was largely due to the high within study bias described in the risk of bias summary plot. Reporting bias was also difficult to robustly assess because direct comparison with active control was often only provided in fewer than 10 studies. Many studies focused on one sex only, older adults, or those with comorbidities, so most arms had some concerns about indirect comparisons. Credible intervals were seldom wide enough to change decision making, so concerns about imprecision were few. Heterogeneity did plausibly change some conclusions around clinical significance. Few studies showed problematic incoherence, meaning direct and indirect evidence usually agreed. Overall, walking or jogging had low confidence, with other modalities being very low.
Moderation by participant characteristics
The optimal modality appeared to be moderated by age and sex. Compared with models that only included exercise modality (R2=0.65), R2was higher for models that included interactions with sex (R2=0.71) and age (R2=0.69). R2showed no substantial increase for models including baseline depression (R2=0.67) or comorbidities (R2=0.66; see supplementary file, section S9).
Effects appeared larger for women than men for strength training and cycling (fig 6). Effects appeared to be larger for men than women when prescribing yoga, tai chi, and aerobic exercise alongside psychotherapy. Yoga and aerobic exercise alongside psychotherapy appeared more effective for older participants than younger people (fig 7). Strength training appeared more effective when prescribed to younger participants than older participants. Some estimates were associated with substantial uncertainty because some modalities were not well studied in some groups (eg, tai chi for younger adults), and mean age of the sample was only available for 71% of the studies.
Moderation by intervention and design characteristics
Across modalities, a clear dose-response curve was observed for intensity of exercise prescribed (fig 8). Although light physical activity (eg, walking, hatha yoga) still provided clinically meaningful effects (g=−0.58, −0.82 to −0.33), expected effects were stronger for vigorous exercise (eg, running, interval training; g=−0.74, −1.10 to −0.38). This finding did not appear to be due to increased weekly energy expenditure: credible intervals were wide, which meant that the dose-response curve for METs/min prescribed per week was unclear (see supplementary file, section S10). Weak evidence suggested that shorter interventions (eg, 10 weeks: g=−0.53, −0.71 to −0.35) worked somewhat better than longer ones (eg, 30 weeks: g=−0.37, −0.79 to 0.03), with wide credible intervals again indicating high uncertainty (see supplementary file, section S11). We also moderated for the lag between the end of treatment and the measurement of the outcome. We found no indication that participants were likely to relapse within the measurement period (see supplementary file, section S12); effects remained steady when measured either directly after the intervention (g=−0.59, −0.80 to −0.39) or up to six months later (g=−0.63, −0.87 to −0.40).
Supplementary file section S13 provides coding for the behaviour change techniques and autonomy for each exercise arm. None of the behaviour change techniques significantly moderated overall effects. Contrary to expectations, studies describing a level of participant autonomy (ie, choice over frequency, intensity, type, or time) tended to show weaker effects (g=−0.28, −0.78 to 0.23) than those that did not (g=−0.75, −1.17 to −0.33; see supplementary file, section S14). This effect was consistent whether or not we included studies that used physical activity counselling (usually high autonomy).
Use of group exercise appeared to moderate the effects: although the overall effects were similar for individual (g=−1.10, −1.57 to −0.64) and group exercise (g=−1.16, −1.61 to −0.73), some interventions were better delivered in groups (yoga) and some were better delivered individually (strength training, mixed aerobic exercise; see supplementary file, section S15).
As preregistered, we tested whether study funding moderated effects. Models that included whether a study was funded did explain more variance (R2=0.70) compared with models that included treatment alone (R2=0.65). Funded studies showed stronger effects (g=−1.01, −1.19 to −0.82) than unfunded studies (g=−0.77, −1.09 to −0.46). We also moderated for the type of measure (self-reportvclinician report). This did not explain a substantial amount of variance in the outcome (R2=0.66).
Sensitivity analyses
Evidence of publication bias was found for overall estimates of exercise on depression compared with active controls, although not enough to nullify effects. The multilevel Egger’s test showed significance (F1,98=23.93, P<0.001). Funnel plots showed asymmetry, but the result of pooled effects remained statistically significant when only including non-significant studies (see supplementary file, section S16). No amount of publication bias would be sufficient to shrink effects to zero (s value=not possible). To reduce effects below clinical significance thresholds, studies with statistically significant results would need to be reported 58 times more frequently than studies with non-significant results.
Qualitative synthesis of mediation effects
Only a few of the studies used explicit mediation analyses to test hypothesised mechanisms of action.545556575859One study found that both aerobic exercise and yoga led to decreased depression because participants ruminated less.54The study found that the effects of aerobic exercise (but not yoga) were mediated by increased acceptance.54“Perceived hassles” and awareness were not statistically significant mediators.54Another study found that the effects of yoga were mediated by increased self-compassion, but not rumination, self-criticism, tolerance of uncertainty, body awareness, body trust, mindfulness, and attentional biases.55One study found that the effects from an aerobic exercise intervention were not mediated by long term physical activity, but instead were mediated by exercise specific affect regulation (eg, self-control for exercise).57Another study found that neither exercise self-efficacy nor depression coping self-efficacy mediated effects of aerobic exercise.56Effects of aerobic exercise were not mediated by the N2 amplitude from electroencephalography, hypothesised as a neuro-correlate of cognitive control deficits.58Increased physical activity did not appear to mediate the effects of physical activity counselling on depression.59It is difficult to infer strong conclusions about mechanisms on the basis of this small number of studies with low power.
Discussion
Summary of evidence
In this systematic review and meta-analysis of randomised controlled trials, exercise showed moderate effects on depression compared with active controls, either alone or in combination with other established treatments such as cognitive behaviour therapy. In isolation, the most effective exercise modalities were walking or jogging, yoga, strength training, and dancing. Although walking or jogging were effective for both men and women, strength training was more effective for women, and yoga or qigong was more effective for men. Yoga was somewhat more effective among older adults, and strength training was more effective among younger people. The benefits from exercise tended to be proportional to the intensity prescribed, with vigorous activity being better. Benefits were equally effective for different weekly doses, for people with different comorbidities, or for different baseline levels of depression. Although confidence in many of the results was low, treatment guidelines may be overly conservative by conditionally recommending exercise as complementary or alternative treatment for patients in whom psychotherapy or pharmacotherapy is either ineffective or unacceptable.60Instead, guidelines for depression ought to include prescriptions for exercise and consider adapting the modality to participants’ characteristics and recommending more vigorous intensity exercis","Objective: To identify the optimal dose and modality of exercise for treating major depressive disorder, compared with psychotherapy, antidepressants, and control conditions.
Design: Systematic review and network meta-analysis.
Methods: Screening, data extraction, coding, and risk of bias assessment were performed independently and in duplicate. Bayesian arm based, multilevel network meta-analyses were performed for the primary analyses. Quality of the evidence for each arm was graded using the confidence in network meta-analysis (CINeMA) online tool.
Data sources: Cochrane Library, Medline, Embase, SPORTDiscus, and PsycINFO databases.
Eligibility criteria for selecting studies: Any randomised trial with exercise arms for participants meeting clinical cut-offs for major depression.
Results: 218 unique studies with a total of 495 arms and 14 170 participants were included. Compared with active controls (eg, usual care, placebo tablet), moderate reductions in depression were found for walking or jogging (n=1210, κ=51, Hedges’ g −0.62, 95% credible interval −0.80 to −0.45), yoga (n=1047, κ=33, g −0.55, −0.73 to −0.36), strength training (n=643, κ=22, g −0.49, −0.69 to −0.29), mixed aerobic exercises (n=1286, κ=51, g −0.43, −0.61 to −0.24), and tai chi or qigong (n=343, κ=12, g −0.42, −0.65 to −0.21). The effects of exercise were proportional to the intensity prescribed. Strength training and yoga appeared to be the most acceptable modalities. Results:  appeared robust to publication bias, but only one study met the Cochrane criteria for low risk of bias. As a result, confidence in accordance with CINeMA was low for walking or jogging and very low for other treatments.
Conclusions: Exercise is an effective treatment for depression, with walking or jogging, yoga, and strength training more effective than other exercises, particularly when intense. Yoga and strength training were well tolerated compared with other treatments. Exercise appeared equally effective for people with and without comorbidities and with different baseline levels of depression. To mitigate expectancy effects, future studies could aim to blind participants and staff. These forms of exercise could be considered alongside psychotherapy and antidepressants as core treatments for depression.
Systematic review registration: PROSPERO CRD42018118040.
"
Association of non-alcoholic fatty liver disease with cardiovascular disease and all cause death in patients with type 2 diabetes,"Introduction
The prevalence of non-alcoholic fatty liver disease (NAFLD) is increasing worldwide.12Many studies have found that NAFLD is associated with various metabolic disorders based on insulin resistance.345NAFLD is a global health problem because of its ability to cause liver related complications (such as cirrhosis and hepatocellular carcinoma) and cardiovascular disease.678910111213Cardiovascular disease is one of the leading causes of death in patients with NAFLD.1415
Type 2 diabetes mellitus (T2DM) is one of the most important risk factors for cardiovascular disease,1617and is strongly associated with greater NAFLD prevalence and severity.918Many studies have described a complex bidirectional association between NAFLD and T2DM910—60-70% of patients with T2DM have NAFLD.9NAFLD and T2DM might have a synergistic association that contributes to cardiovascular risk in affected patients.19However, some studies have reported mixed results when examining the association between NAFLD and cardiovascular disease in patients with T2DM.11122021Although one study found no correlation between NAFLD and cardiovascular disease,11a meta-analysis of 11 studies showed that patients with T2DM and NAFLD had double the risk of cardiovascular disease compared with those with T2DM but without NAFLD.12However, previous studies assessing the risk of cardiovascular disease in patients with T2DM and NAFLD have been limited by small sample sizes or cross sectional design. Large scale, population based, longitudinal studies are needed evaluating the association between NAFLD and cardiovascular disease risk in patients with T2DM. Therefore, the aim of this study was to evaluate the risk of NAFLD for cardiovascular disease and all cause death in patients with T2DM using a nationwide database.
Methods
Data source
This is a nationwide, population based cohort study that used data from the National Health Information Database, which is administered by the National Health Insurance Service with linkage to the National Health Screening Programme. The National Health Insurance Service is a single payer healthcare system that is managed by the Korean government and covers more than 97% of the Korean population. The National Health Information Database contains data on sociodemographic variables, diagnoses (defined by the international classification of diseases, 10th revision—ICD-10), prescriptions, and hospital visit dates. Medical interviews, anthropometric measurements, blood tests, urine tests, and additional assessments were included in the National Health Screening Programme. We obtained mortality data from the National Death Registry. The current study protocol (SSU-202003-HR-201-01) was approved by the institutional review board of Soongsil University. The requirement for informed consent was waived by the institutional review board because the dataset was deidentified to protect personal information.
Study design and participants
We selected 10 601 283 participants from the National Health Screening Programme in 2009. We excluded patients who were younger than 20 years (n=15 431); had type 1 diabetes mellitus (n=172 315); consumed ≥30 g/day alcohol and had chronic hepatitis B, chronic hepatitis C, liver cirrhosis, or hepatocellular carcinoma (n=1 766 863); had cardiovascular disease (myocardial infarction or ischaemic stroke; n=363 151); or who had missing data (n=455 454; supplementary figure S1). To overcome bias, we also excluded those who developed cardiovascular disease within one year (lag period; n=31 306). Finally, 7 796 763 participants were included and followed from baseline to the date of incident cardiovascular disease, all cause death, or until 31 December 2018. The median follow-up duration was 8.13 years.
Measurements
Anthropometric and laboratory data were obtained through the National Health Screening Programme. Blood pressure was measured while participants were seated. After an overnight fast of at least eight hours, venous blood samples were obtained to measure glucose, aspartate aminotransferase, alanine aminotransferase, γ-glutamyl transferase, total cholesterol, triglycerides, high density lipoprotein cholesterol, low density lipoprotein cholesterol, and creatinine levels. The estimated glomerular filtration rate was calculated using the Chronic Kidney Disease Epidemiology Collaboration equation.
We used a standardised self-assessment questionnaire to obtain information about smoking status, alcohol consumption, and regular exercise. Mild drinker was defined as drinking <30 g/day alcohol. Regular exercise was defined as moderate intensity activity performed for at least 30 min, five or more times per week, or high intensity activity performed for at least 20 min, three or more times per week. Low income was defined by the lowest income group or meeting criteria for medical aid benefit.
Definitions
T2DM was defined by fasting plasma glucose concentration ≥126 mg/dL or the presence of at least one prescription claim per year for antidiabetic drugs under ICD-10 codes E11-14. Hypertension was defined as blood pressure ≥140/90 mm Hg or the presence of at least one prescription claim per year for antihypertensive drugs under ICD-10 codes I10-13 or I15. Dyslipidemia was defined using ICD-10 code E78 and at least one claim per year for prescription of a lipid lowering agent, or a total cholesterol level ≥240 mg/dL. Obesity was defined as a body mass index ≥25. Abdominal obesity was defined as waist circumference ≥90 cm in men and ≥85 cm in women. Chronic kidney disease was defined as estimated glomerular filtration rate <60 mL/min/1.73 m2.
Fatty liver index was used to define hepatic steatosis and was calculated using the following equation: (e0.953×ln(TG)+0.139×body mass index+0.718×ln(GGT)+0.053×waist circumference−15.745) ÷ (1+e0.953×ln(TG)+0.139×body mass index+0.718×ln(GGT)+0.053×waist circumference−15.745)×100, where TG=triglyceride and GGT= γ-glutamyl transferase.22NAFLD was defined as the presence of hepatic steatosis without viral hepatitis or excessive alcohol consumption (≥30 g/day). Because we excluded patients who consumed excessive alcohol and those with chronic hepatitis B or hepatitis C, we defined NAFLD by the presence of hepatic steatosis assessed by fatty liver index alone. Patients were divided into three groups: no NAFLD, fatty liver index<30; grade 1 NAFLD, 30≤fatty liver index<60; and grade 2 NAFLD, fatty liver index≥60.
Study outcomes
The endpoint of this study was the development of cardiovascular disease or all cause death. Cardiovascular disease was defined as myocardial infarction or ischaemic stroke. Myocardial infarction was defined as ICD-10 code I21 or I22 during hospital admission. Ischaemic stroke was defined as ICD-10 code I63 or I64 during hospital admission with claims for brain magnetic resonance imaging or computed tomography.
Statistical analysis
Data for continuous variables are presented as means±standard deviation or geometric mean (95% confidence interval). Categorical variables are reported as number (%). Incidence rates are presented as the number of events occurring per 1000 person years. Hazard ratio and 95% confidence interval for cardiovascular disease or all cause death were estimated using a Cox proportional hazards model. The multivariable models were adjusted for basic factors (age, sex), personal characteristics (smoking status, alcohol consumption, physical activity, and low income), and history or condition of disease (hypertension, dyslipidemia, body mass index, diabetes mellitus, and estimated glomerular filtration rate). We constructed Kaplan-Meier survival curves to compare the cumulative incidence rate of cardiovascular disease or all cause death according to the presence of NAFLD. Subgroup analysis was performed under the categories of sex, smoking status, abdominal obesity, aspartate aminotransferase or alanine aminotransferase, income level, alcohol consumption, regular exercise, obesity, hypertension, dyslipidemia, chronic kidney disease, or γ-glutamyl transferase. Variance inflation factor was calculated to assess the collinearity assumption, with variance inflation factor <5 considered to indicate no significant collinearity. Statistical significance was defined as a two sided P value<0.05. Analyses were performed with SAS version 9.4 (SAS Institute, Cary, NC, USA).
Patient and public involvement
No patients were involved in setting the research question or the outcome measures. No patients were involved in developing plans for design or implementation of the study. Patient and public involvement was not commonly used in our discipline in this region when we started the study.
Results
Table 1and supplementary table S1 present the baseline characteristics of the study population. Of 7 796 763 participants, 6.49% (n=505 763) had T2DM; additionally, 22.04% had grade 1 NAFLD and 11.11% had grade 2 NAFLD (supplementary tables S1 and S2). More patients with T2DM had grade 1 NAFLD (34.06%) and grade 2 NAFLD (26.73%) than those without T2DM (grade 1 NAFLD: 21.20%; grade 2 NAFLD: 10.02%).
During a median follow-up of 8.13 years, 34 255 people (6.77%) had cardiovascular disease and there were 42 372 deaths (8.38%) in patients with T2DM, whereas 163 657 people (2.24%) had cardiovascular disease and there were 197 645 deaths (2.71%) in those without T2DM (table 2). Incidence rates for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD, and the incidence rates in patients with T2DM were higher than those in patients without T2DM (table 2, supplementary table S3). The hazard ratios for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death were higher in the order of grade 1 NAFLD and grade 2 NAFLD compared with those in the no NAFLD group in patients with and without T2DM. Furthermore, the five year absolute risk for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD in patients without and with T2DM. In particular, patients with T2DM without NAFLD had a higher five year absolute risk for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death than patients without T2DM with grade 2 NAFLD. Risk differences for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death between no NAFLD and grade 2 NAFLD were higher than those between no NAFLD and grade 1 NAFLD. Additionally, risk differences for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death between no NAFLD and grade 1 or grade 2 NAFLD were higher in patients with T2DM than in those without T2DM.
Kaplan-Meier survival curves showed that NAFLD was associated with a higher risk of cardiovascular disease (fig 1), myocardial infarction (fig 2), ischaemic stroke (fig 3), and all cause death (fig 4) in patients with and without T2DM (all P<0.001). Patients with grade 2 NAFLD had the highest risk of cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death, followed by those with grade 1 NAFLD.
The incidence rate of cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD across all age groups, and increased with age (table 3). These rates were also higher in patients with T2DM than in those without T2DM. In the 20-29 year age group, the incidence rate of cardiovascular disease for patients with T2DM and no NAFLD was 0.40 per 1000 person years, but the rate was 23.25 per 1000 person years in the ≥70 year age group for patients with T2DM and grade 2 NAFLD. The hazard ratios for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death were higher in the order of grade 1 NAFLD and grade 2 NAFLD compared with the hazard ratios for the no NAFLD group in all age groups.
The five year absolute risk for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD across all age groups, and increased with age. The risk was also higher in patients with T2DM than in those without T2DM. Risk differences for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death between no NAFLD and grade 2 NAFLD were higher than those between no NAFLD and grade 1 NAFLD in all age groups, and increased with age. Risk differences for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death between no NAFLD and grade 1 or grade 2 NAFLD were higher in patients with T2DM in all age groups compared with those without T2DM.
Subgroup analysis was performed for the following categories: sex, smoking status, abdominal obesity, aspartate aminotransferase or alanine aminotransferase, income, alcohol consumption, regular exercise, obesity, hypertension, dyslipidemia, chronic kidney disease, or γ-glutamyl transferase (supplementary figure S2). Across all subsets of patients with and without T2DM, NAFLD was associated with a higher risk of cardiovascular disease (supplementary figure S2A and S2B), myocardial infarction (supplementary figure S2C and S2D), ischaemic stroke (supplementary figure S2E and 2F), and all cause death (supplementary figure S2G and S2H). Hazard ratios for cardiovascular disease, myocardial infarction, ischaemic stroke, and all cause death for the grade 2 NAFLD group were mostly higher than those for the grade 1 NAFLD group, although the risks in both groups were higher than those for patients without NAFLD. Collinearity analysis showed no collinearity among the variables (supplementary table S4).
Discussion
This nationwide, population based, longitudinal cohort study showed that NAFLD in patients with T2DM was associated with a higher risk of cardiovascular disease and all cause death. The risk of cardiovascular disease and all cause death in patients with T2DM appears to be increased even in those with grade 1 NAFLD, which is a relatively mild degree of fatty liver. The grade 2 NAFLD group had a higher risk of cardiovascular disease and all cause death than patients in the no NAFLD group and those in the grade 1 NAFLD group. The five year absolute risk for cardiovascular disease and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD in patients without and with T2DM, but it was higher in those with T2DM than in those without T2DM. Risk differences for cardiovascular disease and all cause death between no NAFLD and grade 2 NAFLD were higher than those between no NAFLD and grade 1 NAFLD, and they were also higher in patients with T2DM than in those without T2DM. This large study used real world data obtained from a national database to examine the risk of NAFLD for cardiovascular disease and all cause death in patients with T2DM.
Comparison with other studies
NAFLD is an underappreciated risk factor for atherosclerotic cardiovascular disease.2324In a matched cohort study that enrolled 120 795 adults with NAFLD or non-alcoholic hepatic steatosis, the diagnosis of NAFLD did not appear to be associated with acute myocardial infarction or stroke risk after adjusting for established cardiovascular risk factors including T2DM.25However, a systematic review and meta-analysis of 26 observational studies with 85 395 participants showed that patients with NAFLD had a higher risk of subclinical atherosclerosis than those in the non-NAFLD group (odds ratio 1.60, 95% confidence interval 1.45 to 1.78).26Another meta-analysis of 16 observational studies including 34 043 participants and a median follow-up of 6.9 years showed that NAFLD is associated with increased odds of fatal or non-fatal cardiovascular disease events (random effects odds ratio 1.64, 95% confidence interval 1.26 to 2.13).27Additionally, NAFLD has been associated with an increased long term risk of fatal or non-fatal cardiovascular disease events in a meta-analysis of 36 longitudinal studies with aggregate data on 5 802 226 middle aged participants.13
Because T2DM is a well known risk factor for cardiovascular disease, patients with T2DM and NAFLD might be at greater risk of cardiovascular disease than those without these diseases. However, few studies have focused on people with T2DM, and the results are inconsistent. One study of 300 outpatients with T2DM from a tertiary care teaching hospital in India found no correlation between NAFLD and cardiovascular disease.11In contrast, a prospective case-control study of 2103 patients with T2DM without cardiovascular disease found that NAFLD was associated with an increased risk of cardiovascular disease during a five year follow-up after adjusting for other cardiovascular risk factors.20Another retrospective cohort study of 134 368 patients with T2DM from diabetes registry in Scotland found that patients with NAFLD had increased risk of cardiovascular disease and mortality compared with those without NAFLD during 4.5 years of follow-up.21In our study, we evaluated more than 7.7 million people including half a million patients with T2DM for a median of 8.13 years of follow-up. We were able to show an association between NAFLD and cardiovascular disease or all cause death in patients with T2DM.
Most studies that have investigated NAFLD and cardiovascular disease in patients with T2DM have evaluated hepatic steatosis using ultrasonography. Although it is a non-invasive and widely used procedure, ultrasonography is limited by substantial intraobserver and interobserver variability, can be unreliable with mild degrees of steatosis, and is not suitable for large scale population studies.28We used the fatty liver index instead of ultrasound, which is a simple and accurate surrogate marker of hepatic steatosis that has been validated in many studies.222930Therefore, we were able to evaluate the association between NAFLD and cardiovascular disease in a large nationwide population of more than 7.7 million people. In Western populations, fatty liver index≥60 accurately identified the presence of hepatic steatosis.22When this cut-off value was applied to an Asian population, although the accuracy was similar (area under the curve 0.87), the Youden index decreased to 23-27%.31A study has reported that the optimal cut-off value of the fatty liver index to detect fatty liver with ultrasonography has been validated at ≥30 in the general population of Korea, with an area under the receiver operating characteristic curve of 0.82 (95% confidence interval 0.81 to 0.84).32In middle aged to elderly Chinese participants, the cut-off value of fatty liver index≥30 has presented a maximum Youden's index of 0.51 and achieved a high sensitivity of 79.9% and a specificity of 71.5%.30
The grade 1 NAFLD group had been associated with a higher risk of ischaemic stroke than the no NAFLD group in those with new onset T2DM.33Similarly, another study found that grade 1 and grade 2 fatty liver groups had higher risks of hepatocellular carcinoma and mortality than the no fatty liver group in those with chronic viral hepatitis.34In this study, the risk of cardiovascular disease and all cause death in patients with T2DM appears to be higher in those with grade 1 NAFLD, which is a relatively mild degree of fatty liver. This result shows that even mild NAFLD, which might not be detected by ultrasound, in patients with T2DM was associated with a higher risk of cardiovascular disease and all cause death. These findings suggest that mild NAFLD should be evaluated and managed to reduce the risk of cardiovascular disease or all cause death in patients with T2DM.
According to a previous study, the prevalence of fatty liver disease was highest in young patients (20-39 years) with T2DM compared with middle aged or older T2DM groups; there was also a steep increase in fatty liver disease in young patients with T2DM from 2009 to 2017.2Another study in a general population of people aged 20-39 years showed that NAFLD was associated with a higher risk of myocardial infarction (hazard ratio 1.69, 95% confidence interval 1.61 to 1.77).35A study of the Swedish National Diabetes Registry found that patients with T2DM detected at ≤40 years had an increased risk of myocardial infarction (hazard ratio 3.41, 95% confidence interval 2.88 to 4.04) compared with those without T2DM.36However, studies evaluating the risk of cardiovascular disease in young patients with T2DM and NAFLD have been lacking. In this study, although the incidence rate and absolute risk for cardiovascular disease in younger patients with T2DM were not higher than those for older age groups, the five year absolute risk for cardiovascular disease was higher in young patients with T2DM and grade 1 or grade 2 NAFLD than in those without NAFLD. Considering the recent increase in young patients with T2DM and NAFLD, preventative measures are needed to reduce the risk of cardiovascular disease.
Potential mechanisms for development of cardiovascular disease in patients with T2DM and NAFLD
The mechanisms linking NAFLD with cardiovascular disease in patients with T2DM are not clear. However, there are several potential pathophysiological pathways. NAFLD and T2DM create systemic, low grade inflammatory states that might promote atherosclerosis by secreting multiple cytokines and acute phase proteins.37Additionally, NAFLD and T2DM are proatherogenic conditions that result from increased levels of very low density lipoproteins and small, dense particles of low density lipoprotein cholesterol.38Prothrombotic conditions caused by increased platelet reactivity, higher levels of procoagulant drugs, and lower concentrations of endogenous anticoagulants might influence the development of cardiovascular disease in patients with T2DM and NAFLD.39NAFLD has a deleterious effect on glycaemic control in patients with T2DM; hyperglycaemia could account for the increased risk of cardiovascular disease in patients with T2DM and NAFLD.40Finally, because patients with T2DM and NAFLD tend to have a less healthy lifestyle, they are susceptible to unfavourable metabolic profiles that increase the risk of cardiovascular disease.
Limitations of this study
There are several limitations that should be considered. NAFLD was defined by the fatty liver index because liver biopsy or ultrasonography are not suitable for large scale epidemiologic studies. However, this index is a well known, non-invasive biomarker for predicting hepatic steatosis and has been validated in the Korean population41and worldwide.22We were unable to analyse haemoglobin A1cvariability or changes in diabetes drugs during the follow-up period owing to database limitations.
Although the results of this study might not be generalisable to other ethnicities because of the Korean study population, we believe that the findings are important for Asians who have similar eating habits and body composition to Koreans. However, generalising the results of this study to Western populations with different eating habits, body composition, and genetic factors might require caution. Finally, we could not evaluate hepatic fibrosis, which might influence the risk of cardiovascular disease in patients with T2DM and NAFLD. Overall, however, our study findings are valuable because they show the association between NAFLD and cardiovascular disease risk in patients with T2DM.
Conclusions
NAFLD in patients with T2DM seems to be associated with a higher risk of cardiovascular disease and all cause death even in patients with mild NAFLD. Risk differences for cardiovascular disease and all cause death between no NAFLD and grade 1 or grade 2 NAFLD were higher in patients with T2DM than in those without T2DM. This study suggests that NAFLD screening and prevention are required to reduce the risk of cardiovascular disease and all cause death in patients with T2DM.
Non-alcoholic fatty liver disease and type 2 diabetes mellitus might have a synergistic association that contributes to cardiovascular risk in affected patients
Large scale, population based, longitudinal studies are needed that evaluate the association between non-alcoholic fatty liver disease and cardiovascular disease risk in patients with type 2 diabetes mellitus
Non-alcoholic fatty liver disease in patients with type 2 diabetes mellitus seems to be associated with a higher risk of cardiovascular disease and all cause death, even in patients with a mild degree of fatty liver
Risk differences for cardiovascular disease and all cause death between patients without non-alcoholic fatty liver disease and those with grade 1 or grade 2 non-alcoholic fatty liver disease were higher in those with type 2 diabetes mellitus than in those without this condition
Non-alcoholic fatty liver disease screening and prevention could reduce the risk of cardiovascular disease and all cause death in patients with type 2 diabetes mellitus
","Objective: To investigate the risk of non-alcoholic fatty liver disease (NAFLD) for cardiovascular disease and all cause death in patients with type 2 diabetes mellitus (T2DM).
Design: Nationwide population based study.
Setting: Longitudinal cohort study in Korea.
Participants: 7 796 763 participants in the National Health Screening Programme in 2009 were divided into three groups based on NAFLD status: no NAFLD (fatty liver index<30); grade 1 NAFLD (30≤fatty liver index<60); and grade 2 NAFLD (fatty liver index≥60). Median follow-up was 8.13 years.
Main outcome measures: The primary outcome was incident cardiovascular disease (myocardial infarction, ischaemic stroke) or all cause death.
Results: Of 7 796 763 participants, 6.49% (n=505 763) had T2DM. More patients with T2DM had grade 1 NAFLD (34.06%) and grade 2 NAFLD (26.73%) than those without T2DM (grade 1 NAFLD: 21.20%; grade 2 NAFLD: 10.02%). The incidence rate (per 1000 person years) of cardiovascular disease and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD, and the incidence rates in patients with T2DM were higher than those in patients without T2DM. The five year absolute risk for cardiovascular disease and all cause death increased in the order of no NAFLD, grade 1 NAFLD, and grade 2 NAFLD in patients without and with T2DM (no NAFLD, without T2DM: 1.03, 95% confidence interval 1.02 to 1.04, and 1.25, 1.24 to 1.26, respectively; grade 1 NAFLD, without T2DM: 1.23, 1.22 to 1.25, and 1.50, 1.48 to 1.51, respectively; grade 2 NAFLD, without T2DM: 1.42, 1.40 to 1.45, and 2.09, 2.06 to 2.12, respectively; no NAFLD, with T2DM: 3.34, 3.27 to 3.41, and 3.68, 3.61 to 3.74, respectively; grade 1 NAFLD, with T2DM: 3.94, 3.87 to 4.02, and 4.25, 4.18 to 4.33, respectively; grade 2 NAFLD, with T2DM: 4.66, 4.54 to 4.78, and 5.91, 5.78 to 6.05, respectively). Patients with T2DM and without NAFLD had a higher five year absolute risk for cardiovascular disease and all cause death than those without T2DM and with grade 2 NAFLD. Risk differences for cardiovascular disease and all cause death between no NAFLD and grade 1 or grade 2 NAFLD were higher in patients with T2DM than in those without T2DM.
Conclusions: NAFLD in patients with T2DM seems to be associated with a higher risk of cardiovascular disease and all cause death, even in patients with mild NAFLD. Risk differences for cardiovascular disease and all cause death between the no NAFLD group and the grade 1 or grade 2 NAFLD groups were higher in patients with T2DM than in those without T2DM.
"
Duration of CPR and outcomes for adults with in-hospital cardiac arrest,"Introduction
In-hospital cardiac arrest is an important public health problem, affecting approximately 300 000 adults annually in the United States, with a high mortality rate.12The survival rate after in-hospital cardiac arrest in the US improved from 2000 to 2010 and has remained plateaued after 2010, with approximately 25% of patients surviving to hospital discharge.34
Achieving return of spontaneous circulation is the first step toward long term survival and favorable functional recovery. However, for nearly half of patients with in-hospital cardiac arrest, resuscitative efforts are terminated without achievement of return of spontaneous circulation.5When patients do not achieve return of spontaneous circulation despite cardiopulmonary resuscitation, clinical providers face challenges in deciding how long to continue cardiopulmonary resuscitation. For patients with out-of-hospital cardiac arrest, previous studies showed that longer duration of pre-hospital cardiopulmonary resuscitation before return of spontaneous circulation was associated with poor outcomes for patients.678910However, the association of duration of cardiopulmonary resuscitation with patients’ outcomes has not been fully investigated for in-hospital cardiac arrest. The 2020 International Consensus on Cardiopulmonary Resuscitation and Emergency Cardiovascular Care Science With Treatment Recommendations of the Education, Implementation, and Teams Task Force was unable to make recommendations on when to terminate cardiopulmonary resuscitation for in-hospital cardiac arrest.11This highlights existing gaps in knowledge and the importance of further evaluation of the effect of duration of cardiopulmonary resuscitation on patients’ outcomes after in-hospital cardiac arrest.
Our primary objective was to quantify the time dependent probabilities of favorable outcomes as a function of duration of cardiopulmonary resuscitation among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation who later attained return of spontaneous circulation or had termination of resuscitation. Our secondary objective was to quantify the time dependent probabilities of favorable outcomes as a function of duration of cardiopulmonary resuscitation among patients who had the first return of spontaneous circulation before or at each time point. We also did stratified analyses to investigate whether clinical features and patients’ phenotypes modified the association between duration of cardiopulmonary resuscitation and favorable outcomes.
Methods
Study design and setting
This is an analysis of the Get With The Guidelines—Resuscitation (GWTG-R) registry, a multicenter prospective quality improvement registry of in-hospital cardiac arrest in the US. The GWTG programs are provided by the American Heart Association. Details of the registry were previously reported elsewhere.12Data are collected on all patients with in-hospital cardiac arrest in the participating hospitals who did not have existing do not resuscitate orders and who received cardiopulmonary resuscitation.413Cardiac arrest was defined as pulselessness requiring chest compression, defibrillation, or both.413Several case finding methods were used to consecutively collect cases of in-hospital cardiac arrest, including centralized collection of cardiac arrest flow sheets, review of hospital paging systems, and regular checks of cardiopulmonary resuscitation code carts, pharmacy tracer medication records, and hospital billing charges for use of resuscitation medications.4Research or quality assurance staff collect information on in-hospital cardiac arrest events from hospital medical records and cardiac arrest documentation forms.14The registry used the standardized Utstein template for the definitions of clinical variables and outcomes.1516Data integrity is ensured by rigorous training and certification of hospital staff, use of standardized software with internal data checks, and a periodic re-abstraction process.41213
Study participants
We included adult patients (≥18 years) with an index in-hospital cardiac arrest who received cardiopulmonary resuscitation between 2000 and 2021. We excluded patients with extracorporeal membrane oxygenation in place before the start of cardiopulmonary resuscitation, missing data to classify duration of cardiopulmonary resuscitation, do not resuscitate orders before chest compression, missing data on survival to hospital discharge, duration of cardiopulmonary resuscitation greater than 120 minutes,17termination of resuscitation because of do not resuscitate orders, or extracorporeal membrane oxygenation after the start of cardiopulmonary resuscitation. In an analysis of favorable functional outcome, we further excluded patients with missing functional outcome at hospital discharge.
Exposure
The main exposure was duration of cardiopulmonary resuscitation in minutes, defined as an interval in whole minutes between the start of chest compression and the first return of spontaneous circulation or termination of resuscitation. We defined return of spontaneous circulation as return of adequate pulse by palpation, Doppler, or arterial blood pressure waveform. Data on the start of cardiopulmonary resuscitation, the first return of spontaneous circulation, and termination of resuscitation were initially recoded at the time of the in-hospital cardiac arrest events by the clinical team and subsequently entered onto the GWTG-R database by research or quality assurance staff.14
Outcome measures
Our outcome measures were survival to hospital discharge and favorable functional outcome at hospital discharge, defined as cerebral performance category (CPC) score 1 or 2.16The CPC is a 5 point functional scale; a CPC score of 1 represents good cerebral performance, 2 represents moderate cerebral disability, 3 represents severe cerebral disability, 4 represents coma or vegetative state, and 5 represents brain death.161819
Statistical analysis
We stratified patients by presence or absence of return of spontaneous circulation and reported characteristics of patients and cardiac arrests. We also reported differences in these characteristics with standardized mean differences between patients with and without missing duration of cardiopulmonary resuscitation, survival to hospital discharge, or functional outcome at hospital discharge. We considered an absolute standardized mean difference within 0.25 to be a small difference.20
Using the Kaplan-Meier estimate, we constructed simple curves of the cumulative proportion of patients achieving the first return of spontaneous circulation over time, stratified by survival (among patients who survived to hospital discharge or among patients who had return of spontaneous circulation and subsequently died before hospital discharge) and functional outcome (among patients with favorable functional outcome (CPC score 1 or 2) at hospital discharge, among patients with unfavorable functional outcome (CPC score 3 or 4) at hospital discharge, or among patients who had return of spontaneous circulation and subsequently died before hospital discharge). Using the Greenwood formula for the estimated standard error of the Kaplan-Meier estimate, we also estimated the 95th and 99th centiles of duration of cardiopulmonary resuscitation for each stratified curve with 95% confidence intervals.
We calculated time dependent probabilities for the outcomes as a function of duration of cardiopulmonary resuscitation. Firstly, we calculated time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation. The numerator was the number of patients who were pending the first return of spontaneous circulation at each minute and subsequently had each outcome. The denominator was the number of patients pending the first return of spontaneous circulation at each minute. This time dependent probability represented the probability of subsequently surviving to hospital discharge or having favorable functional outcome if the patients pending the first return of spontaneous circulation at that time point received further cardiopulmonary resuscitation beyond the time point (supplementary methods and figure A).
We calculated two time dependent probabilities of each outcome among patients pending the first return of spontaneous circulation at each minute, as we defined two denominators including and excluding patients with termination of resuscitation before or at each time point (supplementary methods and figure A). As a primary analysis, we included patients who had termination of resuscitation before or at each minute’s duration of cardiopulmonary resuscitation in the denominator (supplementary methods and figure A). Use of this denominator provides probabilities of having outcomes among the overall study population if patients pending the first return of spontaneous circulation had further cardiopulmonary resuscitation beyond that time point, assuming that all decisions on termination of resuscitation were accurate and that the patients who had termination of resuscitation never had outcomes, even if the patients would have had longer duration of cardiopulmonary resuscitation beyond the time point of termination of resuscitation. We reported duration of cardiopulmonary resuscitation when this probability became less than 1%, using traditional medical futility, a likelihood of survival of less than 1%.2122
As a sensitivity analysis, another denominator included only patients who were undergoing cardiopulmonary resuscitation at each minute pending the first return of spontaneous circulation, and excluding patients who had termination of resuscitation before or at each minute (supplementary methods and figure A). This denominator treated termination of resuscitation as a censoring event that is not informative on subsequent time dependent probabilities. Therefore, as duration of cardiopulmonary resuscitation increased, this denominator included only patients who were undergoing cardiopulmonary resuscitation and represented a selected population for whom the resuscitation team chose to provide prolonged cardiopulmonary resuscitation.
Secondly, we calculated the time dependent probability for each outcome among patients who had the first return of spontaneous circulation before or at each time point (supplementary methods and figure A). This time dependent probability quantified the probability of surviving to hospital discharge or having favorable functional outcome once patients achieved the first return of spontaneous circulation before or at each time point. We carried out pointwise estimation of 95% confidence interval of each time dependent probability on the basis of the variance of binomial distribution.
Additionally, we defined clinical features as age group, witness status, and initial rhythm. To evaluate whether the time dependent probabilities differed across clinical features of cardiac arrest, we stratified the time dependent probability curves on the basis of age group (<60 years, 60-79 years, or ≥80 years), witness status (witnessed or unwitnessed), and initial rhythm (shockable or non-shockable rhythm).23
We defined patients’ phenotype as each combination of age group, witness status, and initial rhythm. To investigate whether phenotypes of patients affect the relation between duration of cardiopulmonary resuscitation and outcomes, we plotted the time dependent probabilities for each phenotype.
As the dataset included in-hospital cardiac arrests from 2000 through 2021, we did subgroup analyses including only patients from 2011 through 2021 to evaluate the recent in-hospital cardiac arrest data. We used Stata 16.1 and R software, version 4.0.2, for all statistical analyses
Patient and public involvement
No patients or members of the public were involved in setting the research question or the outcome measures, nor were they involved in developing plans for the design or implementation of the study or asked to advise on interpretation or writing up of results.
Results
We identified 401 697 patients with an index in-hospital cardiac arrest who received cardiopulmonary resuscitation (fig 1). After excluding those who met the exclusion criteria, we included 348 996 patients in our study. We further excluded 15 645 patients who were missing functional outcome at hospital discharge from the analysis of functional outcome.
Table 1andtable 2show characteristics of patients and cardiac arrests. Among 348 996 included patients, 233 551 (66.9%) achieved return of spontaneous circulation and 78 799 (22.6%) survived to hospital discharge. The median interval between the start of chest compressions and the first return of spontaneous circulation was 7 (interquartile range 3-13) minutes among patients who had return of spontaneous circulation. The median interval between start of chest compressions and termination of resuscitation was 20 (14-30) minutes among patients who did not have return of spontaneous circulation. Among 333 351 patients without missing functional outcome at hospital discharge, 52 104 (15.6%) had favorable functional outcome. See supplementary table A for characteristics of patients and cardiac arrests in patients with and without missing duration of cardiopulmonary resuscitation, survival to hospital discharge, or functional outcome at hospital discharge. For all of characteristics of patients and most characteristics of cardiac arrests, the standardized differences were within 0.25, and the characteristics were similar.
Cumulative proportion of patients achieving first return of spontaneous circulation over time stratified by outcomes
Figure 2shows the cumulative proportion of patients achieving the first return of spontaneous circulation, stratified by patients’ outcomes. Almost all (99%) patients who survived to hospital discharge had the first return of spontaneous circulation within 44 (95% confidence interval 43 to 45) minutes’ duration of CPR (fig 2, top). Almost all (99%) patients who had favorable functional outcome at hospital discharge had the first return of spontaneous circulation within 43 (41 to 44) minutes (fig 2, bottom).
Time dependent probabilities of outcomes among patients pending first return of spontaneous circulation at each minute’s duration of CPR
We present time dependent probabilities of survival to hospital discharge (fig 3,fig 4, andfig 5) and favorable functional outcome at hospital discharge (fig 6,fig 7, andfig 8) among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation for overall patients and each clinical feature and patient phenotype, using the denominator including patients who had termination of resuscitation before or at each time point (primary analysis). Among overall patients with in-hospital cardiac arrest, the probabilities of survival and favorable functional outcome among those pending the first return of spontaneous circulation at 1 minute’s duration of cardiopulmonary resuscitation were 22.0% and 15.1%, respectively (fig 3, top;fig 6, top). As duration of cardiopulmonary resuscitation increased, the probabilities of survival and favorable functional outcome decreased and plateaued at 0.3-0.9% and 0.1-0.5% between 40 minutes and 60 minutes, respectively. The probabilities of survival at 39 minutes’ and favorable functional status at 32 minutes’ duration of cardiopulmonary resuscitation were less than 1% (supplementary table B). In terms of clinical features, age younger than 60 years, witnessed arrest, and initial shockable rhythm showed the higher estimates of the time dependent probabilities of survival (fig 3, bottom;fig 4) and favorable functional outcome (fig 6, bottom;fig 7). Across clinical features, stratification by initial rhythm showed the largest change in the time dependent probabilities for each outcome (fig 4, bottomt;fig 7, bottom), and shockable rhythm had the longest duration of cardiopulmonary resuscitation before the time dependent probabilities of survival and functional outcome became less than 1% (supplementary table B).Figure 5shows time dependent probabilities of survival andfigure 8shows time dependent probabilities of favorable functional outcome for patient phenotypes. Across patient phenotypes, patients who were younger than 60 years, had witnessed arrest, and had initial shockable rhythm showed the highest point estimates of the time dependent probabilities of survival (fig 5) and favorable functional outcome (fig 8) and the longest duration of cardiopulmonary resuscitation before the time dependent probabilities became less than 1% (supplementary table B).
Supplementary figures B and C show the time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation, using the denominator excluding patients who had termination of resuscitation before or at each time point (sensitivity analysis). Among overall patients with in-hospital cardiac arrest, the time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at 1 minute’s duration of cardiopulmonary resuscitation were 22.0% and 15.2%, respectively. As duration of cardiopulmonary resuscitation increased from 1 to 20 minutes, the probabilities decreased but then plateaued at 5.4-6.3% for survival and 3.2-3.8% for favorable functional outcome between 20 minutes and 60 minutes.
Time dependent probabilities of outcomes among patients who had first return of spontaneous circulation before or at each minute’s duration of cardiopulmonary resuscitation
In the supplement, we present time dependent probabilities of survival (figure D) and favorable functional outcome (figure E) among patients who had the first return of spontaneous circulation before or at each time point for overall patients and for each clinical feature and patient phenotype. Among clinical features, younger age group and initial shockable rhythm consistently showed higher time dependent probability of survival (supplementary figures D2 and D4) and favorable functional outcome (supplementary figures E2 and E4), and witness status showed crossover of the probability for each outcome (supplementary figures D3 and E3).
Subgroup analysis
The subgroup analysis including the subset of patients with in-hospital cardiac arrest between 2011 and 2021 is shown in supplementary figures F-L and table B. Across most of the clinical features and patient phenotypes, the durations of cardiopulmonary resuscitation before the time dependent probabilities of survival and functional outcome among patients pending the first return of spontaneous circulation before or at each time point became less than 1% were longer in the subgroup of patients between 2011 and 2021 than in the overall study population between 2000 and 2021 (supplementary table B).
Discussion
In this analysis of a large multicenter prospective registry of in-hospital cardiac arrest in the United States between 2000 and 2021, we quantified the time dependent probabilities of survival to hospital discharge and favorable functional outcome at hospital discharge as a function of duration of cardiopulmonary resuscitation. We found that 99% of patients who eventually survived to hospital discharge and who had favorable functional outcome at hospital discharge achieved the first return of spontaneous circulation within 44 minutes’ and 43 minutes’ duration of cardiopulmonary resuscitation, respectively. The time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation using the denominator that included patients who had termination of resuscitation before or at each time point decreased and plateaued at less than 1% as duration of cardiopulmonary resuscitation increased beyond 39 minutes and 32 minutes, respectively. By contrast, the time dependent probabilities using the denominator that excluded patients who had termination of resuscitation before or at each time point plateaued above 5% for survival and 3% for favorable functional outcome as duration of cardiopulmonary resuscitation increased.
Comparison with other studies
A previous observational study using the GWTG-R registry showed that duration of resuscitative efforts before termination of resuscitation varied across hospitals, and resuscitation at hospitals with longer resuscitative efforts before termination of resuscitation was associated with an increased chance of survival to hospital discharge.24Another study using the GWTG-R registry reported that patients with higher predicted survival had longer duration of resuscitative efforts before termination of resuscitation.25However, scarce previous work has evaluated the association between duration of cardiopulmonary resuscitation at the patient level and patients’ outcomes after in-hospital cardiac arrest. A systematic review and meta-analysis in 2019 evaluating pre-arrest and intra-arrest prognostic factors for in-hospital cardiac arrest included only one published study from 1995 and one unpublished study between 2011 and 2018 from the United Kingdom National Cardiac Arrest Audit (UK NCAA) to evaluate the association between duration of cardiopulmonary resuscitation and survival.2627The systematic review and meta-analysis reported that duration of cardiopulmonary resuscitation of more than 15 minutes was associated with decreased odds of survival (adjusted odds ratio 0.06 (95% confidence interval 0.02 to 0.21); adjusted odds ratio 0.13 (0.12 to 0.14) for the UK NCAA data). A Swedish observational study published in 2018 showed the association between longer quarters of duration of cardiopulmonary resuscitation and decreased chance of 30 day survival (adjusted odds ratio 0.69 (0.37 to 1.29) for quarter 2 (duration 3-5 minutes); 0.35 (0.19 to 0.65) for quarter 3 (duration 6-12 minutes); 0.10 (0.05 to 0.20) for quarter 4 (duration ≥13 minutes)), compared with quarter 1 (duration <2 minutes).28A recent observational study of 8727 patients with in-hospital cardiac arrest in 2022, using a national in-hospital cardiac arrest registry in Denmark, reported the association between duration of cardiopulmonary resuscitation and 30 day survival rate (62.0% for quarter 1 (duration <5 minutes), 32.7% for quarter 2 (duration 5-11 minutes), 14.4% for quarter 3 (duration 12-20 minutes), and 8.1% for quarter 4 (duration ≥21 minutes)).29These previous studies have several methodological limitations. The duration of cardiopulmonary resuscitation was treated as a categorical variable, and odds ratios were reported assuming a linear relation between duration of cardiopulmonary resuscitation and outcomes. Avoiding categorizing a continuous variable is recommended, as this can result in loss of information and is rarely justifiable, compared with analyzing the continuous variable on its continuous scale.3031In addition, our results indicate that duration of cardiopulmonary resuscitation and probabilities of favorable outcomes do not show a linear relation. Our findings extend our understanding of the relation between duration of cardiopulmonary resuscitation and outcomes by quantifying time dependent probabilities of subsequent outcomes when patients did not have the first return of spontaneous circulation and when patients achieved the first return of spontaneous circulation in each minute’s duration of cardiopulmonary resuscitation.
In comparison with out-of-hospital cardiac arrest, a retrospective analysis of a multicenter, cluster randomized clinical trial (ROC-PRIMED; Resuscitation Outcomes Consortium Prehospital Resuscitation Using an Impedance Valve and Early Versus Delayed) in the US and Canada showed that 99% of patients who had favorable functional outcome at hospital discharge had return of spontaneous circulation within 37 minutes’ duration of cardiopulmonary resuscitation.10This is shorter than 43 minutes in our results, which is likely explained by the difference in time to start of cardiopulmonary resuscitation in out-of-hospital and in-hospital settings.
Implications of findings
Firstly, we quantified the time dependent changes in the probabilities of patients’ outcomes as a function of duration of cardiopulmonary resuscitation. The time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation provide resuscitation teams, patients, and their surrogates with insights into the likelihood of favorable outcomes if the patient continues to receive cardiopulmonary resuscitation beyond that time point, which is clinically informative for shared decision making to determine whether further cardiopulmonary resuscitation would be beneficial. For example, a pamphlet describing this time dependent probabilities would be helpful for patients’ surrogates to provide objective guidance and make decisions on continuing or discontinuing cardiopulmonary resuscitation. On the other hand, the probabilities of survival and favorable functional outcome among patients who had the first return of spontaneous circulation before or at each time point inform resuscitation teams, patients, and their surrogates of the likelihood of favorable outcomes once the patient has achieved return of spontaneous circulation at or by the time point, which is clinically relevant to estimate subsequent outcomes after return of spontaneous circulation.
Secondly, we observed two distinct features of time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation, depending on the denominators used. Notably, these two probabilities are different in two different populations. When the denominator included patients who had termination of resuscitation before or at each time point, the time dependent probabilities of survival and favorable functional outcome decreased and plateaued below traditional medical futility, the likelihood of survival of less than 1% as patients received longer duration of cardiopulmonary resuscitation.2122By contrast, using the denominator that excluded patients who had termination of resuscitation before or at each time point, the probabilities of survival and favorable functional outcome plateaued above 5% and above 3% respectively after 20 minutes’ cardiopulmonary resuscitation. This difference could be probably explained by two factors: self-fulfilling prophecy—the treating team used duration of cardiopulmonary resuscitation for decisions to terminate resuscitative efforts—and confounding by indication—only a subset of patients for whom the treating providers believed that prolonged cardiopulmonary resuscitation could be beneficial had prolonged cardiopulmonary resuscitation. Therefore, only highly selected patients were included in the denominator when patients who had termination of resuscitation were excluded. These findings would imply that the decision to terminate resuscitation should not be solely dependent on duration of cardiopulmonary resuscitation (for example, time points of <1% of survival in supplementary table B) but should be based on clinical judgment of treating providers.
Thirdly, given the time dependent probabilities of outcomes among patients pending the first return of spontaneous circulation at each minute’s duration of cardiopulmonary resuscitation across the clinical features and patient phenotypes, in-hospital cardiac arrest with younger age, witnessed, and with initial shockable rhythm would benefit from longer duration of cardiopulmonary resuscitation than those with older age, unwitnessed, and with initially non-shockable rhythm.
Fourthly, in the subgroup analysis that included patients with in-hospital cardiac arrest between 2011 and 2021, we found that, for most of the clinical features and patient phenotypes, the durations of cardiopulmonary resuscitation before the time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each time point with the denominators that included termination of resuscitation became less than 1% were longer than those of the overall study population between 2000 and 2021 (supplementary table B). This may be due to improved post-resuscitation care in the 2011-21 time period.432This suggests that outcome rates in the study population affect the time dependent probability. As variations in outcomes across participating hospitals in the GWTG-R are known,33the time dependent probability of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each time point may vary across the hospitals. Our results should be interpreted as the average time dependent probability of outcomes in the dataset, and the probabilities may not be the same at each hospital.
Unanswered questions and future research
The stratified time dependent probabilities of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each time point by clinical features and patient phenotypes provided information about who could benefit from prolonged cardiopulmonary resuscitation, as the probabilities of favorable outcomes differed across features and phenotypes. However, other factors in addition to clinical features and patient phenotypes may also be determinants of the outcomes, and further work is warranted to understand such factors that could justify prolonged cardiopulmonary resuscitation.
In our study, the median interval between start of chest compression and termination of resuscitation was 20 (interquartile range 14-30) minutes among patients without return of spontaneous circulation, whereas we found that the probabilities of survival to hospital discharge and favorable functional outcome become less than 1% at 39 minutes’ and 32 minutes’ duration of cardiopulmonary resuscitation, respectively. Most termination of resuscitation occurred before the time point of traditional medical futility. Further research is needed to evaluate whether patients’ outcomes would improve with prolonged cardiopulmonary resuscitation before termination of resuscitation. Our results might generate a clinical equipoise that justifies a future clinical trial to compare a resuscitation strategy with duration of cardiopulmonary resuscitation at providers’ discretion before termination of resuscitation (a usual care group) versus a resuscitation strategy with prespecified duration of cardiopulmonary resuscitation before termination of resuscitation (an intervention group) for patients with in-hospital cardiac arrest.
Extracorporeal cardiopulmonary resuscitation is an advanced rescue therapy to support circulation in selected patients with refractory cardiac arrest, by an implantation of venoarterial extracorporeal membrane oxygenation.34A recent meta-analysis showed that extracorporeal cardiopulmonary resuscitation for patients with in-hospital cardiac arrest was associated with lower in-hospital morality.35As our study results showed that the probabilities of favorable outcomes decreased as duration of cardiopulmonary resuscitation increased, future research is needed to assess the optimal timing and patient selection to start extracorporeal cardiopulmonary resuscitation when cardiac arrest is refractory to conventional cardiopulmonary resuscitation in hospitals where extracorporeal cardiopulmonary resuscitation is available.
Strengths and limitations of study
Using the largest in-hospital cardiac arrest dataset in the world, we explicitly examined and quantified the changes in the probabilities of favorable outcomes for both patients undergoing cardiopulmonary resuscitation pending the first return of spontaneous circulation at a given moment and those who have achieved the first return of spontaneous circulation before or at the time point.
Our study has several limitations. Firstly, we used two definitions for the denominator of time dependent probability of survival and favorable functional outcome among patients pending the first return of spontaneous circulation at each time point. The denominator that included patient","Objective: To quantify time dependent probabilities of outcomes in patients after in-hospital cardiac arrest as a function of duration of cardiopulmonary resuscitation, defined as the interval between start of chest compression and the first return of spontaneous circulation or termination of resuscitation.
Design: Retrospective cohort study.
Setting: Multicenter prospective in-hospital cardiac arrest registry in the United States.
Participants: 348 996 adult patients (≥18 years) with an index in-hospital cardiac arrest who received cardiopulmonary resuscitation from 2000 through 2021.
Main outcome measures: Survival to hospital discharge and favorable functional outcome at hospital discharge, defined as a cerebral performance category score of 1 (good cerebral performance) or 2 (moderate cerebral disability). Time dependent probabilities of subsequently surviving to hospital discharge or having favorable functional outcome if patients pending the first return of spontaneous circulation at each minute received further cardiopulmonary resuscitation beyond the time point were estimated, assuming that all decisions on termination of resuscitation were accurate (that is, all patients with termination of resuscitation would have invariably failed to survive if cardiopulmonary resuscitation had continued for a longer period of time).
Results: Among 348 996 included patients, 233 551 (66.9%) achieved return of spontaneous circulation with a median interval of 7 (interquartile range 3-13) minutes between start of chest compressions and first return of spontaneous circulation, whereas 115 445 (33.1%) patients did not achieve return of spontaneous circulation with a median interval of 20 (14-30) minutes between start of chest compressions and termination of resuscitation. 78 799 (22.6%) patients survived to hospital discharge. The time dependent probabilities of survival and favorable functional outcome among patients pending return of spontaneous circulation at one minute’s duration of cardiopulmonary resuscitation were 22.0% (75 645/343 866) and 15.1% (49 769/328 771), respectively. The probabilities decreased over time and were <1% for survival at 39 minutes and <1% for favorable functional outcome at 32 minutes’ duration of cardiopulmonary resuscitation.
Conclusions: This analysis of a large multicenter registry of in-hospital cardiac arrest quantified the time dependent probabilities of patients’ outcomes in each minute of duration of cardiopulmonary resuscitation. The findings provide resuscitation teams, patients, and their surrogates with insights into the likelihood of favorable outcomes if patients pending the first return of spontaneous circulation continue to receive further cardiopulmonary resuscitation.
"
Clinical effectiveness of an online physical and mental health rehabilitation programme for post-covid-19 condition,"Introduction
Across the World Health Organization European Region during the first two years of the covid-19 pandemic, more than 17 million people may have experienced covid-19 symptoms lasting more than four weeks.1As of March 2023, 1.9 million people in the UK reported covid-19 symptoms persisting beyond 12 weeks, 1.3 million beyond one year, and 762 000 beyond two years.2Common debilitating symptoms of this complex multisystem condition—post-covid-19 condition (long covid)—include fatigue, shortness of breath, cognitive dysfunction, and muscle ache, all of which can profoundly affect quality of life, participation in society, and economic productivity.3Post-covid-19 condition can result in prolonged and unpredictable disability.
Biomedical research has not fully characterised the underlying pathophysiology of post-covid-19 condition; symptom phenotypes are exceptionally diverse.3Consequently, existing medical management and drug treatments are limited in effectiveness and generalisability. The biopsychosocial model of care may contribute to improved outcomes for people with post-covid-19 condition. Multicomponent physical and mental health rehabilitation can improve breathlessness, fatigue, and quality of life in other long term conditions.456To date, only small quasi-experimental studies have investigated exercise based rehabilitation interventions for people with post-covid-19 condition, and no high quality definitive evidence exists as to the potential benefits or harms of physical and mental health rehabilitation interventions.
We assessed the clinical effectiveness of an eight week live online group rehabilitation programme—the Rehabilitation Exercise and psycholoGical support After COVID-19 InfectioN (REGAIN) intervention—versus a single online session of advice and support for people with post-covid-19 condition.
Methods
Trial design and setting
The REGAIN trial was a pragmatic, multicentre, parallel group, superiority randomised controlled trial with embedded process evaluation, recruiting throughout England and Wales. Each participant identification centre was granted NHS Trust site specific approval. The NHS Digital “Digi-Trials” service was approved to identify and invite potential participants in accordance with Regulation 3(4) of the Health Service (Control of Patient Information, COPI) Regulations 2002, requiring NHS Digital to share confidential patient information with organisations entitled to process this under COPI for the purposes of covid-19 research.36The trial protocol and details of the intervention’s development have been published previously78and are reported in accordance with the template for intervention description and replication (TIDieR)9(see supplementary material).
Participants and procedures
Participants were adults (26-86 years) who had been discharged from hospital three or more months previously after hospital admission with covid-19 and who had ongoing substantial (as defined by participants) covid-19 related physical and/or mental health sequelae. In the absence of agreed diagnostic criteria or clinical coding for post-covid-19 condition, participants were asked to self-report any substantial lasting effects that they attributed to their hospital admission with covid-19. This was confirmed during an eligibility telephone call with the clinical trial team before study enrolment. Exclusion criteria were contraindication to exercise training; severe mental health problems preventing engagement with study procedures; previous randomisation in the present study; already engaged, or planning to engage, in an alternative NHS rehabilitation programme in the next three months; or a household member had been randomised into the REGAIN trial previously.
Randomisation and masking
After completion of the online consent and baseline questionnaires (to ensure allocation concealment), participants were randomly allocated (1:1.03) to the REGAIN intervention or to usual care by a centralised computer generated randomisation sequence using a bespoke web based system, administered independently by Warwick Clinical Trials Unit. We used a minimisation algorithm, stratified by age (<65 yearsv≥65 years), level of hospital care (intensive care unit (ICU) or high dependency unit (HDU)vward), and case level mental health symptomatology (impact of event scale-6 (IES-6) post-traumatic stress disorder score ≥11/24, or hospital anxiety and depression scale (HADS) anxiety subscore ≥11/21, or HADS depression subscore ≥11/21; compared with IES-6 post-traumatic stress disorder score <11/24, or HADS anxiety subscore <11/21, or HADS depression subscore <11/21) (see supplementary material). Participants and practitioners delivering REGAIN could not be masked to group allocation. Follow-up outcome assessments were completed by participants online, or, in a small number of cases, over the telephone by a member of the trial team, blind to treatment allocation.
Procedures
Potential participants were contacted by post, either locally through secondary care NHS Trusts or, for England and Wales, through an NHS Digital “Digi-Trials” mailout. Self-referral to the trial was also possible. Those with persistent physical or mental health sequelae, or both (estimated at 10% of people discharged from hospital after covid-19)10were invited to register their interest by completing a brief online questionnaire for suitability. On confirmation of suitability, the clinical research team contacted participants by telephone to complete further eligibility checks. For those who self-referred, eligibility was confirmed through their general practitioner. Participants then completed an online consent form and baseline outcomes questionnaire before randomisation. We informed general practitioners in writing of participants whose baseline (or follow-up) scores met any of our predefined criteria for case level mental health (see supplementary material). These people continued in the trial. Trial and intervention materials were translated into the five most spoken non-English languages in the UK (Bengali, Gujarati, Urdu, Punjabi, and Mandarin) and a non-English speaking pathway developed to allow access to the trial.
Clinical exercise physiologists or physiotherapists trained in the REGAIN intervention and supported by health psychologists delivered interventions exclusively online from a central trial hub. The hub was based at Atrium Health, a non-profit rehabilitation centre, subcontracted to University Hospitals of Coventry and Warwickshire NHS Trust. Intervention staff included NHS and Atrium Health employees, with some delivering both intervention and usual care treatments. Interventions were informed by a rapid review of existing literature relating to rehabilitation programmes for people affected by chronic obstructive pulmonary disease, chronic fatigue syndrome, and the 2003 severe acute respiratory distress syndrome pandemic. Details on co-development of the intervention are provided elsewhere.8
Interventions
Participants in the usual care group received best practice usual care, consisting of a 30 minute, online, one-to-one consultation with a trained practitioner. A trial booklet was provided that incorporated components of the NHS England “your covid recovery” programme,11information and advice that is freely available online. During the one-to-one consultation, practitioners used the NHS England your covid recovery programme as a template to discuss hospital admission with covid-19, resulting physical and mental health sequalae, and other relevant medical history with participants. The consultation covered generic advice as to how participants might facilitate recovery and undertake self-directed physical activity. A structured physical activity plan was not provided, and no specific psychological techniques were used.
The REGAIN intervention comprised an eight week, online, home based, supervised, group rehabilitation programme (see supplementary figure S1), supported by a workbook for participants (https://wrap.warwick.ac.uk). Participants received a one hour, online, one-to-one consultation with a REGAIN practitioner, which provided an opportunity to discuss hospital admission with covid-19 and sequelae, medical history, and practical ways in which physical and mental health recovery could be supported. Participants subsequently enrolled on weekly practitioner led live (ie, synchronous) online group exercise sessions and six live online group psychological support sessions (one hour each) delivered through Zoom using the Beam platform (https://www.beamfeelgood.com). Individualised, equipment-free exercise sessions performed at home in online groups under the supervision of a REGAIN practitioner aimed to improve cardiovascular fitness, strength, balance, and fatigue while restoring confidence in completing activities of daily living. Semistructured facilitated psychological support sessions were designed to enhance psychological capability and increase knowledge and understanding of covid-19 and its impact on daily living, while giving participants the opportunity to share their own experiences with the group (≤12 participants). Topics of discussion, supported by short introductory videos, included motivation, fear avoidance, activity pacing, managing emotions and set-backs, sleep and fatigue, and stress and anxiety management. Finally, a library of prerecorded, on-demand physical activity videos was made available for participants to access independently online as required. Sessions ranged in duration and intensity from simple breathing exercises, Pilates, and yoga to light seated activity and upright moderate to high intensity exercise.
Outcomes
Outcomes were measured at three, six, and 12 months. The primary outcome was health related quality of life using the patient reported outcomes measurement information system (PROMIS) 29+2 Profile v2.1 at three months post-randomisation.12This measure is one of a portfolio of outcomes from the US National Institutes of Health. The system is reliable, generic, and validated for online use, generating a single overall preference based score (absolute score rather than effect size)—the PROMIS preference (PROPr) score (range −0.022 to 1.0, where 0 indicates a health state equivalent to death and 1.0 indicates perfect health).12The overall score is generated from seven subscores for depression, fatigue, sleep disturbance, pain interference, physical function, social roles or activities, and cognitive function. As with other preference based measures such as the EuroQol 5 dimension 5 level (EQ-5D-5L) instrument, a difference of 0.03 to 0.05 is considered to be clinically important.13
For analysis purposes, we rescaled raw data from the seven PROMIS subscores to standardised T scores with a mean of 50 and a standard deviation (SD) of 10. Therefore, a person with a T score of 40 is 1 SD below the mean. Higher T scores represent more of the concept being measured. For negatively worded concepts such as pain interference, a T score of 60 is 1 SD worse than the mean, whereas for positively worded concepts such as physical function, a T score of 60 is 1 SD better than the mean. For anxiety, depression, fatigue, pain interference, and sleep disturbance, higher scores indicate more severe symptoms. For physical function and social participation, higher scores indicate better health outcomes. A change in T score of between 2.0 and 6.0 is considered clinically important in the PROMIS subscores and subscales.14A further two PROMIS subscales independently measured anxiety and pain intensity. All subscores and subscales were rated over the preceding seven days, apart from physical function and social roles or activities, which do not have a specified timeframe.
Our secondary outcomes were dyspnoea (PROMIS dyspnoea severity short form), cognitive function (PROMIS Neuro-QoL short-form v2.0-cognitive function), quality of life (EQ-5D-5L),15physical activity (international physical activity questionnaire short-form, IPAQ),16severity of post-traumatic stress disorder (impact of events scale-revised, IES-R),17anxiety and depression (hospital anxiety and depression scale, HADS),18general health (self-report of current overall health compared with baseline), and mortality.
Adverse events and serious adverse events were recorded in both trial arms, in line with the principles of good clinical practice.19Additionally, participants in the intervention group were asked to report any events before each exercise session through a confidential, secure online poll. The trial team routinely reviewed responses and contacted participants for further information as required. As the presentation of post-covid-19 condition and chronic fatigue syndrome/myalgic encephalomyelitis overlaps,3we prospectively monitored for post-exertional symptom exacerbation20in the intervention arm during each contact of participants with the intervention team.
Full adherence to the REGAIN intervention was defined as attendance at the initial one-to-one session along with completion of four or more of six support sessions and five or more of eight exercise sessions. Partial adherence was defined as attendance at the initial one-to-one session and completion of fewer than four of six support sessions and fewer than five of eight exercise sessions. To assess fidelity to the intervention, we reviewed a randomly selected 5% subsample of video recorded one-to-one sessions and group exercise and support sessions against predetermined checklist criteria. Results of this and the rest of our process evaluation will be reported elsewhere.
Sample size
The sample size was calculated based on identifying a small to moderate standardised mean difference of 0.3. No data exist on which to base a sample size estimation, nor normative data for the PROPr health related quality of life score in a covid-19 population. Also, there is no indication of what a worthwhile benefit might be from the intervention. We inflated the size of the intervention group to compensate for any clustering effect owing to the delivery of the group intervention. We assumed, based on our experience with other rehabilitation interventions, that groups would comprise a maximum of eight participants. Assuming an intracluster coefficient of 0.01, 90% power, and type I error rate of 5%, with a 10% loss to follow-up, we determined that 535 participants would be required. This equated to 272 participants in the intervention arm (up to 34 intervention groups) and 263 participants in the usual care arm (allocation intervention to usual care ratio of 1.03:1).21To compensate for the slightly higher than anticipated loss to follow-up in the observed data, we recruited a total of 585 participants.
Statistical analysis
Statistical analysis followed a predefined plan.22Our primary analysis was done on an intention-to-treat basis, which included all participants randomly assigned to a treatment group. For the primary outcome (PROPr score) we performed a partially nested heteroscedastic model23to compare health related quality of life at three months between the REGAIN intervention group and usual care group, producing unadjusted and adjusted estimates. Adjustments were made using age, level of hospital care, level of mental health disorder, baseline PROPr score, and therapist effect as a random effect. Deaths were included with a score of zero. The only ordinal categorical outcome was the overall health score. This outcome was fitted using linear regression models (for unadjusted and adjusted variables). We checked normality assumptions and used the Mann-Whitney test to test the treatment effect (unadjusted). To accommodate for non-adherence, we did a complier average causal effect analysis based on a single equation instrumental variable regression model. The complier average causal effect estimates the treatment effect in people randomly assigned to the intervention who actually received it by comparing participants who fully or partially adhered in the intervention group with participants in the control group who would have been classed as adherent had they been allocated to the intervention group.
We performed unadjusted analyses of subgroups defined according to age (<65 yearsv≥65 years), level of hospital care (critical carevstandard ward), HADS depression and HADS anxiety score (<11v≥11), severity of post-traumatic stress disorder (impact of events scale-6 (six item subscale of the impact of events scale-6-revised) score (<11v≥11), ethnicity (whitevnon-white), wave of pandemic (first, second, third, or fourth), and method of recruitment (NHS DigitalvNHS Trusts or self-referral). In a sensitivity analysis, we used the multiple imputation by chained equations procedure,24imputing the primary outcome. To aid interpretation, we report the number needed to treat based on the responses on the global health transition question. These responses are presented as a number needed to treat to be “much better” and at least “somewhat better.” All analyses were conducted using Stata version 17 and R version 4.
Study monitoring
The data monitoring and trial steering committees reviewed the progress of the trial and safety periodically (see supplementary material).
Patient and public involvement
The concept for the trial and grant funding application was driven by our patient partner working group early in the covid-19 pandemic. Patient representatives were involved as co-applicants in the grant funding application. On receipt of the award, our patient and stakeholder working group were integral to the rapid design, co-creation, and pilot testing of the REGAIN intervention and trial processes.8Subsequently, patient partners participated as members of the trial management group and trial steering committee.
Results
Between January 2021 and July 2022, 39 697 people were invited to take part in the study (about 10% (n=4000) were anticipated to meet our definition of post-covid-19 condition)10and 82 self-referred (fig 1). Of 1043 people expressing an interest to participate in the study, 725 (70%) people were contacted and eligible. Overall, 140/725 (19%) people were not randomised for the following reasons: not interested (n=8), consent not received (n=66), baseline outcome questionnaire not completed (n=65), and readmission to hospital with covid-19 (n=1). We randomised 585 people: 298 (51%) to the REGAIN intervention and 287 (49%) to usual care.
Mean age of the study sample was 56 (SD 12) years, more than half were female participants (305/585; 52%), and most were of white ethnicity (517/585, 88%) (table 1). Overall, 508/585 (88%) participants had overweight or obesity, and one third (201/585; 34%) had been admitted to ICU or HDU during their hospital admission with covid-19. Mean time from hospital discharge to randomisation was 323 (SD 144) days (10.6 months). Baseline health related quality of life was low (mean PROPr score 0.20 (SD 0.17))12with a high prevalence of case level mental health disorder (n=251/585; 43%) (table 1; also see supplementary material). Physical activity levels were low (<600 MET (metabolic equivalent of task) mins/week)16for less than half of the participants (235/585; 40%) (table 1). The most common pre-existing medical conditions related to chest or breathing (444/585; 76%) and musculoskeletal conditions (275/585; 47%), and more than one third of participants were unable to work owing to post-covid-19 condition (222/585; 38%) (table 1).
Primary outcome data were collected from 237/298 (80%) in the REGAIN intervention group and 248/287 (86%) participants in the usual care group (table 2). At three months, health related quality of life improved more for participants in the intervention group (mean PROPr score 0.27 (SD 0.18); n=237) than the usual care group (0.23 (SD 0.18); n=248) (also see supplementary tables S9 and S12). We observed a statistically significant difference in health related quality of life between groups at three months (adjusted mean difference in PROPr score 0.03 (95% confidence interval 0.01 to 0.05), P=0.02) (table 2andfig 2). This was driven predominantly by between group differences in three PROMIS subscores: depression (1.39 (0.06 to 2.71), P=0.04), fatigue (2.50 (95% confidence interval 1.19 to 3.81), P<0.001), and pain interference (1.80 (0.50 to 3.11), P=0.01), favouring the REGAIN intervention (fig 3). The effect of the intervention was also evident at 12 months (adjusted mean difference in PROPr score 0.03 (95% confidence interval 0.01 to 0.06), P=0.02), but not at six months (0.02 (−0.003 to 0.05), P=0.08) (table 2andfig 2; also see supplementary tables S9-S11). At 12 months, improvements in the subscores for depression (1.68 (0.20 to 3.15), P=0.03) and fatigue (1.83 (95% confidence interval 0.25, to 3.40), P=0.02) were sustained (see supplementary table S11).
Figure 3andtable 3show the secondary outcomes at three months, and supplementary table S3 shows descriptions of the scale ranges. Supplementary tables S14 and S15 show results at six and 12 months. At three months, all PROMIS subscores and subscales, apart from cognitive function (PROMIS cognitive function subscore; PROMIS Neuro-QoL short-form) were positively influenced to a greater extent by the REGAIN intervention compared with usual care (fig 3). By 12 months, all PROMIS subscores and subscales were improved more in the intervention group. Greater improvements were also noted in favour of the REGAIN intervention for the EQ-5D-5L visual analogue scale at three months (3.37 (95% confidence interval 0.23 to 6.51), P=0.04) and 12 months (3.77 (0.32 to 7.22), P=0.04); and the post-traumatic stress disorder impact of events scale-revised total score at three months (2.61 (0.08 to 5.14), P=0.04) and 12 months. (4.37 (1.66 to 7.07), P=0.002).
In addition to continuous outcome data, we present categorical data for IES-6 post-traumatic stress disorder, HADS anxiety, and HADS depression as these were used to identify case level mental health disorder, defined as a score ≥11 for any of the three measures (table 3). We have not tested the statistical significance of these categorical data, as this was not specified in our statistical analysis plan. However, given a statistical difference between groups in the continuous post-traumatic stress disorder symptom severity data (IESR-R) at three months, it is perhaps noteworthy that 33% of participants in the REGAIN intervention group compared with 46% in the usual care group, exceeded the threshold for case level post-traumatic stress disorder (IES-6).
Participants in the REGAIN intervention group had higher odds (odds ratio 1.66, 95% confidence interval 1.14 to 2.41; P=0.01) of being more physically active compared with participants in the usual care group (table 3). At three months, compared with usual care, 7% more people in the REGAIN intervention group were achieving the UK Chief Medical Officers’ physical activity guidelines25of >150 minutes of moderate intensity activity per week (>600 MET min/week). No effect was seen at six or 12 months. For the category of “overall health compared to three months ago,” a higher proportion of participants in the REGAIN intervention group reported feeling “much better now” (39/237; 17%v20/250; 8%) and “much better now” or “somewhat better now” combined (120/237; 51%v80/250; 32%). This equates to a number needed to treat of 11.9 and 5.4, respectively. The remainder of the secondary outcomes showed no statistically significant differences between groups.
In the usual care and REGAIN intervention groups, we recorded several adverse events (n=16 (6%); n=28 (9%), respectively) and serious adverse events (n=7 (2%); 14 (5%), respectively) (see supplementary material). Of the 21 serious adverse events, 19 concerned admission to hospital or prolongation of admission, and two involved persistent or major disability or incapacity. Only one serious adverse event (syncope with vomiting 24 hours after a live exercise session) was possibly related to the REGAIN intervention. Two adverse events were definitely related (unilateral knee pain during a live exercise session; severe anxiety before a live exercise session) and two were probably related (anxiety before a live exercise session; headache during a live exercise session) to the REGAIN intervention. No instances of post-exertional symptom exacerbation were identified during weekly monitoring.
Adherence to the REGAIN intervention was good. Of 298 participants, 141 (47%) fully adhered, 117 (39%) partially adhered, and 40 (13%) did not receive the intervention (see supplementary material). Median attendance was 5.0 (interquartile range 2.0-7.0) at live group exercise sessions and 5.0 (2.0-6.0) at live group support sessions. At three months, the complier average causal effect analysis for the primary outcome (PROPr) in the groups that fully and fully plus partially adhered was 0.05 (95% confidence interval 0.01 to 0.09), P=0.01) and 0.03 (0.01 to 0.05), P=0.01), respectively (table 2). The supplementary material presents a detailed breakdown of adherence.
No difference in effect was identified in prespecified subgroup analyses relating to age, level of hospital care, HADS depression or anxiety scores, severity of post-traumatic stress disorder, ethnicity, wave of pandemic, or method of recruitment (see supplementary material). In our sensitivity analyses, we observed a statistically significant difference in health related quality of life between groups at three months after adjusting for additional covariates such as sex, body mass index, and ethnicity (adjusted mean difference 0.03 (95% confidence interval 0.01 to 0.05), P=0.01) (see supplementary material), and after multiple imputation for data missingness (0.03 (0.01 to 0.05), P=0.02) (see supplementary material).
Discussion
For adults who experienced post-covid-19 condition after hospital admission with covid-19, a structured programme of physical and mental health rehabilitation (REGAIN), delivered in groups online was clinically effective compared with usual care for improving health related quality of life (PROPr) in our primary analysis at three months post-randomisation. Predominantly, this effect was driven by significantly greater improvements in the PROMIS fatigue, depression, and pain interference subscores with the REGAIN intervention. The intervention was acceptable and safe, as indicated by a single serious adverse event considered to be possibly related to the intervention. Furthermore, the effects of the intervention were also evident at 12 months.
We observed improvements in overall quality of life and in other indices of wellbeing with both the REGAIN intervention and usual care. The relative contributions of the brief intervention, the natural recovery from postviral illness, and regression to the mean in the control group is unclear. Most likely natural recovery played an important part in the improvements witnessed in both groups, as identified in recent observational data.26The REGAIN intervention did, however, show an additional benefit above that which could be attributed to natural recovery and the best practice usual care intervention. Research completed since we started this study suggests a minimally important difference of 0.04 on the PROPr score between groups.13Our observed differences of 0.03 (95% confidence interval 0.01 to 0.05) at three months and 0.03 (0.01 to 0.06) at 12 months are smaller than this suggestion. However, the complier average causal effect analysis showed a larger effect of 0.05 (0.01 to 0.09) at three months and 0.06 (0.01 to 0.10) at 12 months, suggesting that the true effect, in those fully complying with the intervention, might exceed this threshold. Our post hoc analysis showing numbers needed to treat of 11.9 for “much better now” and 5.4 for “much better now” or “somewhat better now” combined at three months will help to interpret the clinical importance of our findings.
The PROPr score for health related quality of life is calculated from seven PROMIS subscores, and in addition we measured four separate PROMIS subscales. At three months, nine of 11 scores were influenced more favourably by the REGAIN intervention compared with usual care. Only two constructs (PROMIS cognitive function subscore; PROMIS Neuro-QoL short-form), both assessing cognitive function, were not improved more by the intervention compared with usual care. By 12 months, all 11 scores were influenced more favourably by the REGAIN intervention. The clinically important improvements we witnessed in the PROMIS fatigue, depression, and pain interference subscores may be important. Fatigue is one of the most prevalent and debilitating symptoms associated with post-covid-19 condition. As with other postviral and autoimmune conditions, post-covid-19 condition is pervasive and enduring. The pathogenesis of the condition is thought to include components of immune dysregulation, disruption to microbiota, autoimmunity, abnormality of clotting and endothelial function, and dysfunctional neurological signalling.3It is beyond the scope of this trial to determine the mechanism of action of the REGAIN intervention, but the reduction in fatigue is likely to be multifactorial. Multiple components of the rehabilitation intervention are likely to have contributed to a reduction in fatigue, which is notoriously complex and treatment resistant. However, carefully prescribed and supervised physical activity along with group education and psychological therapies has been shown to have an impact on fatigue in other clinical populations,5albeit not postviral. The complexity of post-covid-19 condition requires that interventions such as REGAIN are adjuvant—that is, they should be combined with appropriate medical treatment targeted at specific symptom clusters as required.
We were conscious of the potential for post-exertional symptom exacerbation further to physical and mental tasks, as seen in people presenting with chronic fatigue.20Our intervention was tailored and individualised to mitigate this risk. We routinely monitored for signs and symptoms of post-exertional symptom exacerbation but did not observe any instances during the trial or follow-up period, indicating that the intervention was well tolerated. Indeed, the intervention was safe and acceptable overall. The safety profile was such that we did not identify any specific symptom clusters that were exacerbated by physical and mental health rehabilitation. In 33 different intervention groups, totalling more than 1000 participant hours of live exercise and support sessions, only one serious adverse event was possibly related to the intervention, two adverse events were definitely related, and two were probably related.
The REGAIN trial population was severely affected by post-covid-19 condition. At baseline, 43% reported a case level mental health disorder, scoring above accepted clinical thresholds for one or more of anxiety, depression, or post-traumatic stress disorder. Moreover, 38% of participants were unable to work owing to post-covid-19 condition. In addition to improvements in the PROMIS depression subscore, we also witnessed a clinically meaningful reduction in severity of post-traumatic stress disorder with the REGAIN intervention compared with usual care, which was sustained at 12 months. Although the severity of post-traumatic stress disorder reduced in both groups, the magnitude of improvement in the REGAIN intervention group was twofold greater. This is an important finding given the high levels of post-traumatic stress disorder witnessed in this population, and the known impact of this on health related quality of life and social and economic productivity.27Despite observing a reduction in the PROMIS depression subscore at three months, we did not observe a statistically greater effect of the REGAIN intervention in our other measure of depression (HADS). It might be that HADS depression score is less sensitive to change than the PROMIS subscore in post-covid-19 condition. However, both measures indicated a significantly greater effect in favour of the REGAIN intervention at 12 months.
Adherence to the REGAIN intervention was similar to supervised exercise rehabilitation programmes in other clinical conditions.282930Nearly half (47%) of participants attended the initial one-to-one session in addition to at least four of six support sessions and five of eight exercise sessions (full adherence), which resulted in a measurable effect on the outcome. In the complier average causal effect model compared with the intention-to-treat model, there was a 40% greater difference ","Objective: To evaluate whether a structured online supervised group physical and mental health rehabilitation programme can improve health related quality of life compared with usual care in adults with post-covid-19 condition (long covid).
Design: Pragmatic, multicentre, parallel group, superiority randomised controlled trial.
Setting: England and Wales, with home based interventions delivered remotely online from a single trial hub.
Participants: 585 adults (26-86 years) discharged from NHS hospitals at least three months previously after covid-19 and with ongoing physical and/or mental health sequelae (post-covid-19 condition), randomised (1:1.03) to receive the Rehabilitation Exercise and psycholoGical support After covid-19 InfectioN (REGAIN) intervention (n=298) or usual care (n=287).
Interventions: Best practice usual care was a single online session of advice and support with a trained practitioner. The REGAIN intervention was delivered online over eight weeks and consisted of weekly home based, live, supervised, group exercise and psychological support sessions.
Main outcome measures: The primary outcome was health related quality of life using the patient reported outcomes measurement information system (PROMIS) preference (PROPr) score at three months. Secondary outcomes, measured at three, six, and 12 months, included PROMIS subscores (depression, fatigue, sleep disturbance, pain interference, physical function, social roles/activities, and cognitive function), severity of post-traumatic stress disorder, general health, and adverse events.
Results: Between January 2021 and July 2022, 39 697 people were invited to take part in the study and 725 were contacted and eligible. 585 participants were randomised. Mean age was 56 (standard deviation (SD) 12) years, 52% were female participants, mean health related quality of life PROMIS-PROPr score was 0.20 (SD 0.17), and mean time from hospital discharge was 323 (SD 144) days. Compared with usual care, the REGAIN intervention led to improvements in health related quality of life (adjusted mean difference in PROPr score 0.03 (95% confidence interval 0.01 to 0.05), P=0.02) at three months, driven predominantly by greater improvements in the PROMIS subscores for depression (1.39 (0.06 to 2.71), P=0.04), fatigue (2.50 (1.19 to 3.81), P<0.001), and pain interference (1.80 (0.50 to 3.11), P=0.01). Effects were sustained at 12 months (0.03 (0.01 to 0.06), P=0.02). Of 21 serious adverse events, only one was possibly related to the REGAIN intervention. In the intervention group, 141 (47%) participants fully adhered to the programme, 117 (39%) partially adhered, and 40 (13%) did not receive the intervention.
Conclusions: In adults with post-covid-19 condition, at least three months after hospital discharge for covid-19, an online, home based, supervised, group physical and mental health rehabilitation programme was clinically effective at improving health related quality of life at three and 12 months compared with usual care.
Trial registration: ISRCTN registryISRCTN11466448.
"
Atypia detected during breast screening and subsequent development of cancer,"Introduction
Breast screening programmes aim to identify malignancies early when treatment is more effective in reducing breast cancer mortality, but also cause overdiagnosis and overtreatment of cancer that would not have presented symptomatically within the person’s lifetime.1In addition to breast cancer, breast screening programmes identify an increasing number of lesions of uncertain malignant potential (B3), including those with epithelial atypia. Follow-up of atypia might further contribute to overdiagnosis, therefore current management strategies are controversial.
Atypia refers to the histopathological diagnosis of cytological atypia with or without architectural aberration and is diagnosed in 5-10% of needle biopsies performed as part of the English breast screening programme.23However, the term atypia includes diverse abnormalities, including atypical ductal hyperplasia (ADH), flat epithelial atypia (FEA), and lobular neoplasia, which includes atypical lobular hyperplasia (ALH) and lobular carcinoma in situ (LCIS). These processes are not malignant themselves, however cancer can coexist with these lesions.34Additionally, the presence of atypia has been found to confer a fourfold increased long term risk of subsequent breast cancer over a median follow-up of 15.7 years in a meta-analysis of 13 studies, including a total of 1759 women.5This meta-analysis synthesised mainly small studies (median 92 women) from 1987 to 2010, spanning changes to screening programmes, imaging technology, atypia definitions and treatment options, and reported pooled relative risks of cancer development for a range of follow-ups from 6.8 to 21 years. Studies did not consider short term risk at three or six years (time periods reflecting NHS breast screening programme further routine screening rounds). While the overall increased risk is apparent, this is of limited use for policy makers in countries where routine screening is available. In particular, the important question is whether additional mammographic screens for such women are required to detect subsequent cancers earlier.
English guidelines recommend vacuum assisted excision for all atypias (except when associated with a papillary lesion, which requires assessment of the extent in continuity of the atypia) followed by annual mammographic surveillance.6European consensus on the management of B3 lesions with atypia recommends excision by vacuum assisted biopsy of FEA and lobular neoplasia, followed by surveillance imaging for five years and open surgical excision for ADH.7A second and third consensus in 2018 and 2023 stipulated that surveillance can only replace surgical excision of ADH in special situations after discussion at the multidisciplinary meeting.89In the United States, surgical excision is recommended for most ADH, for lobular neoplasia where imaging and pathology are discordant, and for FEA with ADH. For other atypias, surgical excision is not considered necessary and observation with clinical and imaging follow-up can be offered.10Observation and follow-up are not further defined. The recommendations were based on evidence of upgrade rates to cancer on excision and long term cancer risk. However, evidence on the effectiveness of regular surveillance mammography was not available, which is of particular importance in countries where routine breast screening is not annual. Annual surveillance imaging is a safety net to ensure no cancers are missed at excision and provide an opportunity for early detection in women at high risk, but this approach is not evidence based. We need studies examining cancer detection over the short term, after a diagnosis of any type of atypia and after current diagnostic management. In England, for instance, annual surveillance is suggested after vacuum assisted excision of all forms of atypia, but with the provision that this should be amended when more data and national guidance become available.11
This study presents an analysis of the English Sloane Project prospective atypia cohort12and reports the proportion of women with atypia who develop breast cancer by type of atypia and time frame. This evidence base will help policy makers to decide the requirements for surveillance mammography in the first five years after atypia detection.
Methods
Data sources
The Sloane Atypia Project comprises a prospective cohort of women diagnosed as having atypia through the NHS breast screening programme in the United Kingdom from April 2003 to the present. The dataset is formed from a prespecified prospective data collection form submitted to the Sloane Project that is based on preset standardised data collection expectations as part of national quality assurance processes. Centre level participation was voluntary, with processes implemented in recent years to provide participating centres with a list of eligible women to help participation and completeness.12Data included information on women’s atypia type, age at diagnosis, mammographic features, biopsy method, histological features, surgical treatment, and adjuvant treatment up until June 2018 (supplementary method 1.1).
Data were matched by NHS number and date of birth at person level to the English Cancer Registry held by the National Cancer Registration and Analysis Service, and the Mortality and Birth Information System for information on subsequent development of breast cancer and mortality data until December 2018. Data were deidentified before sharing for analysis. Data collection, data cleaning, and verification methods are described in detail elsewhere.13The present analysis followed our published protocol.14
Inclusion criteria
We identified all women on the Sloane database with epithelial atypia, and included those with ADH or atypical intraductal epithelial proliferation (AIDEP), FEA, ALH, and LCIS.Figure 1depicts traditional views of the association between the different types of atypia to show how atypia types were considered in the analysis. We combined ALH, LCIS, and unspecified lobular in situ neoplasia (LISN) or LCIS under the term LISN. Supplementary method 1.2 defines types of atypia.
Exclusion criteria
Women with ductal carcinoma in situ (DCIS) identified on the Sloane database were excluded; this group have been included in previous analyses.151617We excluded women with bilateral primary breast cancer when women had DCIS in one breast and atypia in the other, or the best prognosis atypia of the bilateral primaries in women with atypia in both breasts; patients with DCIS in addition to the atypia; those with pleomorphic LCIS (these are managed similarly to DCIS); those with an unknown type of atypia; women who were not from England; and women without linkage to the Mortality and Birth Information System to determine vital status on 31 December 2018.
Follow-up
We followed women from six months after their atypia diagnosis until death (any cause) or 31 December 2018. For the primary analysis, follow-up was until the date of the first diagnosis of invasive breast cancer in either breast. For the secondary analysis, follow-up was until the date of the first diagnosis of DCIS or invasive breast cancer in either breast.
Outcomes
The primary outcome was subsequent invasive breast cancer (see supplementary method 1.3 for information on collection and definition) per 1000 women diagnosed as having atypia at three years and six years after atypia diagnosis. This outcome was estimated from the cause specific cumulative incidence function calculated using time of observed cancer detection and time of death. Secondary outcomes included location of subsequent breast cancer, nature of subsequent cancer (grade, size, and nodal status), and cancers per 1000 women with atypia one year after their atypia diagnosis.
Analysis
We summarised the characteristics of women with atypia, characteristics of atypia, and histological nature of subsequent cancer events for the whole cohort and by type of atypia using descriptive statistics. We recorded counts of breast cancer at one year, three years, and six years and investigated how diagnostic management changed over time. The number of deaths from breast cancer (see supplementary method 1.4 for definition) and the number of deaths from other causes were also reported.
For the primary analysis, we calculated cause specific cumulative incidence functions for invasive breast cancer (combined and split into ipsilateral and contralateral cancers) and death from any cause in a competing risks framework using the survfit function from the R package survival in R 4.1.2.18The cumulative incidence function for invasive cancer was used to estimate the cumulative incidence of invasive cancers at one year, three years, and six years, with 95% confidence intervals. The three and six year time points represented the first and second rounds of screening after a diagnosis of atypia. The one year time point was a secondary analysis to explore missed cancers at the time of atypia diagnosis. We repeated the analysis for different types of atypia, age at atypia diagnosis, year of atypia diagnosis, and for different diagnostic management strategies to explore their effect on subsequent cancer rates. For the secondary analysis, we considered DCIS and invasive breast cancer as the outcome, with death as the competing risk at all three time points.
We undertook a sensitivity analysis including only consecutive women with atypia to explore the possibility of selective reporting of women with atypia to the Sloane Project, and a sensitivity analysis in which we excluded cancers detected within 12 months of atypia diagnosis as missed cancers. Supplementary method 1.5 reports the justification and approaches for all analyses. We reported the overall patterns of missing data by recording the number of unrecorded or missing items for each variable.
We used flexible parametric models by following the method of Hinchliffe and Lambert19to explore the effect of several explanatory variables on the time to breast cancer since atypia diagnosis using a competing risks framework and considering events as described above. This modelling approach produced hazard ratios with 95% confidence intervals. We considered age at diagnosis, year of diagnosis, type of atypia, management pathway, calcification, and background parenchymal breast density as explanatory variables, and consecutive versus non-consecutive women with atypia. Age was included as a continuous, linear variable (see supplementary method 1.6 for rationale). We calculated model fit statistics, Akaike’s information criterion, and Bayesian information criterion for model selection. A subdistribution model with the same covariates as the chosen model was also fitted (see supplementary method 1.7 for a discussion of both modelling approaches). Results were interpreted by considering major changes to the breast screening programme and the detection and management of atypia during the study period (fig 2).
Patient and public involvement
Patients were involved in all stages of the project from grant application through to dissemination. Patients contributed to monthly project meetings, discussion of findings with patient groups, and to written reports and publicly available information.
Results
Characteristics of women and their atypia in Sloane cohort
A total of 3762 women with an atypia diagnosis after routine breast screening were reported to the Sloane Project in the UK between 1 April 2003 and 30 June 2018. Of these women, 3238 met our inclusion criteria (supplementary figure S1 and supplementary table S1). In total, women were reported from 63 of 77 (81.8%) English breast screening centres, however this proportion fluctuated over the study period. The mean age was 55.6 years (range 46-95) and the total follow-up was 19 087.9 person years. Of 3238 women with atypia, 1350 had ADH, 403 had FEA, 1101 had LISN, and 384 had mixed ductal and lobular atypia. Microcalcifications were present in 2525 (78%) diagnosed atypia.
There was a fourfold increase in the incidence of atypia between 2010 and 2015 (fig 3, upper panel). This increase cannot be explained by the 15% increase in women attending breast screening over the same time period20or the change in age of women screened given the two age extensions during the study window. More women with atypia were recorded in the time period 2013-18 (n=2014) than in the previous two time periods (2003-07, n=534; 2008-12, n=690). This appeared to be a genuine increase in atypia numbers rather than an increase caused by more complete reporting because it was also apparent in centres that reported all women with atypia throughout the study period (fig 3, lower panel). While an increase in FEA diagnoses contributed to the overall increase, it was not the only reason (fig 3, top panel). FEA diagnoses increased over the three time periods in proportion to all atypia diagnoses (2.6% in 2003-07; 16.8% in 2013-18), while the relative numbers of the other atypia types showed minimal change but with an increase in absolute numbers (supplementary table S2). The increase in numbers of atypia coincided with an increase in the proportion of atypia with microcalcifications, which occurred around the time when digital mammography was introduced in screening centres between 2010 and 201321(fig 3, middle panel).
Subsequent breast cancer events after atypia diagnosis
Of 3238 women with atypia with mean follow-up of 5.9 years (range 0.51-15.7), 168 (5.2%) developed breast cancer. Of these, 141 had invasive cancer and 27 had DCIS.Table 1reports characteristics of invasive cancer for all atypia types and separately for each subtype. Supplementary table S3 presents characteristics of DCIS.
The characteristics of the subsequent invasive cancers were similar to those of cancers detected in the general screening population. Most of the invasive cancers recorded were ≤20 mm in size and were node negative. The distribution of grades of the 141 invasive cancers detected was similar to screen detected cancers in the literature (see supplementary table S4): 25 (17.7%) grade 1, 69 (48.9%) grade 2, 28 (19.9%) grade 3, and 19 (13.5%) unrecorded.
The numbers of ipsilateral and contralateral invasive cancers at three years were similar (ipsilateral: 7.7 per 1000 women, 95% confidence interval 4.98 to 11.5; contralateral: 6.5, 3.99 to 10.1). While reporting was incomplete for the location of 22 ipsilateral cancers detected within three years of the initial atypia diagnosis (supplementary table S5), the number of contralateral cancers indicates that many atypia lesions are not direct precursors of subsequent breast cancers within the 15 years of follow-up available for analysis.
Missed cancers at time of atypia diagnosis
The number of cancers diagnosed within 12 months probably reflects missed cancers at the time of the atypia diagnosis rather than cancers that developed after screening. Within six and 12 months after an atypia diagnosis, three invasive cancers were detected in women with atypia—one contralateral cancer after an ADH diagnosis and two ipsilateral cancers after a mixed atypia diagnosis. These findings equate to 0.95 (95% confidence interval 0.28 to 2.69) invasive cancers per 1000 women with atypia.
The main driver for an intensive follow-up of atypia is clinician concern about missing a cancer diagnosis when management of atypia moved from diagnostic surgical excision to vacuum assisted excision as a consequence of possible lower volume tissue removal. In the Sloane atypia cohort, the final atypia diagnosis was based on a single diagnostic procedure (standard core biopsy or vacuum assisted biopsy) in 477 (14.7%) women, a second line vacuum assisted biopsy or vacuum assisted excision in 964 (29.8%) women, and a surgical procedure in 1797 (55.5%) women. However, management with diagnostic surgical excision decreased and second line vacuum assisted excision increased during the study period (supplementary figure S3) in accordance with UK guidelines.6This change in management strategy had little impact on numbers of invasive cancers detected. Second line vacuum assisted biopsy or vacuum assisted excision did not result in more cancers missed than surgery at one year (1.08 (0.11 to 5.9)v1.12 (0.24 to 3.9) cancers per 1000 women, respectively) or three years (9.23 (4.1 to 18.4)v18.5 (12.8 to 25.8) cancers per 1000 women, respectively). This finding applied to all atypia types and was independent of the site of cancer (supplementary table S6). The flexible parametric model confirmed that type of management had no effect (diagnostic surgical excisionvsecond line vacuum assisted biopsy or vacuum assisted excision: hazard ratio 1.029, 95% confidence interval 0.54 to 1.95, P=0.93) when added after including age, year, and density; however, the wide confidence interval reflects the considerable uncertainty and means that a reduction in the hazard cannot be ruled out. Therefore, few cancers were missed at the time of atypia diagnosis and vacuum assisted excision appears to be as safe as surgical excision in the management of atypia.
Cancers at three and six years after atypia and long term risk
The numbers of invasive cancers detected at three years and six years after an atypia diagnosis were estimated using the fitted cumulative incidence functions (three years: 14.2 per 1000 women, 95% confidence interval 10.3 to 19.1; six years: 45.0, 36.3 to 55.1; these figures are based on n=40 and n=94 invasive cancers detected, respectively;fig 4,table 2). While the number of cancers at three years was low, the number was higher at 3.5 years (23.8, 11.4 to 30.3), which presents a more pragmatic estimate because it includes cancers detected at the first routine (three yearly) screen after atypia when not all screens were on time. The numbers of cancers detected at three and six years after atypia diagnosis when an invasive cancer or DCIS was the outcome were estimated to be 18.9 per 1000 women (14.3 to 24.5; n=53) and 52.8 (43.4 to 63.4; n=113), respectively. Only one woman was reclassified in this analysis because she had a DCIS diagnosis followed by an invasive cancer.
Cancers by age at atypia diagnosis increased with age, apart from women aged 66-70 years (supplementary table S8). However, when considering age in combination with breast density and year of diagnosis in the flexible parametric model, neither age nor background parenchymal density had a clinically significant impact on cancer detection (supplementary figure S4). Furthermore, atypia type had no major impact on cancers detected (supplementary table S9). Adding atypia type as a variable to the model including age and year of diagnosis did not improve the model fit (supplementary table S13). Results from the models with cause specific hazards and subdistribution hazards gave the same conclusions (supplementary material 3). Therefore, there was no evidence that atypia management should be risk stratified by subgroup.
Fewer invasive cancers were detected at three years during 2013-18 (estimated to be 6.0 per 1000 women, 3.1 to 10.9) than during the two earlier time periods (2003-07: 24.3, 13.7 to 40.1; 2008-12: 24.6, 14.9 to 38.3) and remained low at 3.5 years (12.6, 7.5 to 20.0). These data suggest that the clinical significance of atypia diagnosed since 2013 was different from the effect of atypia diagnosed in earlier years. This finding was not caused by the lack of follow-up during the latest period (supplementary table S10) or the detection of more women with FEA in that time period. Excluding women with FEA from the analysis did not remove the observed difference (supplementary table S11). Furthermore, the reduced risk cannot be explained by selective reporting of more severe atypia diagnoses in the earlier time periods because the reduction in cancer rates was also substantial in an analysis of women with atypia from centres where all consecutive women with atypia were recorded (supplementary table S12). Additionally, the proportion of non-invasive to invasive breast cancers was higher in the latest time period than in previous time periods. Taken together, there were more atypia diagnoses and fewer cancers (but proportionally more DCIS) in the most recent time period (fig 3, top panel).
The cancer risk continued after six years (n=46 invasive cancers) in line with previous studies, with potentially slightly higher rates for mixed atypia and lowest rates for FEA at the end of follow-up (fig 4). However, care is needed in projecting long term risk from the earlier years to more recent atypia diagnoses, which could represent a different spectrum of atypia, and these lack the long term follow-up available for the earlier time periods.
Mode of detection of subsequent breast cancers
Of 168 invasive breast cancers and DCIS, 57 (33.9%) were detected through screening and 47 (28.0%) were detected symptomatically; 32 (19.0%) cancers were detected by other outpatient appointments, which might have included annual screens. For 32 (19.0%) cancers, the mode of detection was not recorded. Supplementary figure S5 shows the mode of detection of subsequent cancers after an atypia diagnosis. Other outpatient appointments do not show an annual pattern, therefore we cannot assume cancers detected by other outpatient appointments were detected by annual surveillance mammography. A small number (12 of 168) of cancers were picked up symptomatically within the first three years after atypia diagnosis.
Discussion
Main findings
In the English Sloane atypia cohort of 3238 women with any epithelial atypia diagnosis, the incidence of atypia markedly increased from 2012 onwards. At the same time, detection of subsequent breast cancers in women with atypia decreased. Overall, cancer development after atypia was low compared with general population cancer rates and was considerably lower in more recent years than in earlier time periods. We propose that the gradual introduction of digital mammography in England since 2010, which identifies more microcalcifications,2223could explain a large proportion of the increase in atypia from 2012 and might be the reason why lower rates of subsequent invasive cancers were detected in women with atypia from 2012 onwards. The remaining increase in atypia incidence might be because of a shift in atypia definitions and pathologists refining their diagnostic criteria, particularly the diagnosis and terminology of columnar cell lesions. FEA is one form of these lesions and appears to be uncommon before 2012. Another factor possibly relating to the increase in atypia could be the increased size of the biopsy needle that might have been used in recent years, increasing the probability of finding atypia and decreasing the probability of misclassifying atypia as DCIS.
Few cancers appeared to have been missed at the time of atypia diagnosis and non-surgical management was as safe as surgical excision of atypia in this cohort. The characteristics of cancers detected after atypia were similar to cancers detected in the general screening population and no subgroup was identified that was at increased risk of developing invasive cancer. Therefore, the reporting of atypia at screening could contribute to the problem of overdiagnosis in breast cancer screening.
Comparison with previous studies
This study examines the short term risk of breast cancer after screen detected atypia. Previous studies5242526272829303132lack evidence to support a policy on the short term management of women after an atypia diagnosis because they focus on long term relative risks and only two studies have investigated atypia in a screening cohort. In Ireland, Boland and colleagues reported four cancers in 66 women with screen detected lobular neoplasia after mean follow-up of 62.5 months.27Castells and colleagues reported on a cohort of women screened between 1994 and 2011 as part of the Spanish breast screening programme.31In 159 women (0.029% of those screened), they recorded proliferative disease with atypia (although this included 28 benign or uncertain benign phyllodes tumours in this category, which is perhaps unexpected). Of these, six developed breast cancer (invasive or DCIS), which was equivalent to a cancer rate of 8.44 per 1000 person years compared with 7.7 per 1000 person years (9.2 considering invasive cancer and DCIS) in our study. In line with the results presented here, Castells and colleagues concluded that their results showed an association between benign breast disease and subsequent risk of cancer, with only a small number of malignancies misclassified as benign at biopsy and with no impact on cancer risk estimation. Considering all available follow-up (median 6.07 years), Castells and colleagues reported an age adjusted risk ratio of 4.56 (95% confidence interval 2.06 to 10.07) for women with atypia compared with women screened without benign disease (from first screen to cancer diagnosis), but with a similar pattern of time to breast cancer in both groups. However, the authors did not report estimates for the first five years after atypia diagnosis. Furthermore, none of the studies included women with atypia detected after 2011 when, according to our results, invasive cancers developed less frequently.
However, changes over time have been previously reported. An increase in lesions of uncertain malignant potential (B3 lesions), together with a decrease in the positive predictive value of malignancy for B3 lesions (in particular, lobular neoplasia) were reported in 2011 by Rakha and colleagues who compared B3 lesions detected in 1998-2000 with those detected in 2007-08.33They reported a decrease in positive predictive value from 35% to 10% for B3 lesions, suggesting this decline was because of more accurate targeting of lobular neoplasia lesions by radiologists and more DCIS diagnoses with vacuum assisted biopsy (which would have been identified as AIDEP on the limited sampling provided by core biopsy).
Strengths and limitations
The Sloane atypia prospective cohort comprises a large number of women compared with other predominantly retrospective studies or meta-analyses examining women with atypia and follow-up to cancer. However, the data have some limitations. Despite the substantial patient numbers, cancer after atypia diagnosis is rare, limiting the statistical power. Additionally, this is not a complete consecutive cohort across all English breast screening centres for the entire time period, so theoretically atypia lesions that are not included in the Sloane database (which is by voluntary submission) might be systematically different. Therefore, we compared our results for the whole cohort with those for the subset of centres known to have a complete, consecutive sample and they did not differ.
The cohort also encompasses a long time period, which enables assessment of temporal changes in the proportion of women who develop cancer. However, this long time period could complicate interpretation because several concurrent temporal changes play a part, such as improvement in imaging technology, changes in treatment and management of atypia, and changes in atypia terminology and definitions, and data collection forms. Furthermore, the data lacked information on symptomatic versus screen detected subsequent cancer and any data on annual surveillance mammography. Therefore, we gained little insight from the data about how atypia is currently managed, how subsequent cancers were detected, and which management strategy might work best in detecting cancers. Finally, the data lacked a comparator to assess cancer risk in a contemporary general screening population to put our findings into context.
Implications for clinical practice
The results suggest that additional annual mammography for the first three years after a diagnosis of epithelial atypia might not be necessary over and above UK standard screening practice offered to all women (ie, once every three years). The number of women diagnosed as having atypia who developed cancer in the first three years was low. This cohort was not comparative, and so we cannot draw conclusions about the rate of cancers in women with atypia compared with the general screening population. However, the number of cancers detected within 3.5 years (one complete screening round per 1000 women with atypia) was 12.6 (95% confidence interval 7.5 to 20.0) in 2013-18. In the general population of women who have attended screening aged 50-70 years in 2018-19, the total rate of cancers within a three year screening round is comparable at 11.3 per 1000 women (3.5 symptomatically detected interval cancers between screening rounds34and 7.8 detected at the next screening round20). Although without statistical comparisons or a matched cohort, these data suggest that the risk of developing cancer in the first 3.5 years is not high for women with atypia identified recently in a quality assured screening programme. The evidence is less clear for extra screening between three and five years when the rate of cancer is slightly higher than we would expect (58.0 per 1000 women, 95% confidence interval 42.2 to 77.1 at six years after atypia diagnosis). However, this evidence is for atypia diagnoses between 2008 and 2012 when digital mammography was not widely implemented and before the increase in numbers of atypia diagnoses, and with evidence for more recent years not yet available.
This study provides more limited data for longer term risks, although this was not the primary focus of the study. The National Institute for Health and Care Excellence states women in the general population have an 11% chance of developing breast cancer during their lifetime, with moderate risk greater than 17% but less than 30%.35The 15 year risk in the Sloane cohort was 13.1% for the complete study period, however this estimate is less influenced by more recent atypia diagnoses that have shorter follow-up. However, 63 of 77 screening centres contributed data to the Sloane atypia cohort, which suggests that findings are applicable to screening practice generally in England. Other countries should interpret these findings with caution for policy decision making because of potential differences in breast image acquisition, access to vacuum assisted biopsies, the level of quality assurance of the screening programme, and the present management of atypia, which might increase the risk of overdiagnosis or overtreatment.
Conclusion
Overall, data from the Sloane Atypia Project show that invasive breast cancer incidence at three years after a diagnosis of epithelial atypia was low, and even lower in recent years. Few cancers appeared to be missed at the time of an atypia diagnosis. These data, including the similar ipsilateral and contralateral risks, support the suggestion that many epithelial atypia diagnoses might represent risk factors rather than precursor lesions for invasive cancer within 15 years of follow-up. Changes to mammography (digitalvplain film) and biopsy techniques (gauge of biopsy needle and use of vacuum assistance) coincide with the reduction in reported subsequent invasive cancers. One possible interpretation might be that, more recently, milder forms of atypia have been detected, which are more likely to represent overdiagnosis. Annual mammography in the short term after atypia diagnosis might not be beneficial and should be reviewed. Previous studies have shown increased longer term risk of developing cancer with some forms of epithelial atypia, but not all. Even for those lesions with established long term risk (eg, ADH, ALH, LCIS), the data indicate that these women would not benefit from enhanced short term surveillance.
","Objective: To explore how the number and type of breast cancers developed after screen detected atypia compare with the anticipated 11.3 cancers detected per 1000 women screened within one three year screening round in the United Kingdom.
Design: Observational analysis of the Sloane atypia prospective cohort in England.
Setting: Atypia diagnoses through the English NHS breast screening programme reported to the Sloane cohort study. This cohort is linked to the English Cancer Registry and the Mortality and Birth Information System for information on subsequent breast cancer and mortality.
Participants: 3238 women diagnosed as having epithelial atypia between 1 April 2003 and 30 June 2018.
Main outcome measures: Number and type of invasive breast cancers detected at one, three, and six years after atypia diagnosis by atypia type, age, and year of diagnosis.
Results: There was a fourfold increase in detection of atypia after the introduction of digital mammography between 2010 (n=119) and 2015 (n=502). During 19 088 person years of follow-up after atypia diagnosis (until December 2018), 141 women developed breast cancer. Cumulative incidence of cancer per 1000 women with atypia was 0.95 (95% confidence interval 0.28 to 2.69), 14.2 (10.3 to 19.1), and 45.0 (36.3 to 55.1) at one, three, and six years after atypia diagnosis, respectively. Women with atypia detected more recently have lower rates of subsequent cancers detected within three years (6.0 invasive cancers per 1000 women (95% confidence interval 3.1 to 10.9) in 2013-18v24.3 (13.7 to 40.1) in 2003-07, and 24.6 (14.9 to 38.3) in 2008-12). Grade, size, and nodal involvement of subsequent invasive cancers were similar to those of cancers detected in the general screening population, with equal numbers of ipsilateral and contralateral cancers.
Conclusions: Many atypia could represent risk factors rather than precursors of invasive cancer requiring surgery in the short term. Women with atypia detected more recently have lower rates of subsequent cancers detected, which might be associated with changes to mammography and biopsy techniques identifying forms of atypia that are more likely to represent overdiagnosis. Annual mammography in the short term after atypia diagnosis might not be beneficial. More evidence is needed about longer term risks.
"
